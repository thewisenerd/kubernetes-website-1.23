<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes â€“ ç”Ÿäº§çº§åˆ«çš„å®¹å™¨ç¼–æ’ç³»ç»Ÿ</title><link>https://kubernetes.io/zh/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/zh/</link></image><atom:link href="https://kubernetes.io/zh/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Dockershimï¼šå†å²èƒŒæ™¯</title><link>https://kubernetes.io/zh/blog/2022/05/03/dockershim-historical-context/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/05/03/dockershim-historical-context/</guid><description>
&lt;!--
layout: blog
title: "Dockershim: The Historical Context"
date: 2022-05-03
slug: dockershim-historical-context
-->
&lt;!--
**Author:** Kat Cosgrove
Dockershim has been removed as of Kubernetes v1.24, and this is a positive move for the project. However, context is important for fully understanding something, be it socially or in software development, and this deserves a more in-depth review. Alongside the dockershim removal in Kubernetes v1.24, weâ€™ve seen some confusion (sometimes at a panic level) and dissatisfaction with this decision in the community, largely due to a lack of context around this removal. The decision to deprecate and eventually remove dockershim from Kubernetes was not made quickly or lightly. Still, itâ€™s been in the works for so long that many of todayâ€™s users are newer than that decision, and certainly newer than the choices that led to the dockershim being necessary in the first place.
So what is the dockershim, and why is it going away?
-->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong> Kat Cosgrove&lt;/p>
&lt;p>è‡ª Kubernetes v1.24 èµ·ï¼ŒDockershim å·²è¢«åˆ é™¤ï¼Œè¿™å¯¹é¡¹ç›®æ¥è¯´æ˜¯ä¸€ä¸ªç§¯æçš„ä¸¾æªã€‚
ç„¶è€Œï¼ŒèƒŒæ™¯å¯¹äºå……åˆ†ç†è§£æŸäº‹å¾ˆé‡è¦ï¼Œæ— è®ºæ˜¯ç¤¾äº¤è¿˜æ˜¯è½¯ä»¶å¼€å‘ï¼Œè¿™å€¼å¾—æ›´æ·±å…¥çš„å®¡æŸ¥ã€‚
é™¤äº† Kubernetes v1.24 ä¸­çš„ dockershim ç§»é™¤ä¹‹å¤–ï¼Œæˆ‘ä»¬åœ¨ç¤¾åŒºä¸­çœ‹åˆ°äº†ä¸€äº›
æ··ä¹±ï¼ˆæœ‰æ—¶å¤„äºææ…Œçº§åˆ«ï¼‰å’Œå¯¹è¿™ä¸€å†³å®šçš„ä¸æ»¡ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹æœ‰å…³æ­¤åˆ é™¤èƒŒæ™¯çš„äº†è§£ã€‚
å¼ƒç”¨å¹¶æœ€ç»ˆä» Kubernetes ä¸­åˆ é™¤ dockershim çš„å†³å®šå¹¶ä¸æ˜¯è¿…é€Ÿæˆ–è½»ç‡åœ°åšå‡ºçš„ã€‚
å°½ç®¡å¦‚æ­¤ï¼Œå®ƒå·²ç»å·¥ä½œäº†å¾ˆé•¿æ—¶é—´ï¼Œä»¥è‡³äºä»Šå¤©çš„è®¸å¤šç”¨æˆ·éƒ½æ¯”è¿™ä¸ªå†³å®šæ›´æ–°ï¼Œ
æ›´ä¸ç”¨æå½“åˆä¸ºä½•å¼•å…¥ dockershim äº†ã€‚&lt;/p>
&lt;p>é‚£ä¹ˆ dockershim æ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆå®ƒä¼šæ¶ˆå¤±å‘¢ï¼Ÿ&lt;/p>
&lt;!--
In the early days of Kubernetes, we only supported one container runtime. That runtime was Docker Engine. Back then, there werenâ€™t really a lot of other options out there and Docker was the dominant tool for working with containers, so this was not a controversial choice. Eventually, we started adding more container runtimes, like rkt and hypernetes, and it became clear that Kubernetes users want a choice of runtimes working best for them. So Kubernetes needed a way to allow cluster operators the flexibility to use whatever runtime they choose.
-->
&lt;p>åœ¨ Kubernetes çš„æ—©æœŸï¼Œæˆ‘ä»¬åªæ”¯æŒä¸€ä¸ªå®¹å™¨è¿è¡Œæ—¶ï¼Œé‚£ä¸ªè¿è¡Œæ—¶å°±æ˜¯ Docker Engineã€‚
é‚£æ—¶ï¼Œå¹¶æ²¡æœ‰å¤ªå¤šå…¶ä»–é€‰æ‹©ï¼Œè€Œ Docker æ˜¯ä½¿ç”¨å®¹å™¨çš„ä¸»è¦å·¥å…·ï¼Œæ‰€ä»¥è¿™ä¸æ˜¯ä¸€ä¸ªæœ‰äº‰è®®çš„é€‰æ‹©ã€‚
æœ€ç»ˆï¼Œæˆ‘ä»¬å¼€å§‹æ·»åŠ æ›´å¤šçš„å®¹å™¨è¿è¡Œæ—¶ï¼Œæ¯”å¦‚ rkt å’Œ hypernetesï¼Œå¾ˆæ˜æ˜¾ Kubernetes ç”¨æˆ·
å¸Œæœ›é€‰æ‹©æœ€é€‚åˆä»–ä»¬çš„è¿è¡Œæ—¶ã€‚ å› æ­¤ï¼ŒKubernetes éœ€è¦ä¸€ç§æ–¹æ³•æ¥å…è®¸é›†ç¾¤æ“ä½œå‘˜çµæ´»åœ°ä½¿ç”¨
ä»–ä»¬é€‰æ‹©çš„ä»»ä½•è¿è¡Œæ—¶ã€‚&lt;/p>
&lt;!--
The [Container Runtime Interface](/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) (CRI) was released to allow that flexibility. The introduction of CRI was great for the project and users alike, but it did introduce a problem: Docker Engineâ€™s use as a container runtime predates CRI, and Docker Engine is not CRI-compatible. To solve this issue, a small software shim (dockershim) was introduced as part of the kubelet component specifically to fill in the gaps between Docker Engine and CRI, allowing cluster operators to continue using Docker Engine as their container runtime largely uninterrupted.
-->
&lt;p>&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">å®¹å™¨è¿è¡Œæ—¶æ¥å£&lt;/a> (CRI)
å·²å‘å¸ƒä»¥æ”¯æŒè¿™ç§çµæ´»æ€§ã€‚ CRI çš„å¼•å…¥å¯¹é¡¹ç›®å’Œç”¨æˆ·æ¥è¯´éƒ½å¾ˆæ£’ï¼Œä½†å®ƒç¡®å®å¼•å…¥äº†ä¸€ä¸ªé—®é¢˜ï¼šDocker Engine
ä½œä¸ºå®¹å™¨è¿è¡Œæ—¶çš„ä½¿ç”¨æ—©äº CRIï¼Œå¹¶ä¸” Docker Engine ä¸å…¼å®¹ CRIã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨ kubelet ç»„ä»¶
ä¸­å¼•å…¥äº†ä¸€ä¸ªå°å‹è½¯ä»¶ shim (dockershim)ï¼Œä¸“é—¨ç”¨äºå¡«è¡¥ Docker Engine å’Œ CRI ä¹‹é—´çš„ç©ºç™½ï¼Œ
å…è®¸é›†ç¾¤æ“ä½œå‘˜ç»§ç»­ä½¿ç”¨ Docker Engine ä½œä¸ºä»–ä»¬çš„å®¹å™¨è¿è¡Œæ—¶åŸºæœ¬ä¸Šä¸é—´æ–­ã€‚&lt;/p>
&lt;!--
However, this little software shim was never intended to be a permanent solution. Over the course of years, its existence has introduced a lot of unnecessary complexity to the kubelet itself. Some integrations are inconsistently implemented for Docker because of this shim, resulting in an increased burden on maintainers, and maintaining vendor-specific code is not in line with our open source philosophy. To reduce this maintenance burden and move towards a more collaborative community in support of open standards, [KEP-2221 was introduced](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim), proposing the removal of the dockershim. With the release of Kubernetes v1.20, the deprecation was official.
-->
&lt;p>ç„¶è€Œï¼Œè¿™ä¸ªå°è½¯ä»¶ shim ä»æ¥æ²¡æœ‰æ‰“ç®—æˆä¸ºä¸€ä¸ªæ°¸ä¹…çš„è§£å†³æ–¹æ¡ˆã€‚ å¤šå¹´æ¥ï¼Œå®ƒçš„å­˜åœ¨ç»™ kubelet
æœ¬èº«å¸¦æ¥äº†è®¸å¤šä¸å¿…è¦çš„å¤æ‚æ€§ã€‚ ç”±äºè¿™ä¸ª shimï¼ŒDocker çš„ä¸€äº›é›†æˆå®ç°ä¸ä¸€è‡´ï¼Œå¯¼è‡´ç»´æŠ¤äººå‘˜
çš„è´Ÿæ‹…å¢åŠ ï¼Œå¹¶ä¸”ç»´æŠ¤ç‰¹å®šäºä¾›åº”å•†çš„ä»£ç ä¸ç¬¦åˆæˆ‘ä»¬çš„å¼€æºç†å¿µã€‚ ä¸ºäº†å‡å°‘è¿™ç§ç»´æŠ¤è´Ÿæ‹…å¹¶æœç€æ”¯
æŒå¼€æ”¾æ ‡å‡†çš„æ›´å…·åä½œæ€§çš„ç¤¾åŒºè¿ˆè¿›ï¼Œ[å¼•å…¥äº† KEP-2221](&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-">https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-&lt;/a> remove-dockershim)ï¼Œ
å»ºè®®ç§»é™¤ dockershimã€‚ éšç€ Kubernetes v1.20 çš„å‘å¸ƒï¼Œæ­£å¼å¼ƒç”¨ã€‚&lt;/p>
&lt;!--
We didnâ€™t do a great job communicating this, and unfortunately, the deprecation announcement led to some panic within the community. Confusion around what this meant for Docker as a company, if container images built by Docker would still run, and what Docker Engine actually is led to a conflagration on social media. This was our fault; we should have more clearly communicated what was happening and why at the time. To combat this, we released [a blog](/blog/2020/12/02/dont-panic-kubernetes-and-docker/) and [accompanying FAQ](/blog/2020/12/02/dockershim-faq/) to allay the communityâ€™s fears and correct some misconceptions about what Docker is and how containers work within Kubernetes. As a result of the communityâ€™s concerns, Docker and Mirantis jointly agreed to continue supporting the dockershim code in the form of [cri-dockerd](https://www.mirantis.com/blog/the-future-of-dockershim-is-cri-dockerd/), allowing you to continue using Docker Engine as your container runtime if need be. For the interest of users who want to try other runtimes, like containerd or cri-o, [migration documentation was written](/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/).
-->
&lt;p>æˆ‘ä»¬æ²¡æœ‰å¾ˆå¥½åœ°ä¼ è¾¾è¿™ä¸€ç‚¹ï¼Œä¸å¹¸çš„æ˜¯ï¼Œå¼ƒç”¨å…¬å‘Šåœ¨ç¤¾åŒºå†…å¼•èµ·äº†ä¸€äº›ææ…Œã€‚å…³äºè¿™å¯¹ Docker ä½œä¸º
ä¸€å®¶å…¬å¸æ„å‘³ç€ä»€ä¹ˆï¼ŒDocker æ„å»ºçš„å®¹å™¨é•œåƒæ˜¯å¦ä»ç„¶å¯ä»¥è¿è¡Œï¼Œä»¥åŠ Docker Engine ç©¶ç«Ÿæ˜¯
ä»€ä¹ˆå¯¼è‡´äº†ç¤¾äº¤åª’ä½“ä¸Šçš„ä¸€åœºå¤§ç«ï¼Œäººä»¬æ„Ÿåˆ°å›°æƒ‘ã€‚è¿™æ˜¯æˆ‘ä»¬çš„é”™ï¼›æˆ‘ä»¬åº”è¯¥æ›´æ¸…æ¥šåœ°ä¼ è¾¾å½“æ—¶å‘ç”Ÿ
çš„äº‹æƒ…å’ŒåŸå› ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å‘å¸ƒäº†&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/">ä¸€ç¯‡åšå®¢&lt;/a>
å’Œ&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">ç›¸åº”çš„ FAQ&lt;/a> ä»¥å‡è½»ç¤¾åŒºçš„ææƒ§å¹¶çº æ­£å¯¹
Docker æ˜¯ä»€ä¹ˆä»¥åŠå®¹å™¨å¦‚ä½•åœ¨ Kubernetes ä¸­å·¥ä½œçš„ä¸€äº›è¯¯è§£ã€‚ç”±äºç¤¾åŒºçš„å…³æ³¨ï¼ŒDocker å’Œ Mirantis
å…±åŒå†³å®šç»§ç»­ä»¥ [cri-dockerd] çš„å½¢å¼æ”¯æŒ dockershim ä»£ç ï¼ˆhttps://www.mirantis.com/blog/the-future-of-dockershim-is -cri-dockerd/)ï¼Œ
å…è®¸ä½ åœ¨éœ€è¦æ—¶ç»§ç»­ä½¿ç”¨ Docker Engine ä½œä¸ºå®¹å™¨è¿è¡Œæ—¶ã€‚å¯¹äºæƒ³è¦å°è¯•å…¶ä»–è¿è¡Œæ—¶ï¼ˆå¦‚ containerd æˆ– cri-oï¼‰
çš„ç”¨æˆ·ï¼Œ&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">å·²ç¼–å†™è¿ç§»æ–‡æ¡£&lt;/a>ã€‚&lt;/p>
&lt;!--
We later [surveyed the community](https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/) and [discovered that there are still many users with questions and concerns](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim). In response, Kubernetes maintainers and the CNCF committed to addressing these concerns by extending documentation and other programs. In fact, this blog post is a part of this program. With so many end users successfully migrated to other runtimes, and improved documentation, we believe that everyone has a paved way to migration now.
-->
&lt;p>æˆ‘ä»¬åæ¥&lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">è°ƒæŸ¥äº†ç¤¾åŒº&lt;/a>
&lt;a href="https://kubernetes.io/zh/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim">å‘ç°è¿˜æœ‰å¾ˆå¤šç”¨æˆ·æœ‰ç–‘é—®å’Œé¡¾è™‘&lt;/a>ã€‚
ä½œä¸ºå›åº”ï¼ŒKubernetes ç»´æŠ¤äººå‘˜å’Œ CNCF æ‰¿è¯ºé€šè¿‡æ‰©å±•æ–‡æ¡£å’Œå…¶ä»–ç¨‹åºæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ äº‹å®ä¸Šï¼Œè¿™ç¯‡åšæ–‡æ˜¯
è¿™ä¸ªè®¡åˆ’çš„ä¸€éƒ¨åˆ†ã€‚ éšç€å¦‚æ­¤å¤šçš„æœ€ç»ˆç”¨æˆ·æˆåŠŸè¿ç§»åˆ°å…¶ä»–è¿è¡Œæ—¶ï¼Œä»¥åŠæ”¹è¿›çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬ç›¸ä¿¡æ¯ä¸ªäººç°åœ¨éƒ½ä¸ºè¿ç§»é“ºå¹³äº†é“è·¯ã€‚&lt;/p>
&lt;!--
Docker is not going away, either as a tool or as a company. Itâ€™s an important part of the cloud native community and the history of the Kubernetes project. We wouldnâ€™t be where we are without them. That said, removing dockershim from kubelet is ultimately good for the community, the ecosystem, the project, and open source at large. This is an opportunity for all of us to come together to support open standards, and weâ€™re glad to be doing so with the help of Docker and the community.
-->
&lt;p>Docker ä¸ä¼šæ¶ˆå¤±ï¼Œæ— è®ºæ˜¯ä½œä¸ºä¸€ç§å·¥å…·è¿˜æ˜¯ä½œä¸ºä¸€å®¶å…¬å¸ã€‚ å®ƒæ˜¯äº‘åŸç”Ÿç¤¾åŒºçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œ
ä¹Ÿæ˜¯ Kubernetes é¡¹ç›®çš„å†å²ã€‚ æ²¡æœ‰ä»–ä»¬ï¼Œæˆ‘ä»¬å°±ä¸ä¼šæ˜¯ç°åœ¨çš„æ ·å­ã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼Œä» kubelet
ä¸­åˆ é™¤ dockershim æœ€ç»ˆå¯¹ç¤¾åŒºã€ç”Ÿæ€ç³»ç»Ÿã€é¡¹ç›®å’Œæ•´ä¸ªå¼€æºéƒ½æœ‰å¥½å¤„ã€‚ è¿™æ˜¯æˆ‘ä»¬æ‰€æœ‰äººé½å¿ƒååŠ›
æ”¯æŒå¼€æ”¾æ ‡å‡†çš„æœºä¼šï¼Œæˆ‘ä»¬å¾ˆé«˜å…´åœ¨ Docker å’Œç¤¾åŒºçš„å¸®åŠ©ä¸‹è¿™æ ·åšã€‚&lt;/p></description></item><item><title>Blog: Kubernetes 1.24 çš„åˆ é™¤å’Œå¼ƒç”¨</title><link>https://kubernetes.io/zh/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes Removals and Deprecations In 1.24"
date: 2022-04-07
slug: upcoming-changes-in-kubernetes-1-24
-->
&lt;!--
**Author**: Mickey Boxell (Oracle)
As Kubernetes evolves, features and APIs are regularly revisited and removed. New features may offer
an alternative or improved approach to solving existing problems, motivating the team to remove the
old approach.
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼š Mickey Boxell (Oracle)&lt;/p>
&lt;p>éšç€ Kubernetes çš„å‘å±•ï¼Œç‰¹æ€§å’Œ API ä¼šå®šæœŸè¢«é‡æ–°è®¿é—®å’Œåˆ é™¤ã€‚
æ–°ç‰¹æ€§å¯èƒ½ä¼šæä¾›æ›¿ä»£æˆ–æ”¹è¿›çš„æ–¹æ³•ï¼Œæ¥è§£å†³ç°æœ‰çš„é—®é¢˜ï¼Œä»è€Œæ¿€åŠ±å›¢é˜Ÿç§»é™¤æ—§çš„æ–¹æ³•ã€‚&lt;/p>
&lt;!--
We want to make sure you are aware of the changes coming in the Kubernetes 1.24 release. The release will
**deprecate** several (beta) APIs in favor of stable versions of the same APIs. The major change coming
in the Kubernetes 1.24 release is the
[removal of Dockershim](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim).
This is discussed below and will be explored in more depth at release time. For an early look at the
changes coming in Kubernetes 1.24, take a look at the in-progress
[CHANGELOG](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md).
-->
&lt;p>æˆ‘ä»¬å¸Œæœ›ç¡®ä¿ä½ äº†è§£ Kubernetes 1.24 ç‰ˆæœ¬çš„å˜åŒ–ã€‚ è¯¥ç‰ˆæœ¬å°† &lt;strong>å¼ƒç”¨&lt;/strong> ä¸€äº›ï¼ˆæµ‹è¯•ç‰ˆ/betaï¼‰APIï¼Œ
è½¬è€Œæ”¯æŒç›¸åŒ API çš„ç¨³å®šç‰ˆæœ¬ã€‚ Kubernetes 1.24 ç‰ˆæœ¬çš„ä¸»è¦å˜åŒ–æ˜¯
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">ç§»é™¤ Dockershim&lt;/a>ã€‚
è¿™å°†åœ¨ä¸‹é¢è®¨è®ºï¼Œå¹¶å°†åœ¨å‘å¸ƒæ—¶æ›´æ·±å…¥åœ°æ¢è®¨ã€‚
è¦æå‰äº†è§£ Kubernetes 1.24 ä¸­çš„æ›´æ”¹ï¼Œè¯·æŸ¥çœ‹æ­£åœ¨æ›´æ–°ä¸­çš„
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">CHANGELOG&lt;/a>ã€‚&lt;/p>
&lt;!--
## A note about Dockershim
It's safe to say that the removal receiving the most attention with the release of Kubernetes 1.24
is Dockershim. Dockershim was deprecated in v1.20. As noted in the [Kubernetes 1.20 changelog](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation):
"Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet
uses a module called "dockershim" which implements CRI support for Docker and it has seen maintenance
issues in the Kubernetes community." With the upcoming release of Kubernetes 1.24, the Dockershim will
finally be removed.
-->
&lt;h2 id="a-note-about-dockershim">å…³äº Dockershim &lt;/h2>
&lt;p>å¯ä»¥è‚¯å®šåœ°è¯´ï¼Œéšç€ Kubernetes 1.24 çš„å‘å¸ƒï¼Œæœ€å—å…³æ³¨çš„åˆ é™¤æ˜¯ Dockershimã€‚
Dockershim åœ¨ 1.20 ç‰ˆæœ¬ä¸­å·²è¢«å¼ƒç”¨ã€‚ å¦‚
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">Kubernetes 1.20 å˜æ›´æ—¥å¿—&lt;/a>ä¸­æ‰€è¿°ï¼š
&amp;quot;Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet
uses a module called &amp;quot;dockershim&amp;quot; which implements CRI support for Docker and it has seen maintenance
issues in the Kubernetes community.&amp;quot;
éšç€å³å°†å‘å¸ƒçš„ Kubernetes 1.24ï¼ŒDockershim å°†æœ€ç»ˆè¢«åˆ é™¤ã€‚&lt;/p>
&lt;!--
In the article [Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/),
the authors succinctly captured the change's impact and encouraged users to remain calm:
> Docker as an underlying runtime is being deprecated in favor of runtimes that use the
> Container Runtime Interface (CRI) created for Kubernetes. Docker-produced images
> will continue to work in your cluster with all runtimes, as they always have.
-->
&lt;p>åœ¨æ–‡ç« &lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/">åˆ«æ…Œ: Kubernetes å’Œ Docker&lt;/a> ä¸­ï¼Œ
ä½œè€…ç®€æ´åœ°è®°è¿°äº†å˜åŒ–çš„å½±å“ï¼Œå¹¶é¼“åŠ±ç”¨æˆ·ä¿æŒå†·é™ï¼š&lt;/p>
&lt;blockquote>
&lt;p>å¼ƒç”¨ Docker è¿™ä¸ªåº•å±‚è¿è¡Œæ—¶ï¼Œè½¬è€Œæ”¯æŒç¬¦åˆä¸º Kubernetes åˆ›å»ºçš„å®¹å™¨è¿è¡Œæ¥å£
Container Runtime Interface (CRI) çš„è¿è¡Œæ—¶ã€‚
Docker æ„å»ºçš„é•œåƒï¼Œå°†åœ¨ä½ çš„é›†ç¾¤çš„æ‰€æœ‰è¿è¡Œæ—¶ä¸­ç»§ç»­å·¥ä½œï¼Œä¸€å¦‚æ—¢å¾€ã€‚&lt;/p>
&lt;/blockquote>
&lt;!--
Several guides have been created with helpful information about migrating from dockershim
to container runtimes that are directly compatible with Kubernetes. You can find them on the
[Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/)
page in the Kubernetes documentation.
-->
&lt;p>å·²ç»æœ‰ä¸€äº›æ–‡æ¡£æŒ‡å—ï¼Œæä¾›äº†å…³äºä» dockershim è¿ç§»åˆ°ä¸ Kubernetes ç›´æ¥å…¼å®¹çš„å®¹å™¨è¿è¡Œæ—¶çš„æœ‰ç”¨ä¿¡æ¯ã€‚
ä½ å¯ä»¥åœ¨ Kubernetes æ–‡æ¡£ä¸­çš„&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/">ä» dockershim è¿ç§»&lt;/a>
é¡µé¢ä¸Šæ‰¾åˆ°å®ƒä»¬ã€‚&lt;/p>
&lt;!--
For more information about why Kubernetes is moving away from dockershim, check out the aptly
named: [Kubernetes is Moving on From Dockershim](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/)
and the [updated dockershim removal FAQ](/blog/2022/02/17/dockershim-faq/).
Take a look at the [Is Your Cluster Ready for v1.24?](/blog/2022/03/31/ready-for-dockershim-removal/) post to learn about how to ensure your cluster continues to work after upgrading from v1.23 to v1.24.
-->
&lt;p>æœ‰å…³ Kubernetes ä¸ºä½•ä¸å†ä½¿ç”¨ dockershim çš„æ›´å¤šä¿¡æ¯ï¼Œ
è¯·å‚è§ï¼š&lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">Kubernetes æ­£åœ¨ç¦»å¼€ Dockershim&lt;/a>
å’Œ&lt;a href="https://kubernetes.io/zh/blog/2022/02/17/dockershim-faq/">æœ€æ–°çš„å¼ƒç”¨ Dockershim çš„å¸¸è§é—®é¢˜&lt;/a>ã€‚&lt;/p>
&lt;p>æŸ¥çœ‹&lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">ä½ çš„é›†ç¾¤å‡†å¤‡å¥½ä½¿ç”¨ v1.24 äº†å—ï¼Ÿ&lt;/a> ä¸€æ–‡ï¼Œ
äº†è§£å¦‚ä½•ç¡®ä¿ä½ çš„é›†ç¾¤åœ¨ä» 1.23 ç‰ˆæœ¬å‡çº§åˆ° 1.24 ç‰ˆæœ¬åç»§ç»­å·¥ä½œã€‚&lt;/p>
&lt;!--
## The Kubernetes API removal and deprecation process
Kubernetes contains a large number of components that evolve over time. In some cases, this
evolution results in APIs, flags, or entire features, being removed. To prevent users from facing
breaking changes, Kubernetes contributors adopted a feature [deprecation policy](/docs/reference/using-api/deprecation-policy/).
This policy ensures that stable APIs may only be deprecated when a newer stable version of that
same API is available and that APIs have a minimum lifetime as indicated by the following stability levels:
* Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.
* Beta or pre-release API versions must be supported for 3 releases after deprecation.
* Alpha or experimental API versions may be removed in any release without prior deprecation notice.
-->
&lt;h2 id="the-Kubernetes-api-removal-and-deprecation-process">Kubernetes API åˆ é™¤å’Œå¼ƒç”¨æµç¨‹ &lt;/h2>
&lt;p>Kubernetes åŒ…å«å¤§é‡éšæ—¶é—´æ¼”å˜çš„ç»„ä»¶ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™ç§æ¼”å˜ä¼šå¯¼è‡´ APIã€æ ‡å¿—æˆ–æ•´ä¸ªç‰¹æ€§è¢«åˆ é™¤ã€‚
ä¸ºäº†é˜²æ­¢ç”¨æˆ·é¢å¯¹é‡å¤§å˜åŒ–ï¼ŒKubernetes è´¡çŒ®è€…é‡‡ç”¨äº†ä¸€é¡¹ç‰¹æ€§&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/deprecation-policy/">å¼ƒç”¨ç­–ç•¥&lt;/a>ã€‚
æ­¤ç­–ç•¥ç¡®ä¿ä»…å½“åŒä¸€ API çš„è¾ƒæ–°ç¨³å®šç‰ˆæœ¬å¯ç”¨å¹¶ä¸”
API å…·æœ‰ä»¥ä¸‹ç¨³å®šæ€§çº§åˆ«æ‰€æŒ‡ç¤ºçš„æœ€çŸ­ç”Ÿå‘½å‘¨æœŸæ—¶ï¼Œæ‰å¯èƒ½å¼ƒç”¨ç¨³å®šç‰ˆæœ¬ APIï¼š&lt;/p>
&lt;ul>
&lt;li>æ­£å¼å‘å¸ƒ (GA) æˆ–ç¨³å®šçš„ API ç‰ˆæœ¬å¯èƒ½è¢«æ ‡è®°ä¸ºå·²å¼ƒç”¨ï¼Œä½†ä¸å¾—åœ¨ Kubernetes çš„ä¸»ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚&lt;/li>
&lt;li>æµ‹è¯•ç‰ˆï¼ˆbetaï¼‰æˆ–é¢„å‘å¸ƒ API ç‰ˆæœ¬åœ¨å¼ƒç”¨åå¿…é¡»æ”¯æŒ 3 ä¸ªç‰ˆæœ¬ã€‚&lt;/li>
&lt;li>Alpha æˆ–å®éªŒæ€§ API ç‰ˆæœ¬å¯èƒ½ä¼šåœ¨ä»»ä½•ç‰ˆæœ¬ä¸­è¢«åˆ é™¤ï¼Œæ•ä¸å¦è¡Œé€šçŸ¥ã€‚&lt;/li>
&lt;/ul>
&lt;!--
Removals follow the same deprecation policy regardless of whether an API is removed due to a beta feature
graduating to stable or because that API was not proven to be successful. Kubernetes will continue to make
sure migration options are documented whenever APIs are removed.
-->
&lt;p>åˆ é™¤éµå¾ªç›¸åŒçš„å¼ƒç”¨æ”¿ç­–ï¼Œæ— è®º API æ˜¯ç”±äº æµ‹è¯•ç‰ˆï¼ˆbetaï¼‰åŠŸèƒ½é€æ¸ç¨³å®šè¿˜æ˜¯å› ä¸ºè¯¥
API æœªè¢«è¯æ˜æ˜¯æˆåŠŸçš„è€Œè¢«åˆ é™¤ã€‚
Kubernetes å°†ç»§ç»­ç¡®ä¿åœ¨åˆ é™¤ API æ—¶æä¾›ç”¨æ¥è¿ç§»çš„æ–‡æ¡£ã€‚&lt;/p>
&lt;!--
**Deprecated** APIs are those that have been marked for removal in a future Kubernetes release. **Removed**
APIs are those that are no longer available for use in current, supported Kubernetes versions after having
been deprecated. These removals have been superseded by newer, stable/generally available (GA) APIs.
-->
&lt;p>&lt;strong>å¼ƒç”¨çš„&lt;/strong> API æ˜¯æŒ‡é‚£äº›å·²æ ‡è®°ä¸ºåœ¨æœªæ¥ Kubernetes ç‰ˆæœ¬ä¸­è¢«åˆ é™¤çš„ APIã€‚
&lt;strong>åˆ é™¤çš„&lt;/strong> API æ˜¯æŒ‡é‚£äº›åœ¨è¢«å¼ƒç”¨åä¸å†å¯ç”¨äºå½“å‰å—æ”¯æŒçš„ Kubernetes ç‰ˆæœ¬çš„ APIã€‚
è¿™äº›åˆ é™¤å·²è¢«æ›´æ–°çš„ã€ç¨³å®šçš„/æ™®éå¯ç”¨çš„ (GA) API æ‰€å–ä»£ã€‚&lt;/p>
&lt;!--
## API removals, deprecations, and other changes for Kubernetes 1.24
* [Dynamic kubelet configuration](https://github.com/kubernetes/enhancements/issues/281): `DynamicKubeletConfig` is used to enable the dynamic configuration of the kubelet. The `DynamicKubeletConfig` flag was deprecated in Kubernetes 1.22. In v1.24, this feature gate will be removed from the kubelet. See [Reconfigure kubelet](/docs/tasks/administer-cluster/reconfigure-kubelet/). Refer to the ["Dynamic kubelet config is removed" KEP](https://github.com/kubernetes/enhancements/issues/281) for more information.
-->
&lt;h2 id="api-removals-deprecations-and-other-changes-for-kubernetes-1.24">Kubernetes 1.24 çš„ API åˆ é™¤ã€å¼ƒç”¨å’Œå…¶ä»–æ›´æ”¹ &lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/281">åŠ¨æ€ kubelet é…ç½®&lt;/a>: &lt;code>DynamicKubeletConfig&lt;/code>
ç”¨äºå¯ç”¨ kubelet çš„åŠ¨æ€é…ç½®ã€‚Kubernetes 1.22 ä¸­å¼ƒç”¨ &lt;code>DynamicKubeletConfig&lt;/code> æ ‡å¿—ã€‚
åœ¨ 1.24 ç‰ˆæœ¬ä¸­ï¼Œæ­¤ç‰¹æ€§é—¨æ§å°†ä» kubelet ä¸­ç§»é™¤ã€‚è¯·å‚é˜…&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">é‡æ–°é…ç½® kubelet&lt;/a>ã€‚
æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…&lt;a href="https://github.com/kubernetes/enhancements/issues/281">â€œåˆ é™¤åŠ¨æ€ kubelet é…ç½®â€ çš„ KEP&lt;/a>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* [Dynamic log sanitization](https://github.com/kubernetes/kubernetes/pull/107207): The experimental dynamic log sanitization feature is deprecated and will be removed in v1.24. This feature introduced a logging filter that could be applied to all Kubernetes system components logs to prevent various types of sensitive information from leaking via logs. Refer to [KEP-1753: Kubernetes system components logs sanitization](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation) for more information and an [alternative approach](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#alternatives=).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107207">åŠ¨æ€æ—¥å¿—æ¸…æ´—&lt;/a>ï¼šå®éªŒæ€§çš„åŠ¨æ€æ—¥å¿—æ¸…æ´—åŠŸèƒ½å·²è¢«å¼ƒç”¨ï¼Œ
å°†åœ¨ 1.24 ç‰ˆæœ¬ä¸­è¢«åˆ é™¤ã€‚è¯¥åŠŸèƒ½å¼•å…¥äº†ä¸€ä¸ªæ—¥å¿—è¿‡æ»¤å™¨ï¼Œå¯ä»¥åº”ç”¨äºæ‰€æœ‰ Kubernetes ç³»ç»Ÿç»„ä»¶çš„æ—¥å¿—ï¼Œ
ä»¥é˜²æ­¢å„ç§ç±»å‹çš„æ•æ„Ÿä¿¡æ¯é€šè¿‡æ—¥å¿—æ³„æ¼ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯å’Œæ›¿ä»£æ–¹æ³•ï¼Œè¯·å‚é˜…
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation">KEP-1753: Kubernetes ç³»ç»Ÿç»„ä»¶æ—¥å¿—æ¸…æ´—&lt;/a>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* In-tree provisioner to CSI driver migration: This applies to a number of in-tree plugins, including [Portworx](https://github.com/kubernetes/enhancements/issues/2589). Refer to the [In-tree Storage Plugin to CSI Migration Design Doc](https://github.com/kubernetes/design-proposals-archive/blob/main/storage/csi-migration.md#background-and-motivations) for more information.
-->
&lt;ul>
&lt;li>æ ‘å†…é©±åŠ¨ï¼ˆIn-tree provisionerï¼‰å‘ CSI å·è¿ç§»ï¼šè¿™é€‚ç”¨äºè®¸å¤šæ ‘å†…æ’ä»¶ï¼Œ
åŒ…æ‹¬ &lt;a href="https://github.com/kubernetes/enhancements/issues/2589">Portworx&lt;/a>ã€‚
å‚è§&lt;a href="https://github.com/kubernetes/design-proposals-archive/blob/main/storage/csi-migration.md#background-and-motivations">æ ‘å†…å­˜å‚¨æ’ä»¶å‘ CSI å·è¿ç§»çš„è®¾è®¡æ–‡æ¡£&lt;/a>
äº†è§£æ›´å¤šä¿¡æ¯ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* [Removing Dockershim from kubelet](https://github.com/kubernetes/enhancements/issues/2221): the Container Runtime Interface (CRI) for Docker (i.e. Dockershim) is currently a built-in container runtime in the kubelet code base. It was deprecated in v1.20. As of v1.24, the kubelet will no longer have dockershim. Check out this blog on [what you need to do be ready for v1.24](/blog/2022/03/31/ready-for-dockershim-removal/).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2221">ä» kubelet ä¸­ç§»é™¤ Dockershim&lt;/a>ï¼šDocker
çš„å®¹å™¨è¿è¡Œæ—¶æ¥å£(CRI)ï¼ˆå³ Dockershimï¼‰ç›®å‰æ˜¯ kubelet ä»£ç ä¸­å†…ç½®çš„å®¹å™¨è¿è¡Œæ—¶ã€‚ å®ƒåœ¨ 1.20 ç‰ˆæœ¬ä¸­å·²è¢«å¼ƒç”¨ã€‚
ä» 1.24 ç‰ˆæœ¬å¼€å§‹ï¼Œkubelet å·²ç»ç§»é™¤ dockershimã€‚ æŸ¥çœ‹è¿™ç¯‡åšå®¢ï¼Œ
&lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">äº†è§£ä½ éœ€è¦ä¸º 1.24 ç‰ˆæœ¬åšäº›ä»€ä¹ˆ&lt;/a>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* [Storage capacity tracking for pod scheduling](https://github.com/kubernetes/enhancements/issues/1472): The CSIStorageCapacity API supports exposing currently available storage capacity via CSIStorageCapacity objects and enhances scheduling of pods that use CSI volumes with late binding. In v1.24, the CSIStorageCapacity API will be stable. The API graduating to stable initates the deprecation of the v1beta1 CSIStorageCapacity API. Refer to the [Storage Capacity Constraints for Pod Scheduling KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking) for more information.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">pod è°ƒåº¦çš„å­˜å‚¨å®¹é‡è¿½è¸ª&lt;/a>ï¼šCSIStorageCapacity API
æ”¯æŒé€šè¿‡ CSIStorageCapacity å¯¹è±¡æš´éœ²å½“å‰å¯ç”¨çš„å­˜å‚¨å®¹é‡ï¼Œå¹¶å¢å¼ºäº†ä½¿ç”¨å¸¦æœ‰å»¶è¿Ÿç»‘å®šçš„ CSI å·çš„ Pod çš„è°ƒåº¦ã€‚
CSIStorageCapacity API è‡ª 1.24 ç‰ˆæœ¬èµ·æä¾›ç¨³å®šç‰ˆæœ¬ã€‚å‡çº§åˆ°ç¨³å®šç‰ˆçš„ API å°†å¼ƒç”¨ v1beta1 CSIStorageCapacity APIã€‚
æ›´å¤šä¿¡æ¯è¯·å‚è§ &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking">Pod è°ƒåº¦å­˜å‚¨å®¹é‡çº¦æŸ KEP&lt;/a>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* [The `master` label is no longer present on kubeadm control plane nodes](https://github.com/kubernetes/kubernetes/pull/107533). For new clusters, the label 'node-role.kubernetes.io/master' will no longer be added to control plane nodes, only the label 'node-role.kubernetes.io/control-plane' will be added. For more information, refer to [KEP-2067: Rename the kubeadm "master" label and taint](https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107533">kubeadm æ§åˆ¶é¢èŠ‚ç‚¹ä¸Šä¸å†å­˜åœ¨ &lt;code>master&lt;/code> æ ‡ç­¾&lt;/a>ã€‚
å¯¹äºæ–°é›†ç¾¤ï¼Œæ§åˆ¶å¹³é¢èŠ‚ç‚¹å°†ä¸å†æ·»åŠ  'node-role.kubernetes.io/master' æ ‡ç­¾ï¼Œ
åªä¼šæ·»åŠ  'node-role.kubernetes.io/control-plane' æ ‡ç­¾ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067">KEP-2067ï¼šé‡å‘½å kubeadm â€œmasterâ€ æ ‡ç­¾å’Œæ±¡ç‚¹&lt;/a>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* [VolumeSnapshot v1beta1 CRD will be removed](https://github.com/kubernetes/enhancements/issues/177). Volume snapshot and restore functionality for Kubernetes and the [Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md) (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, entered beta in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.21 and is now unsupported. Refer to [KEP-177: CSI Snapshot](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot) and [kubernetes-csi/external-snapshotter](https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v4.1.0) for more information.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD åœ¨ 1.24 ç‰ˆæœ¬ä¸­å°†è¢«ç§»é™¤&lt;/a>ã€‚
Kubernetes å’Œ &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface&lt;/a> (CSI)
çš„å·å¿«ç…§å’Œæ¢å¤åŠŸèƒ½ï¼Œåœ¨ 1.20 ç‰ˆæœ¬ä¸­è¿›å…¥æµ‹è¯•ç‰ˆã€‚è¯¥åŠŸèƒ½æä¾›æ ‡å‡†åŒ– API è®¾è®¡ (CRD ) å¹¶ä¸º CSI å·é©±åŠ¨ç¨‹åºæ·»åŠ äº† PV å¿«ç…§/æ¢å¤æ”¯æŒï¼Œ
VolumeSnapshot v1beta1 åœ¨ 1.21 ç‰ˆæœ¬ä¸­å·²è¢«å¼ƒç”¨ï¼Œç°åœ¨ä¸å—æ”¯æŒã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177ï¼šCSI å¿«ç…§&lt;/a>å’Œ
&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v4.1.0">kubernetes-csi/external-snapshotter&lt;/a>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
## What to do
### Dockershim removal
As stated earlier, there are several guides about
[Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/).
You can start with [Finding what container runtime are on your nodes](/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/).
If your nodes are using dockershim, there are other possible Docker Engine dependencies such as
Pods or third-party tools executing Docker commands or private registries in the Docker configuration file. You can follow the
[Check whether Dockershim deprecation affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/) guide to review possible
Docker Engine dependencies. Before upgrading to v1.24, you decide to either remain using Docker Engine and
[Migrate Docker Engine nodes from dockershim to cri-dockerd](/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/) or migrate to a CRI-compatible runtime. Here's a guide to
[change the container runtime on a node from Docker Engine to containerd](/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/).
-->
&lt;h2 id="what-to-do">éœ€è¦åšä»€ä¹ˆ &lt;/h2>
&lt;h3 id="dockershim-removal">åˆ é™¤ Dockershim &lt;/h3>
&lt;p>å¦‚å‰æ‰€è¿°ï¼Œæœ‰ä¸€äº›å…³äºä» &lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/">dockershim è¿ç§»&lt;/a>çš„æŒ‡å—ã€‚
ä½ å¯ä»¥&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">ä»æŸ¥æ˜èŠ‚ç‚¹ä¸Šæ‰€ä½¿ç”¨çš„å®¹å™¨è¿è¡Œæ—¶&lt;/a>å¼€å§‹ã€‚
å¦‚æœä½ çš„èŠ‚ç‚¹ä½¿ç”¨ dockershimï¼Œåˆ™è¿˜æœ‰å…¶ä»–å¯èƒ½çš„ Docker Engine ä¾èµ–é¡¹ï¼Œ
ä¾‹å¦‚ Pod æˆ–æ‰§è¡Œ Docker å‘½ä»¤çš„ç¬¬ä¸‰æ–¹å·¥å…·æˆ– Docker é…ç½®æ–‡ä»¶ä¸­çš„ç§æœ‰æ³¨å†Œè¡¨ã€‚
ä½ å¯ä»¥æŒ‰ç…§&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">æ£€æŸ¥å¼ƒç”¨ Dockershim å¯¹ä½ çš„å½±å“&lt;/a>
çš„æŒ‡å—æ¥æŸ¥çœ‹å¯èƒ½çš„ Docker å¼•æ“ä¾èµ–é¡¹ã€‚åœ¨å‡çº§åˆ° 1.24 ç‰ˆæœ¬ä¹‹å‰ï¼Œ ä½ å†³å®šè¦ä¹ˆç»§ç»­ä½¿ç”¨ Docker Engine å¹¶
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">å°† Docker Engine èŠ‚ç‚¹ä» dockershim è¿ç§»åˆ° cri-dockerd&lt;/a>ï¼Œ
è¦ä¹ˆè¿ç§»åˆ°ä¸ CRI å…¼å®¹çš„è¿è¡Œæ—¶ã€‚è¿™æ˜¯&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">å°†èŠ‚ç‚¹ä¸Šçš„å®¹å™¨è¿è¡Œæ—¶ä» Docker Engine æ›´æ”¹ä¸º containerd&lt;/a> çš„æŒ‡å—ã€‚&lt;/p>
&lt;!--
### `kubectl convert`
The [`kubectl convert`](/docs/tasks/tools/included/kubectl-convert-overview/) plugin for `kubectl`
can be helpful to address migrating off deprecated APIs. The plugin facilitates the conversion of
manifests between different API versions, for example, from a deprecated to a non-deprecated API
version. More general information about the API migration process can be found in the [Deprecated API Migration Guide](/docs/reference/using-api/deprecation-guide/).
Follow the [install `kubectl convert` plugin](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin)
documentation to download and install the `kubectl-convert` binary.
-->
&lt;h3 id="kubectl-convert">&lt;code>kubectl convert&lt;/code> &lt;/h3>
&lt;p>kubectl çš„ &lt;a href="https://kubernetes.io/zh/docs/tasks/tools/included/kubectl-convert-overview/">&lt;code>kubectl convert&lt;/code>&lt;/a>
æ’ä»¶æœ‰åŠ©äºè§£å†³å¼ƒç”¨ API çš„è¿ç§»é—®é¢˜ã€‚è¯¥æ’ä»¶æ–¹ä¾¿äº†ä¸åŒ API ç‰ˆæœ¬ä¹‹é—´æ¸…å•çš„è½¬æ¢ï¼Œ
ä¾‹å¦‚ï¼Œä»å¼ƒç”¨çš„ API ç‰ˆæœ¬åˆ°éå¼ƒç”¨çš„ API ç‰ˆæœ¬ã€‚å…³äº API è¿ç§»è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/">å·²å¼ƒç”¨ API çš„è¿ç§»æŒ‡å—&lt;/a>ä¸­æ‰¾åˆ°ã€‚æŒ‰ç…§
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin">å®‰è£… &lt;code>kubectl convert&lt;/code> æ’ä»¶&lt;/a>
æ–‡æ¡£ä¸‹è½½å¹¶å®‰è£… &lt;code>kubectl-convert&lt;/code> äºŒè¿›åˆ¶æ–‡ä»¶ã€‚&lt;/p>
&lt;!--
### Looking ahead
The Kubernetes 1.25 and 1.26 releases planned for later this year will stop serving beta versions
of several currently stable Kubernetes APIs. The v1.25 release will also remove PodSecurityPolicy,
which was deprecated with Kubernetes 1.21 and will not graduate to stable. See [PodSecurityPolicy
Deprecation: Past, Present, and Future](/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/) for more information.
-->
&lt;h3 id="looking-ahead">å±•æœ›æœªæ¥ &lt;/h3>
&lt;p>è®¡åˆ’åœ¨ä»Šå¹´æ™šäº›æ—¶å€™å‘å¸ƒçš„ Kubernetes 1.25 å’Œ 1.26 ç‰ˆæœ¬ï¼Œå°†åœæ­¢æä¾›ä¸€äº›
Kubernetes API çš„ beta ç‰ˆæœ¬ï¼Œè¿™äº› API å½“å‰ä¸ºç¨³å®šç‰ˆã€‚1.25 ç‰ˆæœ¬è¿˜å°†åˆ é™¤ PodSecurityPolicyï¼Œ
å®ƒå·²åœ¨ Kubernetes 1.21 ç‰ˆæœ¬ä¸­è¢«å¼ƒç”¨ï¼Œå¹¶ä¸”ä¸ä¼šå‡çº§åˆ°ç¨³å®šç‰ˆã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy å¼ƒç”¨ï¼šè¿‡å»ã€ç°åœ¨å’Œæœªæ¥&lt;/a>ã€‚&lt;/p>
&lt;!--
The official [list of API removals planned for Kubernetes 1.25](/docs/reference/using-api/deprecation-guide/#v1-25) is:
-->
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/deprecation-guide/#v1-25">Kubernetes 1.25 è®¡åˆ’ç§»é™¤çš„ API çš„å®˜æ–¹åˆ—è¡¨&lt;/a>æ˜¯ï¼š&lt;/p>
&lt;ul>
&lt;li>The beta CronJob API (batch/v1beta1)&lt;/li>
&lt;li>The beta EndpointSlice API (discovery.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta Event API (events.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta1)&lt;/li>
&lt;li>The beta PodDisruptionBudget API (policy/v1beta1)&lt;/li>
&lt;li>The beta PodSecurityPolicy API (policy/v1beta1)&lt;/li>
&lt;li>The beta RuntimeClass API (node.k8s.io/v1beta1)&lt;/li>
&lt;/ul>
&lt;!--
The official [list of API removals planned for Kubernetes 1.26](/docs/reference/using-api/deprecation-guide/#v1-26) is:
* The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)
* The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)
-->
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/deprecation-guide/#v1-25">Kubernetes 1.25 è®¡åˆ’ç§»é™¤çš„ API çš„å®˜æ–¹åˆ—è¡¨&lt;/a>æ˜¯ï¼š&lt;/p>
&lt;ul>
&lt;li>The beta FlowSchema å’Œ PriorityLevelConfiguration API (flowcontrol.apiserver.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)&lt;/li>
&lt;/ul>
&lt;!--
### Want to know more?
Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:
* [Kubernetes 1.21](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation)
* [Kubernetes 1.22](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation)
* [Kubernetes 1.23](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation)
* We will formally announce the deprecations that come with [Kubernetes 1.24](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation) as part of the CHANGELOG for that release.
For information on the process of deprecation and removal, check out the official Kubernetes [deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api) document.
-->
&lt;h3 id="want-to-know-more">äº†è§£æ›´å¤š&lt;/h3>
&lt;p>Kubernetes å‘è¡Œè¯´æ˜ä¸­å®£å‘Šäº†å¼ƒç”¨ä¿¡æ¯ã€‚ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç‰ˆæœ¬çš„å‘è¡Œè¯´æ˜ä¸­çœ‹åˆ°å¾…å¼ƒç”¨çš„å…¬å‘Šï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>æˆ‘ä»¬å°†æ­£å¼å®£å¸ƒ &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a> çš„å¼ƒç”¨ä¿¡æ¯ï¼Œ
ä½œä¸ºè¯¥ç‰ˆæœ¬ CHANGELOG çš„ä¸€éƒ¨åˆ†ã€‚&lt;/li>
&lt;/ul>
&lt;p>æœ‰å…³å¼ƒç”¨å’Œåˆ é™¤è¿‡ç¨‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ Kubernetes å®˜æ–¹&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">å¼ƒç”¨ç­–ç•¥&lt;/a> æ–‡æ¡£ã€‚&lt;/p></description></item><item><title>Blog: è®¤è¯†æˆ‘ä»¬çš„è´¡çŒ®è€… - äºšå¤ªåœ°åŒºï¼ˆæ¾³å¤§åˆ©äºš-æ–°è¥¿å…°åœ°åŒºï¼‰</title><link>https://kubernetes.io/zh/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</link><pubDate>Wed, 16 Mar 2022 12:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</guid><description>
&lt;!--
layout: blog
title: "Meet Our Contributors - APAC (Aus-NZ region)"
date: 2022-03-16T12:00:00+0000
slug: meet-our-contributors-au-nz-ep-02
canonicalUrl: https://www.kubernetes.dev/blog/2022/03/14/meet-our-contributors-au-nz-ep-02/
-->
&lt;!--
**Authors &amp; Interviewers:** [Anubhav Vardhan](https://github.com/anubha-v-ardhan), [Atharva Shinde](https://github.com/Atharva-Shinde), [Avinesh Tripathi](https://github.com/AvineshTripathi), [Brad McCoy](https://github.com/bradmccoydev), [Debabrata Panigrahi](https://github.com/Debanitrkl), [Jayesh Srivastava](https://github.com/jayesh-srivastava), [Kunal Verma](https://github.com/verma-kunal), [Pranshu Srivastava](https://github.com/PranshuSrivastava), [Priyanka Saggu](github.com/Priyankasaggu11929/), [Purneswar Prasad](https://github.com/PurneswarPrasad), [Vedant Kakde](https://github.com/vedant-kakde)
-->
&lt;p>&lt;strong>ä½œè€…å’Œé‡‡è®¿è€…ï¼š&lt;/strong>
&lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a>,
&lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a>,
&lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>,
&lt;a href="https://github.com/bradmccoydev">Brad McCoy&lt;/a>,
&lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>,
&lt;a href="https://github.com/jayesh-srivastava">Jayesh Srivastava&lt;/a>,
&lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a>,
&lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a>,
&lt;a href="github.com/Priyankasaggu11929/">Priyanka Saggu&lt;/a>,
&lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>,
&lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;hr>
&lt;!--
Good day, everyone ğŸ‘‹
-->
&lt;p>å¤§å®¶å¥½ğŸ‘‹&lt;/p>
&lt;!--
Welcome back to the second episode of the "Meet Our Contributors" blog post series for APAC.
-->
&lt;p>æ¬¢è¿æ¥åˆ°äºšå¤ªåœ°åŒºçš„â€è®¤è¯†æˆ‘ä»¬çš„è´¡çŒ®è€…â€åšæ–‡ç³»åˆ—ç¬¬äºŒæœŸã€‚&lt;/p>
&lt;!--
This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project.
-->
&lt;p>è¿™ç¯‡æ–‡ç« å°†ä»‹ç»æ¥è‡ªæ¾³å¤§åˆ©äºšå’Œæ–°è¥¿å…°åœ°åŒºçš„å››ä½æ°å‡ºè´¡çŒ®è€…ï¼Œ
ä»–ä»¬åœ¨ä¸Šæ¸¸ Kubernetes é¡¹ç›®ä¸­æ‰¿æ‹…ç€ä¸åŒå­é¡¹ç›®çš„é¢†å¯¼è€…å’Œç¤¾åŒºè´¡çŒ®è€…çš„è§’è‰²ã€‚&lt;/p>
&lt;!--
So, without further ado, let's get straight to the blog.
-->
&lt;p>é—²è¯å°‘è¯´ï¼Œè®©æˆ‘ä»¬ç›´æ¥è¿›å…¥ä¸»é¢˜ã€‚&lt;/p>
&lt;h2 id="caleb-woodbine-https-github-com-bobymcbobs">&lt;a href="https://github.com/BobyMCbobs">Caleb Woodbine&lt;/a>&lt;/h2>
&lt;!--
Caleb Woodbine is currently a member of the ii.nz organisation.
-->
&lt;p>Caleb Woodbine ç›®å‰æ˜¯ ii.nz ç»„ç»‡çš„æˆå‘˜ã€‚&lt;/p>
&lt;!--
He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from [Hippie Hacker](https://github.com/hh), a fellow contributor from New Zealand.
-->
&lt;p>ä»–äº 2018 å¹´ä½œä¸º Kubernetes Conformance å·¥ä½œç»„çš„æˆå‘˜å¼€å§‹ä¸º Kubernetes é¡¹ç›®åšè´¡çŒ®ã€‚
ä»–ç§¯æå‘ä¸Šï¼Œä»–ä»ä¸€ä½æ¥è‡ªæ–°è¥¿å…°çš„è´¡çŒ®è€… &lt;a href="https://github.com/hh">Hippie Hacker&lt;/a> çš„æ—©æœŸæŒ‡å¯¼ä¸­å—ç›ŠåŒªæµ…ã€‚&lt;/p>
&lt;!--
He has made major contributions to Kubernetes project since then through `SIG k8s-infra` and `k8s-conformance` working group.
-->
&lt;p>ä»–åœ¨ &lt;code>SIG k8s-infra&lt;/code> å’Œ &lt;code>k8s-conformance&lt;/code> å·¥ä½œç»„ä¸º Kubernetes é¡¹ç›®åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚&lt;/p>
&lt;!--
Caleb is also a co-organizer of the [CloudNative NZ](https://www.meetup.com/cloudnative-nz/) community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities.
-->
&lt;p>Caleb ä¹Ÿæ˜¯ &lt;a href="https://www.meetup.com/cloudnative-nz/">CloudNative NZ&lt;/a>
ç¤¾åŒºæ´»åŠ¨çš„è”åˆç»„ç»‡è€…ï¼Œè¯¥æ´»åŠ¨æ—¨åœ¨æ‰©å¤§ Kubernetes é¡¹ç›®åœ¨æ•´ä¸ªæ–°è¥¿å…°çš„å½±å“åŠ›ï¼Œä»¥é¼“åŠ±ç§‘æŠ€æ•™è‚²å’Œæ”¹å–„å°±ä¸šæœºä¼šã€‚&lt;/p>
&lt;!--
> _There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally._
-->
&lt;blockquote>
&lt;p>&lt;em>äºšå¤ªåœ°åŒºéœ€è¦æ›´å¤šçš„å¤–è”æ´»åŠ¨ï¼Œæ•™è‚²å·¥ä½œè€…å’Œå¤§å­¦å¿…é¡»å­¦ä¹  Kubernetesï¼Œå› ä¸ºä»–ä»¬éå¸¸ç¼“æ…¢ï¼Œ
è€Œä¸”å·²ç»è½åäº†8å¹´å¤šã€‚æ–°è¥¿å…°å€¾å‘äºåœ¨æµ·å¤–ä»˜è´¹ï¼Œè€Œä¸æ˜¯æ•™è‚²å½“åœ°äººæœ€æ–°çš„äº‘æŠ€æœ¯ã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="dylan-graham-https-github-com-dylangraham">&lt;a href="https://github.com/DylanGraham">Dylan Graham&lt;/a>&lt;/h2>
&lt;!--
Dylan Graham is a cloud engineer from Adeliade, Australia. He has been contributing to the upstream Kubernetes project since 2018.
-->
&lt;p>Dylan Graham æ˜¯æ¥è‡ªæ¾³å¤§åˆ©äºš Adeliade çš„äº‘è®¡ç®—å·¥ç¨‹å¸ˆã€‚è‡ª 2018 å¹´ä»¥æ¥ï¼Œä»–ä¸€ç›´åœ¨ä¸ºä¸Šæ¸¸ Kubernetes é¡¹ç›®åšå‡ºè´¡çŒ®ã€‚&lt;/p>
&lt;!--
He stated that being a part of such a large-scale project was initially overwhelming, but that the community's friendliness and openness assisted him in getting through it.
-->
&lt;p>ä»–è¡¨ç¤ºï¼Œæˆä¸ºå¦‚æ­¤å¤§é¡¹ç›®çš„ä¸€ä»½å­ï¼Œæœ€åˆå‹åŠ›æ˜¯æ¯”è¾ƒå¤§çš„ï¼Œä½†ç¤¾åŒºçš„å‹å¥½å’Œå¼€æ”¾å¸®åŠ©ä»–åº¦è¿‡äº†éš¾å…³ã€‚&lt;/p>
&lt;!--
He began by contributing to the project documentation and is now mostly focused on the community support for the APAC region.
-->
&lt;p>å¼€å§‹åœ¨é¡¹ç›®æ–‡æ¡£æ–¹é¢åšè´¡çŒ®ï¼Œç°åœ¨ä¸»è¦è‡´åŠ›äºä¸ºäºšå¤ªåœ°åŒºæä¾›ç¤¾åŒºæ”¯æŒã€‚&lt;/p>
&lt;!--
He believes that consistent attendance at community/project meetings, taking on project tasks, and seeking community guidance as needed can help new aspiring developers become effective contributors.
-->
&lt;p>ä»–ç›¸ä¿¡ï¼ŒæŒç»­å‚åŠ ç¤¾åŒº/é¡¹ç›®ä¼šè®®ï¼Œæ‰¿æ‹…é¡¹ç›®ä»»åŠ¡ï¼Œå¹¶åœ¨éœ€è¦æ—¶å¯»æ±‚ç¤¾åŒºæŒ‡å¯¼ï¼Œå¯ä»¥å¸®åŠ©æœ‰æŠ±è´Ÿçš„æ–°å¼€å‘äººå‘˜æˆä¸ºæœ‰æ•ˆçš„è´¡çŒ®è€…ã€‚&lt;/p>
&lt;!--
> _The feeling of being a part of a large community is really special. I've met some amazing people, even some before the pandemic in real life :)_
-->
&lt;blockquote>
&lt;p>&lt;em>æˆä¸ºå¤§ç¤¾åŒºçš„ä¸€ä»½å­æ„Ÿè§‰çœŸçš„å¾ˆç‰¹åˆ«ã€‚æˆ‘é‡åˆ°äº†ä¸€äº›äº†ä¸èµ·çš„äººï¼Œç”šè‡³æ˜¯åœ¨ç°å®ç”Ÿæ´»ä¸­ç–«æƒ…å‘ç”Ÿä¹‹å‰ã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="hippie-hacker-https-github-com-hh">&lt;a href="https://github.com/hh">Hippie Hacker&lt;/a>&lt;/h2>
&lt;!--
Hippie has worked for the CNCF.io as a Strategic Initiatives contractor from New Zealand for almost 5+ years. He is an active contributor to k8s-infra, API conformance testing, Cloud provider conformance submissions, and apisnoop.cncf.io domains of the upstream Kubernetes &amp; CNCF projects.
-->
&lt;p>Hippie æ¥è‡ªæ–°è¥¿å…°ï¼Œæ›¾åœ¨ CNCF.io ä½œä¸ºæˆ˜ç•¥è®¡åˆ’æ‰¿åŒ…å•†å·¥ä½œ 5 å¹´å¤šã€‚ä»–æ˜¯ k8s-infraã€
API ä¸€è‡´æ€§æµ‹è¯•ã€äº‘æä¾›å•†ä¸€è‡´æ€§æäº¤ä»¥åŠä¸Šæ¸¸ Kubernetes å’Œ CNCF é¡¹ç›® apisnoop.cncf.io åŸŸçš„ç§¯æè´¡çŒ®è€…ã€‚&lt;/p>
&lt;!--
He recounts their early involvement with the Kubernetes project, which began roughly 5 years ago when their firm, ii.nz, demonstrated [network booting from a Raspberry Pi using PXE and running Gitlab in-cluster to install Kubernetes on servers](https://ii.nz/post/bringing-the-cloud-to-your-community/).
-->
&lt;p>ä»–è®²è¿°äº†ä»–ä»¬æ—©æœŸå‚ä¸ Kubernetes é¡¹ç›®çš„æƒ…å†µï¼Œè¯¥é¡¹ç›®å§‹äºå¤§çº¦ 5 å¹´å‰ï¼Œå½“æ—¶ä»–ä»¬çš„å…¬å¸ ii.nz
æ¼”ç¤ºäº†&lt;a href="https://ii.nz/post/bringing-the-cloud-to-your-community/">ä½¿ç”¨ PXE ä» Raspberry Pi å¯åŠ¨ç½‘ç»œï¼Œå¹¶åœ¨é›†ç¾¤ä¸­è¿è¡ŒGitlabï¼Œä»¥ä¾¿åœ¨æœåŠ¡å™¨ä¸Šå®‰è£… Kubernetes &lt;/a>&lt;/p>
&lt;!--
He describes their own contributing experience as someone who, at first, tried to do all of the hard lifting on their own, but eventually saw the benefit of group contributions which reduced burnout and task division which allowed folks to keep moving forward on their own momentum.
-->
&lt;p>ä»–æè¿°äº†è‡ªå·±çš„è´¡çŒ®ç»å†ï¼šä¸€å¼€å§‹ï¼Œä»–è¯•å›¾ç‹¬è‡ªå®Œæˆæ‰€æœ‰è‰°å·¨çš„ä»»åŠ¡ï¼Œä½†æœ€ç»ˆçœ‹åˆ°äº†å›¢é˜Ÿåä½œè´¡çŒ®çš„å¥½å¤„ï¼Œ
åˆ†å·¥åˆä½œå‡å°‘äº†è¿‡åº¦ç–²åŠ³ï¼Œè¿™è®©äººä»¬èƒ½å¤Ÿå‡­å€Ÿè‡ªå·±çš„åŠ¨åŠ›ç»§ç»­å‰è¿›ã€‚&lt;/p>
&lt;!--
He recommends that new contributors use pair programming.
-->
&lt;p>ä»–å»ºè®®æ–°çš„è´¡çŒ®è€…ç»“å¯¹ç¼–ç¨‹ã€‚&lt;/p>
&lt;!--
> _The cross pollination of approaches and two pairs of eyes on the same work can often yield a much more amplified effect than a PR comment / approval alone can afford._
-->
&lt;blockquote>
&lt;p>&lt;em>é’ˆå¯¹ä¸€ä¸ªé¡¹ç›®ï¼Œå¤šäººå…³æ³¨å’Œäº¤å‰äº¤æµå¾€å¾€æ¯”å•ç‹¬çš„è¯„å®¡ã€æ‰¹å‡† PR èƒ½äº§ç”Ÿæ›´å¤§çš„æ•ˆæœã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="nick-young-https-github-com-youngnick">&lt;a href="https://github.com/youngnick">Nick Young&lt;/a>&lt;/h2>
&lt;!--
Nick Young works at VMware as a technical lead for Contour, a CNCF ingress controller. He was active with the upstream Kubernetes project from the beginning, and eventually became the chair of the LTS working group, where he advocated user concerns. He is currently the SIG Network Gateway API subproject's maintainer.
-->
&lt;p>Nick Young åœ¨ VMware å·¥ä½œï¼Œæ˜¯ CNCF å…¥å£æ§åˆ¶å™¨ Contour çš„æŠ€æœ¯è´Ÿè´£äººã€‚
ä»–ä»ä¸€å¼€å§‹å°±ç§¯æå‚ä¸ä¸Šæ¸¸ Kubernetes é¡¹ç›®ï¼Œæœ€ç»ˆæˆä¸º LTS å·¥ä½œç»„çš„ä¸»å¸­ï¼Œ
ä»–æå€¡å…³æ³¨ç”¨æˆ·ã€‚ä»–ç›®å‰æ˜¯ SIG Network Gateway API å­é¡¹ç›®çš„ç»´æŠ¤è€…ã€‚&lt;/p>
&lt;!--
His contribution path was notable in that he began working on major areas of the Kubernetes project early on, skewing his trajectory.
-->
&lt;p>ä»–çš„è´¡çŒ®ä¹‹è·¯æ˜¯å¼•äººæ³¨ç›®çš„ï¼Œå› ä¸ºä»–å¾ˆæ—©å°±åœ¨ Kubernetes é¡¹ç›®çš„ä¸»è¦é¢†åŸŸå·¥ä½œï¼Œè¿™æ”¹å˜äº†ä»–çš„è½¨è¿¹ã€‚&lt;/p>
&lt;!--
He asserts the best thing a new contributor can do is to "start contributing". Naturally, if it is relevant to their employment, that is excellent; however, investing non-work time in contributing can pay off in the long run in terms of work. He believes that new contributors, particularly those who are currently Kubernetes users, should be encouraged to participate in higher-level project discussions.
-->
&lt;p>ä»–æ–­è¨€ï¼Œä¸€ä¸ªæ–°è´¡çŒ®è€…èƒ½åšçš„æœ€å¥½çš„äº‹æƒ…å°±æ˜¯â€œå¼€å§‹è´¡çŒ®â€ã€‚å½“ç„¶ï¼Œå¦‚æœä¸ä»–çš„å·¥ä½œæ¯æ¯ç›¸å…³ï¼Œé‚£å¥½æäº†;
ç„¶è€Œï¼ŒæŠŠéå·¥ä½œæ—¶é—´æŠ•å…¥åˆ°è´¡çŒ®ä¸­å»ï¼Œä»é•¿è¿œæ¥çœ‹å¯ä»¥åœ¨å·¥ä½œä¸Šè·å¾—å›æŠ¥ã€‚
ä»–è®¤ä¸ºï¼Œåº”è¯¥é¼“åŠ±æ–°çš„è´¡çŒ®è€…ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ç›®å‰æ˜¯ Kubernetes ç”¨æˆ·çš„äººï¼Œå‚ä¸åˆ°æ›´é«˜å±‚æ¬¡çš„é¡¹ç›®è®¨è®ºä¸­æ¥ã€‚&lt;/p>
&lt;!--
> _Just being active and contributing will get you a long way. Once you've been active for a while, you'll find that you're able to answer questions, which will mean you're asked questions, and before you know it you are an expert._
-->
&lt;blockquote>
&lt;p>&lt;em>åªè¦ç§¯æä¸»åŠ¨ï¼Œåšå‡ºè´¡çŒ®ï¼Œä½ å°±å¯ä»¥èµ°å¾ˆè¿œã€‚ä¸€æ—¦ä½ æ´»è·ƒäº†ä¸€æ®µæ—¶é—´ï¼Œä½ ä¼šå‘ç°ä½ èƒ½å¤Ÿè§£ç­”åˆ«äººçš„é—®é¢˜ï¼Œ
è¿™æ„å‘³ç€ä¼šæœ‰äººè¯·æ•™ä½ æˆ–å’Œä½ è®¨è®ºï¼Œåœ¨ä½ æ„è¯†åˆ°è¿™ä¸€ç‚¹ä¹‹å‰ï¼Œä½ å°±å·²ç»æ˜¯ä¸“å®¶äº†ã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;!--
If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.
-->
&lt;p>å¦‚æœä½ å¯¹æˆ‘ä»¬æ¥ä¸‹æ¥åº”è¯¥é‡‡è®¿çš„äººæœ‰ä»»ä½•æ„è§/å»ºè®®ï¼Œè¯·åœ¨ #sig-contribex ä¸­å‘ŠçŸ¥æˆ‘ä»¬ã€‚
éå¸¸æ„Ÿè°¢ä½ çš„å»ºè®®ã€‚æˆ‘ä»¬å¾ˆé«˜å…´æœ‰æ›´å¤šçš„äººå¸®åŠ©æˆ‘ä»¬æ¥è§¦åˆ°ç¤¾åŒºä¸­æ›´ä¼˜ç§€çš„äººã€‚&lt;/p>
&lt;!--
We'll see you all in the next one. Everyone, till then, have a happy contributing! ğŸ‘‹
-->
&lt;p>æˆ‘ä»¬ä¸‹æœŸå†è§ã€‚ç¥ä½ æœ‰ä¸ªæ„‰å¿«çš„è´¡çŒ®ä¹‹æ—…!ğŸ‘‹&lt;/p></description></item><item><title>Blog: æ›´æ–°ï¼šå¼ƒç”¨ Dockershim çš„å¸¸è§é—®é¢˜</title><link>https://kubernetes.io/zh/blog/2022/02/17/dockershim-faq/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/02/17/dockershim-faq/</guid><description>
&lt;!--
layout: blog
title: "Updated: Dockershim Removal FAQ"
linkTitle: "Dockershim Removal FAQ"
date: 2022-02-17
slug: dockershim-faq
aliases: [ '/dockershim' ]
-->
&lt;!--
**This is an update to the original [Dockershim Deprecation FAQ](/blog/2020/12/02/dockershim-faq/) article,
published in late 2020.**
-->
&lt;p>&lt;strong>æœ¬æ–‡æ˜¯é’ˆå¯¹2020å¹´æœ«å‘å¸ƒçš„&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">å¼ƒç”¨ Dockershim çš„å¸¸è§é—®é¢˜&lt;/a>çš„åšå®¢æ›´æ–°ã€‚&lt;/strong>&lt;/p>
&lt;!--
This document goes over some frequently asked questions regarding the
deprecation and removal of _dockershim_, that was
[announced](/blog/2020/12/08/kubernetes-1-20-release-announcement/)
as a part of the Kubernetes v1.20 release. For more detail
on what that means, check out the blog post
[Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/).
-->
&lt;p>æœ¬æ–‡å›é¡¾äº†è‡ª Kubernetes v1.20 ç‰ˆæœ¬&lt;a href="https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/">å®£å¸ƒ&lt;/a>å¼ƒç”¨
Dockershim ä»¥æ¥æ‰€å¼•å‘çš„ä¸€äº›å¸¸è§é—®é¢˜ã€‚å…³äºå¼ƒç”¨ç»†èŠ‚ä»¥åŠè¿™äº›ç»†èŠ‚èƒŒåçš„å«ä¹‰ï¼Œè¯·å‚è€ƒåšæ–‡
&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/">åˆ«æ…Œ: Kubernetes å’Œ Docker&lt;/a>ã€‚&lt;/p>
&lt;!--
Also, you can read [check whether dockershim removal affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/)
to determine how much impact the removal of dockershim would have for you
or for your organization.
-->
&lt;p>ä½ è¿˜å¯ä»¥æŸ¥é˜…ï¼š&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">æ£€æŸ¥å¼ƒç”¨ Dockershim å¯¹ä½ çš„å½±å“&lt;/a>è¿™ç¯‡æ–‡ç« ï¼Œ
ä»¥ç¡®å®šå¼ƒç”¨ dockershim ä¼šå¯¹ä½ æˆ–ä½ çš„ç»„ç»‡å¸¦æ¥å¤šå¤§çš„å½±å“ã€‚&lt;/p>
&lt;!--
As the Kubernetes 1.24 release has become imminent, we've been working hard to try to make this a smooth transition.
-->
&lt;p>éšç€ Kubernetes 1.24 ç‰ˆæœ¬çš„å‘å¸ƒè¿«åœ¨çœ‰ç«ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨åŠªåŠ›å°è¯•ä½¿å…¶èƒ½å¤Ÿå¹³ç¨³å‡çº§é¡ºåˆ©è¿‡æ¸¡ã€‚&lt;/p>
&lt;!--
- We've written a blog post detailing our [commitment and next steps](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/).
- We believe there are no major blockers to migration to [other container runtimes](/docs/setup/production-environment/container-runtimes/#container-runtimes).
- There is also a [Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/) guide available.
- We've also created a page to list
[articles on dockershim removal and on using CRI-compatible runtimes](/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/).
That list includes some of the already mentioned docs, and also covers selected external sources
(including vendor guides).
-->
&lt;ul>
&lt;li>æˆ‘ä»¬å·²ç»å†™äº†ä¸€ç¯‡åšæ–‡ï¼Œè¯¦ç»†è¯´æ˜äº†æˆ‘ä»¬çš„&lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">æ‰¿è¯ºå’Œåç»­æ“ä½œ&lt;/a>ã€‚&lt;/li>
&lt;li>æˆ‘ä»¬æˆ‘ä»¬ç›¸ä¿¡å¯ä»¥æ— éšœç¢çš„è¿ç§»åˆ°å…¶ä»–&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#container-runtimes">å®¹å™¨è¿è¡Œæ—¶&lt;/a>ã€‚&lt;/li>
&lt;li>æˆ‘ä»¬æ’°å†™äº† &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">dockershim è¿ç§»æŒ‡å—&lt;/a>ä¾›ä½ å‚è€ƒã€‚&lt;/li>
&lt;li>æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªé¡µé¢æ¥åˆ—å‡º&lt;a href="https://kubernetes.io/zh/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/">æœ‰å…³ dockershim ç§»é™¤å’Œä½¿ç”¨ CRI å…¼å®¹è¿è¡Œæ—¶çš„æ–‡ç« &lt;/a>ã€‚
è¯¥åˆ—è¡¨åŒ…æ‹¬ä¸€äº›å·²ç»æåˆ°çš„æ–‡æ¡£ï¼Œè¿˜æ¶µç›–äº†é€‰å®šçš„å¤–éƒ¨èµ„æºï¼ˆåŒ…æ‹¬ä¾›åº”å•†æŒ‡å—ï¼‰ã€‚&lt;/li>
&lt;/ul>
&lt;!--
### Why is the dockershim being removed from Kubernetes?
-->
&lt;h3 id="ä¸ºä»€ä¹ˆä¼šä»-kubernetes-ä¸­ç§»é™¤-dockershim">ä¸ºä»€ä¹ˆä¼šä» Kubernetes ä¸­ç§»é™¤ dockershim ï¼Ÿ&lt;/h3>
&lt;!--
Early versions of Kubernetes only worked with a specific container runtime:
Docker Engine. Later, Kubernetes added support for working with other container runtimes.
The CRI standard was [created](/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) to
enable interoperability between orchestrators (like Kubernetes) and many different container
runtimes.
Docker Engine doesn't implement that interface (CRI), so the Kubernetes project created
special code to help with the transition, and made that _dockershim_ code part of Kubernetes
itself.
-->
&lt;p>Kubernetes çš„æ—©æœŸç‰ˆæœ¬ä»…é€‚ç”¨äºç‰¹å®šçš„å®¹å™¨è¿è¡Œæ—¶ï¼šDocker Engineã€‚
åæ¥ï¼ŒKubernetes å¢åŠ äº†å¯¹ä½¿ç”¨å…¶ä»–å®¹å™¨è¿è¡Œæ—¶çš„æ”¯æŒã€‚&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">åˆ›å»º&lt;/a> CRI
æ ‡å‡†æ˜¯ä¸ºäº†å®ç°ç¼–æ’å™¨ï¼ˆå¦‚ Kubernetesï¼‰å’Œè®¸å¤šä¸åŒçš„å®¹å™¨è¿è¡Œæ—¶ä¹‹é—´äº¤äº’æ“ä½œã€‚
Docker Engine æ²¡æœ‰å®ç°ï¼ˆCRIï¼‰æ¥å£ï¼Œå› æ­¤ Kubernetes é¡¹ç›®åˆ›å»ºäº†ç‰¹æ®Šä»£ç æ¥å¸®åŠ©è¿‡æ¸¡ï¼Œ
å¹¶ä½¿ dockershim ä»£ç æˆä¸º Kubernetes çš„ä¸€éƒ¨åˆ†ã€‚&lt;/p>
&lt;!--
The dockershim code was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
In fact, maintaining dockershim had become a heavy burden on the Kubernetes maintainers.
-->
&lt;p>dockershim ä»£ç ä¸€ç›´æ˜¯ä¸€ä¸ªä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼ˆå› æ­¤å¾—åï¼šshimï¼‰ã€‚
ä½ å¯ä»¥é˜…è¯» &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Kubernetes ç§»é™¤ Dockershim å¢å¼ºæ–¹æ¡ˆ&lt;/a>
ä»¥äº†è§£ç›¸å…³çš„ç¤¾åŒºè®¨è®ºå’Œè®¡åˆ’ã€‚
äº‹å®ä¸Šï¼Œç»´æŠ¤ dockershim å·²ç»æˆä¸º Kubernetes ç»´æŠ¤è€…çš„æ²‰é‡è´Ÿæ‹…ã€‚&lt;/p>
&lt;!--
Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.
-->
&lt;p>æ­¤å¤–ï¼Œåœ¨è¾ƒæ–°çš„ CRI è¿è¡Œæ—¶ä¸­å®ç°äº†ä¸ dockershim ä¸å…¼å®¹çš„åŠŸèƒ½ï¼Œä¾‹å¦‚ cgroups v2 å’Œç”¨æˆ·å‘½åç©ºé—´ã€‚
å–æ¶ˆå¯¹ dockershim çš„æ”¯æŒå°†åŠ é€Ÿè¿™äº›é¢†åŸŸçš„å‘å±•ã€‚&lt;/p>
&lt;!--
### Can I still use Docker Engine in Kubernetes 1.23?
-->
&lt;h3 id="åœ¨-kubernetes-1-23-ç‰ˆæœ¬ä¸­è¿˜å¯ä»¥ä½¿ç”¨-docker-engine-å—">åœ¨ Kubernetes 1.23 ç‰ˆæœ¬ä¸­è¿˜å¯ä»¥ä½¿ç”¨ Docker Engine å—ï¼Ÿ&lt;/h3>
&lt;!--
Yes, the only thing changed in 1.20 is a single warning log printed at [kubelet]
startup if using Docker Engine as the runtime. You'll see this warning in all versions up to 1.23. The dockershim removal occurs in Kubernetes 1.24.
-->
&lt;p>å¯ä»¥ä½¿ç”¨ï¼Œåœ¨ 1.20 ç‰ˆæœ¬ä¸­å”¯ä¸€çš„æ”¹åŠ¨æ˜¯ï¼Œå¦‚æœä½¿ç”¨ Docker Engineï¼Œ
åœ¨ &lt;a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
å¯åŠ¨æ—¶ä¼šæ‰“å°ä¸€ä¸ªè­¦å‘Šæ—¥å¿—ã€‚
ä½ å°†åœ¨ 1.23 ç‰ˆæœ¬åŠä»¥å‰ç‰ˆæœ¬çœ‹åˆ°æ­¤è­¦å‘Šã€‚dockershim å°†åœ¨ Kubernetes 1.24 ç‰ˆæœ¬ä¸­ç§»é™¤ ã€‚&lt;/p>
&lt;!--
### When will dockershim be removed?
-->
&lt;h3 id="ä»€ä¹ˆæ—¶å€™ç§»é™¤-dockershim">ä»€ä¹ˆæ—¶å€™ç§»é™¤ dockershim ï¼Ÿ&lt;/h3>
&lt;!--
Given the impact of this change, we are using an extended deprecation timeline.
Removal of dockershim is scheduled for Kubernetes v1.24, see [Dockershim Removal Kubernetes Enhancement Proposal][drkep].
The Kubernetes project will be working closely with vendors and other ecosystem groups to ensure
a smooth transition and will evaluate things as the situation evolves.
-->
&lt;p>è€ƒè™‘åˆ°æ­¤å˜æ›´å¸¦æ¥çš„å½±å“ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŠ é•¿çš„åºŸå¼ƒæ—¶é—´è¡¨ã€‚
dockershim è®¡åˆ’åœ¨ Kubernetes v1.24 ä¸­è¿›è¡Œç§»é™¤ï¼Œ
å‚è§ &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Kubernetes ç§»é™¤ Dockershim å¢å¼ºæ–¹æ¡ˆ&lt;/a>ã€‚
Kubernetes é¡¹ç›®å°†ä¸ä¾›åº”å•†å’Œå…¶ä»–ç”Ÿæ€ç³»ç»Ÿç»„ç»‡å¯†åˆ‡åˆä½œï¼Œä»¥ç¡®ä¿å¹³ç¨³è¿‡æ¸¡ï¼Œå¹¶å°†ä¾æ®äº‹æ€çš„å‘å±•è¯„ä¼°åç»­äº‹é¡¹ã€‚&lt;/p>
&lt;!--
### Can I still use Docker Engine as my container runtime?
-->
&lt;h3 id="æˆ‘è¿˜å¯ä»¥ä½¿ç”¨-docker-engine-ä½œä¸ºæˆ‘çš„å®¹å™¨è¿è¡Œæ—¶å—">æˆ‘è¿˜å¯ä»¥ä½¿ç”¨ Docker Engine ä½œä¸ºæˆ‘çš„å®¹å™¨è¿è¡Œæ—¶å—ï¼Ÿ&lt;/h3>
&lt;!--
First off, if you use Docker on your own PC to develop or test containers: nothing changes.
You can still use Docker locally no matter what container runtime(s) you use for your
Kubernetes clusters. Containers make this kind of interoperability possible.
-->
&lt;p>é¦–å…ˆï¼Œå¦‚æœä½ åœ¨è‡ªå·±çš„ç”µè„‘ä¸Šä½¿ç”¨ Docker ç”¨æ¥åšå¼€å‘æˆ–æµ‹è¯•å®¹å™¨ï¼šå®ƒå°†ä¸ä¹‹å‰æ²¡æœ‰ä»»ä½•å˜åŒ–ã€‚
æ— è®ºä½ ä¸º Kubernetes é›†ç¾¤ä½¿ç”¨ä»€ä¹ˆå®¹å™¨è¿è¡Œæ—¶ï¼Œä½ éƒ½å¯ä»¥åœ¨æœ¬åœ°ä½¿ç”¨ Dockerã€‚å®¹å™¨ä½¿è¿™ç§äº¤äº’æˆä¸ºå¯èƒ½ã€‚&lt;/p>
&lt;!--
Mirantis and Docker have [committed][mirantis] to maintaining a replacement adapter for
Docker Engine, and to maintain that adapter even after the in-tree dockershim is removed
from Kubernetes. The replacement adapter is named [`cri-dockerd`](https://github.com/Mirantis/cri-dockerd).
-->
&lt;p>Mirantis å’Œ Docker å·²&lt;a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/">æ‰¿è¯º&lt;/a>
ä¸º Docker Engine ç»´æŠ¤ä¸€ä¸ªæ›¿ä»£é€‚é…å™¨ï¼Œ
å¹¶åœ¨ dockershim ä» Kubernetes ç§»é™¤åç»´æŠ¤è¯¥é€‚é…å™¨ã€‚
æ›¿ä»£é€‚é…å™¨åä¸º &lt;a href="https://github.com/Mirantis/cri-dockerd">&lt;code>cri-dockerd&lt;/code>&lt;/a>ã€‚&lt;/p>
&lt;!--
### Will my existing container images still work?
-->
&lt;h3 id="æˆ‘ç°æœ‰çš„å®¹å™¨é•œåƒè¿˜èƒ½æ­£å¸¸å·¥ä½œå—">æˆ‘ç°æœ‰çš„å®¹å™¨é•œåƒè¿˜èƒ½æ­£å¸¸å·¥ä½œå—ï¼Ÿ&lt;/h3>
&lt;!--
Yes, the images produced from `docker build` will work with all CRI implementations.
All your existing images will still work exactly the same.
-->
&lt;p>å½“ç„¶å¯ä»¥ï¼Œ&lt;code>docker build&lt;/code> åˆ›å»ºçš„é•œåƒé€‚ç”¨äºä»»ä½• CRI å®ç°ã€‚
æ‰€æœ‰ä½ çš„ç°æœ‰é•œåƒå°†å’Œå¾€å¸¸ä¸€æ ·å·¥ä½œã€‚&lt;/p>
&lt;!--
#### What about private images?
-->
&lt;h3 id="ç§æœ‰é•œåƒå‘¢">ç§æœ‰é•œåƒå‘¢ï¼Ÿ&lt;/h3>
&lt;!--
Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.
-->
&lt;p>å½“ç„¶å¯ä»¥ã€‚æ‰€æœ‰ CRI è¿è¡Œæ—¶å‡æ”¯æŒåœ¨ Kubernetes ä¸­ç›¸åŒçš„æ‹‰å–ï¼ˆpullï¼‰Secret é…ç½®ï¼Œ
æ— è®ºæ˜¯é€šè¿‡ PodSpec è¿˜æ˜¯ ServiceAccountã€‚&lt;/p>
&lt;!--
### Are Docker and containers the same thing?
-->
&lt;h3 id="docker-å’Œå®¹å™¨æ˜¯ä¸€å›äº‹å—">Docker å’Œå®¹å™¨æ˜¯ä¸€å›äº‹å—ï¼Ÿ&lt;/h3>
&lt;!--
Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.
-->
&lt;p>Docker æ™®åŠäº† Linux å®¹å™¨æ¨¡å¼ï¼Œå¹¶åœ¨å¼€å‘åº•å±‚æŠ€æœ¯æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œ
ä½†æ˜¯ Linux ä¸­çš„å®¹å™¨å·²ç»å­˜åœ¨äº†å¾ˆé•¿æ—¶é—´ã€‚å®¹å™¨çš„ç”Ÿæ€ç›¸æ¯”äº Docker å…·æœ‰æ›´å®½å¹¿çš„é¢†åŸŸã€‚
OCI å’Œ CRI ç­‰æ ‡å‡†å¸®åŠ©è®¸å¤šå·¥å…·åœ¨æˆ‘ä»¬çš„ç”Ÿæ€ç³»ç»Ÿä¸­å‘å±•å£®å¤§ï¼Œ
å…¶ä¸­ä¸€äº›æ›¿ä»£äº† Docker çš„æŸäº›æ–¹é¢ï¼Œè€Œå¦ä¸€äº›åˆ™å¢å¼ºäº†ç°æœ‰åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
### Are there examples of folks using other runtimes in production today?
-->
&lt;h3 id="ç°åœ¨æ˜¯å¦æœ‰åœ¨ç”Ÿäº§ç³»ç»Ÿä¸­ä½¿ç”¨å…¶ä»–è¿è¡Œæ—¶çš„ä¾‹å­">ç°åœ¨æ˜¯å¦æœ‰åœ¨ç”Ÿäº§ç³»ç»Ÿä¸­ä½¿ç”¨å…¶ä»–è¿è¡Œæ—¶çš„ä¾‹å­ï¼Ÿ&lt;/h3>
&lt;!--
All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.
-->
&lt;p>Kubernetes æ‰€æœ‰é¡¹ç›®åœ¨æ‰€æœ‰ç‰ˆæœ¬ä¸­å‡ºäº§çš„å·¥ä»¶ï¼ˆKubernetes äºŒè¿›åˆ¶æ–‡ä»¶ï¼‰éƒ½ç»è¿‡äº†éªŒè¯ã€‚&lt;/p>
&lt;!--
Additionally, the [kind] project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the [CRI-O] runtime in production since June 2019.
-->
&lt;p>æ­¤å¤–ï¼Œ&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> é¡¹ç›®ä½¿ç”¨ containerd å·²ç»æœ‰ä¸€æ®µæ—¶é—´äº†ï¼Œå¹¶ä¸”æé«˜äº†å…¶ç”¨ä¾‹çš„ç¨³å®šæ€§ã€‚
Kind å’Œ containerd æ¯å¤©éƒ½ä¼šè¢«å¤šæ¬¡ä½¿ç”¨æ¥éªŒè¯å¯¹ Kubernetes ä»£ç åº“çš„ä»»ä½•æ›´æ”¹ã€‚
å…¶ä»–ç›¸å…³é¡¹ç›®ä¹Ÿéµå¾ªåŒæ ·çš„æ¨¡å¼ï¼Œä»è€Œå±•ç¤ºäº†å…¶ä»–å®¹å™¨è¿è¡Œæ—¶çš„ç¨³å®šæ€§å’Œå¯ç”¨æ€§ã€‚
ä¾‹å¦‚ï¼ŒOpenShift 4.x ä» 2019 å¹´ 6 æœˆä»¥æ¥ï¼Œå°±ä¸€ç›´åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ &lt;a href="https://cri-o.io/">CRI-O&lt;/a> è¿è¡Œæ—¶ã€‚&lt;/p>
&lt;!--
For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation ([CNCF]).
-->
&lt;p>è‡³äºå…¶ä»–ç¤ºä¾‹å’Œå‚è€ƒèµ„æ–™ï¼Œä½ å¯ä»¥æŸ¥çœ‹ containerd å’Œ CRI-O çš„ä½¿ç”¨è€…åˆ—è¡¨ï¼Œ
è¿™ä¸¤ä¸ªå®¹å™¨è¿è¡Œæ—¶æ˜¯äº‘åŸç”ŸåŸºé‡‘ä¼šï¼ˆ&lt;a href="https://cncf.io">CNCF&lt;/a>ï¼‰ä¸‹çš„é¡¹ç›®ã€‚&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### People keep referencing OCI, what is that?
-->
&lt;h3 id="äººä»¬æ€»åœ¨è°ˆè®º-oci-å®ƒæ˜¯ä»€ä¹ˆ">äººä»¬æ€»åœ¨è°ˆè®º OCIï¼Œå®ƒæ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h3>
&lt;!--
OCI stands for the [Open Container Initiative], which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of [runc], which is the underlying default runtime for both
[containerd] and [CRI-O]. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.
-->
&lt;p>OCI æ˜¯ &lt;a href="https://opencontainers.org/about/overview/">Open Container Initiative&lt;/a> çš„ç¼©å†™ï¼Œ
å®ƒæ ‡å‡†åŒ–äº†å®¹å™¨å·¥å…·å’Œåº•å±‚å®ç°ä¹‹é—´çš„å¤§é‡æ¥å£ã€‚
å®ƒä»¬ç»´æŠ¤äº†æ‰“åŒ…å®¹å™¨é•œåƒï¼ˆOCI imageï¼‰å’Œè¿è¡Œæ—¶ï¼ˆOCI runtimeï¼‰çš„æ ‡å‡†è§„èŒƒã€‚
å®ƒä»¬è¿˜ä»¥ &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> çš„å½¢å¼ç»´æŠ¤äº†ä¸€ä¸ª runtime-spec çš„çœŸå®å®ç°ï¼Œ
è¿™ä¹Ÿæ˜¯ &lt;a href="https://containerd.io/">containerd&lt;/a> å’Œ &lt;a href="https://cri-o.io/">CRI-O&lt;/a> ä¾èµ–çš„é»˜è®¤è¿è¡Œæ—¶ã€‚
CRI å»ºç«‹åœ¨è¿™äº›åº•å±‚è§„èŒƒä¹‹ä¸Šï¼Œä¸ºç®¡ç†å®¹å™¨æä¾›ç«¯åˆ°ç«¯çš„æ ‡å‡†ã€‚&lt;/p>
&lt;!--
### Which CRI implementation should I use?
-->
&lt;h3 id="æˆ‘åº”è¯¥ç”¨å“ªä¸ª-cri-å®ç°">æˆ‘åº”è¯¥ç”¨å“ªä¸ª CRI å®ç°ï¼Ÿ&lt;/h3>
&lt;!--
Thatâ€™s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the [CNCF landscape] in case another would be an
even better fit for your environment.
-->
&lt;p>è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œä¾èµ–äºè®¸å¤šå› ç´ ã€‚
å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ Dockerï¼Œè¿ç§»åˆ° containerd åº”è¯¥æ˜¯ä¸€ä¸ªç›¸å¯¹å®¹æ˜“åœ°è½¬æ¢ï¼Œå¹¶å°†è·å¾—æ›´å¥½çš„æ€§èƒ½å’Œæ›´å°‘çš„å¼€é”€ã€‚
ç„¶è€Œï¼Œæˆ‘ä»¬é¼“åŠ±ä½ æ¢ç´¢ &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape&lt;/a>
æä¾›çš„æ‰€æœ‰é€‰é¡¹ï¼Œåšå‡ºæ›´é€‚åˆä½ çš„é€‰æ‹©ã€‚&lt;/p>
&lt;!--
### What should I look out for when changing CRI implementations?
-->
&lt;h3 id="å½“åˆ‡æ¢-cri-å®ç°æ—¶-åº”è¯¥æ³¨æ„ä»€ä¹ˆ">å½“åˆ‡æ¢ CRI å®ç°æ—¶ï¼Œåº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ&lt;/h3>
&lt;!--
While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:
-->
&lt;p>è™½ç„¶ Docker å’Œå¤§å¤šæ•° CRIï¼ˆåŒ…æ‹¬ containerdï¼‰ä¹‹é—´çš„åº•å±‚å®¹å™¨åŒ–ä»£ç æ˜¯ç›¸åŒçš„ï¼Œ
ä½†å…¶å‘¨è¾¹éƒ¨åˆ†å´å­˜åœ¨å·®å¼‚ã€‚è¿ç§»æ—¶è¦è€ƒè™‘å¦‚ä¸‹å¸¸è§äº‹é¡¹ï¼š&lt;/p>
&lt;!--
- Logging configuration
- Runtime resource limitations
- Node provisioning scripts that call docker or use docker via it's control socket
- Kubectl plugins that require docker CLI or the control socket
- Tools from the Kubernetes project that require direct access to Docker Engine
(for example: the deprecated `kube-imagepuller` tool)
- Configuration of functionality like `registry-mirrors` and insecure registries
- Other support scripts or daemons that expect Docker Engine to be available and are run
outside of Kubernetes (for example, monitoring or security agents)
- GPUs or special hardware and how they integrate with your runtime and Kubernetes
-->
&lt;ul>
&lt;li>æ—¥å¿—é…ç½®&lt;/li>
&lt;li>è¿è¡Œæ—¶çš„èµ„æºé™åˆ¶&lt;/li>
&lt;li>è°ƒç”¨ docker æˆ–é€šè¿‡å…¶æ§åˆ¶å¥—æ¥å­—ä½¿ç”¨ docker çš„èŠ‚ç‚¹é…ç½®è„šæœ¬&lt;/li>
&lt;li>éœ€è¦è®¿é—® docker å‘½ä»¤æˆ–æ§åˆ¶å¥—æ¥å­—çš„ kubectl æ’ä»¶&lt;/li>
&lt;li>éœ€è¦ç›´æ¥è®¿é—® Docker Engine çš„ Kubernetes å·¥å…·ï¼ˆä¾‹å¦‚ï¼šå·²å¼ƒç”¨çš„ 'kube-imagepuller' å·¥å…·ï¼‰&lt;/li>
&lt;li>&lt;code>registry-mirrors&lt;/code> å’Œä¸å®‰å…¨æ³¨å†Œè¡¨ç­‰åŠŸèƒ½çš„é…ç½®&lt;/li>
&lt;li>ä¿éšœ Docker Engine å¯ç”¨ã€ä¸”è¿è¡Œåœ¨ Kubernetes ä¹‹å¤–çš„è„šæœ¬æˆ–å®ˆæŠ¤è¿›ç¨‹ï¼ˆä¾‹å¦‚ï¼šç›‘è§†æˆ–å®‰å…¨ä»£ç†ï¼‰&lt;/li>
&lt;li>GPU æˆ–ç‰¹æ®Šç¡¬ä»¶ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ä¸ä½ çš„è¿è¡Œæ—¶å’Œ Kubernetes é›†æˆ&lt;/li>
&lt;/ul>
&lt;!--
If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if youâ€™ve customized
your `dockerd` configuration, youâ€™ll need to adapt that for your new container
runtime where possible.
-->
&lt;p>å¦‚æœä½ åªæ˜¯ç”¨äº† Kubernetes èµ„æºè¯·æ±‚/é™åˆ¶æˆ–åŸºäºæ–‡ä»¶çš„æ—¥å¿—æ”¶é›† DaemonSetï¼Œå®ƒä»¬å°†ç»§ç»­ç¨³å®šå·¥ä½œï¼Œ
ä½†æ˜¯å¦‚æœä½ ç”¨äº†è‡ªå®šä¹‰äº† dockerd é…ç½®ï¼Œåˆ™å¯èƒ½éœ€è¦ä¸ºæ–°çš„å®¹å™¨è¿è¡Œæ—¶åšä¸€äº›é€‚é…å·¥ä½œã€‚&lt;/p>
&lt;!--
Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the [`crictl`][cr] tool as a drop-in replacement (see [mapping from docker cli to crictl](https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl)) and for the
latter you can use newer container build options like [img], [buildah],
[kaniko], or [buildkit-cli-for-kubectl] that donâ€™t require Docker.
-->
&lt;p>å¦å¤–è¿˜æœ‰ä¸€ä¸ªéœ€è¦å…³æ³¨çš„ç‚¹ï¼Œé‚£å°±æ˜¯å½“åˆ›å»ºé•œåƒæ—¶ï¼Œç³»ç»Ÿç»´æŠ¤æˆ–åµŒå…¥å®¹å™¨æ–¹é¢çš„ä»»åŠ¡å°†æ— æ³•å·¥ä½œã€‚
å¯¹äºå‰è€…ï¼Œå¯ä»¥ç”¨ &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> å·¥å…·ä½œä¸ºä¸´æ—¶æ›¿ä»£æ–¹æ¡ˆ
(å‚é˜…&lt;a href="https://kubernetes.io/zh/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl">ä» docker cli åˆ° crictl çš„æ˜ å°„&lt;/a>)ã€‚
å¯¹äºåè€…ï¼Œå¯ä»¥ç”¨æ–°çš„å®¹å™¨åˆ›å»ºé€‰é¡¹ï¼Œä¾‹å¦‚
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>ã€
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>ã€
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a> æˆ–
&lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a>ï¼Œ
ä»–ä»¬éƒ½ä¸éœ€è¦ Dockerã€‚&lt;/p>
&lt;!--
For containerd, you can start with their [documentation] to see what configuration
options are available as you migrate things over.
-->
&lt;p>å¯¹äº containerdï¼Œä½ å¯æŸ¥é˜…æœ‰å…³å®ƒçš„&lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">æ–‡æ¡£&lt;/a>ï¼Œ
è·å–è¿ç§»æ—¶å¯ç”¨çš„é…ç½®é€‰é¡¹ã€‚&lt;/p>
&lt;!--
For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on [Container Runtimes].
-->
&lt;p>æœ‰å…³å¦‚ä½•åœ¨ Kubernetes ä¸­ä½¿ç”¨ containerd å’Œ CRI-O çš„è¯´æ˜ï¼Œ
è¯·å‚é˜… &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">Kubernetes ç›¸å…³æ–‡æ¡£&lt;/a>ã€‚&lt;/p>
&lt;!--
### What if I have more questions?
-->
&lt;h3 id="æˆ‘è¿˜æœ‰å…¶ä»–é—®é¢˜æ€ä¹ˆåŠ">æˆ‘è¿˜æœ‰å…¶ä»–é—®é¢˜æ€ä¹ˆåŠï¼Ÿ&lt;/h3>
&lt;!--
If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: https://discuss.kubernetes.io/.
-->
&lt;p>å¦‚æœä½ ä½¿ç”¨äº†ä¾›åº”å•†æ”¯æŒçš„ Kubernetes å‘è¡Œç‰ˆï¼Œä½ å¯ä»¥å’¨è¯¢ä¾›åº”å•†ä»–ä»¬äº§å“çš„å‡çº§è®¡åˆ’ã€‚
å¯¹äºæœ€ç»ˆç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·æŠŠé—®é¢˜å‘åˆ°æˆ‘ä»¬çš„æœ€ç»ˆç”¨æˆ·ç¤¾åŒºçš„è®ºå›ï¼šhttps://discuss.kubernetes.io/ã€‚&lt;/p>
&lt;!--
You can discuss the decision to remove dockershim via a dedicated
[GitHub issue](https://github.com/kubernetes/kubernetes/issues/106917).
-->
&lt;p>ä½ å¯ä»¥é€šè¿‡ä¸“ç”¨ &lt;a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub é—®é¢˜&lt;/a>
è®¨è®ºåˆ é™¤ dockershim çš„å†³å®šã€‚&lt;/p>
&lt;!--
You can also check out the excellent blog post
[Wait, Docker is deprecated in Kubernetes now?][dep] a more in-depth technical
discussion of the changes.
-->
&lt;p>ä½ ä¹Ÿå¯ä»¥çœ‹çœ‹è¿™ç¯‡ä¼˜ç§€çš„åšå®¢æ–‡ç« ï¼š&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">ç­‰ç­‰ï¼ŒDocker è¢« Kubernetes å¼ƒç”¨äº†?&lt;/a>
å¯¹è¿™äº›å˜åŒ–è¿›è¡Œæ›´æ·±å…¥çš„æŠ€æœ¯è®¨è®ºã€‚&lt;/p>
&lt;!--
### Is there any tooling that can help me find dockershim in use
-->
&lt;h3 id="æ˜¯å¦æœ‰ä»»ä½•å·¥å…·å¯ä»¥å¸®åŠ©æˆ‘æ‰¾åˆ°æ­£åœ¨ä½¿ç”¨çš„-dockershim">æ˜¯å¦æœ‰ä»»ä½•å·¥å…·å¯ä»¥å¸®åŠ©æˆ‘æ‰¾åˆ°æ­£åœ¨ä½¿ç”¨çš„ dockershim&lt;/h3>
&lt;!--
Yes! The [Detector for Docker Socket (DDS)][dds] is a kubectl plugin that you can
install and then use to check your cluster. DDS can detect if active Kubernetes workloads
are mounting the Docker Engine socket (`docker.sock`) as a volume.
Find more details and usage patterns in the DDS project's [README][dds].
-->
&lt;p>æ˜¯çš„ï¼ &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">Docker Socket æ£€æµ‹å™¨ (DDS)&lt;/a> æ˜¯ä¸€ä¸ª kubectl æ’ä»¶ï¼Œ
ä½ å¯ä»¥å®‰è£…å®ƒç”¨äºæ£€æŸ¥ä½ çš„é›†ç¾¤ã€‚ DDS å¯ä»¥æ£€æµ‹è¿è¡Œä¸­çš„ Kubernetes
å·¥ä½œè´Ÿè½½æ˜¯å¦å°† Docker å¼•æ“å¥—æ¥å­— (&lt;code>docker.sock&lt;/code>) ä½œä¸ºå·æŒ‚è½½ã€‚
åœ¨ DDS é¡¹ç›®çš„ &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">README&lt;/a> ä¸­æŸ¥æ‰¾æ›´å¤šè¯¦ç»†ä¿¡æ¯å’Œä½¿ç”¨æ–¹æ³•ã€‚&lt;/p>
&lt;!--
### Can I have a hug?
-->
&lt;h3 id="æˆ‘å¯ä»¥åŠ å…¥å—">æˆ‘å¯ä»¥åŠ å…¥å—ï¼Ÿ&lt;/h3>
&lt;!--
Yes, we're still giving hugs as requested. ğŸ¤—ğŸ¤—ğŸ¤—
-->
&lt;p>å½“ç„¶ï¼Œåªè¦ä½ æ„¿æ„ï¼Œéšæ—¶éšåœ°æ¬¢è¿ã€‚ğŸ¤—ğŸ¤—ğŸ¤—&lt;/p></description></item><item><title>Blog: SIG Node CI å­é¡¹ç›®åº†ç¥æµ‹è¯•æ”¹è¿›ä¸¤å‘¨å¹´</title><link>https://kubernetes.io/zh/blog/2022/02/16/sig-node-ci-subproject-celebrates/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/02/16/sig-node-ci-subproject-celebrates/</guid><description>
&lt;!--
---
layout: blog
title: 'SIG Node CI Subproject Celebrates Two Years of Test Improvements'
date: 2022-02-16
slug: sig-node-ci-subproject-celebrates
canonicalUrl: https://www.kubernetes.dev/blog/2022/02/16/sig-node-ci-subproject-celebrates-two-years-of-test-improvements/
url: /zh/blog/2022/02/sig-node-ci-subproject-celebrates
---
-->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong> Sergey Kanzhelev (Google), Elana Hashman (Red Hat)&lt;/p>
&lt;!--**Authors:** Sergey Kanzhelev (Google), Elana Hashman (Red Hat)-->
&lt;!--Ensuring the reliability of SIG Node upstream code is a continuous effort
that takes a lot of behind-the-scenes effort from many contributors.
There are frequent releases of Kubernetes, base operating systems,
container runtimes, and test infrastructure that result in a complex matrix that
requires attention and steady investment to "keep the lights on."
In May 2020, the Kubernetes node special interest group ("SIG Node") organized a new
subproject for continuous integration (CI) for node-related code and tests. Since its
inauguration, the SIG Node CI subproject has run a weekly meeting, and even the full hour
is often not enough to complete triage of all bugs, test-related PRs and issues, and discuss all
related ongoing work within the subgroup.-->
&lt;p>ä¿è¯ SIG èŠ‚ç‚¹ä¸Šæ¸¸ä»£ç çš„å¯é æ€§æ˜¯ä¸€é¡¹æŒç»­çš„å·¥ä½œï¼Œéœ€è¦è®¸å¤šè´¡çŒ®è€…åœ¨å¹•åä»˜å‡ºå¤§é‡åŠªåŠ›ã€‚
Kubernetesã€åŸºç¡€æ“ä½œç³»ç»Ÿã€å®¹å™¨è¿è¡Œæ—¶å’Œæµ‹è¯•åŸºç¡€æ¶æ„çš„é¢‘ç¹å‘å¸ƒï¼Œå¯¼è‡´äº†ä¸€ä¸ªå¤æ‚çš„çŸ©é˜µï¼Œ
éœ€è¦å…³æ³¨å’Œç¨³å®šçš„æŠ•èµ„æ¥â€œä¿æŒç¯ç«é€šæ˜â€ã€‚2020 å¹´ 5 æœˆï¼ŒKubernetes Node ç‰¹æ®Šå…´è¶£å°ç»„
ï¼ˆâ€œSIG Nodeâ€ï¼‰ä¸ºèŠ‚ç‚¹ç›¸å…³ä»£ç å’Œæµ‹è¯•ç»„ç»‡äº†ä¸€ä¸ªæ–°çš„æŒç»­é›†æˆï¼ˆCIï¼‰å­é¡¹ç›®ã€‚è‡ªæˆç«‹ä»¥æ¥ï¼ŒSIG Node CI
å­é¡¹ç›®æ¯å‘¨ä¸¾è¡Œä¸€æ¬¡ä¼šè®®ï¼Œå³ä½¿ä¸€æ•´ä¸ªå°æ—¶é€šå¸¸ä¹Ÿä¸è¶³ä»¥å®Œæˆå¯¹æ‰€æœ‰ç¼ºé™·ã€æµ‹è¯•ç›¸å…³çš„ PR å’Œé—®é¢˜çš„åˆ†ç±»ï¼Œ
å¹¶è®¨è®ºç»„å†…æ‰€æœ‰ç›¸å…³çš„æ­£åœ¨è¿›è¡Œçš„å·¥ä½œã€‚&lt;/p>
&lt;!--Over the past two years, we've fixed merge-blocking and release-blocking tests, reducing time to merge Kubernetes contributors' pull requests thanks to reduced test flakes. When we started, Node test jobs only passed 42% of the time, and through our efforts, we now ensure a consistent >90% job pass rate. We've closed 144 test failure issues and merged 176 pull requests just in kubernetes/kubernetes. And we've helped subproject participants ascend the Kubernetes contributor ladder, with 3 new org members, 6 new reviewers, and 2 new approvers.-->
&lt;p>åœ¨è¿‡å»ä¸¤å¹´ä¸­ï¼Œæˆ‘ä»¬ä¿®å¤äº†é˜»å¡åˆå¹¶å’Œé˜»å¡å‘å¸ƒçš„æµ‹è¯•ï¼Œç”±äºå‡å°‘äº†æµ‹è¯•ç¼ºé™·ï¼Œç¼©çŸ­äº†åˆå¹¶ Kubernetes
è´¡çŒ®è€…çš„æ‹‰å–è¯·æ±‚çš„æ—¶é—´ã€‚é€šè¿‡æˆ‘ä»¬çš„åŠªåŠ›ï¼Œä»»åŠ¡é€šè¿‡ç‡ç”±å¼€å§‹æ—¶ 42% æé«˜è‡³ç¨³å®šå¤§äº 90% ã€‚æˆ‘ä»¬å·²ç»è§£å†³äº† 144 ä¸ªæµ‹è¯•å¤±è´¥é—®é¢˜ï¼Œ
å¹¶åœ¨ kubernetes/kubernetes ä¸­åˆå¹¶äº† 176 ä¸ªæ‹‰å–è¯·æ±‚ã€‚
æˆ‘ä»¬è¿˜å¸®åŠ©å­é¡¹ç›®å‚ä¸è€…æå‡äº† Kubernetes è´¡çŒ®è€…çš„ç­‰çº§ï¼Œæ–°å¢äº† 3 åç»„ç»‡æˆå‘˜ã€6 åè¯„å®¡å‘˜å’Œ 2 åå®¡æ‰¹å‘˜ã€‚&lt;/p>
&lt;!--The Node CI subproject is an approachable first stop to help new contributors
get started with SIG Node. There is a low barrier to entry for new contributors
to address high-impact bugs and test fixes, although there is a long
road before contributors can climb the entire contributor ladder:
it took over a year to establish two new approvers for the group.
The complexity of all the different components that power Kubernetes nodes
and its test infrastructure requires a sustained investment over a long period
for developers to deeply understand the entire system,
both at high and low levels of detail.-->
&lt;p>Node CI å­é¡¹ç›®æ˜¯ä¸€ä¸ªå¯å…¥é—¨çš„ç¬¬ä¸€ç«™ï¼Œå¸®åŠ©æ–°å‚ä¸è€…å¼€å§‹ä½¿ç”¨ SIG Nodeã€‚å¯¹äºæ–°è´¡çŒ®è€…æ¥è¯´ï¼Œ
è§£å†³å½±å“è¾ƒå¤§çš„ç¼ºé™·å’Œæµ‹è¯•ä¿®å¤çš„é—¨æ§›å¾ˆä½ï¼Œå°½ç®¡è´¡çŒ®è€…è¦æ”€ç™»æ•´ä¸ªè´¡çŒ®è€…é˜¶æ¢¯è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ï¼š
ä¸ºè¯¥å›¢é˜ŸåŸ¹å…»äº†ä¸¤ä¸ªæ–°çš„å®¡æ‰¹äººèŠ±äº†ä¸€å¹´å¤šçš„æ—¶é—´ã€‚ä¸º Kubernetes èŠ‚ç‚¹åŠå…¶æµ‹è¯•åŸºç¡€è®¾æ–½æä¾›åŠ¨åŠ›çš„æ‰€æœ‰
ä¸åŒç»„ä»¶çš„å¤æ‚æ€§è¦æ±‚å¼€å‘äººå‘˜åœ¨å¾ˆé•¿ä¸€æ®µæ—¶é—´å†…è¿›è¡ŒæŒç»­æŠ•èµ„ï¼Œ
ä»¥æ·±å…¥äº†è§£æ•´ä¸ªç³»ç»Ÿï¼Œä»å®è§‚åˆ°å¾®è§‚ã€‚&lt;/p>
&lt;!--We have several regular contributors at our meetings, however; our reviewers
and approvers pool is still small. It is our goal to continue to grow
contributors to ensure a sustainable distribution of work
that does not just fall to a few key approvers.-->
&lt;p>è™½ç„¶åœ¨æˆ‘ä»¬çš„ä¼šè®®ä¸Šæœ‰å‡ ä¸ªæ¯”è¾ƒå›ºå®šçš„è´¡çŒ®è€…ï¼›ä½†æ˜¯æˆ‘ä»¬çš„è¯„å®¡å‘˜å’Œå®¡æ‰¹å‘˜ä»ç„¶å¾ˆå°‘ã€‚
æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç»§ç»­å¢åŠ è´¡çŒ®è€…ï¼Œä»¥ç¡®ä¿å·¥ä½œçš„å¯æŒç»­åˆ†é…ï¼Œè€Œä¸ä»…ä»…æ˜¯å°‘æ•°å…³é”®æ‰¹å‡†è€…ã€‚&lt;/p>
&lt;!--It's not always obvious how subprojects within SIGs are formed, operate,
and work. Each is unique to its sponsoring SIG and tailored to the projects
that the group is intended to support. As a group that has welcomed many
first-time SIG Node contributors, we'd like to share some of the details and
accomplishments over the past two years,
helping to demystify our inner workings and celebrate the hard work
of all our dedicated contributors!-->
&lt;p>SIG ä¸­çš„å­é¡¹ç›®å¦‚ä½•å½¢æˆã€è¿è¡Œå’Œå·¥ä½œå¹¶ä¸æ€»æ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚æ¯ä¸€ä¸ªéƒ½æ˜¯å…¶èƒŒåçš„ SIG æ‰€ç‹¬æœ‰çš„ï¼Œ
å¹¶æ ¹æ®è¯¥å°ç»„æ‰“ç®—æ”¯æŒçš„é¡¹ç›®é‡èº«å®šåˆ¶ã€‚ä½œä¸ºä¸€ä¸ªæ¬¢è¿äº†è®¸å¤šç¬¬ä¸€æ¬¡ SIG Node è´¡çŒ®è€…çš„å›¢é˜Ÿï¼Œ
æˆ‘ä»¬æƒ³åˆ†äº«è¿‡å»ä¸¤å¹´çš„ä¸€äº›ç»†èŠ‚å’Œæˆå°±ï¼Œå¸®åŠ©æ­å¼€æˆ‘ä»¬å†…éƒ¨å·¥ä½œçš„ç¥ç§˜é¢çº±ï¼Œå¹¶åº†ç¥æˆ‘ä»¬æ‰€æœ‰ä¸“æ³¨è´¡çŒ®è€…çš„è¾›å‹¤å·¥ä½œï¼&lt;/p>
&lt;!--## Timeline-->
&lt;h2 id="æ—¶é—´çº¿">æ—¶é—´çº¿&lt;/h2>
&lt;!--***May 2020.*** SIG Node CI group was formed on May 11, 2020, with more than
[30 volunteers](https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib)
signed up, to improve SIG Node CI signal and overall observability.
Victor Pickard focused on getting
[testgrid jobs](https://testgrid.k8s.io/sig-node) passing
when Ning Liao suggested forming a group around this effort and came up with
the [original group charter document](https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf).
The SIG Node chairs sponsored group creation with Victor as a subproject lead.
Sergey Kanzhelev joined Victor shortly after as a co-lead.-->
&lt;p>&lt;em>&lt;strong>2020 å¹´ 5 æœˆ&lt;/strong>&lt;/em> SIG Node CI ç»„äº 2020 å¹´ 5 æœˆ 11 æ—¥æˆç«‹ï¼Œè¶…è¿‡
&lt;a href="https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib">30 åå¿—æ„¿è€…&lt;/a>
æ³¨å†Œï¼Œä»¥æ”¹è¿› SIG Node CI ä¿¡å·å’Œæ•´ä½“å¯è§‚æµ‹æ€§ã€‚
Victor Pickard ä¸“æ³¨äºè®© &lt;a href="https://testgrid.k8s.io/sig-node">testgrid å¯ä»¥è¿è¡Œ&lt;/a> ï¼Œ
å½“æ—¶ Ning Liao å»ºè®®å›´ç»•è¿™é¡¹å·¥ä½œç»„å»ºä¸€ä¸ªå°ç»„ï¼Œå¹¶æå‡º
&lt;a href="https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf">æœ€åˆçš„å°ç»„ç« ç¨‹æ–‡ä»¶&lt;/a> ã€‚
SIG Node èµåŠ©æˆç«‹ä»¥ Victor ä½œä¸ºå­é¡¹ç›®è´Ÿè´£äººçš„å°ç»„ã€‚Sergey Kanzhelev ä¸ä¹…åå°±åŠ å…¥ Victorï¼Œæ‹…ä»»è”åˆé¢†å¯¼äººã€‚&lt;/p>
&lt;!--At the kick-off meeting, we discussed which tests to concentrate on fixing first
and discussed merge-blocking and release-blocking tests, many of which were failing due
to infrastructure issues or buggy test code.-->
&lt;p>åœ¨å¯åŠ¨ä¼šè®®ä¸Šï¼Œæˆ‘ä»¬è®¨è®ºäº†åº”è¯¥é¦–å…ˆé›†ä¸­ç²¾åŠ›ä¿®å¤å“ªäº›æµ‹è¯•ï¼Œå¹¶è®¨è®ºäº†é˜»å¡åˆå¹¶å’Œé˜»å¡å‘å¸ƒçš„æµ‹è¯•ï¼Œ
å…¶ä¸­è®¸å¤šæµ‹è¯•ç”±äºåŸºç¡€è®¾æ–½é—®é¢˜æˆ–é”™è¯¯çš„æµ‹è¯•ä»£ç è€Œå¤±è´¥ã€‚&lt;/p>
&lt;!--The subproject launched weekly hour-long meetings to discuss ongoing work
discussion and triage.-->
&lt;p>è¯¥å­é¡¹ç›®æ¯å‘¨å¬å¼€ä¸€å°æ—¶çš„ä¼šè®®ï¼Œè®¨è®ºæ­£åœ¨è¿›è¡Œçš„å·¥ä½œä¼šè°ˆå’Œåˆ†ç±»ã€‚&lt;/p>
&lt;!--***June 2020.*** Morgan Bauer, Karan Goel, and Jorge Alarcon Ochoa were
recognized as reviewers for the SIG Node CI group for their contributions,
helping significantly with the early stages of the subproject.
David Porter and Roy Yang also joined the SIG test failures GitHub team.-->
&lt;p>&lt;em>&lt;strong>2020 å¹´ 6 æœˆ&lt;/strong>&lt;/em> Morgan Bauer ã€ Karan Goel å’Œ Jorge Alarcon Ochoa
å› å…¶è´¡çŒ®è€Œè¢«å…¬è®¤ä¸º SIG Node CI å°ç»„çš„è¯„å®¡å‘˜ï¼Œä¸ºè¯¥å­é¡¹ç›®çš„æ—©æœŸé˜¶æ®µæä¾›äº†é‡è¦å¸®åŠ©ã€‚
David Porter å’Œ Roy Yang ä¹ŸåŠ å…¥äº† SIG æ£€æµ‹å¤±è´¥çš„ GitHub æµ‹è¯•å›¢é˜Ÿã€‚&lt;/p>
&lt;!--***August 2020.*** All merge-blocking and release-blocking tests were passing,
with some flakes. However, only 42% of all SIG Node test jobs were green, as there
were many flakes and failing tests.-->
&lt;p>&lt;em>&lt;strong>2020 å¹´ 8 æœˆ&lt;/strong>&lt;/em> æ‰€æœ‰çš„é˜»å¡åˆå¹¶å’Œé˜»å¡å‘å¸ƒçš„æµ‹è¯•éƒ½é€šè¿‡äº†ï¼Œä¼´æœ‰ä¸€äº›é€»è¾‘é—®é¢˜ã€‚
ç„¶è€Œï¼Œåªæœ‰ 42% çš„ SIG Node æµ‹è¯•ä½œä¸šæ˜¯ç»¿è‰²çš„ï¼Œ
å› ä¸ºæœ‰è®¸å¤šé€»è¾‘é”™è¯¯å’Œå¤±è´¥çš„æµ‹è¯•ã€‚&lt;/p>
&lt;!--***October 2020.*** Amim Knabben becomes a Kubernetes org member for his
contributions to the subproject.-->
&lt;p>&lt;em>&lt;strong>2020 å¹´ 10 æœˆ&lt;/strong>&lt;/em> Amim Knabben å› å¯¹å­é¡¹ç›®çš„è´¡çŒ®æˆä¸º Kubernetes ç»„ç»‡æˆå‘˜ã€‚&lt;/p>
&lt;!--***January 2021.*** With healthy presubmit and critical periodic jobs passing,
the subproject discussed its goal for cleaning up the rest of periodic tests
and ensuring they passed without flakes.-->
&lt;p>&lt;em>&lt;strong>2021 å¹´ 1 æœˆ&lt;/strong>&lt;/em> éšç€å¥å…¨çš„é¢„æäº¤å’Œå…³é”®å®šæœŸå·¥ä½œçš„é€šè¿‡ï¼Œå­é¡¹ç›®è®¨è®ºäº†æ¸…ç†å…¶ä½™å®šæœŸæµ‹è¯•å¹¶ç¡®ä¿å…¶é¡ºåˆ©é€šè¿‡çš„ç›®æ ‡ã€‚&lt;/p>
&lt;!--Elana Hashman joined the subproject, stepping up to help lead it after
Victor's departure.-->
&lt;p>Elana Hashman åŠ å…¥äº†è¿™ä¸ªå­é¡¹ç›®ï¼Œåœ¨ Victor ç¦»å¼€åå¸®åŠ©é¢†å¯¼è¯¥é¡¹ç›®ã€‚&lt;/p>
&lt;!--***February 2021.*** Artyom Lukianov becomes a Kubernetes org member for his
contributions to the subproject.-->
&lt;p>&lt;em>&lt;strong>2021 å¹´ 2 æœˆ&lt;/strong>&lt;/em> Artyom Lukianov å› å…¶å¯¹å­é¡¹ç›®çš„è´¡çŒ®æˆä¸º Kubernetes ç»„ç»‡æˆå‘˜ã€‚&lt;/p>
&lt;!--***August 2021.*** After SIG Node successfully ran a [bug scrub](https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ)
to clean up its bug backlog, the scope of the meeting was extended to
include bug triage to increase overall reliability, anticipating issues
before they affect the CI signal.-->
&lt;p>&lt;em>&lt;strong>2021 å¹´ 8 æœˆ&lt;/strong>&lt;/em> åœ¨ SIG Node æˆåŠŸè¿è¡Œ &lt;a href="https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ">bug scrub&lt;/a>
ä»¥æ¸…ç†å…¶ç´¯ç§¯çš„ç¼ºé™·ä¹‹åï¼Œä¼šè®®çš„èŒƒå›´æ‰©å¤§åˆ°åŒ…æ‹¬ç¼ºé™·åˆ†ç±»ä»¥æé«˜æ•´ä½“å¯é æ€§ï¼Œ
åœ¨é—®é¢˜å½±å“ CI ä¿¡å·ä¹‹å‰é¢„æµ‹é—®é¢˜ã€‚&lt;/p>
&lt;!--Subproject leads Elana Hashman and Sergey Kanzhelev are both recognized as
approvers on all node test code, supported by SIG Node and SIG Testing.-->
&lt;p>å­é¡¹ç›®è´Ÿè´£äºº Elana Hashman å’Œ Sergey Kanzhelev éƒ½è¢«è®¤ä¸ºæ˜¯æ‰€æœ‰èŠ‚ç‚¹æµ‹è¯•ä»£ç çš„å®¡æ‰¹äººï¼Œç”± SIG node å’Œ SIG Testing æ”¯æŒã€‚&lt;/p>
&lt;!--***September 2021.*** After significant deflaking progress with serial tests in
the 1.22 release spearheaded by Francesco Romani, the subproject set a goal
for getting the serial job fully passing by the 1.23 release date.-->
&lt;p>&lt;em>&lt;strong>2021 å¹´ 9 æœˆ&lt;/strong>&lt;/em> åœ¨ Francesco Romani ç‰µå¤´çš„ 1.22 ç‰ˆæœ¬ç³»åˆ—æµ‹è¯•å–å¾—é‡å¤§è¿›å±•åï¼Œ
è¯¥å­é¡¹ç›®è®¾å®šäº†ä¸€ä¸ªç›®æ ‡ï¼Œå³åœ¨ 1.23 å‘å¸ƒæ—¥æœŸä¹‹å‰è®©ä¸²è¡Œä»»åŠ¡å®Œå…¨é€šè¿‡ã€‚&lt;/p>
&lt;!--Mike Miranda becomes a Kubernetes org member for his contributions
to the subproject.-->
&lt;p>Mike Miranda å› å…¶å¯¹å­é¡¹ç›®çš„è´¡çŒ®æˆä¸º Kubernetes ç»„ç»‡æˆå‘˜ã€‚&lt;/p>
&lt;!--***November 2021.*** Throughout 2021, SIG Node had no merge or
release-blocking test failures. Many flaky tests from past releases are removed
from release-blocking dashboards as they had been fully cleaned up.-->
&lt;p>&lt;em>&lt;strong>2021 å¹´ 11 æœˆ&lt;/strong>&lt;/em> åœ¨æ•´ä¸ª 2021 å¹´ï¼Œ SIG Node æ²¡æœ‰åˆå¹¶æˆ–å‘å¸ƒçš„æµ‹è¯•å¤±è´¥ã€‚
è¿‡å»ç‰ˆæœ¬ä¸­çš„è®¸å¤šå¤æ€ªæµ‹è¯•éƒ½å·²ä»é˜»æ­¢å‘å¸ƒçš„ä»ªè¡¨æ¿ä¸­åˆ é™¤ï¼Œå› ä¸ºå®ƒä»¬å·²è¢«å®Œå…¨æ¸…ç†ã€‚&lt;/p>
&lt;!--Danielle Lancashire was recognized as a reviewer for SIG Node's subgroup, test code.-->
&lt;p>Danielle Lancashire è¢«å…¬è®¤ä¸º SIG Node å­ç»„æµ‹è¯•ä»£ç çš„è¯„å®¡å‘˜ã€‚&lt;/p>
&lt;!--The final node serial tests were completely fixed. The serial tests consist of
many disruptive and slow tests which tend to be flakey and are hard
to troubleshoot. By the 1.23 release freeze, the last serial tests were
fixed and the job was passing without flakes.-->
&lt;p>æœ€ç»ˆèŠ‚ç‚¹ç³»åˆ—æµ‹è¯•å·²å®Œå…¨ä¿®å¤ã€‚ç³»åˆ—æµ‹è¯•ç”±è®¸å¤šä¸­æ–­æ€§å’Œç¼“æ…¢çš„æµ‹è¯•ç»„æˆï¼Œè¿™äº›æµ‹è¯•å¾€å¾€æ˜¯ç¢ç‰‡åŒ–çš„ï¼Œå¾ˆéš¾æ’é™¤æ•…éšœã€‚
åˆ° 1.23 ç‰ˆæœ¬å†»ç»“æ—¶ï¼Œæœ€åä¸€æ¬¡ç³»åˆ—æµ‹è¯•å·²ä¿®å¤ï¼Œä½œä¸šé¡ºåˆ©é€šè¿‡ã€‚&lt;/p>
&lt;!--[![Slack announcement that Serial tests are green](serial-tests-green.png)](https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900)-->
&lt;p>&lt;a href="https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900">&lt;img src="serial-tests-green.png" alt="å®£å¸ƒç³»åˆ—æµ‹è¯•ä¸ºç»¿è‰²">&lt;/a>&lt;/p>
&lt;!--The 1.23 release got a special shout out for the tests quality and CI signal.
The SIG Node CI subproject was proud to have helped contribute to such
a high-quality release, in part due to our efforts in identifying
and fixing flakes in Node and beyond.-->
&lt;p>1.23 ç‰ˆæœ¬åœ¨æµ‹è¯•è´¨é‡å’Œ CI ä¿¡å·æ–¹é¢å¾—åˆ°äº†ç‰¹åˆ«çš„å…³æ³¨ã€‚SIG Node CI å­é¡¹ç›®å¾ˆè‡ªè±ªèƒ½å¤Ÿä¸ºè¿™æ ·ä¸€ä¸ªé«˜è´¨é‡çš„å‘å¸ƒåšå‡ºè´¡çŒ®ï¼Œ
éƒ¨åˆ†åŸå› æ˜¯æˆ‘ä»¬åœ¨è¯†åˆ«å’Œä¿®å¤èŠ‚ç‚¹å†…å¤–çš„ç¢ç‰‡æ–¹é¢æ‰€åšçš„åŠªåŠ›ã€‚&lt;/p>
&lt;!--[![Slack shoutout that release was mostly green](release-mostly-green.png)](https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200)-->
&lt;p>&lt;a href="https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200">&lt;img src="release-mostly-green.png" alt="Slack å¤§å£°å®£å¸ƒå‘å¸ƒçš„ç‰ˆæœ¬å¤§å¤šæ˜¯ç»¿è‰²çš„">&lt;/a>&lt;/p>
&lt;!--***December 2021.*** An estimated 90% of test jobs were passing at the time of
the 1.23 release (up from 42% in August 2020).-->
&lt;p>&lt;em>&lt;strong>2021 å¹´ 12 æœˆ&lt;/strong>&lt;/em> åœ¨ 1.23 ç‰ˆæœ¬å‘å¸ƒæ—¶ï¼Œä¼°è®¡æœ‰ 90% çš„æµ‹è¯•å·¥ä½œé€šè¿‡äº†æµ‹è¯•ï¼ˆ2020 å¹´ 8 æœˆä¸º 42%ï¼‰ã€‚&lt;/p>
&lt;!--Dockershim code was removed from Kubernetes. This affected nearly half of SIG Node's
test jobs and the SIG Node CI subproject reacted quickly and retargeted all the
tests. SIG Node was the first SIG to complete test migrations off dockershim,
providing examples for other affected SIGs. The vast majority of new jobs passed
at the time of introduction without further fixes required. The [effort of
removing dockershim](https://k8s.io/dockershim)) from Kubernetes is ongoing.
There are still some wrinkles from the dockershim removal as we uncover more
dependencies on dockershim, but we plan to stabilize all test jobs
by the 1.24 release.-->
&lt;p>Dockershim ä»£ç å·²ä» Kubernetes ä¸­åˆ é™¤ã€‚è¿™å½±å“äº† SIG Node è¿‘ä¸€åŠçš„æµ‹è¯•ä½œä¸šï¼Œ
SIG Node CI å­é¡¹ç›®ååº”è¿…é€Ÿï¼Œå¹¶é‡æ–°ç¡®å®šäº†æ‰€æœ‰æµ‹è¯•çš„ç›®æ ‡ã€‚
SIG Node æ˜¯ç¬¬ä¸€ä¸ªå®Œæˆ dockershim å¤–æµ‹è¯•è¿ç§»çš„ SIG ï¼Œä¸ºå…¶ä»–å—å½±å“çš„ SIG æä¾›äº†ç¤ºä¾‹ã€‚
ç»å¤§å¤šæ•°æ–°å·¥ä½œåœ¨å¼•å…¥æ—¶éƒ½å·²é€šè¿‡ï¼Œæ— éœ€è¿›ä¸€æ­¥ä¿®å¤ã€‚
ä» Kubernetes ä¸­&lt;a href="https://k8s.io/dockershim">å°† dockershim é™¤åçš„å·¥ä½œ&lt;/a> æ­£åœ¨è¿›è¡Œä¸­ã€‚
éšç€æˆ‘ä»¬å‘ç° dockershim å¯¹ dockershim çš„ä¾èµ–æ€§è¶Šæ¥è¶Šå¤§ï¼Œdockershim çš„åˆ é™¤ä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œ
ä½†æˆ‘ä»¬è®¡åˆ’åœ¨ 1.24 ç‰ˆæœ¬ä¹‹å‰ç¡®ä¿æ‰€æœ‰æµ‹è¯•ä»»åŠ¡ç¨³å®šã€‚&lt;/p>
&lt;!--## Statistics-->
&lt;h2 id="ç»Ÿè®¡æ•°æ®">ç»Ÿè®¡æ•°æ®&lt;/h2>
&lt;!--Our regular meeting attendees and subproject participants for the past few months:-->
&lt;p>æˆ‘ä»¬è¿‡å»å‡ ä¸ªæœˆçš„å®šæœŸä¼šè®®ä¸ä¼šè€…å’Œå­é¡¹ç›®å‚ä¸è€…ï¼š&lt;/p>
&lt;ul>
&lt;li>Aditi Sharma&lt;/li>
&lt;li>Artyom Lukianov&lt;/li>
&lt;li>Arnaud Meukam&lt;/li>
&lt;li>Danielle Lancashire&lt;/li>
&lt;li>David Porter&lt;/li>
&lt;li>Davanum Srinivas&lt;/li>
&lt;li>Elana Hashman&lt;/li>
&lt;li>Francesco Romani&lt;/li>
&lt;li>Matthias Bertschy&lt;/li>
&lt;li>Mike Miranda&lt;/li>
&lt;li>Paco Xu&lt;/li>
&lt;li>Peter Hunt&lt;/li>
&lt;li>Ruiwen Zhao&lt;/li>
&lt;li>Ryan Phillips&lt;/li>
&lt;li>Sergey Kanzhelev&lt;/li>
&lt;li>Skyler Clark&lt;/li>
&lt;li>Swati Sehgal&lt;/li>
&lt;li>Wenjun Wu&lt;/li>
&lt;/ul>
&lt;!--The [kubernetes/test-infra](https://github.com/kubernetes/test-infra/) source code repository contains test definitions. The number of
Node PRs just in that repository:
- 2020 PRs (since May): [183](https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+)
- 2021 PRs: [264](https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+)-->
&lt;p>&lt;a href="https://github.com/kubernetes/test-infra/">kubernetes/test-infra&lt;/a> æºä»£ç å­˜å‚¨åº“åŒ…å«æµ‹è¯•å®šä¹‰ã€‚è¯¥å­˜å‚¨åº“ä¸­çš„èŠ‚ç‚¹ PR æ•°ï¼š&lt;/p>
&lt;ul>
&lt;li>2020 å¹´ PRï¼ˆè‡ª 5 æœˆèµ·ï¼‰ï¼š&lt;a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+">183&lt;/a>&lt;/li>
&lt;li>2021 å¹´ PRï¼š&lt;a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+">264&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--Triaged issues and PRs on CI board (including triaging away from the subgroup scope):
- 2020 (since May)ï¼š[132](https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31)
- 2021: [532](httpsï¼š//github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+)-->
&lt;p>CI å§”å‘˜ä¼šä¸Šçš„é—®é¢˜å’Œ PRs åˆ†ç±»ï¼ˆåŒ…æ‹¬å­ç»„èŒƒå›´ä¹‹å¤–çš„åˆ†ç±»ï¼‰ï¼š&lt;/p>
&lt;ul>
&lt;li>2020 å¹´ï¼ˆè‡ª 5 æœˆèµ·ï¼‰ï¼š&lt;a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31">132&lt;/a>&lt;/li>
&lt;li>2021 å¹´ï¼š&lt;a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+">532&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--## Future-->
&lt;h2 id="æœªæ¥">æœªæ¥&lt;/h2>
&lt;!--Just "keeping the lights on" is a bold task and we are committed to improving this experience.
We are working to simplify the triage and review processes for SIG Node.
Specifically, we are working on better test organization, naming,
and tracking:-->
&lt;p>åªæ˜¯â€œä¿æŒç¯äº®â€æ˜¯ä¸€é¡¹å¤§èƒ†çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬è‡´åŠ›äºæ”¹å–„è¿™ç§ä½“éªŒã€‚
æˆ‘ä»¬æ­£åœ¨åŠªåŠ›ç®€åŒ– SIG Node çš„åˆ†ç±»å’Œå®¡æŸ¥æµç¨‹ã€‚&lt;/p>
&lt;p>å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ­£åœ¨è‡´åŠ›äºæ›´å¥½çš„æµ‹è¯•ç»„ç»‡ã€å‘½åå’Œè·Ÿè¸ªï¼š&lt;/p>
&lt;!-- - https://github.com/kubernetes/enhancements/pull/3042
- https://github.com/kubernetes/test-infra/issues/24641
- [Kubernetes SIG-Node CI Testgrid Tracker](https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0)-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/pull/3042">https://github.com/kubernetes/enhancements/pull/3042&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/issues/24641">https://github.com/kubernetes/test-infra/issues/24641&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0">Kubernetes SIG Node CI æµ‹è¯•ç½‘æ ¼è·Ÿè¸ªå™¨&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--We are also constantly making progress on improved tests debuggability and de-flaking.
If any of this interests you, we'd love for you to join us!
There's plenty to learn in debugging test failures, and it will help you gain
familiarity with the code that SIG Node maintains.-->
&lt;p>æˆ‘ä»¬è¿˜åœ¨æ”¹è¿›æµ‹è¯•çš„å¯è°ƒè¯•æ€§å’Œå»å‰¥è½æ–¹é¢ä¸æ–­å–å¾—è¿›å±•ã€‚&lt;/p>
&lt;p>å¦‚æœä½ å¯¹æ­¤æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬å¾ˆä¹æ„æ‚¨èƒ½åŠ å…¥æˆ‘ä»¬ï¼
åœ¨è°ƒè¯•æµ‹è¯•å¤±è´¥ä¸­æœ‰å¾ˆå¤šä¸œè¥¿éœ€è¦å­¦ä¹ ï¼Œå®ƒå°†å¸®åŠ©ä½ ç†Ÿæ‚‰ SIG Node ç»´æŠ¤çš„ä»£ç ã€‚&lt;/p>
&lt;!--You can always find information about the group on the
[SIG Node](https://github.com/kubernetes/community/tree/master/sig-node) page.
We give group updates at our maintainer track sessions, such as
[KubeCon + CloudNativeCon Europe 2021](https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google) å’Œ
[KubeCon + CloudNative North America 2021](https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;w=100%&amp;sidebar=yes&amp;bg=no)ã€‚
Join us in our mission to keep the kubelet and other SIG Node components reliable and ensure smooth and uneventful releases!-->
&lt;p>ä½ å¯ä»¥åœ¨ &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> é¡µé¢ä¸Šæ‰¾åˆ°æœ‰å…³è¯¥ç»„çš„ä¿¡æ¯ã€‚
æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„ç»´æŠ¤è€…è½¨é“ä¼šè®®ä¸Šæä¾›ç»„æ›´æ–°ï¼Œä¾‹å¦‚ï¼š
&lt;a href="https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google">KubeCon + CloudNativeCon Europe 2021&lt;/a> å’Œ
&lt;a href="https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;amp;w=100%25&amp;amp;sidebar=yes&amp;amp;bg=no">KubeCon + CloudNative North America 2021&lt;/a>ã€‚
åŠ å…¥æˆ‘ä»¬çš„ä½¿å‘½ï¼Œä¿æŒ kubelet å’Œå…¶ä»– SIG Node ç»„ä»¶çš„å¯é æ€§ï¼Œç¡®ä¿é¡ºé¡ºåˆ©åˆ©å‘å¸ƒï¼&lt;/p></description></item><item><title>Blog: ç¡®ä¿å‡†å…¥æ§åˆ¶å™¨çš„å®‰å…¨</title><link>https://kubernetes.io/zh/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</link><pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</guid><description>
&lt;!--
layout: blog
title: "Securing Admission Controllers"
date: 2022-01-19
slug: secure-your-admission-controllers-and-webhooks
-->
&lt;!--
**Author:** Rory McCune (Aqua Security)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> Rory McCune (Aqua Security)&lt;/p>
&lt;!--
[Admission control](/docs/reference/access-authn-authz/admission-controllers/) is a key part of Kubernetes security, alongside authentication and authorization.
Webhook admission controllers are extensively used to help improve the security of Kubernetes clusters in a variety of ways including restricting the privileges of workloads and ensuring that images deployed to the cluster meet organizationâ€™s security requirements.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/">å‡†å…¥æ§åˆ¶&lt;/a>å’Œè®¤è¯ã€æˆæƒéƒ½æ˜¯ Kubernetes å®‰å…¨æ€§çš„å…³é”®éƒ¨åˆ†ã€‚
Webhook å‡†å…¥æ§åˆ¶å™¨è¢«å¹¿æ³›ç”¨äºä»¥å¤šç§æ–¹å¼å¸®åŠ©æé«˜ Kubernetes é›†ç¾¤çš„å®‰å…¨æ€§ï¼Œ
åŒ…æ‹¬é™åˆ¶å·¥ä½œè´Ÿè½½æƒé™å’Œç¡®ä¿éƒ¨ç½²åˆ°é›†ç¾¤çš„é•œåƒæ»¡è¶³ç»„ç»‡å®‰å…¨è¦æ±‚ã€‚&lt;/p>
&lt;!--
However, as with any additional component added to a cluster, security risks can present themselves.
A security risk example is if the deployment and management of the admission controller are not handled correctly. To help admission controller users and designers manage these risks appropriately,
the [security documentation](https://github.com/kubernetes/community/tree/master/sig-security#security-docs) subgroup of SIG Security has spent some time developing a [threat model for admission controllers](https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control).
This threat model looks at likely risks which may arise from the incorrect use of admission controllers, which could allow security policies to be bypassed, or even allow an attacker to get unauthorised access to the cluster.
-->
&lt;p>ç„¶è€Œï¼Œä¸æ·»åŠ åˆ°é›†ç¾¤ä¸­çš„ä»»ä½•å…¶ä»–ç»„ä»¶ä¸€æ ·ï¼Œå®‰å…¨é£é™©ä¹Ÿä¼šéšä¹‹å‡ºç°ã€‚
ä¸€ä¸ªå®‰å…¨é£é™©ç¤ºä¾‹æ˜¯æ²¡æœ‰æ­£ç¡®å¤„ç†å‡†å…¥æ§åˆ¶å™¨çš„éƒ¨ç½²å’Œç®¡ç†ã€‚
ä¸ºäº†å¸®åŠ©å‡†å…¥æ§åˆ¶å™¨ç”¨æˆ·å’Œè®¾è®¡äººå‘˜é€‚å½“åœ°ç®¡ç†è¿™äº›é£é™©ï¼Œ
SIG Security çš„&lt;a href="https://github.com/kubernetes/community/tree/master/sig-security#security-docs">å®‰å…¨æ–‡æ¡£&lt;/a>å°ç»„
èŠ±è´¹äº†ä¸€äº›æ—¶é—´æ¥å¼€å‘ä¸€ä¸ª&lt;a href="https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control">å‡†å…¥æ§åˆ¶å™¨å¨èƒæ¨¡å‹&lt;/a>ã€‚
è¿™ç§å¨èƒæ¨¡å‹ç€çœ¼äºç”±äºä¸æ­£ç¡®ä½¿ç”¨å‡†å…¥æ§åˆ¶å™¨è€Œäº§ç”Ÿçš„å¯èƒ½çš„é£é™©ï¼Œå¯èƒ½å…è®¸ç»•è¿‡å®‰å…¨ç­–ç•¥ï¼Œç”šè‡³å…è®¸æ”»å‡»è€…æœªç»æˆæƒè®¿é—®é›†ç¾¤ã€‚&lt;/p>
&lt;!--
From the threat model, we developed a set of security best practices that should be adopted to ensure that cluster operators can get the security benefits of admission controllers whilst avoiding any risks from using them.
-->
&lt;p>åŸºäºè¿™ä¸ªå¨èƒæ¨¡å‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—å®‰å…¨æœ€ä½³å®è·µã€‚
ä½ åº”è¯¥é‡‡ç”¨è¿™äº›å®è·µæ¥ç¡®ä¿é›†ç¾¤æ“ä½œå‘˜å¯ä»¥è·å¾—å‡†å…¥æ§åˆ¶å™¨å¸¦æ¥çš„å®‰å…¨ä¼˜åŠ¿ï¼ŒåŒæ—¶é¿å…ä½¿ç”¨å®ƒä»¬å¸¦æ¥çš„ä»»ä½•é£é™©ã€‚&lt;/p>
&lt;!--
## Admission controllers and good practices for security
-->
&lt;h2 id="å‡†å…¥æ§åˆ¶å™¨å’Œå®‰å…¨çš„è‰¯å¥½åšæ³•">å‡†å…¥æ§åˆ¶å™¨å’Œå®‰å…¨çš„è‰¯å¥½åšæ³•&lt;/h2>
&lt;!--
From the threat model, a couple of themes emerged around how to ensure the security of admission controllers.
-->
&lt;p>åŸºäºè¿™ä¸ªå¨èƒæ¨¡å‹ï¼Œå›´ç»•ç€å¦‚ä½•ç¡®ä¿å‡†å…¥æ§åˆ¶å™¨çš„å®‰å…¨æ€§å‡ºç°äº†å‡ ä¸ªä¸»é¢˜ã€‚&lt;/p>
&lt;!--
### Secure webhook configuration
-->
&lt;h3 id="å®‰å…¨çš„-webhook-é…ç½®">å®‰å…¨çš„ webhook é…ç½®&lt;/h3>
&lt;!--
Itâ€™s important to ensure that any security component in a cluster is well configured and admission controllers are no different here. There are a couple of security best practices to consider when using admission controllers
-->
&lt;p>ç¡®ä¿é›†ç¾¤ä¸­çš„ä»»ä½•å®‰å…¨ç»„ä»¶éƒ½é…ç½®è‰¯å¥½æ˜¯å¾ˆé‡è¦çš„ï¼Œåœ¨è¿™é‡Œå‡†å…¥æ§åˆ¶å™¨ä¹Ÿå¹¶ä¸ä¾‹å¤–ã€‚
ä½¿ç”¨å‡†å…¥æ§åˆ¶å™¨æ—¶éœ€è¦è€ƒè™‘å‡ ä¸ªå®‰å…¨æœ€ä½³å®è·µï¼š&lt;/p>
&lt;!--
* **Correctly configured TLS for all webhook traffic**. Communications between the API server and the admission controller webhook should be authenticated and encrypted to ensure that attackers who may be in a network position to view or modify this traffic cannot do so. To achieve this access the API server and webhook must be using certificates from a trusted certificate authority so that they can validate their mutual identities
-->
&lt;ul>
&lt;li>&lt;strong>ä¸ºæ‰€æœ‰ webhook æµé‡æ­£ç¡®é…ç½®äº† TLS&lt;/strong>ã€‚
API æœåŠ¡å™¨å’Œå‡†å…¥æ§åˆ¶å™¨ webhook ä¹‹é—´çš„é€šä¿¡åº”è¯¥ç»è¿‡èº«ä»½éªŒè¯å’ŒåŠ å¯†ï¼Œä»¥ç¡®ä¿å¤„äºç½‘ç»œä¸­æŸ¥çœ‹æˆ–ä¿®æ”¹æ­¤æµé‡çš„æ”»å‡»è€…æ— æ³•æŸ¥çœ‹æˆ–ä¿®æ”¹ã€‚
è¦å®ç°æ­¤è®¿é—®ï¼ŒAPI æœåŠ¡å™¨å’Œ webhook å¿…é¡»ä½¿ç”¨æ¥è‡ªå—ä¿¡ä»»çš„è¯ä¹¦é¢å‘æœºæ„çš„è¯ä¹¦ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥éªŒè¯ç›¸äº’çš„èº«ä»½ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **Only authenticated access allowed**. If an attacker can send an admission controller large numbers of requests, they may be able to overwhelm the service causing it to fail. Ensuring all access requires strong authentication should mitigate that risk.
-->
&lt;ul>
&lt;li>&lt;strong>åªå…è®¸ç»è¿‡èº«ä»½éªŒè¯çš„è®¿é—®&lt;/strong>ã€‚
å¦‚æœæ”»å‡»è€…å¯ä»¥å‘å‡†å…¥æ§åˆ¶å™¨å‘é€å¤§é‡è¯·æ±‚ï¼Œä»–ä»¬å¯èƒ½ä¼šå‹å®æœåŠ¡å¯¼è‡´å…¶å¤±è´¥ã€‚
ç¡®ä¿æ‰€æœ‰è®¿é—®éƒ½éœ€è¦å¼ºèº«ä»½éªŒè¯å¯ä»¥é™ä½è¿™ç§é£é™©ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **Admission controller fails closed**. This is a security practice that has a tradeoff, so whether a cluster operator wants to configure it will depend on the clusterâ€™s threat model. If an admission controller fails closed, when the API server canâ€™t get a response from it, all deployments will fail. This stops attackers bypassing the admission controller by disabling it, but, can disrupt the clusterâ€™s operation. As clusters can have multiple webhooks, one approach to hit a middle ground might be to have critical controls on a fail closed setups and less critical controls allowed to fail open.
-->
&lt;ul>
&lt;li>&lt;strong>å‡†å…¥æ§åˆ¶å™¨å…³é—­å¤±è´¥&lt;/strong>ã€‚
è¿™æ˜¯ä¸€ç§éœ€è¦æƒè¡¡çš„å®‰å…¨å®è·µï¼Œé›†ç¾¤æ“ä½œå‘˜æ˜¯å¦è¦å¯¹å…¶è¿›è¡Œé…ç½®å–å†³äºé›†ç¾¤çš„å¨èƒæ¨¡å‹ã€‚
å¦‚æœä¸€ä¸ªå‡†å…¥æ§åˆ¶å™¨å…³é—­å¤±è´¥ï¼Œå½“ API æœåŠ¡å™¨æ— æ³•ä»å®ƒå¾—åˆ°å“åº”æ—¶ï¼Œæ‰€æœ‰çš„éƒ¨ç½²éƒ½ä¼šå¤±è´¥ã€‚
è¿™å¯ä»¥é˜»æ­¢æ”»å‡»è€…é€šè¿‡ç¦ç”¨å‡†å…¥æ§åˆ¶å™¨ç»•è¿‡å‡†å…¥æ§åˆ¶å™¨ï¼Œä½†å¯èƒ½ä¼šç ´åé›†ç¾¤çš„è¿è¡Œã€‚
ç”±äºé›†ç¾¤å¯ä»¥æœ‰å¤šä¸ª webhookï¼Œå› æ­¤ä¸€ç§æŠ˜ä¸­çš„æ–¹æ³•æ˜¯å¯¹å…³é”®æ§åˆ¶å…è®¸æ•…éšœå…³é—­ï¼Œ
å¹¶å…è®¸ä¸å¤ªå…³é”®çš„æ§åˆ¶è¿›è¡Œæ•…éšœæ‰“å¼€ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **Regular reviews of webhook configuration**. Configuration mistakes can lead to security issues, so itâ€™s important that the admission controller webhook configuration is checked to make sure the settings are correct. This kind of review could be done automatically by an Infrastructure As Code scanner or manually by an administrator.
-->
&lt;ul>
&lt;li>&lt;strong>å®šæœŸå®¡æŸ¥ webhook é…ç½®&lt;/strong>ã€‚
é…ç½®é”™è¯¯å¯èƒ½å¯¼è‡´å®‰å…¨é—®é¢˜ï¼Œå› æ­¤æ£€æŸ¥å‡†å…¥æ§åˆ¶å™¨ webhook é…ç½®ä»¥ç¡®ä¿è®¾ç½®æ­£ç¡®éå¸¸é‡è¦ã€‚
è¿™ç§å®¡æŸ¥å¯ä»¥ç”±åŸºç¡€è®¾æ–½å³ä»£ç æ‰«æç¨‹åºè‡ªåŠ¨å®Œæˆï¼Œä¹Ÿå¯ä»¥ç”±ç®¡ç†å‘˜æ‰‹åŠ¨å®Œæˆã€‚&lt;/li>
&lt;/ul>
&lt;!--
### Secure cluster configuration for admission control
-->
&lt;h3 id="ä¸ºå‡†å…¥æ§åˆ¶ä¿æŠ¤é›†ç¾¤é…ç½®">ä¸ºå‡†å…¥æ§åˆ¶ä¿æŠ¤é›†ç¾¤é…ç½®&lt;/h3>
&lt;!--
In most cases, the admission controller webhook used by a cluster will be installed as a workload in the cluster. As a result, itâ€™s important to ensure that Kubernetes' security features that could impact its operation are well configured.
-->
&lt;p>åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œé›†ç¾¤ä½¿ç”¨çš„å‡†å…¥æ§åˆ¶å™¨ webhook å°†ä½œä¸ºå·¥ä½œè´Ÿè½½å®‰è£…åœ¨é›†ç¾¤ä¸­ã€‚
å› æ­¤ï¼Œç¡®ä¿æ­£ç¡®é…ç½®äº†å¯èƒ½å½±å“å…¶æ“ä½œçš„ Kubernetes å®‰å…¨ç‰¹æ€§éå¸¸é‡è¦ã€‚&lt;/p>
&lt;!--
* **Restrict [RBAC](/docs/reference/access-authn-authz/rbac/) rights**. Any user who has rights which would allow them to modify the configuration of the webhook objects or the workload that the admission controller uses could disrupt its operation. So itâ€™s important to make sure that only cluster administrators have those rights.
-->
&lt;ul>
&lt;li>&lt;strong>é™åˆ¶ &lt;a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/">RBAC&lt;/a> æƒé™&lt;/strong>ã€‚
ä»»ä½•æœ‰æƒä¿®æ”¹ webhook å¯¹è±¡çš„é…ç½®æˆ–å‡†å…¥æ§åˆ¶å™¨ä½¿ç”¨çš„å·¥ä½œè´Ÿè½½çš„ç”¨æˆ·éƒ½å¯ä»¥ç ´åå…¶è¿è¡Œã€‚
å› æ­¤ï¼Œç¡®ä¿åªæœ‰é›†ç¾¤ç®¡ç†å‘˜æ‹¥æœ‰è¿™äº›æƒé™éå¸¸é‡è¦ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **Prevent privileged workloads**. One of the realities of container systems is that if a workload is given certain privileges, it will be possible to break out to the underlying cluster node and impact other containers on that node. Where admission controller services run in the cluster theyâ€™re protecting, itâ€™s important to ensure that any requirement for privileged workloads is carefully reviewed and restricted as much as possible.
-->
&lt;ul>
&lt;li>&lt;strong>é˜²æ­¢ç‰¹æƒå·¥ä½œè´Ÿè½½&lt;/strong>ã€‚
å®¹å™¨ç³»ç»Ÿçš„ä¸€ä¸ªç°å®æ˜¯ï¼Œå¦‚æœå·¥ä½œè´Ÿè½½è¢«èµ‹äºˆæŸäº›ç‰¹æƒï¼Œ
åˆ™æœ‰å¯èƒ½é€ƒé€¸åˆ°ä¸‹å±‚çš„é›†ç¾¤èŠ‚ç‚¹å¹¶å½±å“è¯¥èŠ‚ç‚¹ä¸Šçš„å…¶ä»–å®¹å™¨ã€‚
å¦‚æœå‡†å…¥æ§åˆ¶å™¨æœåŠ¡åœ¨å®ƒä»¬æ‰€ä¿æŠ¤çš„é›†ç¾¤ä¸Šè¿è¡Œï¼Œ
ä¸€å®šè¦ç¡®ä¿å¯¹ç‰¹æƒå·¥ä½œè´Ÿè½½çš„æ‰€æœ‰è¯·æ±‚éƒ½è¦ç»è¿‡ä»”ç»†å®¡æŸ¥å¹¶å°½å¯èƒ½åœ°åŠ ä»¥é™åˆ¶ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **Strictly control external system access**. As a security service in a cluster admission controller systems will have access to sensitive information like credentials. To reduce the risk of this information being sent outside the cluster, [network policies](/docs/concepts/services-networking/network-policies/) should be used to restrict the admission controller services access to external networks.
-->
&lt;ul>
&lt;li>&lt;strong>ä¸¥æ ¼æ§åˆ¶å¤–éƒ¨ç³»ç»Ÿè®¿é—®&lt;/strong>ã€‚
ä½œä¸ºé›†ç¾¤ä¸­çš„å®‰å…¨æœåŠ¡ï¼Œå‡†å…¥æ§åˆ¶å™¨ç³»ç»Ÿå°†æœ‰æƒè®¿é—®æ•æ„Ÿä¿¡æ¯ï¼Œå¦‚å‡­è¯ã€‚
ä¸ºäº†é™ä½æ­¤ä¿¡æ¯è¢«å‘é€åˆ°é›†ç¾¤å¤–çš„é£é™©ï¼Œ
åº”ä½¿ç”¨&lt;a href="https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/">ç½‘ç»œç­–ç•¥&lt;/a>
æ¥é™åˆ¶å‡†å…¥æ§åˆ¶å™¨æœåŠ¡å¯¹å¤–éƒ¨ç½‘ç»œçš„è®¿é—®ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **Each cluster has a dedicated webhook**. Whilst it may be possible to have admission controller webhooks that serve multiple clusters, there is a risk when using that model that an attack on the webhook service would have a larger impact where itâ€™s shared. Also where multiple clusters use an admission controller there will be increased complexity and access requirements, making it harder to secure.
-->
&lt;ul>
&lt;li>&lt;strong>æ¯ä¸ªé›†ç¾¤éƒ½æœ‰ä¸€ä¸ªä¸“ç”¨çš„ webhook&lt;/strong>ã€‚
è™½ç„¶å¯èƒ½è®©å‡†å…¥æ§åˆ¶å™¨ webhook æœåŠ¡äºå¤šä¸ªé›†ç¾¤çš„ï¼Œ
ä½†åœ¨ä½¿ç”¨è¯¥æ¨¡å‹æ—¶å­˜åœ¨å¯¹ webhook æœåŠ¡çš„æ”»å‡»ä¼šå¯¹å…±äº«å®ƒçš„åœ°æ–¹äº§ç”Ÿæ›´å¤§å½±å“çš„é£é™©ã€‚
æ­¤å¤–ï¼Œåœ¨å¤šä¸ªé›†ç¾¤ä½¿ç”¨å‡†å…¥æ§åˆ¶å™¨çš„æƒ…å†µä¸‹ï¼Œå¤æ‚æ€§å’Œè®¿é—®è¦æ±‚ä¹Ÿä¼šå¢åŠ ï¼Œä»è€Œæ›´éš¾ä¿æŠ¤å…¶å®‰å…¨ã€‚&lt;/li>
&lt;/ul>
&lt;!--
### Admission controller rules
-->
&lt;h3 id="å‡†å…¥æ§åˆ¶å™¨è§„åˆ™">å‡†å…¥æ§åˆ¶å™¨è§„åˆ™&lt;/h3>
&lt;!--
A key element of any admission controller used for Kubernetes security is the rulebase it uses. The rules need to be able to accurately meet their goals avoiding false positive and false negative results.
-->
&lt;p>å¯¹äºç”¨äº Kubernetes å®‰å…¨çš„æ‰€æœ‰å‡†å…¥æ§åˆ¶å™¨è€Œè¨€ï¼Œä¸€ä¸ªå…³é”®å…ƒç´ æ˜¯å®ƒä½¿ç”¨çš„è§„åˆ™åº“ã€‚
è§„åˆ™éœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°æ»¡è¶³å…¶ç›®æ ‡ï¼Œé¿å…å‡é˜³æ€§å’Œå‡é˜´æ€§ç»“æœã€‚&lt;/p>
&lt;!--
* **Regularly test and review rules**. Admission controller rules need to be tested to ensure their accuracy. They also need to be regularly reviewed as the Kubernetes API will change with each new version, and rules need to be assessed with each Kubernetes release to understand any changes that may be required to keep them up to date.
-->
&lt;ul>
&lt;li>&lt;strong>å®šæœŸæµ‹è¯•å’Œå®¡æŸ¥è§„åˆ™&lt;/strong>ã€‚
éœ€è¦æµ‹è¯•å‡†å…¥æ§åˆ¶å™¨è§„åˆ™ä»¥ç¡®ä¿å…¶å‡†ç¡®æ€§ã€‚
è¿˜éœ€è¦å®šæœŸå®¡æŸ¥ï¼Œå› ä¸º Kubernetes API ä¼šéšç€æ¯ä¸ªæ–°ç‰ˆæœ¬è€Œæ”¹å˜ï¼Œ
å¹¶ä¸”éœ€è¦åœ¨æ¯ä¸ª Kubernetes ç‰ˆæœ¬ä¸­è¯„ä¼°è§„åˆ™ï¼Œä»¥äº†è§£ä½¿ä»–ä»¬ä¿æŒæœ€æ–°ç‰ˆæœ¬æ‰€éœ€è¦åšçš„ä»»ä½•æ”¹å˜ã€‚&lt;/li>
&lt;/ul></description></item><item><title>Blog: è®¤è¯†æˆ‘ä»¬çš„è´¡çŒ®è€… - äºšå¤ªåœ°åŒºï¼ˆå°åº¦åœ°åŒºï¼‰</title><link>https://kubernetes.io/zh/blog/2022/01/10/meet-our-contributors-india-ep-01/</link><pubDate>Mon, 10 Jan 2022 12:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/01/10/meet-our-contributors-india-ep-01/</guid><description>
&lt;!--
layout: blog
title: "Meet Our Contributors - APAC (India region)"
date: 2022-01-10T12:00:00+0000
slug: meet-our-contributors-india-ep-01
canonicalUrl: https://kubernetes.dev/blog/2022/01/10/meet-our-contributors-india-ep-01/
-->
&lt;!--
**Authors &amp; Interviewers:** [Anubhav Vardhan](https://github.com/anubha-v-ardhan), [Atharva Shinde](https://github.com/Atharva-Shinde), [Avinesh Tripathi](https://github.com/AvineshTripathi), [Debabrata Panigrahi](https://github.com/Debanitrkl), [Kunal Verma](https://github.com/verma-kunal), [Pranshu Srivastava](https://github.com/PranshuSrivastava), [Pritish Samal](https://github.com/CIPHERTron), [Purneswar Prasad](https://github.com/PurneswarPrasad), [Vedant Kakde](https://github.com/vedant-kakde)
-->
&lt;p>&lt;strong>ä½œè€…å’Œé‡‡è®¿è€…ï¼š&lt;/strong> &lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a> ï¼Œ &lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a> ï¼Œ &lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a> ï¼Œ &lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a> ï¼Œ &lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a> ï¼Œ &lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a> ï¼Œ &lt;a href="https://github.com/CIPHERTron">Pritish Samal&lt;/a> ï¼Œ &lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a> ï¼Œ &lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;!--
**Editor:** [Priyanka Saggu](https://psaggu.com)
-->
&lt;p>&lt;strong>ç¼–è¾‘ï¼š&lt;/strong> &lt;a href="https://psaggu.com">Priyanka Saggu&lt;/a>&lt;/p>
&lt;hr>
&lt;!--
Good day, everyone ğŸ‘‹
-->
&lt;p>å¤§å®¶å¥½ ğŸ‘‹&lt;/p>
&lt;!--
Welcome to the first episode of the APAC edition of the "Meet Our Contributors" blog post series.
-->
&lt;p>æ¬¢è¿æ¥åˆ°äºšå¤ªåœ°åŒºçš„â€œè®¤è¯†æˆ‘ä»¬çš„è´¡çŒ®è€…â€åšæ–‡ç³»åˆ—ç¬¬ä¸€æœŸã€‚&lt;/p>
&lt;!--
In this post, we'll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives.
-->
&lt;p>åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨ä»‹ç»æ¥è‡ªå°åº¦åœ°åŒºçš„äº”ä½ä¼˜ç§€è´¡çŒ®è€…ï¼Œä»–ä»¬ä¸€ç›´åœ¨ä»¥å„ç§æ–¹å¼ç§¯æåœ°ä¸ºä¸Šæ¸¸ Kubernetes é¡¹ç›®åšè´¡çŒ®ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¼—å¤šç¤¾åŒºå€¡è®®çš„é¢†å¯¼è€…å’Œç»´æŠ¤è€…ã€‚&lt;/p>
&lt;!--
ğŸ’« *Let's get started, so without further adoâ€¦*
-->
&lt;p>ğŸ’« &lt;em>é—²è¯å°‘è¯´ï¼Œæˆ‘ä»¬å¼€å§‹å§ã€‚&lt;/em>&lt;/p>
&lt;h2 id="arsh-sharma-https-github-com-rinkiyakedad">&lt;a href="https://github.com/RinkiyaKeDad">Arsh Sharma&lt;/a>&lt;/h2>
&lt;!--
Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project.
-->
&lt;p>Arsh ç›®å‰åœ¨ Okteto å…¬å¸ä¸­æ‹…ä»»å¼€å‘è€…ä½“éªŒå·¥ç¨‹å¸ˆèŒåŠ¡ã€‚ä½œä¸ºä¸€åæ–°çš„è´¡çŒ®è€…ï¼Œä»–æ„è¯†åˆ°ä¸€å¯¹ä¸€çš„æŒ‡å¯¼æœºä¼šè®©ä»–åœ¨å¼€å§‹ä¸Šæ¸¸é¡¹ç›®ä¸­å—ç›ŠåŒªæµ…ã€‚&lt;/p>
&lt;!--
He is presently a CI Signal shadow on the Kubernetes 1.23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the [cert-manager](https://github.com/cert-manager/infrastructure) tools development work that is being done under the aegis of SIG Architecture.
-->
&lt;p>ä»–ç›®å‰æ˜¯ Kubernetes 1.23 ç‰ˆæœ¬å›¢é˜Ÿçš„ CI Signal ç»ç†ã€‚ä»–è¿˜è‡´åŠ›äºä¸º SIG Testing å’Œ SIG Docs é¡¹ç›®æä¾›è´¡çŒ®ï¼Œå¹¶ä¸”åœ¨ SIG Architecture é¡¹ç›®ä¸­è´Ÿè´£ &lt;a href="https://github.com/cert-manager/infrastructure">è¯ä¹¦ç®¡ç†å™¨&lt;/a> å·¥å…·çš„å¼€å‘å·¥ä½œã€‚&lt;/p>
&lt;!--
To the newcomers, Arsh helps plan their early contributions sustainably.
-->
&lt;p>å¯¹äºæ–°äººæ¥è¯´ï¼ŒArsh å¸®åŠ©ä»–ä»¬å¯æŒç»­åœ°è®¡åˆ’æ—©æœŸè´¡çŒ®ã€‚&lt;/p>
&lt;!--
> _I would encourage folks to contribute in a way that's sustainable. What I mean by that
> is that it's easy to be very enthusiastic early on and take up more stuff than one can
> actually handle. This can often lead to burnout in later stages. It's much more sustainable
> to work on things iteratively._
-->
&lt;blockquote>
&lt;p>&lt;em>æˆ‘é¼“åŠ±å¤§å®¶ä»¥å¯æŒç»­çš„æ–¹å¼ä¸ºç¤¾åŒºåšè´¡çŒ®ã€‚æˆ‘çš„æ„æ€æ˜¯ï¼Œä¸€ä¸ªäººå¾ˆå®¹æ˜“åœ¨æ—©æœŸçš„æ—¶å€™éå¸¸æœ‰çƒ­æƒ…ï¼Œå¹¶ä¸”æ‰¿æ‹…äº†å¾ˆå¤šè¶…å‡ºä¸ªäººå®é™…èƒ½åŠ›çš„äº‹æƒ…ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´åæœŸçš„å€¦æ€ ã€‚è¿­ä»£åœ°å¤„ç†äº‹æƒ…ä¼šè®©å¤§å®¶å¯¹ç¤¾åŒºçš„è´¡çŒ®å˜å¾—å¯æŒç»­ã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="kunal-kushwaha-https-github-com-kunal-kushwaha">&lt;a href="https://github.com/kunal-kushwaha">Kunal Kushwaha&lt;/a>&lt;/h2>
&lt;!--
Kunal Kushwaha is a core member of the Kubernetes marketing council. He is also a CNCF ambassador and one of the founders of the [CNCF Students Program](https://community.cncf.io/cloud-native-students/).. He also served as a Communications role shadow during the 1.22 release cycle.
-->
&lt;p>Kunal Kushwaha æ˜¯ Kubernetes è¥é”€å§”å‘˜ä¼šçš„æ ¸å¿ƒæˆå‘˜ã€‚ä»–åŒæ—¶ä¹Ÿæ˜¯ &lt;a href="https://community.cncf.io/cloud-native-students/">CNCF å­¦ç”Ÿè®¡åˆ’&lt;/a> çš„åˆ›å§‹äººä¹‹ä¸€ã€‚ä»–è¿˜åœ¨ 1.22 ç‰ˆæœ¬å‘¨æœŸä¸­æ‹…ä»»é€šä¿¡ç»ç†ä¸€èŒã€‚&lt;/p>
&lt;!--
At the end of his first year, Kunal began contributing to the [fabric8io kubernetes-client](https://github.com/fabric8io/kubernetes-client) project. He was then selected to work on the same project as part of Google Summer of Code. Kunal mentored people on the same project, first through Google Summer of Code then through Google Code-in.
-->
&lt;p>åœ¨ä»–çš„ç¬¬ä¸€å¹´ç»“æŸæ—¶ï¼ŒKunal å¼€å§‹ä¸º &lt;a href="https://github.com/fabric8io/kubernetes-client">fabric8io kubernetes-client&lt;/a> é¡¹ç›®åšè´¡çŒ®ã€‚ç„¶åï¼Œä»–è¢«æ¨é€‰ä»äº‹åŒä¸€é¡¹ç›®ï¼Œæ­¤é¡¹ç›®æ˜¯ Google Summer of Code çš„ä¸€éƒ¨åˆ†ã€‚Kunal åœ¨ Google Summer of Codeã€Google Code-in ç­‰é¡¹ç›®ä¸­æŒ‡å¯¼è¿‡å¾ˆå¤šäººã€‚&lt;/p>
&lt;!--
As an open-source enthusiast, he believes that diverse participation in the community is beneficial since it introduces new perspectives and opinions and respect for one's peers. He has worked on various open-source projects, and his participation in communities has considerably assisted his development as a developer.
-->
&lt;p>ä½œä¸ºä¸€åå¼€æºçˆ±å¥½è€…ï¼Œä»–åšä¿¡ï¼Œç¤¾åŒºçš„å¤šå…ƒåŒ–å‚ä¸æ˜¯éå¸¸æœ‰ç›Šçš„ï¼Œå› ä¸ºä»–å¼•å…¥äº†æ–°çš„è§‚å¿µå’Œè§‚ç‚¹ï¼Œå¹¶å°Šé‡è‡ªå·±çš„ä¼™ä¼´ã€‚å®ƒæ›¾å‚ä¸è¿‡å„ç§å¼€æºé¡¹ç›®ï¼Œä»–åœ¨è¿™äº›ç¤¾åŒºä¸­çš„å‚ä¸å¯¹ä»–ä½œä¸ºå¼€å‘è€…çš„å‘å±•æœ‰å¾ˆå¤§å¸®åŠ©ã€‚&lt;/p>
&lt;!--
> _I believe if you find yourself in a place where you do not know much about the
> project, that's a good thing because now you can learn while contributing and the
> community is there to help you. It has helped me a lot in gaining skills, meeting
> people from around the world and also helping them. You can learn on the go,
> you don't have to be an expert. Make sure to also check out no code contributions
> because being a beginner is a skill and you can bring new perspectives to the
> organisation._
-->
&lt;blockquote>
&lt;p>&lt;em>æˆ‘ç›¸ä¿¡ï¼Œå¦‚æœä½ å‘ç°è‡ªå·±åœ¨ä¸€ä¸ªäº†è§£ä¸å¤šçš„é¡¹ç›®å½“ä¸­ï¼Œé‚£æ˜¯ä»¶å¥½äº‹ï¼Œå› ä¸ºç°åœ¨ä½ å¯ä»¥ä¸€è¾¹è´¡çŒ®ä¸€è¾¹å­¦ä¹ ï¼Œç¤¾åŒºä¹Ÿä¼šå¸®åŠ©ä½ ã€‚å®ƒå¸®åŠ©æˆ‘è·å¾—äº†å¾ˆå¤šæŠ€èƒ½ï¼Œè®¤è¯†äº†æ¥è‡ªä¸–ç•Œå„åœ°çš„äººï¼Œä¹Ÿå¸®åŠ©äº†ä»–ä»¬ã€‚ä½ å¯ä»¥åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å­¦ä¹ ï¼Œè‡ªå·±ä¸ä¸€å®šå¿…é¡»æ˜¯ä¸“å®¶ã€‚è¯·é‡è§†éä»£ç è´¡çŒ®ï¼Œå› ä¸ºä½œä¸ºåˆå­¦è€…è¿™æ˜¯ä¸€é¡¹æŠ€èƒ½ï¼Œä½ å¯ä»¥ä¸ºç»„ç»‡å¸¦æ¥æ–°çš„è§†è§’ã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="madhav-jivarajani-https-github-com-madhavjivrajani">&lt;a href="https://github.com/MadhavJivrajani">Madhav Jivarajani&lt;/a>&lt;/h2>
&lt;!--
Madhav Jivarajani works on the VMware Upstream Kubernetes stability team. He began contributing to the Kubernetes project in January 2021 and has since made significant contributions to several areas of work under SIG Architecture, SIG API Machinery, and SIG ContribEx (contributor experience).
-->
&lt;p>Madhav Jivarajani åœ¨ VMware ä¸Šæ¸¸ Kubernetes ç¨³å®šæ€§å›¢é˜Ÿå·¥ä½œã€‚ä»–äº 2021 å¹´ 1 æœˆå¼€å§‹ä¸º Kubernetes é¡¹ç›®åšè´¡çŒ®ï¼Œæ­¤ååœ¨ SIG Architectureã€SIG API Machinery å’Œ SIG ContribExï¼ˆè´¡çŒ®è€…ç»éªŒï¼‰ç­‰é¡¹ç›®çš„å‡ ä¸ªå·¥ä½œé¢†åŸŸåšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚&lt;/p>
&lt;!--
Among several significant contributions are his recent efforts toward the Archival of [design proposals](https://github.com/kubernetes/community/issues/6055), refactoring the ["groups" codebase](https://github.com/kubernetes/k8s.io/pull/2713) under k8s-infra repository to make it mockable and testable, and improving the functionality of the [GitHub k8s bot](https://github.com/kubernetes/test-infra/issues/23129).
-->
&lt;p>åœ¨è¿™å‡ ä¸ªé‡è¦é¡¹ç›®ä¸­ï¼Œä»–æœ€è¿‘è‡´åŠ›äº &lt;a href="https://github.com/kubernetes/community/issues/6055">è®¾è®¡æ–¹æ¡ˆ&lt;/a> çš„å­˜æ¡£å·¥ä½œï¼Œé‡æ„ k8s-infra å­˜å‚¨åº“ä¸‹çš„ &lt;a href="https://github.com/kubernetes/k8s.io/pull/2713">&amp;quot;ç»„&amp;quot;ä»£ç åº“&lt;/a> ï¼Œä½¿å…¶å…·æœ‰å¯æ¨¡æ‹Ÿæ€§å’Œå¯æµ‹è¯•æ€§ï¼Œä»¥åŠæ”¹è¿› &lt;a href="https://github.com/kubernetes/test-infra/issues/23129">GitHub k8s æœºå™¨äºº&lt;/a> çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
In addition to his technical efforts, Madhav oversees many projects aimed at assisting new contributors. He organises bi-weekly "KEP reading club" sessions to help newcomers understand the process of adding new features, deprecating old ones, and making other key changes to the upstream project. He has also worked on developing [Katacoda scenarios](https://github.com/kubernetes-sigs/contributor-katacoda) to assist new contributors to become acquainted with the process of contributing to k/k. In addition to his current efforts to meet with community members every week, he has organised several [new contributors workshops (NCW)](https://www.youtube.com/watch?v=FgsXbHBRYIc).
-->
&lt;p>é™¤äº†åœ¨æŠ€æœ¯æ–¹é¢çš„è´¡çŒ®ï¼ŒMadhav è¿˜ç›‘ç£è®¸å¤šæ—¨åœ¨å¸®åŠ©æ–°è´¡çŒ®è€…çš„é¡¹ç›®ã€‚ä»–æ¯ä¸¤å‘¨ç»„ç»‡ä¸€æ¬¡çš„â€œKEP é˜…è¯»ä¿±ä¹éƒ¨â€ä¼šè®®ï¼Œå¸®åŠ©æ–°äººäº†è§£æ·»åŠ æ–°åŠŸèƒ½ã€æ‘’å¼ƒæ—§åŠŸèƒ½ä»¥åŠå¯¹ä¸Šæ¸¸é¡¹ç›®è¿›è¡Œå…¶ä»–å…³é”®æ›´æ”¹çš„è¿‡ç¨‹ã€‚ä»–è¿˜è‡´åŠ›äºå¼€å‘ &lt;a href="https://github.com/kubernetes-sigs/contributor-katacoda">Katacoda åœºæ™¯&lt;/a> ï¼Œä»¥å¸®åŠ©æ–°çš„è´¡çŒ®è€…åœ¨ä¸º k/k åšè´¡çŒ®çš„è¿‡ç¨‹æ›´åŠ ç†Ÿç»ƒã€‚ç›®å‰é™¤äº†æ¯å‘¨ä¸ç¤¾åŒºæˆå‘˜ä¼šé¢å¤–ï¼Œä»–è¿˜ç»„ç»‡äº†å‡ ä¸ª &lt;a href="https://www.youtube.com/watch?v=FgsXbHBRYIc">æ–°è´¡çŒ®è€…è®²ä¹ ç­ï¼ˆNCWï¼‰&lt;/a> ã€‚&lt;/p>
&lt;!--
> _I initially did not know much about Kubernetes. I joined because the community was
> super friendly. But what made me stay was not just the people, but the project itself.
> My solution to not feeling overwhelmed in the community was to gain as much context
> and knowledge into the topics that I was interested in and were being discussed. And
> as a result I continued to dig deeper into Kubernetes and the design of it.
> I am a systems nut &amp; thus Kubernetes was an absolute goldmine for me._
-->
&lt;blockquote>
&lt;p>&lt;em>ä¸€å¼€å§‹æˆ‘å¯¹ Kubernetes äº†è§£å¹¶ä¸å¤šã€‚æˆ‘åŠ å…¥ç¤¾åŒºæ˜¯å› ä¸ºç¤¾åŒºè¶…çº§å‹å¥½ã€‚ä½†è®©æˆ‘ç•™ä¸‹æ¥çš„ä¸ä»…ä»…æ˜¯äººï¼Œè¿˜æœ‰é¡¹ç›®æœ¬èº«ã€‚æˆ‘åœ¨ç¤¾åŒºä¸­ä¸ä¼šæ„Ÿåˆ°ä¸çŸ¥æ‰€æªï¼Œè¿™æ˜¯å› ä¸ºæˆ‘èƒ½å¤Ÿåœ¨æ„Ÿå…´è¶£çš„å’Œæ­£åœ¨è®¨è®ºçš„ä¸»é¢˜ä¸­è·å¾—å°½å¯èƒ½å¤šçš„èƒŒæ™¯å’ŒçŸ¥è¯†ã€‚å› æ­¤ï¼Œæˆ‘å°†ç»§ç»­æ·±å…¥æ¢è®¨ Kubernetes åŠå…¶è®¾è®¡ã€‚æˆ‘æ˜¯ä¸€ä¸ªç³»ç»Ÿè¿·ï¼Œkubernetes å¯¹æˆ‘æ¥è¯´ç»å¯¹æ˜¯ä¸€ä¸ªé‡‘çŸ¿ã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="rajas-kakodkar-https-github-com-rajaskakodkar">&lt;a href="https://github.com/rajaskakodkar">Rajas Kakodkar&lt;/a>&lt;/h2>
&lt;!--
Rajas Kakodkar currently works at VMware as a Member of Technical Staff. He has been engaged in many aspects of the upstream Kubernetes project since 2019.
-->
&lt;p>Rajas Kakodkar ç›®å‰åœ¨ VMware æ‹…ä»»æŠ€æœ¯äººå‘˜ã€‚è‡ª 2019 å¹´ä»¥æ¥ï¼Œä»–ä¸€ç›´å¤šæ–¹é¢åœ°ä»äº‹ä¸Šæ¸¸ kubernetes é¡¹ç›®ã€‚&lt;/p>
&lt;!--
He is now a key contributor to the Testing special interest group. He is also active in the SIG Network community. Lately, Rajas has contributed significantly to the [NetworkPolicy++](https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/) and [`kpng`](https://github.com/kubernetes-sigs/kpng) sub-projects.
-->
&lt;p>ä»–ç°åœ¨æ˜¯ Testing ç‰¹åˆ«å…´è¶£å°ç»„çš„å…³é”®è´¡çŒ®è€…ã€‚ä»–è¿˜æ´»è·ƒåœ¨ SIG Network ç¤¾åŒºã€‚æœ€è¿‘ï¼ŒRajas ä¸º &lt;a href="https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/">NetworkPolicy++&lt;/a> å’Œ &lt;a href="https://github.com/kubernetes-sigs/kpng">&lt;code>kpng&lt;/code>&lt;/a> å­é¡¹ç›®åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚&lt;/p>
&lt;!--
One of the first challenges he ran across was that he was in a different time zone than the upstream project's regular meeting hours. However, async interactions on community forums progressively corrected that problem.
-->
&lt;p>ä»–é‡åˆ°çš„ç¬¬ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œä»–æ‰€å¤„çš„æ—¶åŒºä¸ä¸Šæ¸¸é¡¹ç›®çš„æ—¥å¸¸ä¼šè®®æ—¶é—´ä¸åŒã€‚ä¸è¿‡ï¼Œç¤¾åŒºè®ºå›ä¸Šçš„å¼‚æ­¥äº¤äº’é€æ¸è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚&lt;/p>
&lt;!--
> _I enjoy contributing to Kubernetes not just because I get to work on
> cutting edge tech but more importantly because I get to work with
> awesome people and help in solving real world problems._
-->
&lt;blockquote>
&lt;p>&lt;em>æˆ‘å–œæ¬¢ä¸º kubernetes åšå‡ºè´¡çŒ®ï¼Œä¸ä»…å› ä¸ºæˆ‘å¯ä»¥ä»äº‹å°–ç«¯æŠ€æœ¯å·¥ä½œï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘å¯ä»¥å’Œä¼˜ç§€çš„äººä¸€èµ·å·¥ä½œï¼Œå¹¶å¸®åŠ©è§£å†³ç°å®é—®é¢˜ã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="rajula-vineet-reddy-https-github-com-rajula96reddy">&lt;a href="https://github.com/rajula96reddy">Rajula Vineet Reddy&lt;/a>&lt;/h2>
&lt;!--
Rajula Vineet Reddy, a Junior Engineer at CERN, is a member of the Marketing Council team under SIG ContribEx . He also served as a release shadow for SIG Release during the 1.22 and 1.23 Kubernetes release cycles.
-->
&lt;p>Rajula Vineet Reddyï¼ŒCERN çš„åˆçº§å·¥ç¨‹å¸ˆï¼Œæ˜¯ SIG ContribEx é¡¹ç›®ä¸‹è¥é”€å§”å‘˜ä¼šçš„æˆå‘˜ã€‚åœ¨ Kubernetes 1.22 å’Œ 1.23 ç‰ˆæœ¬å‘¨æœŸä¸­ï¼Œä»–è¿˜æ‹…ä»» SIG Release çš„ç‰ˆæœ¬ç»ç†ã€‚&lt;/p>
&lt;!--
He started looking at the Kubernetes project as part of a university project with the help of one of his professors. Over time, he spent a significant amount of time reading the project's documentation, Slack discussions, GitHub issues, and blogs, which helped him better grasp the Kubernetes project and piqued his interest in contributing upstream. One of his key contributions was his assistance with automation in the SIG ContribEx Upstream Marketing subproject.
-->
&lt;p>åœ¨ä»–çš„ä¸€ä½æ•™æˆçš„å¸®åŠ©ä¸‹ï¼Œä»–å¼€å§‹å°† kubernetes é¡¹ç›®ä½œä¸ºå¤§å­¦é¡¹ç›®çš„ä¸€éƒ¨åˆ†ã€‚æ…¢æ…¢åœ°ï¼Œä»–èŠ±è´¹äº†å¤§é‡çš„æ—¶é—´é˜…è¯»é¡¹ç›®çš„æ–‡æ¡£ã€Slack è®¨è®ºã€GitHub issues å’Œåšå®¢ï¼Œè¿™æœ‰åŠ©äºä»–æ›´å¥½åœ°æŒæ¡ kubernetes é¡¹ç›®ï¼Œå¹¶æ¿€å‘äº†ä»–å¯¹ä¸Šæ¸¸é¡¹ç›®åšè´¡çŒ®çš„å…´è¶£ã€‚ä»–çš„ä¸»è¦è´¡çŒ®ä¹‹ä¸€æ˜¯ä»–åœ¨SIG ContribExä¸Šæ¸¸è¥é”€å­é¡¹ç›®ä¸­ååŠ©å®ç°äº†è‡ªåŠ¨åŒ–ã€‚&lt;/p>
&lt;!--
According to Rajula, attending project meetings and shadowing various project roles are vital for learning about the community.
-->
&lt;p>Rajas è¯´ï¼Œå‚ä¸é¡¹ç›®ä¼šè®®å’Œè·Ÿè¸ªå„ç§é¡¹ç›®è§’è‰²å¯¹äºäº†è§£ç¤¾åŒºè‡³å…³é‡è¦ã€‚&lt;/p>
&lt;!--
> _I find the community very helpful and it's always_
> â€œyou get back as much as you contributeâ€.
> _The more involved you are, the more you will understand, get to learn and
> contribute new things._
>
> _The first step to_ â€œcome forward and startâ€ _is hard. But it's all gonna be
> smooth after that. Just take that jump._
-->
&lt;blockquote>
&lt;p>&lt;em>æˆ‘å‘ç°ç¤¾åŒºéå¸¸æœ‰å¸®åŠ©ï¼Œè€Œä¸”æ€»æ˜¯â€œä½ å¾—åˆ°çš„å›æŠ¥å’Œä½ è´¡çŒ®çš„ä¸€æ ·å¤šâ€ã€‚ä½ å‚ä¸å¾—è¶Šå¤šï¼Œä½ å°±è¶Šä¼šäº†è§£ã€å­¦ä¹ å’Œè´¡çŒ®æ–°ä¸œè¥¿ã€‚&lt;/em>
&lt;em>â€œæŒºèº«è€Œå‡ºâ€çš„ç¬¬ä¸€æ­¥æ˜¯è‰°éš¾çš„ã€‚ä½†åœ¨é‚£ä¹‹åä¸€åˆ‡éƒ½ä¼šé¡ºåˆ©çš„ã€‚å‹‡æ•¢åœ°å‚ä¸è¿›æ¥å§ã€‚&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;!--
If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. We're thrilled to have other folks assisting us in reaching out to even more wonderful individuals of the community. Your suggestions would be much appreciated.
-->
&lt;p>å¦‚æœæ‚¨å¯¹æˆ‘ä»¬ä¸‹ä¸€æ­¥åº”è¯¥é‡‡è®¿è°æœ‰ä»»ä½•æ„è§/å»ºè®®ï¼Œè¯·åœ¨ #sig-contribex ä¸­å‘ŠçŸ¥æˆ‘ä»¬ã€‚æˆ‘ä»¬å¾ˆé«˜å…´æœ‰å…¶ä»–äººå¸®åŠ©æˆ‘ä»¬æ¥è§¦ç¤¾åŒºä¸­æ›´ä¼˜ç§€çš„äººã€‚æˆ‘ä»¬å°†ä¸èƒœæ„Ÿæ¿€ã€‚&lt;/p>
&lt;!--
We'll see you all in the next one. Everyone, till then, have a happy contributing! ğŸ‘‹
-->
&lt;p>æˆ‘ä»¬ä¸‹æœŸè§ã€‚æœ€åï¼Œç¥å¤§å®¶éƒ½èƒ½å¿«ä¹åœ°ä¸ºç¤¾åŒºåšè´¡çŒ®ï¼ğŸ‘‹&lt;/p></description></item><item><title>Blog: Kubernetes å³å°†ç§»é™¤ Dockershimï¼šæ‰¿è¯ºå’Œä¸‹ä¸€æ­¥</title><link>https://kubernetes.io/zh/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</link><pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes is Moving on From Dockershim: Commitments and Next Steps"
date: 2022-01-07
slug: kubernetes-is-moving-on-from-dockershim
-->
&lt;!--
**Authors:** Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)
-->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong> Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)&lt;/p>
&lt;!--
Kubernetes is removing dockershim in the upcoming v1.24 release. We're excited
to reaffirm our community values by supporting open source container runtimes,
enabling a smaller kubelet, and increasing engineering velocity for teams using
Kubernetes. If you [use Docker Engine as a container runtime](/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/)
for your Kubernetes cluster, get ready to migrate in 1.24! To check if you're
affected, refer to [Check whether dockershim deprecation affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/).
-->
&lt;p>Kubernetes å°†åœ¨å³å°†å‘å¸ƒçš„ 1.24 ç‰ˆæœ¬ä¸­ç§»é™¤ dockershimã€‚æˆ‘ä»¬å¾ˆé«˜å…´èƒ½å¤Ÿé€šè¿‡æ”¯æŒå¼€æºå®¹å™¨è¿è¡Œæ—¶ã€æ”¯æŒæ›´å°çš„
kubelet ä»¥åŠä¸ºä½¿ç”¨ Kubernetes çš„å›¢é˜Ÿæé«˜å·¥ç¨‹é€Ÿåº¦æ¥é‡ç”³æˆ‘ä»¬çš„ç¤¾åŒºä»·å€¼ã€‚
å¦‚æœä½ &lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">ä½¿ç”¨ Docker Engine ä½œä¸º Kubernetes é›†ç¾¤çš„å®¹å™¨è¿è¡Œæ—¶&lt;/a>ï¼Œ
è¯·å‡†å¤‡å¥½åœ¨ 1.24 ä¸­è¿ç§»ï¼è¦æ£€æŸ¥ä½ æ˜¯å¦å—åˆ°å½±å“ï¼Œ
è¯·å‚è€ƒ&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">æ£€æŸ¥å¼ƒç”¨ Dockershim å¯¹ä½ çš„å½±å“&lt;/a>ã€‚&lt;/p>
&lt;!--
## Why weâ€™re moving away from dockershim
Docker was the first container runtime used by Kubernetes. This is one of the
reasons why Docker is so familiar to many Kubernetes users and enthusiasts.
Docker support was hardcoded into Kubernetes â€“ a component the project refers to
as dockershim.
-->
&lt;h2 id="why-we-re-moving-away-from-dockershim">ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ç¦»å¼€ dockershim &lt;/h2>
&lt;p>Docker æ˜¯ Kubernetes ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªå®¹å™¨è¿è¡Œæ—¶ã€‚
è¿™ä¹Ÿæ˜¯è®¸å¤š Kubernetes ç”¨æˆ·å’Œçˆ±å¥½è€…å¦‚æ­¤ç†Ÿæ‚‰ Docker çš„åŸå› ä¹‹ä¸€ã€‚
å¯¹ Docker çš„æ”¯æŒè¢«ç¡¬ç¼–ç åˆ° Kubernetes ä¸­â€”â€”ä¸€ä¸ªè¢«é¡¹ç›®ç§°ä¸º dockershim çš„ç»„ä»¶ã€‚&lt;/p>
&lt;!--
As containerization became an industry standard, the Kubernetes project added support
for additional runtimes. This culminated in the implementation of the
container runtime interface (CRI), letting system components (like the kubelet)
talk to container runtimes in a standardized way. As a result, dockershim became
an anomaly in the Kubernetes project.
-->
&lt;p>éšç€å®¹å™¨åŒ–æˆä¸ºè¡Œä¸šæ ‡å‡†ï¼ŒKubernetes é¡¹ç›®å¢åŠ äº†å¯¹å…¶ä»–è¿è¡Œæ—¶çš„æ”¯æŒã€‚
æœ€ç»ˆå®ç°äº†å®¹å™¨è¿è¡Œæ—¶æ¥å£ï¼ˆCRIï¼‰ï¼Œè®©ç³»ç»Ÿç»„ä»¶ï¼ˆå¦‚ kubeletï¼‰ä»¥æ ‡å‡†åŒ–çš„æ–¹å¼ä¸å®¹å™¨è¿è¡Œæ—¶é€šä¿¡ã€‚
å› æ­¤ï¼Œdockershim æˆä¸ºäº† Kubernetes é¡¹ç›®ä¸­çš„ä¸€ä¸ªå¼‚å¸¸ç°è±¡ã€‚&lt;/p>
&lt;!--
Dependencies on Docker and dockershim have crept into various tools
and projects in the CNCF ecosystem ecosystem, resulting in fragile code.
By removing the
dockershim CRI, we're embracing the first value of CNCF: "[Fast is better than
slow](https://github.com/cncf/foundation/blob/master/charter.md#3-values)".
Stay tuned for future communications on the topic!
-->
&lt;p>å¯¹ Docker å’Œ dockershim çš„ä¾èµ–å·²ç»æ¸—é€åˆ° CNCF ç”Ÿæ€ç³»ç»Ÿä¸­çš„å„ç§å·¥å…·å’Œé¡¹ç›®ä¸­ï¼Œè¿™å¯¼è‡´äº†ä»£ç è„†å¼±ã€‚&lt;/p>
&lt;p>é€šè¿‡åˆ é™¤ dockershim CRIï¼Œæˆ‘ä»¬æ‹¥æŠ±äº† CNCF çš„ç¬¬ä¸€ä¸ªä»·å€¼ï¼š
â€œ&lt;a href="https://github.com/cncf/foundation/blob/master/charter.md#3-values">å¿«æ¯”æ…¢å¥½&lt;/a>â€ã€‚
è¯·ç»§ç»­å…³æ³¨æœªæ¥å…³äºè¿™ä¸ªè¯é¢˜çš„äº¤æµ!&lt;/p>
&lt;!--
## Deprecation timeline
We [formally announced](/blog/2020/12/08/kubernetes-1-20-release-announcement/) the dockershim deprecation in December 2020. Full removal is targeted
in Kubernetes 1.24, in April 2022. This timeline
aligns with our [deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior),
which states that deprecated behaviors must function for at least 1 year
after their announced deprecation.
-->
&lt;h2 id="deprecation-timeline">å¼ƒç”¨æ—¶é—´çº¿ &lt;/h2>
&lt;p>æˆ‘ä»¬&lt;a href="https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/">æ­£å¼å®£å¸ƒ&lt;/a>äº
2020 å¹´ 12 æœˆå¼ƒç”¨ dockershimã€‚ç›®æ ‡æ˜¯åœ¨ 2022 å¹´ 4 æœˆï¼Œ
Kubernetes 1.24 ä¸­å®Œå…¨ç§»é™¤ dockershimã€‚
æ­¤æ—¶é—´çº¿ä¸æˆ‘ä»¬çš„[å¼ƒç”¨ç­–ç•¥](/zh/docs/reference/using api/deprecation-policy/#deprecating-a-feature-or-behavior)ä¸€è‡´ï¼Œ
å³è§„å®šå·²å¼ƒç”¨çš„è¡Œä¸ºå¿…é¡»åœ¨å…¶å®£å¸ƒå¼ƒç”¨åè‡³å°‘è¿è¡Œ 1 å¹´ã€‚&lt;/p>
&lt;!--
We'll support Kubernetes version 1.23, which includes
dockershim, for another year in the Kubernetes project. For managed
Kubernetes providers, vendor support is likely to last even longer, but this is
dependent on the companies themselves. Regardless, we're confident all cluster operations will have
time to migrate. If you have more questions about the dockershim removal, refer
to the [Dockershim Deprecation FAQ](/dockershim).
-->
&lt;p>åŒ…æ‹¬ dockershim çš„ Kubernetes 1.23 ç‰ˆæœ¬ï¼Œåœ¨ Kubernetes é¡¹ç›®ä¸­å°†å†æ”¯æŒä¸€å¹´ã€‚
å¯¹äºæ‰˜ç®¡ Kubernetes çš„ä¾›åº”å•†ï¼Œä¾›åº”å•†æ”¯æŒå¯èƒ½ä¼šæŒç»­æ›´é•¿æ—¶é—´ï¼Œä½†è¿™å–å†³äºå…¬å¸æœ¬èº«ã€‚
æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬ç›¸ä¿¡æ‰€æœ‰é›†ç¾¤æ“ä½œéƒ½æœ‰æ—¶é—´è¿›è¡Œè¿ç§»ã€‚å¦‚æœä½ æœ‰æ›´å¤šå…³äº dockershim ç§»é™¤çš„é—®é¢˜ï¼Œ
è¯·å‚è€ƒ&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">å¼ƒç”¨ Dockershim çš„å¸¸è§é—®é¢˜&lt;/a>ã€‚&lt;/p>
&lt;!--
We asked you whether you feel prepared for the migration from dockershim in this
survey: [Are you ready for Dockershim removal](/blog/2021/11/12/are-you-ready-for-dockershim-removal/).
We had over 600 responses. To everybody who took time filling out the survey,
thank you.
-->
&lt;p>åœ¨è¿™ä¸ª&lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">ä½ æ˜¯å¦ä¸º dockershim çš„åˆ é™¤åšå¥½äº†å‡†å¤‡&lt;/a>çš„è°ƒæŸ¥ä¸­ï¼Œ
æˆ‘ä»¬è¯¢é—®ä½ æ˜¯å¦ä¸º dockershim çš„è¿ç§»åšå¥½äº†å‡†å¤‡ã€‚æˆ‘ä»¬æ”¶åˆ°äº† 600 å¤šä¸ªå›å¤ã€‚
æ„Ÿè°¢æ‰€æœ‰èŠ±æ—¶é—´å¡«å†™è°ƒæŸ¥é—®å·çš„äººã€‚&lt;/p>
&lt;!--
The results show that we still have a lot of ground to cover to help you to
migrate smoothly. Other container runtimes exist, and have been promoted
extensively. However, many users told us they still rely on dockershim,
and sometimes have dependencies that need to be re-worked. Some of these
dependencies are outside of your control. Based on your feedback, here are some
of the steps we are taking to help.
-->
&lt;p>ç»“æœè¡¨æ˜ï¼Œåœ¨å¸®åŠ©ä½ é¡ºåˆ©è¿ç§»æ–¹é¢ï¼Œæˆ‘ä»¬è¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åšã€‚
å­˜åœ¨å…¶ä»–å®¹å™¨è¿è¡Œæ—¶ï¼Œå¹¶ä¸”å·²è¢«å¹¿æ³›æ¨å¹¿ã€‚ä½†æ˜¯ï¼Œè®¸å¤šç”¨æˆ·å‘Šè¯‰æˆ‘ä»¬ä»–ä»¬ä»ç„¶ä¾èµ– dockershimï¼Œ
å¹¶ä¸”æœ‰æ—¶éœ€è¦é‡æ–°å¤„ç†ä¾èµ–é¡¹ã€‚å…¶ä¸­ä¸€äº›ä¾èµ–é¡¹è¶…å‡ºæ§åˆ¶èŒƒå›´ã€‚
æ ¹æ®æ”¶é›†åˆ°çš„åé¦ˆï¼Œæˆ‘ä»¬é‡‡å–äº†ä¸€äº›æªæ–½æä¾›å¸®åŠ©ã€‚&lt;/p>
&lt;!--
## Our next steps
Based on the feedback you provided:
- CNCF and the 1.24 release team are committed to delivering documentation in
time for the 1.24 release. This includes more informative blog posts like this
one, updating existing code samples, tutorials, and tasks, and producing a
migration guide for cluster operators.
- We are reaching out to the rest of the CNCF community to help prepare them for
this change.
-->
&lt;h2 id="our-next-steps">æˆ‘ä»¬çš„ä¸‹ä¸€ä¸ªæ­¥éª¤&lt;/h2>
&lt;p>æ ¹æ®æä¾›çš„åé¦ˆï¼š&lt;/p>
&lt;ul>
&lt;li>CNCF å’Œ 1.24 ç‰ˆæœ¬å›¢é˜Ÿè‡´åŠ›äºåŠæ—¶äº¤ä»˜ 1.24 ç‰ˆæœ¬çš„æ–‡æ¡£ã€‚è¿™åŒ…æ‹¬åƒæœ¬æ–‡è¿™æ ·çš„åŒ…å«æ›´å¤šä¿¡æ¯çš„åšå®¢æ–‡ç« ï¼Œ
æ›´æ–°ç°æœ‰çš„ä»£ç ç¤ºä¾‹ã€æ•™ç¨‹å’Œä»»åŠ¡ï¼Œå¹¶ä¸ºé›†ç¾¤æ“ä½œäººå‘˜ç”Ÿæˆè¿ç§»æŒ‡å—ã€‚&lt;/li>
&lt;li>æˆ‘ä»¬æ­£åœ¨è”ç³» CNCF ç¤¾åŒºçš„å…¶ä»–æˆå‘˜ï¼Œå¸®åŠ©ä»–ä»¬ä¸ºè¿™ä¸€å˜åŒ–åšå¥½å‡†å¤‡ã€‚&lt;/li>
&lt;/ul>
&lt;!--
If you're part of a project with dependencies on dockershim, or if you're
interested in helping with the migration effort, please join us! There's always
room for more contributors, whether to our transition tools or to our
documentation. To get started, say hello in the
[#sig-node](https://kubernetes.slack.com/archives/C0BP8PW9G)
channel on [Kubernetes Slack](https://slack.kubernetes.io/)!
-->
&lt;p>å¦‚æœä½ æ˜¯ä¾èµ– dockershim çš„é¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼Œæˆ–è€…å¦‚æœä½ æœ‰å…´è¶£å¸®åŠ©å‚ä¸è¿ç§»å·¥ä½œï¼Œè¯·åŠ å…¥æˆ‘ä»¬ï¼
æ— è®ºæ˜¯æˆ‘ä»¬çš„è¿ç§»å·¥å…·è¿˜æ˜¯æˆ‘ä»¬çš„æ–‡æ¡£ï¼Œæ€»æ˜¯æœ‰æ›´å¤šè´¡çŒ®è€…çš„ç©ºé—´ã€‚
ä½œä¸ºèµ·æ­¥ï¼Œè¯·åœ¨ &lt;a href="https://slack.kubernetes.io/">Kubernetes Slack&lt;/a> ä¸Šçš„
&lt;a href="https://kubernetes.slack.com/archives/C0BP8PW9G">#sig-node&lt;/a> é¢‘é“æ‰“ä¸ªæ‹›å‘¼ï¼&lt;/p>
&lt;!--
## Final thoughts
As a project, we've already seen cluster operators increasingly adopt other
container runtimes through 2021.
We believe there are no major blockers to migration. The steps we're taking to
improve the migration experience will light the path more clearly for you.
-->
&lt;h2 id="final-thoughts">æœ€ç»ˆæƒ³æ³• &lt;/h2>
&lt;p>ä½œä¸ºä¸€ä¸ªé¡¹ç›®ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°é›†ç¾¤è¿è¥å•†åœ¨ 2021 å¹´ä¹‹å‰è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨å…¶ä»–å®¹å™¨è¿è¡Œæ—¶ã€‚
æˆ‘ä»¬ç›¸ä¿¡è¿ç§»æ²¡æœ‰ä¸»è¦éšœç¢ã€‚æˆ‘ä»¬ä¸ºæ”¹å–„è¿ç§»ä½“éªŒè€Œé‡‡å–çš„æ­¥éª¤å°†ä¸ºä½ æŒ‡æ˜æ›´æ¸…æ™°çš„é“è·¯ã€‚&lt;/p>
&lt;!--
We understand that migration from dockershim is yet another action you may need to
do to keep your Kubernetes infrastructure up to date. For most of you, this step
will be straightforward and transparent. In some cases, you will encounter
hiccups or issues. The community has discussed at length whether postponing the
dockershim removal would be helpful. For example, we recently talked about it in
the [SIG Node discussion on November 11th](https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid)
and in the [Kubernetes Steering committee meeting held on December 6th](https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx).
We already [postponed](https://github.com/kubernetes/enhancements/pull/2481/) it
once in 2021 because the adoption rate of other
runtimes was lower than we wanted, which also gave us more time to identify
potential blocking issues.
-->
&lt;p>æˆ‘ä»¬çŸ¥é“ï¼Œä» dockershim è¿ç§»æ˜¯ä½ å¯èƒ½éœ€è¦æ‰§è¡Œçš„å¦ä¸€é¡¹æ“ä½œï¼Œä»¥ä¿è¯ä½ çš„ Kubernetes åŸºç¡€æ¶æ„ä¿æŒæœ€æ–°ã€‚
å¯¹äºä½ ä»¬ä¸­çš„å¤§å¤šæ•°äººæ¥è¯´ï¼Œè¿™ä¸€æ­¥å°†æ˜¯ç®€å•æ˜äº†çš„ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ ä¼šé‡åˆ°é—®é¢˜ã€‚
ç¤¾åŒºå·²ç»è¯¦ç»†è®¨è®ºäº†æ¨è¿Ÿ dockershim åˆ é™¤æ˜¯å¦ä¼šæœ‰æ‰€å¸®åŠ©ã€‚
ä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ€è¿‘åœ¨ &lt;a href="https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid">11 æœˆ 11 æ—¥çš„ SIG Node è®¨è®º&lt;/a>å’Œ
&lt;a href="https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx">12 æœˆ 6 æ—¥ Kubernetes Steering ä¸¾è¡Œçš„å§”å‘˜ä¼šä¼šè®®&lt;/a>è°ˆåˆ°äº†å®ƒã€‚
æˆ‘ä»¬å·²ç»åœ¨ 2021 å¹´&lt;a href="https://github.com/kubernetes/enhancements/pull/2481/">æ¨è¿Ÿ&lt;/a>å®ƒä¸€æ¬¡ï¼Œ
å› ä¸ºå…¶ä»–è¿è¡Œæ—¶çš„é‡‡ç”¨ç‡ä½äºæˆ‘ä»¬çš„é¢„æœŸï¼Œè¿™ä¹Ÿç»™äº†æˆ‘ä»¬æ›´å¤šçš„æ—¶é—´æ¥è¯†åˆ«æ½œåœ¨çš„é˜»å¡é—®é¢˜ã€‚&lt;/p>
&lt;!--
At this point, we believe that the value that you (and Kubernetes) gain from
dockershim removal makes up for the migration effort you'll have. Start planning
now to avoid surprises. We'll have more updates and guides before Kubernetes
1.24 is released.
-->
&lt;p>åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä»¬ç›¸ä¿¡ä½ ï¼ˆå’Œ Kubernetesï¼‰ä»ç§»é™¤ dockershim ä¸­è·å¾—çš„ä»·å€¼å¯ä»¥å¼¥è¡¥ä½ å°†è¦è¿›è¡Œçš„è¿ç§»å·¥ä½œã€‚
ç°åœ¨å°±å¼€å§‹è®¡åˆ’ä»¥é¿å…å‡ºç°æ„å¤–ã€‚åœ¨ Kubernetes 1.24 å‘å¸ƒä¹‹å‰ï¼Œæˆ‘ä»¬å°†æä¾›æ›´å¤šæ›´æ–°ä¿¡æ¯å’ŒæŒ‡å—ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes 1.23: StatefulSet PVC è‡ªåŠ¨åˆ é™¤ (alpha)</title><link>https://kubernetes.io/zh/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.23: StatefulSet PVC Auto-Deletion (alpha)'
date: 2021-12-16
slug: kubernetes-1-23-statefulset-pvc-auto-deletion
-->
&lt;!--
**Author:** Matthew Cary (Google)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> Matthew Cary (è°·æ­Œ)&lt;/p>
&lt;!--
Kubernetes v1.23 introduced a new, alpha-level policy for
[StatefulSets](/docs/concepts/workloads/controllers/statefulset/) that controls the lifetime of
[PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/) (PVCs) generated from the
StatefulSet spec template for cases when they should be deleted automatically when the StatefulSet
is deleted or pods in the StatefulSet are scaled down.
-->
&lt;p>Kubernetes v1.23 ä¸º &lt;a href="https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets&lt;/a>
å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ alpha çº§ç­–ç•¥ï¼Œç”¨æ¥æ§åˆ¶ç”± StatefulSet è§„çº¦æ¨¡æ¿ç”Ÿæˆçš„
&lt;a href="https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaims&lt;/a> (PVCs) çš„ç”Ÿå‘½å‘¨æœŸï¼Œ
ç”¨äºå½“åˆ é™¤ StatefulSet æˆ–å‡å°‘ StatefulSet ä¸­çš„ Pods æ•°é‡æ—¶ PVCs åº”è¯¥è¢«è‡ªåŠ¨åˆ é™¤çš„åœºæ™¯ã€‚&lt;/p>
&lt;!--
## What problem does this solve?
-->
&lt;h2 id="å®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜">å®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ&lt;/h2>
&lt;!--
A StatefulSet spec can include Pod and PVC templates. When a replica is first created, the
Kubernetes control plane creates a PVC for that replica if one does not already exist. The behavior
before Kubernetes v1.23 was that the control plane never cleaned up the PVCs created for
StatefulSets - this was left up to the cluster administrator, or to some add-on automation that
youâ€™d have to find, check suitability, and deploy. The common pattern for managing PVCs, either
manually or through tools such as Helm, is that the PVCs are tracked by the tool that manages them,
with explicit lifecycle. Workflows that use StatefulSets must determine on their own what PVCs are
created by a StatefulSet and what their lifecycle should be.
-->
&lt;p>StatefulSet è§„çº¦ä¸­å¯ä»¥åŒ…å« Pod å’Œ PVC çš„æ¨¡æ¿ã€‚å½“å‰¯æœ¬å…ˆè¢«åˆ›å»ºæ—¶ï¼Œå¦‚æœ PVC è¿˜ä¸å­˜åœ¨ï¼Œ
Kubernetes æ§åˆ¶é¢ä¼šä¸ºè¯¥å‰¯æœ¬è‡ªåŠ¨åˆ›å»ºä¸€ä¸ª PVCã€‚åœ¨ Kubernetes 1.23 ç‰ˆæœ¬ä¹‹å‰ï¼Œ
æ§åˆ¶é¢ä¸ä¼šåˆ é™¤ StatefulSet åˆ›å»ºçš„ PVCsâ€”â€”è¿™ä¾èµ–é›†ç¾¤ç®¡ç†å‘˜æˆ–ä½ éœ€è¦éƒ¨ç½²ä¸€äº›é¢å¤–çš„é€‚ç”¨çš„è‡ªåŠ¨åŒ–å·¥å…·æ¥å¤„ç†ã€‚
ç®¡ç† PVC çš„å¸¸è§æ¨¡å¼æ˜¯é€šè¿‡æ‰‹åŠ¨æˆ–ä½¿ç”¨ Helm ç­‰å·¥å…·ï¼ŒPVC çš„å…·ä½“ç”Ÿå‘½å‘¨æœŸç”±ç®¡ç†å®ƒçš„å·¥å…·è·Ÿè¸ªã€‚
ä½¿ç”¨ StatefulSet æ—¶å¿…é¡»è‡ªè¡Œç¡®å®š StatefulSet åˆ›å»ºå“ªäº› PVCï¼Œä»¥åŠå®ƒä»¬çš„ç”Ÿå‘½å‘¨æœŸåº”è¯¥æ˜¯ä»€ä¹ˆã€‚&lt;/p>
&lt;!--
Before this new feature, when a StatefulSet-managed replica disappears, either because the
StatefulSet is reducing its replica count, or because its StatefulSet is deleted, the PVC and its
backing volume remains and must be manually deleted. While this behavior is appropriate when the
data is critical, in many cases the persistent data in these PVCs is either temporary, or can be
reconstructed from another source. In those cases, PVCs and their backing volumes remaining after
their StatefulSet or replicas have been deleted are not necessary, incur cost, and require manual
cleanup.
-->
&lt;p>åœ¨è¿™ä¸ªæ–°ç‰¹æ€§ä¹‹å‰ï¼Œå½“ä¸€ä¸ª StatefulSet ç®¡ç†çš„å‰¯æœ¬æ¶ˆå¤±æ—¶ï¼Œæ— è®ºæ˜¯å› ä¸º StatefulSet å‡å°‘äº†å®ƒçš„å‰¯æœ¬æ•°ï¼Œ
è¿˜æ˜¯å› ä¸ºå®ƒçš„ StatefulSet è¢«åˆ é™¤äº†ï¼ŒPVC åŠå…¶ä¸‹å±‚çš„å·ä»ç„¶å­˜åœ¨ï¼Œéœ€è¦æ‰‹åŠ¨åˆ é™¤ã€‚
å½“å­˜å‚¨æ•°æ®æ¯”è¾ƒé‡è¦æ—¶ï¼Œè¿™æ ·åšæ˜¯åˆç†çš„ï¼Œä½†åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™äº› PVC ä¸­çš„æŒä¹…åŒ–æ•°æ®è¦ä¹ˆæ˜¯ä¸´æ—¶çš„ï¼Œ
è¦ä¹ˆå¯ä»¥ä»å¦ä¸€ä¸ªæºç«¯é‡å»ºã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œåˆ é™¤ StatefulSet æˆ–å‡å°‘å‰¯æœ¬åç•™ä¸‹çš„ PVC åŠå…¶ä¸‹å±‚çš„å·æ˜¯ä¸å¿…è¦çš„ï¼Œ
è¿˜ä¼šäº§ç”Ÿæˆæœ¬ï¼Œéœ€è¦æ‰‹åŠ¨æ¸…ç†ã€‚&lt;/p>
&lt;!--
## The new StatefulSet PVC retention policy
-->
&lt;h2 id="æ–°çš„-statefulset-pvc-ä¿ç•™ç­–ç•¥">æ–°çš„ StatefulSet PVC ä¿ç•™ç­–ç•¥&lt;/h2>
&lt;!--
If you enable the alpha feature, a StatefulSet spec includes a PersistentVolumeClaim retention
policy. This is used to control if and when PVCs created from a StatefulSetâ€™s `volumeClaimTemplate`
are deleted. This first iteration of the retention policy contains two situations where PVCs may be
deleted.
-->
&lt;p>å¦‚æœä½ å¯ç”¨è¿™ä¸ªæ–° alpha ç‰¹æ€§ï¼ŒStatefulSet è§„çº¦ä¸­å°±å¯ä»¥åŒ…å« PersistentVolumeClaim çš„ä¿ç•™ç­–ç•¥ã€‚
è¯¥ç­–ç•¥ç”¨äºæ§åˆ¶æ˜¯å¦ä»¥åŠä½•æ—¶åˆ é™¤åŸºäº StatefulSet çš„ &lt;code>volumeClaimTemplate&lt;/code> å±æ€§æ‰€åˆ›å»ºçš„ PVCsã€‚
ä¿ç•™ç­–ç•¥çš„é¦–æ¬¡è¿­ä»£åŒ…å«ä¸¤ç§å¯èƒ½åˆ é™¤ PVC çš„æƒ…å†µã€‚&lt;/p>
&lt;!--
The first situation is when the StatefulSet resource is deleted (which implies that all replicas are
also deleted). This is controlled by the `whenDeleted` policy. The second situation, controlled by
`whenScaled` is when the StatefulSet is scaled down, which removes some but not all of the replicas
in a StatefulSet. In both cases the policy can either be `Retain`, where the corresponding PVCs are
not touched, or `Delete`, which means that PVCs are deleted. The deletion is done with a normal
[object deletion](/docs/concepts/architecture/garbage-collection/), so that, for example, all
retention policies for the underlying PV are respected.
-->
&lt;p>ç¬¬ä¸€ç§æƒ…å†µæ˜¯ StatefulSet èµ„æºè¢«åˆ é™¤æ—¶ï¼ˆè¿™æ„å‘³ç€æ‰€æœ‰å‰¯æœ¬ä¹Ÿè¢«åˆ é™¤ï¼‰ï¼Œè¿™ç”± &lt;code>whenDeleted&lt;/code> ç­–ç•¥æ§åˆ¶çš„ã€‚
ç¬¬äºŒç§æƒ…å†µæ˜¯ StatefulSet ç¼©å°æ—¶ï¼Œå³åˆ é™¤ StatefulSet éƒ¨åˆ†å‰¯æœ¬ï¼Œè¿™ç”± &lt;code>whenScaled&lt;/code> ç­–ç•¥æ§åˆ¶ã€‚
åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œç­–ç•¥å³å¯ä»¥æ˜¯ &lt;code>Retain&lt;/code> ä¸æ¶‰åŠç›¸åº” PVCs çš„æ”¹å˜ï¼Œä¹Ÿå¯ä»¥æ˜¯ &lt;code>Delete&lt;/code> å³åˆ é™¤å¯¹åº”çš„ PVCsã€‚
åˆ é™¤æ˜¯é€šè¿‡æ™®é€šçš„&lt;a href="https://kubernetes.io/zh/docs/concepts/architecture/garbage-collection/">å¯¹è±¡åˆ é™¤&lt;/a>å®Œæˆçš„ï¼Œ
å› æ­¤ï¼Œçš„æ‰€æœ‰ä¿ç•™ç­–ç•¥éƒ½ä¼šè¢«éµç…§æ‰§è¡Œã€‚&lt;/p>
&lt;!--
This policy forms a matrix with four cases. Iâ€™ll walk through and give an example for each one.
-->
&lt;p>è¯¥ç­–ç•¥å½¢æˆåŒ…å«å››ç§æƒ…å†µçš„çŸ©é˜µã€‚æˆ‘å°†é€ä¸€ä»‹ç»ï¼Œå¹¶ä¸ºæ¯ä¸€ç§æƒ…å†µç»™å‡ºä¸€ä¸ªä¾‹å­ã€‚&lt;/p>
&lt;!--
* **`whenDeleted` and `whenScaled` are both `Retain`.** This matches the existing behavior for
StatefulSets, where no PVCs are deleted. This is also the default retention policy. Itâ€™s
appropriate to use when data on StatefulSet volumes may be irreplaceable and should only be
deleted manually.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> å’Œ &lt;code>whenScaled&lt;/code> éƒ½æ˜¯ &lt;code>Retain&lt;/code>ã€‚&lt;/strong> è¿™ä¸ StatefulSets çš„ç°æœ‰è¡Œä¸ºä¸€è‡´ï¼Œ
å³ä¸åˆ é™¤ PVCsã€‚ è¿™ä¹Ÿæ˜¯é»˜è®¤çš„ä¿ç•™ç­–ç•¥ã€‚å®ƒé€‚ç”¨äº StatefulSet
å·ä¸Šçš„æ•°æ®æ˜¯ä¸å¯æ›¿ä»£çš„ä¸”åªèƒ½æ‰‹åŠ¨åˆ é™¤çš„æƒ…å†µã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` is `Delete` and `whenScaled` is `Retain`.** In this case, PVCs are deleted only when
the entire StatefulSet is deleted. If the StatefulSet is scaled down, PVCs are not touched,
meaning they are available to be reattached if a scale-up occurs with any data from the previous
replica. This might be used for a temporary StatefulSet, such as in a CI instance or ETL
pipeline, where the data on the StatefulSet is needed only during the lifetime of the
StatefulSet lifetime, but while the task is running the data is not easily reconstructible. Any
retained state is needed for any replicas that scale down and then up.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> æ˜¯ &lt;code>Delete&lt;/code> ä½† &lt;code>whenScaled&lt;/code> æ˜¯ &lt;code>Retain&lt;/code>ã€‚&lt;/strong> åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ
åªæœ‰å½“æ•´ä¸ª StatefulSet è¢«åˆ é™¤æ—¶ï¼ŒPVCs æ‰ä¼šè¢«åˆ é™¤ã€‚
å¦‚æœå‡å°‘ StatefulSet å‰¯æœ¬ï¼ŒPVCs ä¸ä¼šåˆ é™¤ï¼Œè¿™æ„å‘³ç€å¦‚æœå¢åŠ å‰¯æœ¬æ—¶ï¼Œå¯ä»¥ä»å‰ä¸€ä¸ªå‰¯æœ¬é‡æ–°è¿æ¥æ‰€æœ‰æ•°æ®ã€‚
è¿™å¯èƒ½ç”¨äºä¸´æ—¶çš„ StatefulSetï¼Œä¾‹å¦‚åœ¨ CI å®ä¾‹æˆ– ETL ç®¡é“ä¸­ï¼Œ
StatefulSet ä¸Šçš„æ•°æ®ä»…åœ¨ StatefulSet ç”Ÿå‘½å‘¨æœŸå†…æ‰éœ€è¦ï¼Œä½†åœ¨ä»»åŠ¡è¿è¡Œæ—¶æ•°æ®ä¸æ˜“é‡æ„ã€‚
ä»»ä½•ä¿ç•™çŠ¶æ€å¯¹äºæ‰€æœ‰å…ˆç¼©å°åæ‰©å¤§çš„å‰¯æœ¬éƒ½æ˜¯å¿…éœ€çš„ã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` and `whenScaled` are both `Delete`.** PVCs are deleted immediately when their
replica is no longer needed. Note this does not include when a Pod is deleted and a new version
rescheduled, for example when a node is drained and Pods need to migrate elsewhere. The PVC is
deleted only when the replica is no longer needed as signified by a scale-down or StatefulSet
deletion. This use case is for when data does not need to live beyond the life of its
replica. Perhaps the data is easily reconstructable and the cost savings of deleting unused PVCs
is more important than quick scale-up, or perhaps that when a new replica is created, any data
from a previous replica is not usable and must be reconstructed anyway.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> å’Œ &lt;code>whenScaled&lt;/code> éƒ½æ˜¯ &lt;code>Delete&lt;/code>ã€‚&lt;/strong> å½“å…¶å‰¯æœ¬ä¸å†è¢«éœ€è¦æ—¶ï¼ŒPVCs ä¼šç«‹å³è¢«åˆ é™¤ã€‚
æ³¨æ„ï¼Œè¿™å¹¶ä¸åŒ…æ‹¬ Pod è¢«åˆ é™¤ä¸”æœ‰æ–°ç‰ˆæœ¬è¢«è°ƒåº¦çš„æƒ…å†µï¼Œä¾‹å¦‚å½“èŠ‚ç‚¹è¢«è…¾ç©ºè€Œ Pod éœ€è¦è¿ç§»åˆ°åˆ«å¤„æ—¶ã€‚
åªæœ‰å½“å‰¯æœ¬ä¸å†è¢«éœ€è¦æ—¶ï¼Œå¦‚æŒ‰æ¯”ä¾‹ç¼©å°æˆ–åˆ é™¤ StatefulSet æ—¶ï¼Œæ‰ä¼šåˆ é™¤ PVCã€‚
æ­¤ç­–ç•¥é€‚ç”¨äºæ•°æ®ç”Ÿå‘½å‘¨æœŸçŸ­äºå‰¯æœ¬ç”Ÿå‘½å‘¨æœŸçš„æƒ…å†µã€‚å³æ•°æ®å¾ˆå®¹æ˜“é‡æ„ï¼Œ
ä¸”åˆ é™¤æœªä½¿ç”¨çš„ PVC æ‰€èŠ‚çœçš„æˆæœ¬æ¯”å¿«é€Ÿå¢åŠ å‰¯æœ¬æ›´é‡è¦ï¼Œæˆ–è€…å½“åˆ›å»ºä¸€ä¸ªæ–°çš„å‰¯æœ¬æ—¶ï¼Œ
æ¥è‡ªä»¥å‰å‰¯æœ¬çš„ä»»ä½•æ•°æ®éƒ½ä¸å¯ç”¨ï¼Œå¿…é¡»é‡æ–°æ„å»ºã€‚&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` is `Retain` and `whenScaled` is `Delete`.** This is similar to the previous case,
when there is little benefit to keeping PVCs for fast reuse during scale-up. An example of a
situation where you might use this is an Elasticsearch cluster. Typically you would scale that
workload up and down to match demand, whilst ensuring a minimum number of replicas (for example:
3). When scaling down, data is migrated away from removed replicas and there is no benefit to
retaining those PVCs. However, it can be useful to bring the entire Elasticsearch cluster down
temporarily for maintenance. If you need to take the Elasticsearch system offline, you can do
this by temporarily deleting the StatefulSet, and then bringing the Elasticsearch cluster back
by recreating the StatefulSet. The PVCs holding the Elasticsearch data will still exist and the
new replicas will automatically use them.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> æ˜¯ &lt;code>Retain&lt;/code> ä½† &lt;code>whenScaled&lt;/code> æ˜¯ &lt;code>Delete&lt;/code>ã€‚&lt;/strong> è¿™ä¸å‰ä¸€ç§æƒ…å†µç±»ä¼¼ï¼Œ
åœ¨å¢åŠ å‰¯æœ¬æ—¶ç”¨ä¿ç•™çš„ PVCs å¿«é€Ÿé‡æ„å‡ ä¹æ²¡æœ‰ä»€ä¹ˆç›Šå¤„ã€‚ä¾‹å¦‚ Elasticsearch é›†ç¾¤å°±æ˜¯ä½¿ç”¨çš„è¿™ç§æ–¹å¼ã€‚
é€šå¸¸ï¼Œä½ éœ€è¦å¢å¤§æˆ–ç¼©å°å·¥ä½œè´Ÿè½½æ¥æ»¡è¶³ä¸šåŠ¡è¯‰æ±‚ï¼ŒåŒæ—¶ç¡®ä¿æœ€å°æ•°é‡çš„å‰¯æœ¬ï¼ˆä¾‹å¦‚ï¼š3ï¼‰ã€‚
å½“å‡å°‘å‰¯æœ¬æ—¶ï¼Œæ•°æ®å°†ä»å·²åˆ é™¤çš„å‰¯æœ¬è¿ç§»å‡ºå»ï¼Œä¿ç•™è¿™äº› PVCs æ²¡æœ‰ä»»ä½•ç”¨å¤„ã€‚
ä½†æ˜¯ï¼Œè¿™å¯¹ä¸´æ—¶å…³é—­æ•´ä¸ª Elasticsearch é›†ç¾¤è¿›è¡Œç»´æŠ¤æ—¶æ˜¯å¾ˆæœ‰ç”¨çš„ã€‚
å¦‚æœéœ€è¦ä½¿ Elasticsearch ç³»ç»Ÿè„±æœºï¼Œå¯ä»¥é€šè¿‡ä¸´æ—¶åˆ é™¤ StatefulSet æ¥å®ç°ï¼Œ
ç„¶åé€šè¿‡é‡æ–°åˆ›å»º StatefulSet æ¥æ¢å¤ Elasticsearch é›†ç¾¤ã€‚
ä¿å­˜ Elasticsearch æ•°æ®çš„ PVCs ä¸ä¼šè¢«åˆ é™¤ï¼Œæ–°çš„å‰¯æœ¬å°†è‡ªåŠ¨ä½¿ç”¨å®ƒä»¬ã€‚&lt;/li>
&lt;/ul>
&lt;!--
Visit the
[documentation](/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies) to
see all the details.
-->
&lt;p>æŸ¥é˜…&lt;a href="https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies">æ–‡æ¡£&lt;/a>
è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
## Whatâ€™s next?
-->
&lt;h2 id="ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆ">ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h2>
&lt;!--
Enable the feature and try it out! Enable the `StatefulSetAutoDeletePVC` feature gate on a cluster,
then create a StatefulSet using the new policy. Test it out and tell us what you think!
-->
&lt;p>å¯ç”¨è¯¥åŠŸèƒ½å¹¶å°è¯•ä¸€ä¸‹ï¼åœ¨é›†ç¾¤ä¸Šå¯ç”¨ &lt;code>StatefulSetAutoDeletePVC&lt;/code> åŠŸèƒ½ï¼Œç„¶åä½¿ç”¨æ–°ç­–ç•¥åˆ›å»º StatefulSetã€‚
æµ‹è¯•ä¸€ä¸‹ï¼Œå‘Šè¯‰æˆ‘ä»¬ä½ çš„ä½“éªŒï¼&lt;/p>
&lt;!--
I'm very curious to see if this owner reference mechanism works well in practice. For example, we
realized there is no mechanism in Kubernetes for knowing who set a reference, so itâ€™s possible that
the StatefulSet controller may fight with custom controllers that set their own
references. Fortunately, maintaining the existing retention behavior does not involve any new owner
references, so default behavior will be compatible.
-->
&lt;p>æˆ‘å¾ˆå¥½å¥‡è¿™ä¸ªå±ä¸»å¼•ç”¨æœºåˆ¶åœ¨å®è·µä¸­æ˜¯å¦æœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ„è¯†åˆ° Kubernetes ä¸­æ²¡æœ‰å¯ä»¥çŸ¥é“è°è®¾ç½®äº†å¼•ç”¨çš„æœºåˆ¶ï¼Œ
å› æ­¤ StatefulSet æ§åˆ¶å™¨å¯èƒ½ä¼šä¸è®¾ç½®è‡ªå·±çš„å¼•ç”¨çš„è‡ªå®šä¹‰æ§åˆ¶å™¨å‘ç”Ÿå†²çªã€‚
å¹¸è¿çš„æ˜¯ï¼Œç»´æŠ¤ç°æœ‰çš„ä¿ç•™è¡Œä¸ºä¸æ¶‰åŠä»»ä½•æ–°å±ä¸»å¼•ç”¨ï¼Œå› æ­¤é»˜è®¤è¡Œä¸ºæ˜¯å…¼å®¹çš„ã€‚&lt;/p>
&lt;!--
Please tag any issues you report with the label `sig/apps` and assign them to Matthew Cary
([@mattcary](https://github.com/mattcary) at GitHub).
-->
&lt;p>è¯·ç”¨æ ‡ç­¾ &lt;code>sig/apps&lt;/code> æ ‡è®°ä½ æŠ¥å‘Šçš„ä»»ä½•é—®é¢˜ï¼Œå¹¶å°†å®ƒä»¬åˆ†é…ç»™ Matthew Cary
(åœ¨ GitHubä¸Š &lt;a href="https://github.com/mattcary">@mattcary&lt;/a>)ã€‚&lt;/p>
&lt;!--
Enjoy!
-->
&lt;p>å°½æƒ…ä½“éªŒå§ï¼&lt;/p></description></item><item><title>Blog: Kubernetes 1.23ï¼šæ ‘å†…å­˜å‚¨å‘ CSI å·è¿ç§»å·¥ä½œçš„è¿›å±•æ›´æ–°</title><link>https://kubernetes.io/zh/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/</guid><description>
&lt;!---
layout: blog
title: "Kubernetes 1.23: Kubernetes In-Tree to CSI Volume Migration Status Update"
date: 2021-12-10
slug: storage-in-tree-to-csi-migration-status-update
-->
&lt;!---
**Author:** Jiawei Wang (Google)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Jiawei Wangï¼ˆè°·æ­Œï¼‰&lt;/p>
&lt;!---
The Kubernetes in-tree storage plugin to [Container Storage Interface (CSI)](/blog/2019/01/15/container-storage-interface-ga/) migration infrastructure has already been [beta](/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/) since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
-->
&lt;p>è‡ª Kubernetes v1.14 å¼•å…¥å®¹å™¨å­˜å‚¨æ¥å£ï¼ˆ&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface, CSI&lt;/a>ï¼‰çš„å·¥ä½œè¾¾åˆ° alpha é˜¶æ®µåï¼Œè‡ª v1.17 èµ·ï¼ŒKubernetes æ ‘å†…å­˜å‚¨æ’ä»¶ï¼ˆin-tree storage pluginï¼‰å‘ CSI çš„è¿ç§»åŸºç¡€è®¾æ–½å·²æ­¥å…¥ &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">beta é˜¶æ®µ&lt;/a>ã€‚&lt;/p>
&lt;!---
Since then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for GA.
This article is intended to give a status update to the feature as well as changes between Kubernetes 1.17 and 1.23. In addition, I will also cover the future roadmap for the CSI migration feature GA for each storage plugin.
-->
&lt;p>è‡ªé‚£æ—¶èµ·ï¼ŒKubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„ï¼ˆspecial interest groups, SIGï¼‰åŠå…¶ä»– Kubernetes ç‰¹åˆ«å…´è¶£ç»„å°±åœ¨åŠªåŠ›ç¡®ä¿è¿™ä¸€åŠŸèƒ½çš„ç¨³å®šæ€§å’Œå…¼å®¹æ€§ï¼Œä¸ºæ­£å¼å‘å¸ƒåšå‡†å¤‡ã€‚
æœ¬æ–‡æ—¨åœ¨ä»‹ç»è¯¥åŠŸèƒ½çš„æœ€æ–°å¼€å‘è¿›å±•ï¼Œä»¥åŠ Kubernetes v1.17 åˆ° v1.23 ä¹‹é—´çš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜å°†ä»‹ç»æ¯ä¸ªå­˜å‚¨æ’ä»¶çš„ CSI è¿ç§»åŠŸèƒ½è¾¾åˆ°æ­£å¼å‘å¸ƒé˜¶æ®µçš„æœªæ¥è·¯çº¿å›¾ã€‚&lt;/p>
&lt;!---
## Quick recap: What is CSI Migration, and why migrate?
The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins.
Kubernetes support for the [Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md#README) has been
[generally available](/blog/2019/01/15/container-storage-interface-ga/) since Kubernetes v1.13.
Support for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).
-->
&lt;h2 id="quick-recap-what-is-csi-migration-and-why-migrate">å¿«é€Ÿå›é¡¾ï¼šCSI è¿ç§»åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¦è¿ç§»ï¼Ÿ &lt;/h2>
&lt;p>å®¹å™¨å­˜å‚¨æ¥å£æ—¨åœ¨å¸®åŠ© Kubernetes å–ä»£å…¶ç°æœ‰çš„æ ‘å†…å­˜å‚¨é©±åŠ¨æœºåˆ¶â”€â”€ç‰¹åˆ«æ˜¯ä¾›åº”å•†çš„ç‰¹å®šæ’ä»¶ã€‚è‡ª v1.13 èµ·ï¼ŒKubernetes å¯¹&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#README">å®¹å™¨å­˜å‚¨æ¥å£&lt;/a>çš„æ”¯æŒå·¥ä½œå·²è¾¾åˆ°&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">æ­£å¼å‘å¸ƒé˜¶æ®µ&lt;/a>ã€‚å¼•å…¥å¯¹ CSI é©±åŠ¨çš„æ”¯æŒï¼Œå°†ä½¿å¾— Kubernetes å’Œå­˜å‚¨åç«¯æŠ€æœ¯ä¹‹é—´çš„é›†æˆå·¥ä½œæ›´æ˜“å»ºç«‹å’Œç»´æŠ¤ã€‚ä½¿ç”¨ CSI é©±åŠ¨å¯ä»¥å®ç°æ›´å¥½çš„å¯ç»´æŠ¤æ€§ï¼ˆé©±åŠ¨ä½œè€…å¯ä»¥å†³å®šè‡ªå·±çš„å‘å¸ƒå‘¨æœŸå’Œæ”¯æŒç”Ÿå‘½å‘¨æœŸï¼‰ã€å‡å°‘å‡ºç°æ¼æ´çš„æœºä¼šï¼ˆå¾—ç›Šäºæ›´å°‘çš„æ ‘å†…ä»£ç ï¼Œå‡ºç°é”™è¯¯çš„é£é™©ä¼šé™ä½ã€‚å¦å¤–ï¼Œé›†ç¾¤æ“ä½œå‘˜å¯ä»¥åªé€‰æ‹©é›†ç¾¤éœ€è¦çš„å­˜å‚¨é©±åŠ¨ï¼‰ã€‚&lt;/p>
&lt;!---
As more CSI Drivers were created and became production ready, SIG Storage group wanted all Kubernetes users to benefit from the CSI model. However, we cannot break API compatibility with the existing storage API types. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.
-->
&lt;p>éšç€æ›´å¤šçš„ CSI é©±åŠ¨è¯ç”Ÿå¹¶è¿›å…¥ç”Ÿäº§å°±ç»ªé˜¶æ®µï¼ŒKubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„å¸Œæœ›æ‰€æœ‰ Kubernetes ç”¨æˆ·éƒ½èƒ½ä» CSI æ¨¡å‹ä¸­å—ç›Šâ”€â”€ç„¶è€Œï¼Œæˆ‘ä»¬ä¸åº”ç ´åä¸ç°æœ‰å­˜å‚¨ API ç±»å‹çš„ API å…¼å®¹æ€§ã€‚å¯¹æ­¤ï¼Œæˆ‘ä»¬ç»™å‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯ CSI è¿ç§»ï¼šè¯¥åŠŸèƒ½å®ç°å°†æ ‘å†…å­˜å‚¨ API ç¿»è¯‘æˆç­‰æ•ˆçš„ CSI APIï¼Œå¹¶æŠŠæ“ä½œå§”æ‰˜ç»™ä¸€ä¸ªæ›¿æ¢çš„ CSI é©±åŠ¨æ¥å®Œæˆã€‚&lt;/p>
&lt;!---
The CSI migration effort enables the replacement of existing in-tree storage plugins such as `kubernetes.io/gce-pd` or `kubernetes.io/aws-ebs` with a corresponding [CSI driver](https://kubernetes-csi.github.io/docs/introduction.html) from the storage backend.
If CSI Migration is working properly, Kubernetes end users shouldnâ€™t notice a difference. Existing `StorageClass`, `PersistentVolume` and `PersistentVolumeClaim` objects should continue to work.
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have.
However, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.
-->
&lt;p>CSI è¿ç§»å·¥ä½œä½¿å­˜å‚¨åç«¯ç°æœ‰çš„æ ‘å†…å­˜å‚¨æ’ä»¶ï¼ˆå¦‚ &lt;code>kubernetes.io/gce-pd&lt;/code> æˆ– &lt;code>kubernetes.io/aws-ebs&lt;/code>ï¼‰èƒ½å¤Ÿè¢«ç›¸åº”çš„ &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI é©±åŠ¨&lt;/a> æ‰€å–ä»£ã€‚å¦‚æœ CSI è¿ç§»åŠŸèƒ½æ­£ç¡®å‘æŒ¥ä½œç”¨ï¼ŒKubernetes ç»ˆç«¯ç”¨æˆ·åº”è¯¥ä¸ä¼šæ³¨æ„åˆ°æœ‰ä»€ä¹ˆå˜åŒ–ã€‚ç°æœ‰çš„ &lt;code>StorageClass&lt;/code>ã€&lt;code>PersistentVolume&lt;/code> å’Œ &lt;code>PersistentVolumeClaim&lt;/code> å¯¹è±¡åº”ç»§ç»­å·¥ä½œã€‚å½“ Kubernetes é›†ç¾¤ç®¡ç†å‘˜æ›´æ–°é›†ç¾¤ä»¥å¯ç”¨ CSI è¿ç§»åŠŸèƒ½æ—¶ï¼Œåˆ©ç”¨åˆ° PVCs&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>ï¼ˆç”±æ ‘å†…å­˜å‚¨æ’ä»¶æ”¯æŒï¼‰çš„ç°æœ‰å·¥ä½œè´Ÿè½½å°†ç»§ç»­åƒä»¥å‰ä¸€æ ·è¿ä½œâ”€â”€ä¸è¿‡åœ¨å¹•åï¼ŒKubernetes å°†æ‰€æœ‰å­˜å‚¨ç®¡ç†æ“ä½œï¼ˆä»¥å‰é¢å‘æ ‘å†…å­˜å‚¨é©±åŠ¨çš„ï¼‰äº¤ç»™ CSI é©±åŠ¨æ§åˆ¶ã€‚&lt;/p>
&lt;!---
For example, suppose you are a `kubernetes.io/gce-pd` user, after CSI migration, you can still use `kubernetes.io/gce-pd` to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing API/Interface will still function correctly. However, the underlying function calls are all going through the [GCE PD CSI driver](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver) instead of the in-tree Kubernetes function.
-->
&lt;p>ä¸¾ä¸ªä¾‹å­ã€‚å‡è®¾ä½ æ˜¯ &lt;code>kubernetes.io/gce-pd&lt;/code> ç”¨æˆ·ï¼Œåœ¨å¯ç”¨ CSI è¿ç§»åŠŸèƒ½åï¼Œä½ ä»ç„¶å¯ä»¥ä½¿ç”¨ &lt;code>kubernetes.io/gce-pd&lt;/code> æ¥é…ç½®æ–°å·ã€æŒ‚è½½ç°æœ‰çš„ GCE-PD å·æˆ–åˆ é™¤ç°æœ‰å·ã€‚æ‰€æœ‰ç°æœ‰çš„ API/æ¥å£ ä»å°†æ­£å¸¸å·¥ä½œâ”€â”€åªä¸è¿‡ï¼Œåº•å±‚åŠŸèƒ½è°ƒç”¨éƒ½å°†é€šå‘ &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI é©±åŠ¨&lt;/a>ï¼Œè€Œä¸æ˜¯ Kubernetes çš„æ ‘å†…å­˜å‚¨åŠŸèƒ½ã€‚&lt;/p>
&lt;!---
This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.
-->
&lt;p>è¿™ä½¿å¾— Kubernetes ç»ˆç«¯ç”¨æˆ·å¯ä»¥é¡ºåˆ©è¿‡æ¸¡ã€‚å¦å¤–ï¼Œå¯¹äºå­˜å‚¨æ’ä»¶çš„å¼€å‘è€…ï¼Œæˆ‘ä»¬å¯ä»¥å‡å°‘ä»–ä»¬ç»´æŠ¤æ ‘å†…å­˜å‚¨æ’ä»¶çš„è´Ÿæ‹…ï¼Œå¹¶æœ€ç»ˆå°†è¿™äº›æ’ä»¶ä» Kubernetes æ ¸å¿ƒçš„äºŒè¿›åˆ¶ä¸­ç§»é™¤ã€‚&lt;/p>
&lt;!---
## What has been changed, and what's new?
Building on the work done in Kubernetes v1.17 and earlier, the releases since then have
made a series of changes:
-->
&lt;h2 id="what-has-been-changed-and-what-s-new">æ”¹è¿›ä¸æ›´æ–° &lt;/h2>
&lt;p>åœ¨ Kubernetes v1.17 åŠæ›´æ—©çš„å·¥ä½œåŸºç¡€ä¸Šï¼Œæ­¤åçš„å‘å¸ƒæœ‰äº†ä»¥ä¸‹ä¸€ç³»åˆ—æ”¹å˜ï¼š&lt;/p>
&lt;!---
### New feature gates
The Kubernetes v1.21 release deprecated the `CSIMigration{provider}Complete` feature flags, and stopped honoring them. In their place came new feature flags named `InTreePlugin{vendor}Unregister`, that replace the old feature flag and retain all the functionality that `CSIMigration{provider}Complete` provided.
-->
&lt;h3 id="new-feature-gates">æ–°çš„ç‰¹æ€§é—¨æ§ï¼ˆfeature gateï¼‰ &lt;/h3>
&lt;p>Kubernetes v1.21 å¼ƒç”¨äº† &lt;code>CSIMigration{provider}Complete&lt;/code> ç‰¹æ€§å‚æ•°ï¼ˆfeature flagï¼‰ï¼Œå®ƒä»¬ä¸å†ç”Ÿæ•ˆã€‚å–è€Œä»£ä¹‹çš„æ˜¯åä¸º &lt;code>InTreePlugin{vendor}Unregister&lt;/code> çš„æ–°ç‰¹æ€§å‚æ•°ï¼Œå®ƒä»¬ä¿ç•™äº† &lt;code>CSIMigration{provider}Complete&lt;/code> æä¾›çš„æ‰€æœ‰åŠŸèƒ½ã€‚&lt;/p>
&lt;!---
`CSIMigration{provider}Complete` was introduced before as a supplementary feature gate once CSI migration is enabled on all of the nodes. This flag unregisters the in-tree storage plugin you specify with the `{provider}` part of the flag name.
-->
&lt;p>&lt;code>CSIMigration{provider}Complete&lt;/code> æ˜¯ä½œä¸º CSI è¿ç§»åŠŸèƒ½åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šå¯ç”¨åçš„è¡¥å……ç‰¹æ€§é—¨æ§äºä¹‹å‰å¼•å…¥çš„ã€‚è¿™ä¸ªå‚æ•°å¯æ³¨é”€å‚æ•°åç§°ä¸­ &lt;code>{provider}&lt;/code> éƒ¨åˆ†æ‰€æŒ‡å®šçš„æ ‘å†…å­˜å‚¨æ’ä»¶ã€‚&lt;/p>
&lt;!---
When you enable that feature gate, then instead of using the in-tree driver code, your cluster directly selects and uses the relevant CSI driver. This happens without any check for whether CSI migration is enabled on the node, or whether you have in fact deployed that CSI driver.
-->
&lt;p>å½“ä½ å¯ç”¨è¯¥ç‰¹æ€§é—¨æ§æ—¶ï¼Œä½ çš„é›†ç¾¤ä¸å†ä½¿ç”¨æ ‘å†…é©±åŠ¨ä»£ç ï¼Œè€Œæ˜¯ç›´æ¥é€‰æ‹©å¹¶ä½¿ç”¨ç›¸åº”çš„ CSI é©±åŠ¨ã€‚åŒæ—¶ï¼Œé›†ç¾¤å¹¶ä¸æ£€æŸ¥èŠ‚ç‚¹ä¸Š CSI è¿ç§»åŠŸèƒ½æ˜¯å¦å¯ç”¨ï¼Œä»¥åŠ CSI é©±åŠ¨æ˜¯å¦å®é™…éƒ¨ç½²ã€‚&lt;/p>
&lt;!---
While this feature gate is a great helper, SIG Storage (and, I'm sure, lots of cluster operators) also wanted a feature gate that lets you disable an in-tree storage plugin, even without also enabling CSI migration. For example, you might want to disable the EBS storage plugin on a GCE cluster, because EBS volumes are specific to a different vendor's cloud (AWS).
-->
&lt;p>è™½ç„¶è¿™ä¸€ç‰¹æ€§é—¨æ§æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¸®æ‰‹ï¼Œä½† Kubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„ï¼ˆä»¥åŠï¼Œæˆ‘ç›¸ä¿¡è¿˜æœ‰å¾ˆå¤šé›†ç¾¤æ“ä½œå‘˜ï¼‰åŒæ ·å¸Œæœ›æœ‰ä¸€ä¸ªç‰¹æ€§é—¨æ§å¯ä»¥è®©ä½ å³ä½¿åœ¨ä¸å¯ç”¨ CSI è¿ç§»åŠŸèƒ½æ—¶ï¼Œä¹Ÿèƒ½ç¦ç”¨æ ‘å†…å­˜å‚¨æ’ä»¶ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½å¸Œæœ›åœ¨ä¸€ä¸ª GCE é›†ç¾¤ä¸Šç¦ç”¨ EBS å­˜å‚¨æ’ä»¶ï¼Œå› ä¸º EBS å·æ˜¯å…¶ä»–ä¾›åº”å•†çš„äº‘ï¼ˆAWSï¼‰æ‰€ä¸“æœ‰çš„ã€‚&lt;/p>
&lt;!---
To make this possible, Kubernetes v1.21 introduced a new feature flag set: `InTreePlugin{vendor}Unregister`.
`InTreePlugin{vendor}Unregister` is a standalone feature gate that can be enabled and disabled independently from CSI Migration. When enabled, the component will not register the specific in-tree storage plugin to the supported list. If the cluster operator only enables this flag, end users will get an error from PVC saying it cannot find the plugin when the plugin is used. The cluster operator may want to enable this regardless of CSI Migration if they do not want to support the legacy in-tree APIs and only support CSI moving forward.
-->
&lt;p>ä¸ºäº†ä½¿è¿™æˆä¸ºå¯èƒ½ï¼ŒKubernetes v1.21 å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç‰¹æ€§å‚æ•°é›†åˆï¼š&lt;code>InTreePlugin{vendor}Unregister&lt;/code>ã€‚&lt;/p>
&lt;p>&lt;code>InTreePlugin{vendor}Unregister&lt;/code> æ˜¯ä¸€ç§ç‰¹æ€§é—¨æ§ï¼Œå¯ä»¥ç‹¬ç«‹äº CSI è¿ç§»åŠŸèƒ½æ¥å¯ç”¨æˆ–ç¦ç”¨ã€‚å½“å¯ç”¨æ­¤ç§ç‰¹æ€§é—¨æ§æ—¶ï¼Œç»„ä»¶å°†ä¸ä¼šæŠŠç›¸åº”çš„æ ‘å†…å­˜å‚¨æ’ä»¶æ³¨å†Œåˆ°æ”¯æŒçš„åˆ—è¡¨ä¸­ã€‚å¦‚æœé›†ç¾¤æ“ä½œå‘˜åªå¯ç”¨äº†è¿™ç§å‚æ•°ï¼Œç»ˆç«¯ç”¨æˆ·å°†åœ¨ä½¿ç”¨è¯¥æ’ä»¶çš„ PVC&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> å¤„é‡åˆ°é”™è¯¯ï¼Œæç¤ºå…¶æ‰¾ä¸åˆ°æ’ä»¶ã€‚å¦‚æœé›†ç¾¤æ“ä½œå‘˜ä¸æƒ³æ”¯æŒè¿‡æ—¶çš„æ ‘å†…å­˜å‚¨ APIï¼Œåªæ”¯æŒ CSIï¼Œé‚£ä¹ˆä»–ä»¬å¯èƒ½å¸Œæœ›å¯ç”¨è¿™ç§ç‰¹æ€§é—¨æ§è€Œä¸è€ƒè™‘ CSI è¿ç§»åŠŸèƒ½ã€‚&lt;/p>
&lt;!---
### Observability
Kubernetes v1.21 introduced [metrics](https://github.com/kubernetes/kubernetes/issues/98279) for tracking CSI migration.
You can use these metrics to observe how your cluster is using storage services and whether access to that storage is using the legacy in-tree driver or its CSI-based replacement.
-->
&lt;h3 id="observability">å¯è§‚å¯Ÿæ€§ &lt;/h3>
&lt;p>Kubernetes v1.21 å¼•å…¥äº†è·Ÿè¸ª CSI è¿ç§»åŠŸèƒ½çš„&lt;a href="https://github.com/kubernetes/kubernetes/issues/98279">æŒ‡æ ‡&lt;/a>ã€‚ä½ å¯ä»¥ä½¿ç”¨è¿™äº›æŒ‡æ ‡æ¥è§‚å¯Ÿä½ çš„é›†ç¾¤æ˜¯å¦‚ä½•ä½¿ç”¨å­˜å‚¨æœåŠ¡çš„ï¼Œä»¥åŠå¯¹è¯¥å­˜å‚¨çš„è®¿é—®ä½¿ç”¨çš„æ˜¯ä¼ ç»Ÿçš„æ ‘å†…é©±åŠ¨è¿˜æ˜¯åŸºäº CSI çš„æ›¿ä»£ã€‚&lt;/p>
&lt;!---
| Components | Metrics | Notes |
| -------------------------------------------- | ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Kube-Controller-Manager | storage_operation_duration_seconds | A new label `migrated` is added to the metric to indicate whether this storage operation is a CSI migration operation(string value `true` for enabled and `false` for not enabled). |
| Kubelet | csi_operations_seconds | The new metric exposes labels including `driver_name`, `method_name`, `grpc_status_code` and `migrated`. The meaning of these labels is identical to `csi_sidecar_operations_seconds`. |
| CSI Sidecars(provisioner, attacher, resizer) | csi_sidecar_operations_seconds | A new label `migrated` is added to the metric to indicate whether this storage operation is a CSI migration operation(string value `true` for enabled and `false` for not enabled). |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ç»„ä»¶&lt;/th>
&lt;th>æŒ‡æ ‡&lt;/th>
&lt;th>æ³¨é‡Š&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kube-Controller-Manager&lt;/td>
&lt;td>storage_operation_duration_seconds&lt;/td>
&lt;td>ä¸€ä¸ªæ–°çš„æ ‡ç­¾ &lt;code>migrated&lt;/code> è¢«æ·»åŠ åˆ°æŒ‡æ ‡ä¸­ï¼Œä»¥è¡¨æ˜è¯¥å­˜å‚¨æ“ä½œæ˜¯å¦ç”± CSI è¿ç§»åŠŸèƒ½æ“ä½œï¼ˆå­—ç¬¦ä¸²å€¼ä¸º &lt;code>true&lt;/code> è¡¨ç¤ºå¯ç”¨ï¼Œ&lt;code>false&lt;/code> è¡¨ç¤ºæœªå¯ç”¨ï¼‰ã€‚&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kubelet&lt;/td>
&lt;td>csi_operations_seconds&lt;/td>
&lt;td>æ–°çš„æŒ‡æ ‡æä¾›çš„æ ‡ç­¾åŒ…æ‹¬ &lt;code>driver_name&lt;/code>ã€&lt;code>method_name&lt;/code>ã€&lt;code>grpc_status_code&lt;/code> å’Œ &lt;code>migrated&lt;/code>ã€‚è¿™äº›æ ‡ç­¾çš„å«ä¹‰ä¸ &lt;code>csi_sidecar_operations_seconds&lt;/code> ç›¸åŒã€‚&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSI Sidecars(provisioner, attacher, resizer)&lt;/td>
&lt;td>csi_sidecar_operations_seconds&lt;/td>
&lt;td>ä¸€ä¸ªæ–°çš„æ ‡ç­¾ &lt;code>migrated&lt;/code> è¢«æ·»åŠ åˆ°æŒ‡æ ‡ä¸­ï¼Œä»¥è¡¨æ˜è¯¥å­˜å‚¨æ“ä½œæ˜¯å¦ç”± CSI è¿ç§»åŠŸèƒ½æ“ä½œï¼ˆå­—ç¬¦ä¸²å€¼ä¸º &lt;code>true&lt;/code> è¡¨ç¤ºå¯ç”¨ï¼Œ&lt;code>false&lt;/code> è¡¨ç¤ºæœªå¯ç”¨ï¼‰ã€‚&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
### Bug fixes and feature improvement
We have fixed numerous bugs like dangling attachment, garbage collection, incorrect topology label through the help of our beta testers.
-->
&lt;h3 id="bug-fixes-and-feature-improvement">é”™è¯¯ä¿®å¤å’ŒåŠŸèƒ½æ”¹è¿› &lt;/h3>
&lt;p>ç±ç”± beta æµ‹è¯•äººå‘˜çš„å¸®åŠ©ï¼Œæˆ‘ä»¬ä¿®å¤äº†è®¸å¤šé”™è¯¯â”€â”€å¦‚æ‚¬ç©ºé™„ä»¶ã€åƒåœ¾æ”¶é›†ã€æ‹“æ‰‘æ ‡ç­¾é”™è¯¯ç­‰ã€‚&lt;/p>
&lt;!---
### Cloud Provider &amp;&amp; Cluster Lifecycle Collaboration
SIG Storage has been working closely with SIG Cloud Provider and SIG Cluster Lifecycle on the rollout of CSI migration.
If you are a user of a managed Kubernetes service, check with your provider if anything needs to be done. In many cases, the provider will manage the migration and no additional work is required.
-->
&lt;h3 id="cloud-provider-cluster-lifecycle-collaboration">ä¸ Kubernetes äº‘æä¾›å•†ç‰¹åˆ«å…´è¶£ç»„ã€é›†ç¾¤ç”Ÿå‘½å‘¨æœŸç‰¹åˆ«å…´è¶£ç»„çš„åˆä½œ &lt;/h3>
&lt;p>Kubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„ä¸äº‘æä¾›å•†ç‰¹åˆ«å…´è¶£ç»„å’Œé›†ç¾¤ç”Ÿå‘½å‘¨æœŸç‰¹åˆ«å…´è¶£ç»„ï¼Œæ­£ä¸ºäº† CSI è¿ç§»åŠŸèƒ½ä¸Šçº¿è€Œå¯†åˆ‡åˆä½œã€‚&lt;/p>
&lt;p>å¦‚æœä½ é‡‡ç”¨æ‰˜ç®¡ Kubernetes æœåŠ¡ï¼Œè¯·è¯¢é—®ä½ çš„ä¾›åº”å•†æ˜¯å¦æœ‰ä»€ä¹ˆå·¥ä½œéœ€è¦å®Œæˆã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œä¾›åº”å•†å°†ç®¡ç†è¿ç§»ï¼Œä½ ä¸éœ€è¦åšé¢å¤–çš„å·¥ä½œã€‚&lt;/p>
&lt;!---
If you use a distribution of Kubernetes, check its official documentation for information about support for this feature. For the CSI Migration feature graduation to GA, SIG Storage and SIG Cluster Lifecycle are collaborating towards making the migration mechanisms available in tooling (such as kubeadm) as soon as they're available in Kubernetes itself.
-->
&lt;p>å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Kubernetes çš„å‘è¡Œç‰ˆï¼Œè¯·æŸ¥çœ‹å…¶å®˜æ–¹æ–‡æ¡£ï¼Œäº†è§£å¯¹è¯¥åŠŸèƒ½çš„æ”¯æŒæƒ…å†µã€‚å¯¹äºå·²è¿›å…¥æ­£å¼å‘å¸ƒé˜¶æ®µçš„ CSI è¿ç§»åŠŸèƒ½ï¼ŒKubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„æ­£ä¸Kubernetes é›†ç¾¤ç”Ÿå‘½å‘¨æœŸç‰¹åˆ«å…´è¶£ç»„åˆä½œï¼Œä»¥ä¾¿åœ¨è¿™äº›åŠŸèƒ½äº Kubernetes ä¸­å¯ç”¨æ—¶ï¼Œä½¿è¿ç§»æœºåˆ¶ä¹Ÿè¿›å…¥åˆ°å‘¨è¾¹å·¥å…·ï¼ˆå¦‚ kubeadmï¼‰ä¸­ã€‚&lt;/p>
&lt;!---
## What is the timeline / status? {#timeline-and-status}
The current and targeted releases for each individual driver is shown in the table below:
-->
&lt;h2 id="timeline-and-status">æ—¶é—´è®¡åˆ’åŠå½“å‰çŠ¶æ€ &lt;/h2>
&lt;p>å„é©±åŠ¨çš„å½“å‰å‘å¸ƒåŠç›®æ ‡å‘å¸ƒå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š&lt;/p>
&lt;!---
| Driver | Alpha | Beta (in-tree deprecated) | Beta (on-by-default) | GA | Target "in-tree plugin" removal |
| ---------------- | ----- | ------------------------- | -------------------- | ------------- | ------------------------------- |
| AWS EBS | 1.14 | 1.17 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| GCE PD | 1.14 | 1.17 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| OpenStack Cinder | 1.14 | 1.18 | 1.21 | 1.24 (Target) | 1.26 (Target) |
| Azure Disk | 1.15 | 1.19 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| Azure File | 1.15 | 1.21 | 1.24 (Target) | 1.25 (Target) | 1.27 (Target) |
| vSphere | 1.18 | 1.19 | 1.24 (Target) | 1.25 (Target) | 1.27 (Target) |
| Ceph RBD | 1.23 |
| Portworx | 1.23 |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>é©±åŠ¨&lt;/th>
&lt;th>Alpha&lt;/th>
&lt;th>Betaï¼ˆå¯ç”¨æ ‘å†…æ’ä»¶ï¼‰&lt;/th>
&lt;th>Betaï¼ˆé»˜è®¤å¯ç”¨ï¼‰&lt;/th>
&lt;th>æ­£å¼å‘å¸ƒ&lt;/th>
&lt;th>ç›®æ ‡ï¼šç§»é™¤â€œæ ‘å†…å­˜å‚¨æ’ä»¶â€&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AWS EBS&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GCE PD&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenStack Cinder&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure Disk&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure File&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vSphere&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ceph RBD&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Portworx&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
The following storage drivers will not have CSI migration support. The ScaleIO driver was already removed; the others are deprecated and will be removed from core Kubernetes.
-->
&lt;p>ä»¥ä¸‹å­˜å‚¨é©±åŠ¨å°†ä¸ä¼šæ”¯æŒ CSI è¿ç§»åŠŸèƒ½ã€‚å…¶ä¸­ ScaleIO é©±åŠ¨å·²ç»è¢«ç§»é™¤ï¼›å…¶ä»–é©±åŠ¨éƒ½è¢«å¼ƒç”¨ï¼Œå¹¶å°†ä» Kubernetes æ ¸å¿ƒä¸­åˆ é™¤ã€‚&lt;/p>
&lt;!---
| Driver | Deprecated | Code Removal |
| --------- | ---------- | ------------- |
| ScaleIO | 1.16 | 1.22 |
| Flocker | 1.22 | 1.25 (Target) |
| Quobyte | 1.22 | 1.25 (Target) |
| StorageOS | 1.22 | 1.25 (Target) |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>é©±åŠ¨&lt;/th>
&lt;th>è¢«å¼ƒç”¨&lt;/th>
&lt;th>ä»£ç ç§»é™¤&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ScaleIO&lt;/td>
&lt;td>1.16&lt;/td>
&lt;td>1.22&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Flocker&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Quobyte&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>StorageOS&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
## What's next?
With more CSI drivers graduating to GA, we hope to soon mark the overall CSI Migration feature as GA. We are expecting cloud provider in-tree storage plugins code removal to happen by Kubernetes v1.26 and v1.27.
-->
&lt;h2 id="what-s-next">ä¸‹ä¸€æ­¥çš„è®¡åˆ’ &lt;/h2>
&lt;p>éšç€æ›´å¤šçš„ CSI é©±åŠ¨è¿›å…¥æ­£å¼å‘å¸ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¿«å°†æ•´ä¸ª CSI è¿ç§»åŠŸèƒ½æ ‡è®°ä¸ºæ­£å¼å‘å¸ƒçŠ¶æ€ã€‚æˆ‘ä»¬è®¡åˆ’åœ¨ Kubernetes v1.26 å’Œ v1.27 ä¹‹å‰ç§»é™¤äº‘æä¾›å•†æ ‘å†…å­˜å‚¨æ’ä»¶çš„ä»£ç ã€‚&lt;/p>
&lt;!---
## What should I do as a user?
Note that all new features for the Kubernetes storage system (such as volume snapshotting) will only be added to the CSI interface. Therefore, if you are starting up a new cluster, creating stateful applications for the first time, or require these new features we recommend using CSI drivers natively (instead of the in-tree volume plugin API). Follow the [updated user guides for CSI drivers](https://kubernetes-csi.github.io/docs/drivers.html) and use the new CSI APIs.
-->
&lt;h2 id="what-should-i-do-as-a-user">ä½œä¸ºç”¨æˆ·ï¼Œæˆ‘åº”è¯¥åšä»€ä¹ˆï¼Ÿ &lt;/h2>
&lt;p>è¯·æ³¨æ„ï¼ŒKubernetes å­˜å‚¨ç³»ç»Ÿçš„æ‰€æœ‰æ–°åŠŸèƒ½ï¼ˆå¦‚å·å¿«ç…§ï¼‰å°†åªè¢«æ·»åŠ åˆ° CSI æ¥å£ã€‚å› æ­¤ï¼Œå¦‚æœä½ æ­£åœ¨å¯åŠ¨ä¸€ä¸ªæ–°çš„é›†ç¾¤ã€é¦–æ¬¡åˆ›å»ºæœ‰çŠ¶æ€çš„åº”ç”¨ç¨‹åºï¼Œæˆ–è€…éœ€è¦è¿™äº›æ–°åŠŸèƒ½ï¼Œæˆ‘ä»¬å»ºè®®ä½ åœ¨æœ¬åœ°ä½¿ç”¨ CSI é©±åŠ¨ï¼ˆè€Œä¸æ˜¯æ ‘å†…å·æ’ä»¶ APIï¼‰ã€‚éµå¾ª&lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">æœ€æ–°çš„ CSI é©±åŠ¨ç”¨æˆ·æŒ‡å—&lt;/a>å¹¶ä½¿ç”¨æ–°çš„ CSI APIã€‚&lt;/p>
&lt;!---
However, if you choose to roll a cluster forward or continue using specifications with the legacy volume APIs, CSI Migration will ensure we continue to support those deployments with the new CSI drivers. However, if you want to leverage new features like snapshot, it will require a manual migration to re-import an existing intree PV as a CSI PV.
-->
&lt;p>ç„¶è€Œï¼Œå¦‚æœæ‚¨é€‰æ‹©æ²¿ç”¨ç°æœ‰é›†ç¾¤æˆ–ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿå· API çš„è§„çº¦ï¼ŒCSI è¿ç§»åŠŸèƒ½å°†ç¡®ä¿æˆ‘ä»¬é€šè¿‡æ–° CSI é©±åŠ¨ç»§ç»­æ”¯æŒè¿™äº›éƒ¨ç½²ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åˆ©ç”¨å¿«ç…§ç­‰æ–°åŠŸèƒ½ï¼Œåˆ™éœ€è¦è¿›è¡Œæ‰‹åŠ¨è¿ç§»ï¼Œå°†ç°æœ‰çš„æ ‘å†…æŒä¹…å·é‡æ–°å¯¼å…¥ä¸º CSI æŒä¹…å·ã€‚&lt;/p>
&lt;!---
## How do I get involved?
The Kubernetes Slack channel [#csi-migration](https://kubernetes.slack.com/messages/csi-migration) along with any of the standard [SIG Storage communication channels](https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact) are great mediums to reach out to the SIG Storage and migration working group teams.
-->
&lt;h2 id="how-do-i-get-involved">æˆ‘å¦‚ä½•å‚ä¸å…¶ä¸­ï¼Ÿ &lt;/h2>
&lt;p>Kubernetes Slack é¢‘é“ &lt;a href="https://kubernetes.slack.com/messages/csi-migration">#csi-migration&lt;/a> ä»¥åŠä»»ä½•ä¸€ä¸ªæ ‡å‡†çš„ &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage é€šä¿¡é¢‘é“&lt;/a>éƒ½æ˜¯ä¸ Kubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„å’Œè¿ç§»å·¥ä½œç»„å›¢é˜Ÿè”ç³»çš„ç»ä½³åª’ä»‹ã€‚&lt;/p>
&lt;!---
This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:
* Michelle Au (msau42)
* Jan Å afrÃ¡nek (jsafrane)
* Hemant Kumar (gnufied)
-->
&lt;p>è¯¥é¡¹ç›®ï¼Œå’Œå…¶ä»–æ‰€æœ‰ Kubernetes é¡¹ç›®ä¸€æ ·ï¼Œæ˜¯è®¸å¤šæ¥è‡ªä¸åŒèƒŒæ™¯çš„è´¡çŒ®è€…å…±åŒåŠªåŠ›çš„ç»“æœã€‚æˆ‘ä»¬éå¸¸æ„Ÿè°¢åœ¨è¿‡å»å‡ ä¸ªå­£åº¦é‡ŒæŒºèº«è€Œå‡ºå¸®åŠ©æ¨åŠ¨é¡¹ç›®å‘å±•çš„è´¡çŒ®è€…ä»¬ï¼š&lt;/p>
&lt;ul>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;li>Jan Å afrÃ¡nek (jsafrane)&lt;/li>
&lt;li>Hemant Kumar (gnufied)&lt;/li>
&lt;/ul>
&lt;!---
Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:
* Andy Zhang (andyzhangz)
* Divyen Patel (divyenpatel)
* Deep Debroy (ddebroy)
* Humble Devassy Chirammal (humblec)
* Jing Xu (jingxu97)
* Jordan Liggitt (liggitt)
* Matthew Cary (mattcary)
* Matthew Wong (wongma7)
* Neha Arora (nearora-msft)
* Oksana Naumov (trierra)
* Saad Ali (saad-ali)
* Tim Bannister (sftim)
* Xing Yang (xing-yang)
-->
&lt;p>ç‰¹åˆ«æ„Ÿè°¢ä»¥ä¸‹äººå£«å¯¹ CSI è¿ç§»åŠŸèƒ½çš„ç²¾è¾Ÿè¯„è®ºã€å…¨é¢è€ƒè™‘å’Œå®è´µè´¡çŒ®ï¼š&lt;/p>
&lt;ul>
&lt;li>Andy Zhang (andyzhangz)&lt;/li>
&lt;li>Divyen Patel (divyenpatel)&lt;/li>
&lt;li>Deep Debroy (ddebroy)&lt;/li>
&lt;li>Humble Devassy Chirammal (humblec)&lt;/li>
&lt;li>Jing Xu (jingxu97)&lt;/li>
&lt;li>Jordan Liggitt (liggitt)&lt;/li>
&lt;li>Matthew Cary (mattcary)&lt;/li>
&lt;li>Matthew Wong (wongma7)&lt;/li>
&lt;li>Neha Arora (nearora-msft)&lt;/li>
&lt;li>Oksana Naumov (trierra)&lt;/li>
&lt;li>Saad Ali (saad-ali)&lt;/li>
&lt;li>Tim Bannister (sftim)&lt;/li>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;/ul>
&lt;!---
Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the [Kubernetes Storage Special Interest Group (SIG)](https://github.com/kubernetes/community/tree/master/sig-storage). Weâ€™re rapidly growing and always welcome new contributors.
-->
&lt;p>æœ‰å…´è¶£å‚ä¸ CSI æˆ– Kubernetes å­˜å‚¨ç³»ç»Ÿä»»ä½•éƒ¨åˆ†çš„è®¾è®¡å’Œå¼€å‘çš„äººï¼Œè¯·åŠ å…¥ &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„&lt;/a>ã€‚æˆ‘ä»¬æ­£åœ¨è¿…é€Ÿæˆé•¿ï¼Œå¹¶ä¸€ç›´æ¬¢è¿æ–°çš„è´¡çŒ®è€…ã€‚&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>æŒä¹…å·ç”³é¢†ï¼ˆPersistentVolumeClaimï¼ŒPVCï¼‰&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Blog: Kubernetes 1.23ï¼šIPv4/IPv6 åŒåè®®æ ˆç½‘ç»œè¾¾åˆ° GA</title><link>https://kubernetes.io/zh/blog/2021/12/08/dual-stack-networking-ga/</link><pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/12/08/dual-stack-networking-ga/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.23: Dual-stack IPv4/IPv6 Networking Reaches GA'
date: 2021-12-08
slug: dual-stack-networking-ga
-->
&lt;!--
**Author:** Bridget Kromhout (Microsoft)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> Bridget Kromhout (å¾®è½¯)&lt;/p>
&lt;!--
"When will Kubernetes have IPv6?" This question has been asked with increasing frequency ever since alpha support for IPv6 was first added in k8s v1.9. While Kubernetes has supported IPv6-only clusters since v1.18, migration from IPv4 to IPv6 was not yet possible at that point. At long last, [dual-stack IPv4/IPv6 networking](https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack/) has reached general availability (GA) in Kubernetes v1.23.
What does dual-stack networking mean for you? Letâ€™s take a lookâ€¦
-->
&lt;p>â€œKubernetes ä½•æ—¶æ”¯æŒ IPv6ï¼Ÿâ€ è‡ªä» k8s v1.9 ç‰ˆæœ¬ä¸­é¦–æ¬¡æ·»åŠ å¯¹ IPv6 çš„ alpha æ”¯æŒä»¥æ¥ï¼Œè¿™ä¸ªé—®é¢˜çš„è®¨è®ºè¶Šæ¥è¶Šé¢‘ç¹ã€‚
è™½ç„¶ Kubernetes ä» v1.18 ç‰ˆæœ¬å¼€å§‹å°±æ”¯æŒçº¯ IPv6 é›†ç¾¤ï¼Œä½†å½“æ—¶è¿˜æ— æ³•æ”¯æŒ IPv4 è¿ç§»åˆ° IPv6ã€‚
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack/">IPv4/IPv6 åŒåè®®æ ˆç½‘ç»œ&lt;/a>
åœ¨ Kubernetes v1.23 ç‰ˆæœ¬ä¸­è¿›å…¥æ­£å¼å‘å¸ƒï¼ˆGAï¼‰é˜¶æ®µã€‚&lt;/p>
&lt;p>è®©æˆ‘ä»¬æ¥çœ‹çœ‹åŒåè®®æ ˆç½‘ç»œå¯¹ä½ æ¥è¯´æ„å‘³ç€ä»€ä¹ˆï¼Ÿ&lt;/p>
&lt;!--
## Service API updates
-->
&lt;h2 id="æ›´æ–°-service-api">æ›´æ–° Service API&lt;/h2>
&lt;!--
[Services](/docs/concepts/services-networking/service/) were single-stack before 1.20, so using both IP families meant creating one Service per IP family. The user experience was simplified in 1.20, when Services were re-implemented to allow both IP families, meaning a single Service can handle both IPv4 and IPv6 workloads. Dual-stack load balancing is possible between services running any combination of IPv4 and IPv6.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/">Services&lt;/a> åœ¨ 1.20 ç‰ˆæœ¬ä¹‹å‰æ˜¯å•åè®®æ ˆçš„ï¼Œ
å› æ­¤ï¼Œä½¿ç”¨ä¸¤ä¸ª IP åè®®æ—æ„å‘³ç€éœ€ä¸ºæ¯ä¸ª IP åè®®æ—åˆ›å»ºä¸€ä¸ª Serviceã€‚åœ¨ 1.20 ç‰ˆæœ¬ä¸­å¯¹ç”¨æˆ·ä½“éªŒè¿›è¡Œç®€åŒ–ï¼Œ
é‡æ–°å®ç°äº† Service ä»¥æ”¯æŒä¸¤ä¸ª IP åè®®æ—ï¼Œè¿™æ„å‘³ç€ä¸€ä¸ª Service å°±å¯ä»¥å¤„ç† IPv4 å’Œ IPv6 åè®®ã€‚
å¯¹äº Service è€Œè¨€ï¼Œä»»æ„çš„ IPv4 å’Œ IPv6 åè®®ç»„åˆéƒ½å¯ä»¥å®ç°è´Ÿè½½å‡è¡¡ã€‚&lt;/p>
&lt;!--
The Service API now has new fields to support dual-stack, replacing the single ipFamily field.
* You can select your choice of IP family by setting `ipFamilyPolicy` to one of three options: SingleStack, PreferDualStack, or RequireDualStack. A service can be changed between single-stack and dual-stack (within some limits).
* Setting `ipFamilies` to a list of families assigned allows you to set the order of families used.
* `clusterIPs` is inclusive of the previous `clusterIP` but allows for multiple entries, so itâ€™s no longer necessary to run duplicate services, one in each of the two IP families. Instead, you can assign cluster IP addresses in both IP families.
-->
&lt;p>Service API ç°åœ¨æœ‰äº†æ”¯æŒåŒåè®®æ ˆçš„æ–°å­—æ®µï¼Œå–ä»£äº†å•ä¸€çš„ ipFamily å­—æ®µã€‚&lt;/p>
&lt;ul>
&lt;li>ä½ å¯ä»¥é€šè¿‡å°† &lt;code>ipFamilyPolicy&lt;/code> å­—æ®µè®¾ç½®ä¸º &lt;code>SingleStack&lt;/code>ã€&lt;code>PreferDualStack&lt;/code> æˆ–
&lt;code>RequireDualStack&lt;/code> æ¥è®¾ç½® IP åè®®æ—ã€‚Service å¯ä»¥åœ¨å•åè®®æ ˆå’ŒåŒåè®®æ ˆä¹‹é—´è¿›è¡Œè½¬æ¢(åœ¨æŸäº›é™åˆ¶å†…)ã€‚&lt;/li>
&lt;li>è®¾ç½® &lt;code>ipFamilies&lt;/code> ä¸ºæŒ‡å®šçš„åè®®æ—åˆ—è¡¨ï¼Œå¯ç”¨æ¥è®¾ç½®ä½¿ç”¨åè®®æ—çš„é¡ºåºã€‚&lt;/li>
&lt;li>'clusterIPs' çš„èƒ½åŠ›åœ¨æ¶µç›–äº†ä¹‹å‰çš„ 'clusterIP'çš„æƒ…å†µä¸‹ï¼Œè¿˜å…è®¸è®¾ç½®å¤šä¸ª IP åœ°å€ã€‚
æ‰€ä»¥ä¸å†éœ€è¦è¿è¡Œé‡å¤çš„ Serviceï¼Œåœ¨ä¸¤ä¸ª IP åè®®æ—ä¸­å„è¿è¡Œä¸€ä¸ªã€‚ä½ å¯ä»¥åœ¨ä¸¤ä¸ª IP åè®®æ—ä¸­åˆ†é…é›†ç¾¤ IP åœ°å€ã€‚&lt;/li>
&lt;/ul>
&lt;!--
Note that Pods are also dual-stack. For a given pod, there is no possibility of setting multiple IP addresses in the same family.
-->
&lt;p>è¯·æ³¨æ„ï¼ŒPods ä¹Ÿæ˜¯åŒåè®®æ ˆçš„ã€‚å¯¹äºä¸€ä¸ªç»™å®šçš„ Podï¼Œä¸å¯èƒ½åœ¨åŒä¸€åè®®æ—ä¸­è®¾ç½®å¤šä¸ª IP åœ°å€ã€‚&lt;/p>
&lt;!--
## Default behavior remains single-stack
-->
&lt;h2 id="é»˜è®¤è¡Œä¸ºä»ç„¶æ˜¯å•åè®®æ ˆ">é»˜è®¤è¡Œä¸ºä»ç„¶æ˜¯å•åè®®æ ˆ&lt;/h2>
&lt;!--
Starting in 1.20 with the re-implementation of dual-stack services as alpha, the underlying networking for Kubernetes has included dual-stack whether or not a cluster was configured with the feature flag to enable dual-stack.
-->
&lt;p>ä» 1.20 ç‰ˆæœ¬å¼€å§‹ï¼Œé‡æ–°å®ç°çš„åŒåè®®æ ˆæœåŠ¡å¤„äº Alpha é˜¶æ®µï¼Œæ— è®ºé›†ç¾¤æ˜¯å¦é…ç½®äº†å¯ç”¨åŒåè®®æ ˆçš„ç‰¹æ€§æ ‡å¿—ï¼Œ
Kubernetes çš„åº•å±‚ç½‘ç»œéƒ½å·²ç»åŒ…æ‹¬äº†åŒåè®®æ ˆã€‚&lt;/p>
&lt;!--
Kubernetes 1.23 removed that feature flag as part of graduating the feature to stable. Dual-stack networking is always available if you want to configure it. You can set your cluster network to operate as single-stack IPv4, as single-stack IPv6, or as dual-stack IPv4/IPv6.
-->
&lt;p>Kubernetes 1.23 åˆ é™¤äº†è¿™ä¸ªç‰¹æ€§æ ‡å¿—ï¼Œè¯´æ˜è¯¥ç‰¹æ€§å·²ç»ç¨³å®šã€‚
å¦‚æœä½ æƒ³è¦é…ç½®åŒåè®®æ ˆç½‘ç»œï¼Œè¿™ä¸€èƒ½åŠ›æ€»æ˜¯å­˜åœ¨çš„ã€‚
ä½ å¯ä»¥å°†é›†ç¾¤ç½‘ç»œè®¾ç½®ä¸º IPv4 å•åè®®æ ˆ ã€IPv6 å•åè®®æ ˆæˆ– IPV4/IPV6 åŒåè®®æ ˆ ã€‚&lt;/p>
&lt;!--
While Services are set according to what you configure, Pods default to whatever the CNI plugin sets. If your CNI plugin assigns single-stack IPs, you will have single-stack unless `ipFamilyPolicy` specifies PreferDualStack or RequireDualStack. If your CNI plugin assigns dual-stack IPs, `pod.status.PodIPs` defaults to dual-stack.
-->
&lt;p>è™½ç„¶ Service æ˜¯æ ¹æ®ä½ çš„é…ç½®è®¾ç½®çš„ï¼Œä½† Pod é»˜è®¤æ˜¯ç”± CNI æ’ä»¶è®¾ç½®çš„ã€‚
å¦‚æœä½ çš„ CNI æ’ä»¶åˆ†é…å•åè®®æ ˆ IPï¼Œé‚£ä¹ˆå°±æ˜¯å•åè®®æ ˆï¼Œé™¤é &lt;code>ipFamilyPolicy&lt;/code> è®¾ç½®ä¸º &lt;code>PreferDualStack&lt;/code> æˆ– &lt;code>RequireDualStack&lt;/code>ã€‚
å¦‚æœä½ çš„ CNI æ’ä»¶åˆ†é…åŒåè®®æ ˆ IPï¼Œåˆ™ &lt;code>pod.status.PodIPs&lt;/code> é»˜è®¤ä¸ºåŒåè®®æ ˆã€‚&lt;/p>
&lt;!--
Even though dual-stack is possible, it is not mandatory to use it. Examples in the documentation show the variety possible in [dual-stack service configurations](/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios).
-->
&lt;p>å°½ç®¡åŒåè®®æ ˆæ˜¯å¯ç”¨çš„ï¼Œä½†å¹¶ä¸å¼ºåˆ¶ä½ ä½¿ç”¨å®ƒã€‚
åœ¨&lt;a href="https://kubernetes.io/zh/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios">åŒåè®®æ ˆæœåŠ¡é…ç½®&lt;/a>
æ–‡æ¡£ä¸­çš„ç¤ºä¾‹åˆ—å‡ºäº†å¯èƒ½å‡ºç°çš„å„ç§åœºæ™¯.&lt;/p>
&lt;!--
## Try dual-stack right now
-->
&lt;h2 id="ç°åœ¨å°è¯•åŒåè®®æ ˆ">ç°åœ¨å°è¯•åŒåè®®æ ˆ&lt;/h2>
&lt;!--
While upstream Kubernetes now supports [dual-stack networking](/docs/concepts/services-networking/dual-stack/) as a GA or stable feature, each providerâ€™s support of dual-stack Kubernetes may vary. Nodes need to be provisioned with routable IPv4/IPv6 network interfaces. Pods need to be dual-stack. The [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) is what assigns the IP addresses to the Pods, so it's the network plugin being used for the cluster that needs to support dual-stack. Some Container Network Interface (CNI) plugins support dual-stack, as does kubenet.
-->
&lt;p>è™½ç„¶ç°åœ¨ä¸Šæ¸¸ Kubernetes æ”¯æŒ&lt;a href="https://kubernetes.io/zh/docs/concepts/services-networking/dual-stack/">åŒåè®®æ ˆç½‘ç»œ&lt;/a>
ä½œä¸º GA æˆ–ç¨³å®šç‰¹æ€§ï¼Œä½†æ¯ä¸ªæä¾›å•†å¯¹åŒåè®®æ ˆ Kubernetes çš„æ”¯æŒå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚èŠ‚ç‚¹éœ€è¦æä¾›å¯è·¯ç”±çš„ IPv4/IPv6 ç½‘ç»œæ¥å£ã€‚
Pod éœ€è¦æ˜¯åŒåè®®æ ˆçš„ã€‚&lt;a href="https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">ç½‘ç»œæ’ä»¶&lt;/a>
æ˜¯ç”¨æ¥ä¸º Pod åˆ†é… IP åœ°å€çš„ï¼Œæ‰€ä»¥é›†ç¾¤éœ€è¦æ”¯æŒåŒåè®®æ ˆçš„ç½‘ç»œæ’ä»¶ã€‚ä¸€äº›å®¹å™¨ç½‘ç»œæ¥å£ï¼ˆCNIï¼‰æ’ä»¶æ”¯æŒåŒåè®®æ ˆï¼Œä¾‹å¦‚ kubenetã€‚&lt;/p>
&lt;!--
Ecosystem support of dual-stack is increasing; you can create [dual-stack clusters with kubeadm](/docs/setup/production-environment/tools/kubeadm/dual-stack-support/), try a [dual-stack cluster locally with KIND](https://kind.sigs.k8s.io/docs/user/configuration/#ip-family), and deploy dual-stack clusters in cloud providers (after checking docs for CNI or kubenet availability).
-->
&lt;p>æ”¯æŒåŒåè®®æ ˆçš„ç”Ÿæ€ç³»ç»Ÿåœ¨ä¸æ–­å£®å¤§ï¼›ä½ å¯ä»¥ä½¿ç”¨
&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/dual-stack-support/">kubeadm åˆ›å»ºåŒåè®®æ ˆé›†ç¾¤&lt;/a>,
åœ¨æœ¬åœ°å°è¯•ç”¨ &lt;a href="https://kind.sigs.k8s.io/docs/user/configuration/#ip-family">KIND åˆ›å»ºåŒåè®®æ ˆé›†ç¾¤&lt;/a>ï¼Œ
è¿˜å¯ä»¥å°†åŒåè®®æ ˆé›†ç¾¤éƒ¨ç½²åˆ°äº‘ä¸Šï¼ˆåœ¨æŸ¥é˜… CNI æˆ– kubenet å¯ç”¨æ€§çš„æ–‡æ¡£ä¹‹åï¼‰&lt;/p>
&lt;!--
## Get involved with SIG Network
-->
&lt;h2 id="åŠ å…¥-network-sig">åŠ å…¥ Network SIG&lt;/h2>
&lt;!--
SIG-Network wants to learn from community experiences with dual-stack networking to find out more about evolving needs and your use cases. The [SIG-network update video from KubeCon NA 2021](https://www.youtube.com/watch?v=uZ0WLxpmBbY&amp;list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP&amp;index=4) summarizes the SIGâ€™s recent updates, including dual-stack going to stable in 1.23.
-->
&lt;p>SIG-Network å¸Œæœ›ä»åŒåè®®æ ˆç½‘ç»œçš„ç¤¾åŒºä½“éªŒä¸­å­¦ä¹ ï¼Œä»¥äº†è§£æ›´å¤šä¸æ–­å˜åŒ–çš„éœ€æ±‚å’Œä½ çš„ç”¨ä¾‹ä¿¡æ¯ã€‚
&lt;a href="https://www.youtube.com/watch?v=uZ0WLxpmBbY&amp;amp;list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP&amp;amp;index=4">SIG-network æ›´æ–°äº†æ¥è‡ª KubeCon 2021 åŒ—ç¾å¤§ä¼šçš„è§†é¢‘&lt;/a>
æ€»ç»“äº† SIG æœ€è¿‘çš„æ›´æ–°ï¼ŒåŒ…æ‹¬åŒåè®®æ ˆå°†åœ¨ 1.23 ç‰ˆæœ¬ä¸­ç¨³å®šã€‚&lt;/p>
&lt;!--
The current SIG-Network [KEPs](https://github.com/orgs/kubernetes/projects/10) and [issues](https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork) on GitHub illustrate the SIGâ€™s areas of emphasis. The [dual-stack API server](https://github.com/kubernetes/enhancements/issues/2438) is one place to consider contributing.
-->
&lt;p>å½“å‰ SIG-Network åœ¨ GitHub ä¸Šçš„ &lt;a href="https://github.com/orgs/kubernetes/projects/10">KEPs&lt;/a> å’Œ
&lt;a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork">issues&lt;/a>
è¯´æ˜äº†è¯¥ SIG çš„é‡ç‚¹é¢†åŸŸã€‚&lt;a href="https://github.com/kubernetes/enhancements/issues/2438">åŒåè®®æ ˆ API æœåŠ¡å™¨&lt;/a>
æ˜¯ä¸€ä¸ªè€ƒè™‘è´¡çŒ®çš„æ–¹å‘ã€‚&lt;/p>
&lt;!--
[SIG-Network meetings](https://github.com/kubernetes/community/tree/master/sig-network#meetings) are a friendly, welcoming venue for you to connect with the community and share your ideas. Looking forward to hearing from you!
-->
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network#meetings">SIG-Network ä¼šè®®&lt;/a>
æ˜¯ä¸€ä¸ªå‹å¥½ã€çƒ­æƒ…çš„åœºæ‰€ï¼Œä½ å¯ä»¥ä¸ç¤¾åŒºè”ç³»å¹¶åˆ†äº«ä½ çš„æƒ³æ³•ã€‚æœŸå¾…ä½ çš„åŠ å…¥ï¼&lt;/p>
&lt;!--
## Acknowledgments
-->
&lt;h2 id="è‡´è°¢">è‡´è°¢&lt;/h2>
&lt;!--
The dual-stack networking feature represents the work of many Kubernetes contributors. Thanks to all who contributed code, experience reports, documentation, code reviews, and everything in between. Bridget Kromhout details this community effort in [Dual-Stack Networking in Kubernetes](https://containerjournal.com/features/dual-stack-networking-in-kubernetes/). KubeCon keynotes by Tim Hockin &amp; Khaled (Kal) Henidak in 2019 ([The Long Road to IPv4/IPv6 Dual-stack Kubernetes](https://www.youtube.com/watch?v=o-oMegdZcg4)) and by Lachlan Evenson in 2021 ([And Here We Go: Dual-stack Networking in Kubernetes](https://www.youtube.com/watch?v=lVrt8F2B9CM)) talk about the dual-stack journey, spanning five years and a great many lines of code.
-->
&lt;p>è®¸å¤š Kubernetes è´¡çŒ®è€…ä¸ºåŒåè®®æ ˆç½‘ç»œåšå‡ºäº†è´¡çŒ®ã€‚æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®äº†ä»£ç ã€ç»éªŒæŠ¥å‘Šã€æ–‡æ¡£ã€ä»£ç å®¡æŸ¥ä»¥åŠå…¶ä»–å·¥ä½œçš„äººã€‚
Bridget Kromhout åœ¨ &lt;a href="https://containerjournal.com/features/dual-stack-networking-in-kubernetes/">Kubernetesçš„åŒåè®®æ ˆç½‘ç»œ&lt;/a>
ä¸­è¯¦ç»†ä»‹ç»äº†è¿™é¡¹ç¤¾åŒºå·¥ä½œã€‚Tim Hockin å’Œ Khaled (Kal) Henidak åœ¨ 2019 å¹´çš„ KubeCon å¤§ä¼šæ¼”è®²
ï¼ˆ&lt;a href="https://www.youtube.com/watch?v=o-oMegdZcg4">Kubernetes é€šå¾€ IPv4/IPv6 åŒåè®®æ ˆçš„æ¼«æ¼«é•¿è·¯&lt;/a>ï¼‰
å’Œ Lachlan Evenson åœ¨ 2021 å¹´æ¼”è®²ï¼ˆ&lt;a href="https://www.youtube.com/watch?v=o-oMegdZcg4">æˆ‘ä»¬æ¥å•¦ï¼ŒKubernetes åŒåè®®æ ˆç½‘ç»œ&lt;/a>ï¼‰
ä¸­è®¨è®ºäº†åŒåè®®æ ˆçš„å‘å±•æ—…ç¨‹ï¼Œè€—æ—¶ 5 å¹´å’Œæµ·é‡ä»£ç ã€‚&lt;/p></description></item><item><title>Blog: å…¬å¸ƒ 2021 å¹´æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾ç»“æœ</title><link>https://kubernetes.io/zh/blog/2021/11/08/steering-committee-results-2021/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/11/08/steering-committee-results-2021/</guid><description>
&lt;!--
layout: blog
title: "Announcing the 2021 Steering Committee Election Results"
date: 2021-11-08
slug: steering-committee-results-2021
-->
&lt;!--
**Author**: Kaslin Fields
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šKaslin Fields&lt;/p>
&lt;!--
The [2021 Steering Committee Election](https://github.com/kubernetes/community/tree/master/events/elections/2021) is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2021. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.
-->
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/events/elections/2021">2021 å¹´æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾&lt;/a>ç°å·²å®Œæˆã€‚
Kubernetes æŒ‡å¯¼å§”å‘˜ä¼šç”± 7 ä¸ªå¸­ä½ç»„æˆï¼Œå…¶ä¸­ 4 ä¸ªå¸­ä½å°†åœ¨ 2021 å¹´è¿›è¡Œé€‰ä¸¾ã€‚
æ–°ä»»å§”å‘˜ä¼šæˆå‘˜ä»»æœŸ 2 å¹´ï¼Œæ‰€æœ‰æˆå‘˜å‡ç”± Kubernetes ç¤¾åŒºé€‰ä¸¾äº§ç”Ÿã€‚&lt;/p>
&lt;!--
This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committeeâ€™s role in their [charter](https://github.com/kubernetes/steering/blob/master/charter.md).
-->
&lt;p>è¿™ä¸ªç¤¾åŒºæœºæ„éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒç›‘ç£æ•´ä¸ª Kubernetes é¡¹ç›®çš„æ²»ç†ã€‚
ä½ å¯ä»¥åœ¨å…¶&lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">ç« ç¨‹&lt;/a>ä¸­äº†è§£æ›´å¤šå…³äºæŒ‡å¯¼å§”å‘˜ä¼šçš„è§’è‰²ã€‚&lt;/p>
&lt;!--
## Results
-->
&lt;h2 id="é€‰ä¸¾ç»“æœ">é€‰ä¸¾ç»“æœ&lt;/h2>
&lt;!--
Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):
-->
&lt;p>ç¥è´ºå½“é€‰çš„å§”å‘˜ä¼šæˆå‘˜ï¼Œä»–ä»¬çš„ä¸¤å¹´ä»»æœŸå³åˆ»ç”Ÿæ•ˆï¼ˆæŒ‰ GitHub handle å­—æ¯æ’åºï¼‰:&lt;/p>
&lt;!--
* **Christoph Blecker ([@cblecker](https://github.com/cblecker)), Red Hat**
* **Stephen Augustus ([@justaugustus](https://github.com/justaugustus)), Cisco**
* **Paris Pittman ([@parispittman](https://github.com/parispittman)), Apple**
* **Tim Pepper ([@tpepper](https://github.com/tpepper)), VMware**
-->
&lt;ul>
&lt;li>&lt;strong>Christoph Bleckerï¼ˆ&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>ï¼‰ï¼Œ çº¢å¸½&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Stephen Augustusï¼ˆ&lt;a href="https://github.com/justaugustus">@justaugustus&lt;/a>ï¼‰ï¼Œ æ€ç§‘&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittmanï¼ˆ&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>)ï¼Œ è‹¹æœ&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Tim Pepperï¼ˆ&lt;a href="https://github.com/tpepper">@tpepper&lt;/a>ï¼‰ï¼Œ VMware&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
They join continuing members:
-->
&lt;p>ä»–ä»¬åŠ å…¥æ°¸ä¹…æˆå‘˜ï¼š&lt;/p>
&lt;!--
* **Davanum Srinivas ([@dims](https://github.com/dims)), VMware**
* **Jordan Liggitt ([@liggitt](https://github.com/liggitt)), Google**
* **Bob Killen ([@mrbobbytables](https://github.com/mrbobbytables)), Google**
-->
&lt;ul>
&lt;li>&lt;strong>Davanum Srinivasï¼ˆ&lt;a href="https://github.com/dims">@dims&lt;/a>ï¼‰ï¼Œ VMware&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Jordan Liggitt ï¼ˆ&lt;a href="https://github.com/liggitt">@liggitt&lt;/a>ï¼‰ï¼Œ è°·æ­Œ&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Bob Killen ï¼ˆ&lt;a href="https://github.com/mrbobbytables">@mrbobbytables&lt;/a>ï¼‰ï¼Œ è°·æ­Œ&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
Paris Pittman and Christoph Blecker are returning Steering Committee Members.
-->
&lt;p>Paris Pittman å’Œ Christoph Blecker å°†å›åˆ°æŒ‡å¯¼å§”å‘˜ä¼šã€‚&lt;/p>
&lt;!--
## Big Thanks
-->
&lt;h2 id="éå¸¸æ„Ÿè°¢">éå¸¸æ„Ÿè°¢&lt;/h2>
&lt;!--
Thank you and congratulations on a successful election to this roundâ€™s election officers:
-->
&lt;p>æ„Ÿè°¢å¹¶ç¥è´ºå®Œæˆæœ¬è½®æˆåŠŸé€‰ä¸¾çš„é€‰ä¸¾å®˜ä»¬:&lt;/p>
&lt;ul>
&lt;li>Alison Dowdney, (&lt;a href="https://github.com/alisondy">@alisondy&lt;/a>)&lt;/li>
&lt;li>Noah Kantrowitz (&lt;a href="https://github.com/coderanger">@coderanger&lt;/a>)&lt;/li>
&lt;li>Josh Berkus (&lt;a href="https://github.com/jberkus">@jberkus&lt;/a>)&lt;/li>
&lt;/ul>
&lt;!--
Special thanks to Arnaud Meukam ([@ameukam](https://github.com/ameukam)), k8s-infra liaison, who enabled our voting software on community-owned infrastructure.
-->
&lt;p>ç‰¹åˆ«æ„Ÿè°¢ k8s-infra è”ç»œå‘˜ Arnaud Meukamï¼ˆ&lt;a href="https://github.com/ameukam">@ameukam&lt;/a>ï¼‰ï¼Œ
ä»–åœ¨ç¤¾åŒºçš„åŸºç¡€è®¾æ–½ä¸Šå¯åŠ¨äº†æˆ‘ä»¬çš„æŠ•ç¥¨è½¯ä»¶ã€‚&lt;/p>
&lt;!--
Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:
-->
&lt;p>æ„Ÿè°¢è£èª‰é€€ä¼‘çš„æŒ‡å¯¼å§”å‘˜ä¼šæˆå‘˜ã€‚å¯¹ä½ ä»¬ä¹‹å‰å¯¹ç¤¾åŒºçš„è´¡çŒ®è¡¨ç¤ºæ„Ÿè°¢:&lt;/p>
&lt;ul>
&lt;li>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>)&lt;/li>
&lt;li>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>)&lt;/li>
&lt;/ul>
&lt;!--
And thank you to all the candidates who came forward to run for election.
-->
&lt;p>æ„Ÿè°¢æ‰€æœ‰å‰æ¥å‚åŠ ç«é€‰çš„å€™é€‰äººã€‚&lt;/p>
&lt;!--
## Get Involved with the Steering Committee
-->
&lt;h2 id="å‚ä¸æŒ‡å¯¼å§”å‘˜ä¼š">å‚ä¸æŒ‡å¯¼å§”å‘˜ä¼š&lt;/h2>
&lt;!--
This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee [backlog items](https://github.com/kubernetes/steering/projects/1) and weigh in by filing an issue or creating a PR against their [repo](https://github.com/kubernetes/steering). They have an open meeting on [the first Monday at 9:30am PT of every month](https://github.com/kubernetes/steering) and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list steering@kubernetes.io.
-->
&lt;p>ä¸æ‰€æœ‰ Kubernetes ä¸€æ ·ï¼Œè¿™ä¸ªç®¡ç†æœºæ„å¯¹æ‰€æœ‰äººå¼€æ”¾ã€‚
ä½ å¯ä»¥æŸ¥çœ‹æŒ‡å¯¼å§”å‘˜ä¼šçš„&lt;a href="https://github.com/kubernetes/steering/projects/1">å¾…åŠäº‹é¡¹&lt;/a>ï¼Œ
é€šè¿‡åœ¨ä»–ä»¬çš„ &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>
ä¸­æäº¤ä¸€ä¸ª issue æˆ–åˆ›å»ºä¸€ä¸ª PR æ¥å‚ä¸è®¨è®ºã€‚
ä»–ä»¬åœ¨&lt;a href="https://github.com/kubernetes/steering">æ¯æœˆçš„ç¬¬ä¸€ä¸ªæ˜ŸæœŸä¸€ä¸Šåˆ 9:30&lt;/a> ä¸¾è¡Œå…¬å¼€ä¼šè®®ï¼Œ
å¹¶å®šæœŸå‚åŠ ä¼šè§æˆ‘ä»¬çš„è´¡çŒ®è€…æ´»åŠ¨ã€‚ä¹Ÿå¯ä»¥é€šè¿‡ä»–ä»¬çš„å…¬å…±é‚®ä»¶åˆ—è¡¨ &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a> è”ç³»ä»–ä»¬ã€‚&lt;/p>
&lt;!--
You can see what the Steering Committee meetings are all about by watching past meetings on the [YouTube Playlist](https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM).
-->
&lt;p>ä½ å¯ä»¥åœ¨ &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube æ’­æ”¾åˆ—è¡¨&lt;/a>
ä¸Šè§‚çœ‹ä¹‹å‰çš„ä¼šè®®è§†é¢‘ï¼Œäº†è§£æŒ‡å¯¼å§”å‘˜ä¼šçš„ä¼šè®®è®¨è®ºå†…å®¹ã€‚&lt;/p>
&lt;hr>
&lt;!--
_This post was written by the [Upstream Marketing Working Group](https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing). If you want to write stories about the Kubernetes community, learn more about us._
-->
&lt;p>&lt;em>æœ¬æ–‡æ˜¯ç”±&lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing">ä¸Šæ¸¸è¥é”€å·¥ä½œç»„&lt;/a>æ’°å†™çš„ã€‚
å¦‚æœä½ æƒ³æ’°å†™æœ‰å…³ Kubernetes ç¤¾åŒºçš„æ•…äº‹ï¼Œè¯·äº†è§£æ›´å¤šå…³äºæˆ‘ä»¬çš„ä¿¡æ¯ã€‚&lt;/em>&lt;/p></description></item><item><title>Blog: å…³æ³¨ SIG Node</title><link>https://kubernetes.io/zh/blog/2021/09/27/sig-node-spotlight-2021/</link><pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/09/27/sig-node-spotlight-2021/</guid><description>
&lt;!--
---
layout: blog
title: "Spotlight on SIG Node"
date: 2021-09-27
slug: sig-node-spotlight-2021
---
-->
&lt;p>&lt;strong>Author:&lt;/strong> Dewan Ahmed, Red Hat&lt;/p>
&lt;!--
**Author:** Dewan Ahmed, Red Hat
-->
&lt;!--
## Introduction
In Kubernetes, a _Node_ is a representation of a single machine in your cluster. [SIG Node](https://github.com/kubernetes/community/tree/master/sig-node) owns that very important Node component and supports various subprojects such as Kubelet, Container Runtime Interface (CRI) and more to support how the pods and host resources interact. In this blog, we have summarized our conversation with [Elana Hashman (EH)](https://twitter.com/ehashdn) &amp; [Sergey Kanzhelev (SK)](https://twitter.com/SergeyKanzhelev), who walk us through the various aspects of being a part of the SIG and share some insights about how others can get involved.
-->
&lt;h2 id="ä»‹ç»">ä»‹ç»&lt;/h2>
&lt;p>åœ¨ Kubernetes ä¸­ï¼Œä¸€ä¸ª &lt;em>Node&lt;/em> æ˜¯ä½ é›†ç¾¤ä¸­çš„æŸå°æœºå™¨ã€‚
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> è´Ÿè´£è¿™ä¸€éå¸¸é‡è¦çš„ Node ç»„ä»¶å¹¶æ”¯æŒå„ç§å­é¡¹ç›®ï¼Œ
å¦‚ Kubelet, Container Runtime Interface (CRI) ä»¥åŠå…¶ä»–æ”¯æŒ Pod å’Œä¸»æœºèµ„æºé—´äº¤äº’çš„å­é¡¹ç›®ã€‚
åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ€»ç»“äº†å’Œ &lt;a href="https://twitter.com/ehashdn">Elana Hashman (EH)&lt;/a> &amp;amp; &lt;a href="https://twitter.com/SergeyKanzhelev">Sergey Kanzhelev (SK)&lt;/a> çš„å¯¹è¯ï¼Œæ˜¯ä»–ä»¬å¸¦é¢†æˆ‘ä»¬äº†è§£ä½œä¸ºæ­¤ SIG ä¸€ä»½å­çš„å„ä¸ªæ–¹é¢ï¼Œå¹¶åˆ†äº«ä¸€äº›å…³äºå…¶ä»–äººå¦‚ä½•å‚ä¸çš„è§è§£ã€‚&lt;/p>
&lt;!--
## A summary of our conversation
### Could you tell us a little about what SIG Node does?
SK: SIG Node is a vertical SIG responsible for the components that support the controlled interactions between the pods and host resources. We manage the lifecycle of pods that are scheduled to a node. This SIG's focus is to enable a broad set of workload types, including workloads with hardware specific or performance sensitive requirements. All while maintaining isolation boundaries between pods on a node, as well as the pod and the host. This SIG maintains quite a few components and has many external dependencies (like container runtimes or operating system features), which makes the complexity we deal with huge. We tame the complexity and aim to continuously improve node reliability.
-->
&lt;h2 id="æˆ‘ä»¬çš„å¯¹è¯æ€»ç»“">æˆ‘ä»¬çš„å¯¹è¯æ€»ç»“&lt;/h2>
&lt;h3 id="ä½ èƒ½å‘Šè¯‰æˆ‘ä»¬ä¸€äº›å…³äº-sig-node-çš„å·¥ä½œå—">ä½ èƒ½å‘Šè¯‰æˆ‘ä»¬ä¸€äº›å…³äº SIG Node çš„å·¥ä½œå—ï¼Ÿ&lt;/h3>
&lt;p>SKï¼šSIG Node æ˜¯ä¸€ä¸ªå‚ç›´ SIGï¼Œè´Ÿè´£æ”¯æŒ Pod å’Œä¸»æœºèµ„æºä¹‹é—´å—æ§äº’åŠ¨çš„ç»„ä»¶ã€‚æˆ‘ä»¬ç®¡ç†è¢«è°ƒåº¦åˆ°èŠ‚ç‚¹ä¸Šçš„ Pod çš„ç”Ÿå‘½å‘¨æœŸã€‚
è¿™ä¸ª SIG çš„é‡ç‚¹æ˜¯æ”¯æŒå¹¿æ³›çš„å·¥ä½œè´Ÿè½½ç±»å‹ï¼ŒåŒ…æ‹¬å…·æœ‰ç¡¬ä»¶ç‰¹æ€§æˆ–æ€§èƒ½æ•æ„Ÿè¦æ±‚çš„å·¥ä½œè´Ÿè½½ã€‚åŒæ—¶ä¿æŒèŠ‚ç‚¹ä¸Š Pod ä¹‹é—´çš„éš”ç¦»è¾¹ç•Œï¼Œä»¥åŠ Pod å’Œä¸»æœºçš„éš”ç¦»è¾¹ç•Œã€‚
è¿™ä¸ª SIG ç»´æŠ¤äº†ç›¸å½“å¤šçš„ç»„ä»¶ï¼Œå¹¶æœ‰è®¸å¤šå¤–éƒ¨ä¾èµ–ï¼ˆå¦‚å®¹å™¨è¿è¡Œæ—¶é—´æˆ–æ“ä½œç³»ç»ŸåŠŸèƒ½ï¼‰ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¤„ç†èµ·æ¥ååˆ†å¤æ‚ã€‚ä½†æˆ‘ä»¬æˆ˜èƒœäº†è¿™ç§å¤æ‚åº¦ï¼Œæ—¨åœ¨ä¸æ–­æé«˜èŠ‚ç‚¹çš„å¯é æ€§ã€‚&lt;/p>
&lt;!--
### "SIG Node is a vertical SIG" could you explain a bit more?
EH: There are two kinds of SIGs: horizontal and vertical. Horizontal SIGs are concerned with a particular function of every component in Kubernetes: for example, SIG Security considers security aspects of every component in Kubernetes, or SIG Instrumentation looks at the logs, metrics, traces and events of every component in Kubernetes. Such SIGs don't tend to own a lot of code.
Vertical SIGs, on the other hand, own a single component, and are responsible for approving and merging patches to that code base. SIG Node owns the "Node" vertical, pertaining to the kubelet and its lifecycle. This includes the code for the kubelet itself, as well as the node controller, the container runtime interface, and related subprojects like the node problem detector.
-->
&lt;h3 id="ä½ èƒ½å†è§£é‡Šä¸€ä¸‹-sig-node-æ˜¯ä¸€ç§å‚ç›´-sig-çš„å«ä¹‰å—">ä½ èƒ½å†è§£é‡Šä¸€ä¸‹ â€œSIG Node æ˜¯ä¸€ç§å‚ç›´ SIGâ€ çš„å«ä¹‰å—ï¼Ÿ&lt;/h3>
&lt;p>EHï¼šæœ‰ä¸¤ç§ SIGï¼šæ¨ªå‘å’Œå‚ç›´ã€‚æ¨ªå‘ SIG å…³æ³¨ Kubernetes ä¸­æ¯ä¸ªç»„ä»¶çš„ç‰¹å®šåŠŸèƒ½ï¼šä¾‹å¦‚ï¼ŒSIG Security è€ƒè™‘ Kubernetes ä¸­æ¯ä¸ªç»„ä»¶çš„å®‰å…¨æ–¹é¢ï¼Œæˆ–è€… SIG Instrumentation å…³æ³¨ Kubernetes ä¸­æ¯ä¸ªç»„ä»¶çš„æ—¥å¿—ã€åº¦é‡ã€è·Ÿè¸ªå’Œäº‹ä»¶ã€‚
è¿™æ ·çš„ SIG å¹¶ä¸å¤ªä¼šæ‹¥æœ‰å¤§é‡çš„ä»£ç ã€‚&lt;/p>
&lt;p>ç›¸åï¼Œå‚ç›´ SIG æ‹¥æœ‰ä¸€ä¸ªå•ä¸€çš„ç»„ä»¶ï¼Œå¹¶è´Ÿè´£æ‰¹å‡†å’Œåˆå¹¶è¯¥ä»£ç åº“çš„è¡¥ä¸ã€‚
SIG Node æ‹¥æœ‰ &amp;quot;Node&amp;quot; çš„å‚ç›´æ€§ï¼Œä¸ kubelet å’Œå®ƒçš„ç”Ÿå‘½å‘¨æœŸæœ‰å…³ã€‚è¿™åŒ…æ‹¬ kubelet æœ¬èº«çš„ä»£ç ï¼Œä»¥åŠèŠ‚ç‚¹æ§åˆ¶å™¨ã€å®¹å™¨è¿è¡Œæ—¶æ¥å£å’Œç›¸å…³çš„å­é¡¹ç›®ï¼Œæ¯”å¦‚èŠ‚ç‚¹é—®é¢˜æ£€æµ‹å™¨ã€‚&lt;/p>
&lt;!--
### How did the CI subproject start? Is this specific to SIG Node and how does it help the SIG?
SK: The subproject started as a follow up after one of the releases was blocked by numerous test failures of critical tests. These tests havenâ€™t started falling all at once, rather continuous lack of attention led to slow degradation of tests quality. SIG Node was always prioritizing quality and reliability, and forming of the subproject was a way to highlight this priority.
-->
&lt;h3 id="ci-å­é¡¹ç›®æ˜¯å¦‚ä½•å¼€å§‹çš„-è¿™æ˜¯ä¸“é—¨é’ˆå¯¹-sig-node-çš„å—-å®ƒå¯¹-sig-æœ‰ä»€ä¹ˆå¸®åŠ©">CI å­é¡¹ç›®æ˜¯å¦‚ä½•å¼€å§‹çš„ï¼Ÿè¿™æ˜¯ä¸“é—¨é’ˆå¯¹ SIG Node çš„å—ï¼Ÿå®ƒå¯¹ SIG æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ&lt;/h3>
&lt;p>SKï¼šè¯¥å­é¡¹ç›®æ˜¯åœ¨å…¶ä¸­ä¸€ä¸ªç‰ˆæœ¬å› å…³é”®æµ‹è¯•çš„å¤§é‡æµ‹è¯•å¤±è´¥è€Œå—é˜»åå¼€å§‹è·Ÿè¿›çš„ã€‚
è¿™äº›æµ‹è¯•å¹¶ä¸æ˜¯ä¸€ä¸‹å­å°±å¼€å§‹ä¸‹é™çš„ï¼Œè€Œæ˜¯æŒç»­çš„ç¼ºä¹å…³æ³¨å¯¼è‡´äº†æµ‹è¯•è´¨é‡çš„ç¼“æ…¢ä¸‹é™ã€‚
SIG Node ä¸€ç›´å°†è´¨é‡å’Œå¯é æ€§æ”¾åœ¨é¦–ä½ï¼Œç»„å»ºè¿™ä¸ªå­é¡¹ç›®æ˜¯å¼ºè°ƒè¿™ä¸€ä¼˜å…ˆäº‹é¡¹çš„ä¸€ç§æ–¹å¼ã€‚&lt;/p>
&lt;!--
### As the 3rd largest SIG in terms of number of issues and PRs, how does your SIG juggle so much work?
EH: It helps to be organized. When I increased my contributions to the SIG in January of 2021, I found myself overwhelmed by the volume of pull requests and issues and wasn't sure where to start. We were already tracking test-related issues and pull requests on the CI subproject board, but that was missing a lot of our bugfixes and feature work. So I began putting together a triage board for the rest of our pull requests, which allowed me to sort each one by status and what actions to take, and documented its use for other contributors. We closed or merged over 500 issues and pull requests tracked by our two boards in each of the past two releases. The Kubernetes devstats showed that we have significantly increased our velocity as a result.
In June, we ran our first bug scrub event to work through the backlog of issues filed against SIG Node, ensuring they were properly categorized. We closed over 130 issues over the course of this 48 hour global event, but as of writing we still have 333 open issues.
-->
&lt;h3 id="ä½œä¸º-issue-å’Œ-pr-æ•°é‡ç¬¬ä¸‰å¤§çš„-sig-ä½ ä»¬-sig-æ˜¯å¦‚ä½•å…¼é¡¾è¿™ä¹ˆå¤šå·¥ä½œçš„">ä½œä¸º issue å’Œ PR æ•°é‡ç¬¬ä¸‰å¤§çš„ SIGï¼Œä½ ä»¬ SIG æ˜¯å¦‚ä½•å…¼é¡¾è¿™ä¹ˆå¤šå·¥ä½œçš„ï¼Ÿ&lt;/h3>
&lt;p>EHï¼šè¿™å½’åŠŸäºæœ‰ç»„ç»‡æ€§ã€‚å½“æˆ‘åœ¨ 2021 å¹´ 1 æœˆå¢åŠ å¯¹ SIG çš„è´¡çŒ®æ—¶ï¼Œæˆ‘å‘ç°è‡ªå·±è¢«å¤§é‡çš„ PR å’Œ issue æ·¹æ²¡äº†ï¼Œä¸çŸ¥é“è¯¥ä»å“ªé‡Œå¼€å§‹ã€‚
æˆ‘ä»¬å·²ç»åœ¨ CI å­é¡¹ç›®æ¿ä¸Šè·Ÿè¸ªä¸æµ‹è¯•æœ‰å…³çš„ issue å’Œ PR è¯·æ±‚ï¼Œä½†è¿™ç¼ºå°‘äº†å¾ˆå¤š bug ä¿®å¤å’ŒåŠŸèƒ½å·¥ä½œã€‚
å› æ­¤ï¼Œæˆ‘å¼€å§‹ä¸ºæˆ‘ä»¬å‰©ä½™çš„ PR å»ºç«‹ä¸€ä¸ªåˆ†æµæ¿ï¼Œè¿™ä½¿æˆ‘èƒ½å¤Ÿæ ¹æ®çŠ¶æ€å’Œé‡‡å–çš„è¡ŒåŠ¨å¯¹å…¶è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä¸ºå…¶ä»–è´¡çŒ®è€…è®°å½•å®ƒçš„ç”¨é€”ã€‚
åœ¨è¿‡å»çš„ä¸¤ä¸ªç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å…³é—­æˆ–åˆå¹¶äº†è¶…è¿‡ 500 ä¸ª issue å’Œ PRã€‚Kubernetes devstats æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„é€Ÿåº¦å› æ­¤è€Œå¤§å¤§æå‡ã€‚&lt;/p>
&lt;p>6æœˆï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç¬¬ä¸€æ¬¡ bug æ¸…é™¤æ´»åŠ¨ï¼Œä»¥è§£å†³é’ˆå¯¹ SIG Node çš„ç§¯å‹é—®é¢˜ï¼Œç¡®ä¿å®ƒä»¬è¢«æ­£ç¡®å½’ç±»ã€‚
åœ¨è¿™æ¬¡ 48 å°æ—¶çš„å…¨çƒæ´»åŠ¨ä¸­ï¼Œæˆ‘ä»¬å…³é—­äº† 130 å¤šä¸ªé—®é¢˜ï¼Œä½†æˆªè‡³å‘ç¨¿æ—¶ï¼Œæˆ‘ä»¬ä»æœ‰ 333 ä¸ªé—®é¢˜æ²¡æœ‰è§£å†³ã€‚&lt;/p>
&lt;!--
### Why should new and existing contributors consider joining SIG Node?
SK: Being a SIG Node contributor gives you skills and recognition that are rewarding and useful. Understanding under the hood of a kubelet helps architecting better apps, tune and optimize those apps, and gives leg up in issues troubleshooting. If you are a new contributor, SIG Node gives you the foundational knowledge that is key to understanding why other Kubernetes components are designed the way they are. Existing contributors may benefit as many features will require SIG Node changes one way or another. So being a SIG Node contributor helps building features in other SIGs faster.
SIG Node maintains numerous components, many of which have dependency on external projects or OS features. This makes the onboarding process quite lengthy and demanding. But if you are up for a challenge, there is always a place for you, and a group of people to support.
-->
&lt;h3 id="ä¸ºä»€ä¹ˆæ–°çš„å’Œç°æœ‰çš„è´¡çŒ®è€…åº”è¯¥è€ƒè™‘åŠ å…¥-node-å…´è¶£å°ç»„å‘¢">ä¸ºä»€ä¹ˆæ–°çš„å’Œç°æœ‰çš„è´¡çŒ®è€…åº”è¯¥è€ƒè™‘åŠ å…¥ Node å…´è¶£å°ç»„å‘¢ï¼Ÿ&lt;/h3>
&lt;p>SKï¼šä½œä¸º SIG Node çš„è´¡çŒ®è€…ä¼šå¸¦ç»™ä½ æœ‰æ„ä¹‰ä¸”æœ‰ç”¨çš„æŠ€èƒ½å’Œè®¤å¯åº¦ã€‚
äº†è§£ Kubelet çš„å†…éƒ¨ç»“æ„æœ‰åŠ©äºæ„å»ºæ›´å¥½çš„åº”ç”¨ç¨‹åºï¼Œè°ƒæ•´å’Œä¼˜åŒ–è¿™äº›åº”ç”¨ç¨‹åºï¼Œå¹¶åœ¨ issue æ’æŸ¥ä¸Šè·å¾—ä¼˜åŠ¿ã€‚
å¦‚æœä½ æ˜¯ä¸€ä¸ªæ–°æ‰‹è´¡çŒ®è€…ï¼ŒSIG Node ä¸ºä½ æä¾›äº†åŸºç¡€çŸ¥è¯†ï¼Œè¿™æ˜¯ç†è§£å…¶ä»– Kubernetes ç»„ä»¶çš„è®¾è®¡æ–¹å¼çš„å…³é”®ã€‚
ç°åœ¨çš„è´¡çŒ®è€…å¯èƒ½ä¼šå—ç›Šäºè®¸å¤šåŠŸèƒ½éƒ½éœ€è¦ SIG Node çš„è¿™ç§æˆ–é‚£ç§å˜åŒ–ã€‚æ‰€ä»¥æˆä¸º SIG Node çš„è´¡çŒ®è€…æœ‰åŠ©äºæ›´å¿«åœ°å»ºç«‹å…¶ä»– SIG çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;p>SIG Node ç»´æŠ¤ç€è®¸å¤šç»„ä»¶ï¼Œå…¶ä¸­è®¸å¤šç»„ä»¶éƒ½ä¾èµ–äºå¤–éƒ¨é¡¹ç›®æˆ–æ“ä½œç³»ç»ŸåŠŸèƒ½ã€‚è¿™ä½¿å¾—å…¥èŒè¿‡ç¨‹ç›¸å½“å†—é•¿å’Œè‹›åˆ»ã€‚
ä½†å¦‚æœä½ æ„¿æ„æ¥å—æŒ‘æˆ˜ï¼Œæ€»æœ‰ä¸€ä¸ªåœ°æ–¹é€‚åˆä½ ï¼Œä¹Ÿæœ‰ä¸€ç¾¤äººæ”¯æŒä½ ã€‚&lt;/p>
&lt;!--
### What do you do to help new contributors get started?
EH: Getting started in SIG Node can be intimidating, since there is so much work to be done, our SIG meetings are very large, and it can be hard to find a place to start.
I always encourage new contributors to work on things that they have some investment in already. In SIG Node, that might mean volunteering to help fix a bug that you have personally been affected by, or helping to triage bugs you care about by priority.
To come up to speed on any open source code base, there are two strategies you can take: start by exploring a particular issue deeply, and follow that to expand the edges of your knowledge as needed, or briefly review as many issues and change requests as you possibly can to get a higher level picture of how the component works. Ultimately, you will need to do both if you want to become a Node reviewer or approver.
[Davanum Srinivas](https://twitter.com/dims) and I each ran a cohort of group mentoring to help teach new contributors the skills to become Node reviewers, and if there's interest we can work to find a mentor to run another session. I also encourage new contributors to attend our Node CI Subproject meeting: it's a smaller audience and we don't record the triage sessions, so it can be a less intimidating way to get started with the SIG.
-->
&lt;h3 id="ä½ æ˜¯å¦‚ä½•å¸®åŠ©æ–°æ‰‹è´¡çŒ®è€…å¼€å§‹å·¥ä½œçš„">ä½ æ˜¯å¦‚ä½•å¸®åŠ©æ–°æ‰‹è´¡çŒ®è€…å¼€å§‹å·¥ä½œçš„ï¼Ÿ&lt;/h3>
&lt;p>EHï¼šåœ¨ SIG Node çš„èµ·æ­¥å·¥ä½œå¯èƒ½æ˜¯ä»¤äººç”Ÿç•çš„ï¼Œå› ä¸ºæœ‰å¤ªå¤šçš„å·¥ä½œè¦åšï¼Œæˆ‘ä»¬çš„ SIG ä¼šè®®éå¸¸å¤§ï¼Œè€Œä¸”å¾ˆéš¾æ‰¾åˆ°ä¸€ä¸ªå¼€å§‹çš„åœ°æ–¹ã€‚&lt;/p>
&lt;p>æˆ‘æ€»æ˜¯é¼“åŠ±æ–°æ‰‹è´¡çŒ®è€…åœ¨ä»–ä»¬å·²ç»æœ‰ä¸€äº›æŠ•å…¥çš„æ–¹å‘ä¸Šæ›´è¿›ä¸€æ­¥ã€‚
åœ¨ SIG Node ä¸­ï¼Œè¿™å¯èƒ½æ„å‘³ç€è‡ªæ„¿å¸®åŠ©ä¿®å¤ä¸€ä¸ªåªå½±å“åˆ°ä½ ä¸ªäººçš„ bugï¼Œæˆ–è€…æŒ‰ä¼˜å…ˆçº§å»åˆ†æµä½ å…³å¿ƒçš„ bugã€‚&lt;/p>
&lt;p>ä¸ºäº†å°½å¿«äº†è§£ä»»ä½•å¼€æºä»£ç åº“ï¼Œä½ å¯ä»¥é‡‡å–ä¸¤ç§ç­–ç•¥ï¼šä»æ·±å…¥æ¢ç´¢ä¸€ä¸ªç‰¹å®šçš„é—®é¢˜å¼€å§‹ï¼Œç„¶åæ ¹æ®éœ€è¦æ‰©å±•ä½ çš„çŸ¥è¯†è¾¹ç¼˜ï¼Œæˆ–è€…å•çº¯åœ°å°½å¯èƒ½å¤šçš„å®¡æŸ¥ issues å’Œå˜æ›´è¯·æ±‚ï¼Œä»¥äº†è§£æ›´é«˜å±‚æ¬¡çš„ç»„ä»¶å·¥ä½œæ–¹å¼ã€‚
æœ€ç»ˆï¼Œå¦‚æœä½ æƒ³æˆä¸ºä¸€å Node reviewer æˆ– approverï¼Œä¸¤ä»¶äº‹æ˜¯ä¸å¯é¿å…çš„ã€‚&lt;/p>
&lt;p>&lt;a href="https://twitter.com/dims">Davanum Srinivas&lt;/a> å’Œæˆ‘å„è‡ªä¸¾åŠäº†ä¸€æ¬¡å°ç»„è¾…å¯¼ï¼Œä»¥å¸®åŠ©æ•™å¯¼æ–°æ‰‹è´¡çŒ®è€…æˆä¸º Node reviewer çš„æŠ€èƒ½ï¼Œå¦‚æœæœ‰å…´è¶£ï¼Œæˆ‘ä»¬å¯ä»¥åŠªåŠ›å¯»æ‰¾ä¸€ä¸ªå¯¼å¸ˆæ¥ä¸¾åŠå¦ä¸€æ¬¡ä¼šè®®ã€‚
æˆ‘ä¹Ÿé¼“åŠ±æ–°æ‰‹è´¡çŒ®è€…å‚åŠ æˆ‘ä»¬çš„ Node CI å­é¡¹ç›®ä¼šè®®ï¼šå®ƒçš„å¬ä¼—è¾ƒå°‘ï¼Œè€Œä¸”æˆ‘ä»¬ä¸è®°å½•åˆ†æµä¼šè®®ï¼Œæ‰€ä»¥å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªæ¯”è¾ƒæ¸©å’Œçš„æ–¹å¼æ¥å¼€å§‹ SIG ä¹‹æ—…ã€‚&lt;/p>
&lt;!--
### Are there any particular skills youâ€™d like to recruit for? What skills are contributors to SIG Usability likely to learn?
SK: SIG Node works on many workstreams in very different areas. All of these areas are on system level. For the typical code contributions you need to have a passion for building and utilizing low level APIs and writing performant and reliable components. Being a contributor you will learn how to debug and troubleshoot, profile, and monitor these components, as well as user workload that is run by these components. Often, with the limited to no access to Nodes, as they are running production workloads.
The other way of contribution is to help document SIG node features. This type of contribution requires a deep understanding of features, and ability to explain them in simple terms.
Finally, we are always looking for feedback on how best to run your workload. Come and explain specifics of it, and what features in SIG Node components may help to run it better.
-->
&lt;h3 id="æœ‰ä»€ä¹ˆç‰¹åˆ«çš„æŠ€èƒ½è€…æ˜¯ä½ æƒ³æ‹›å‹Ÿçš„å—-å¯¹-sig-å¯ç”¨æ€§çš„è´¡çŒ®è€…å¯èƒ½ä¼šå­¦åˆ°ä»€ä¹ˆæŠ€èƒ½">æœ‰ä»€ä¹ˆç‰¹åˆ«çš„æŠ€èƒ½è€…æ˜¯ä½ æƒ³æ‹›å‹Ÿçš„å—ï¼Ÿå¯¹ SIG å¯ç”¨æ€§çš„è´¡çŒ®è€…å¯èƒ½ä¼šå­¦åˆ°ä»€ä¹ˆæŠ€èƒ½ï¼Ÿ&lt;/h3>
&lt;p>SKï¼šSIG Node åœ¨å¤§ç›¸å¾„åº­çš„é¢†åŸŸä»äº‹è®¸å¤šå·¥ä½œæµã€‚æ‰€æœ‰è¿™äº›é¢†åŸŸéƒ½æ˜¯ç³»ç»Ÿçº§çš„ã€‚
å¯¹äºå…¸å‹çš„ä»£ç è´¡çŒ®ï¼Œä½ éœ€è¦å¯¹å»ºç«‹å’Œå–„ç”¨ä½çº§åˆ«çš„ API ä»¥åŠç¼–å†™é«˜æ€§èƒ½å’Œå¯é çš„ç»„ä»¶æœ‰çƒ­æƒ…ã€‚
ä½œä¸ºä¸€ä¸ªè´¡çŒ®è€…ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•è°ƒè¯•å’Œæ’é™¤æ•…éšœï¼Œå‰–æå’Œç›‘æ§è¿™äº›ç»„ä»¶ï¼Œä»¥åŠç”±è¿™äº›ç»„ä»¶è¿è¡Œçš„ç”¨æˆ·å·¥ä½œè´Ÿè½½ã€‚
é€šå¸¸æƒ…å†µä¸‹ï¼Œç”±äºèŠ‚ç‚¹æ­£åœ¨è¿è¡Œç”Ÿäº§å·¥ä½œè´Ÿè½½ï¼Œæ‰€ä»¥å¯¹èŠ‚ç‚¹çš„è®¿é—®æ˜¯æœ‰é™çš„ï¼Œç”šè‡³æ˜¯æ²¡æœ‰çš„ã€‚&lt;/p>
&lt;p>å¦ä¸€ç§è´¡çŒ®æ–¹å¼æ˜¯å¸®åŠ©è®°å½• SIG Node çš„åŠŸèƒ½ã€‚è¿™ç§ç±»å‹çš„è´¡çŒ®éœ€è¦å¯¹åŠŸèƒ½æœ‰æ·±åˆ»çš„ç†è§£ï¼Œå¹¶æœ‰èƒ½åŠ›ç”¨ç®€å•çš„æœ¯è¯­è§£é‡Šå®ƒä»¬ã€‚&lt;/p>
&lt;p>æœ€åï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨å¯»æ‰¾å…³äºå¦‚ä½•æœ€å¥½åœ°è¿è¡Œä½ çš„å·¥ä½œè´Ÿè½½çš„åé¦ˆã€‚æ¥è§£é‡Šä¸€ä¸‹å®ƒçš„å…·ä½“æƒ…å†µï¼Œä»¥åŠ SIG Node ç»„ä»¶ä¸­çš„å“ªäº›åŠŸèƒ½å¯èƒ½æœ‰åŠ©äºæ›´å¥½åœ°è¿è¡Œå®ƒã€‚&lt;/p>
&lt;!--
### What are you getting positive feedback on, and whatâ€™s coming up next for SIG Node?
EH: Over the past year SIG Node has adopted some new processes to help manage our feature development and Kubernetes enhancement proposals, and other SIGs have looked to us for inspiration in managing large workloads. I hope that this is an area we can continue to provide leadership in and further iterate on.
We have a great balance of new features and deprecations in flight right now. Deprecations of unused or difficult to maintain features help us keep technical debt and maintenance load under control, and examples include the dockershim and DynamicKubeletConfiguration deprecations. New features will unlock additional functionality in end users' clusters, and include exciting features like support for cgroups v2, swap memory, graceful node shutdowns, and device management policies.
-->
&lt;h3 id="ä½ åœ¨å“ªäº›æ–¹é¢å¾—åˆ°äº†ç§¯æçš„åé¦ˆ-ä»¥åŠ-sig-node-çš„ä¸‹ä¸€æ­¥è®¡åˆ’æ˜¯ä»€ä¹ˆ">ä½ åœ¨å“ªäº›æ–¹é¢å¾—åˆ°äº†ç§¯æçš„åé¦ˆï¼Œä»¥åŠ SIG Node çš„ä¸‹ä¸€æ­¥è®¡åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h3>
&lt;p>EHï¼šåœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼ŒSIG Node é‡‡ç”¨äº†ä¸€äº›æ–°çš„æµç¨‹æ¥å¸®åŠ©ç®¡ç†æˆ‘ä»¬çš„åŠŸèƒ½å¼€å‘å’Œ Kubernetes å¢å¼ºæè®®ï¼Œå…¶ä»– SIG ä¹Ÿå‘æˆ‘ä»¬å¯»æ±‚åœ¨ç®¡ç†å¤§å‹å·¥ä½œè´Ÿè½½æ–¹é¢çš„çµæ„Ÿã€‚
æˆ‘å¸Œæœ›è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¯ä»¥ç»§ç»­é¢†å¯¼å¹¶è¿›ä¸€æ­¥è¿­ä»£çš„é¢†åŸŸã€‚&lt;/p>
&lt;p>ç°åœ¨ï¼Œæˆ‘ä»¬åœ¨æ–°åŠŸèƒ½å’ŒåºŸå¼ƒåŠŸèƒ½ä¹‹é—´ä¿æŒäº†å¾ˆå¥½çš„å¹³è¡¡ã€‚
åºŸå¼ƒæœªä½¿ç”¨æˆ–éš¾ä»¥ç»´æŠ¤çš„åŠŸèƒ½æœ‰åŠ©äºæˆ‘ä»¬æ§åˆ¶æŠ€æœ¯å€ºåŠ¡å’Œç»´æŠ¤è´Ÿè·ï¼Œä¾‹å­åŒ…æ‹¬ dockershim å’Œ DynamicKubeletConfiguration çš„åºŸå¼ƒã€‚
æ–°åŠŸèƒ½å°†åœ¨ç»ˆç«¯ç”¨æˆ·çš„é›†ç¾¤ä¸­é‡Šæ”¾æ›´å¤šçš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ä»¤äººå…´å¥‹çš„åŠŸèƒ½ï¼Œå¦‚æ”¯æŒ cgroups v2ã€äº¤æ¢å†…å­˜ã€ä¼˜é›…çš„èŠ‚ç‚¹å…³é—­å’Œè®¾å¤‡ç®¡ç†ç­–ç•¥ã€‚&lt;/p>
&lt;!--
### Any closing thoughts/resources youâ€™d like to share?
SK/EH: It takes time and effort to get to any open source community. SIG Node may overwhelm you at first with the number of participants, volume of work, and project scope. But it is totally worth it. Join our welcoming community! [SIG Node GitHub Repo](https://github.com/kubernetes/community/tree/master/sig-node) contains many useful resources including Slack, mailing list and other contact info.
-->
&lt;h3 id="æœ€åä½ æœ‰ä»€ä¹ˆæƒ³æ³•-èµ„æºè¦åˆ†äº«å—">æœ€åä½ æœ‰ä»€ä¹ˆæƒ³æ³•/èµ„æºè¦åˆ†äº«å—ï¼Ÿ&lt;/h3>
&lt;p>SK/EHï¼šè¿›å…¥ä»»ä½•å¼€æºç¤¾åŒºéƒ½éœ€è¦æ—¶é—´å’ŒåŠªåŠ›ã€‚ä¸€å¼€å§‹ SIG Node å¯èƒ½ä¼šå› ä¸ºå‚ä¸è€…çš„æ•°é‡ã€å·¥ä½œé‡å’Œé¡¹ç›®èŒƒå›´è€Œè®©ä½ ä¸çŸ¥æ‰€æªã€‚ä½†è¿™æ˜¯å®Œå…¨å€¼å¾—çš„ã€‚
è¯·åŠ å…¥æˆ‘ä»¬è¿™ä¸ªçƒ­æƒ…çš„ç¤¾åŒº! [SIG Node GitHub Repo]ï¼ˆhttps://github.com/kubernetes/community/tree/master/sig-nodeï¼‰åŒ…å«è®¸å¤šæœ‰ç”¨çš„èµ„æºï¼ŒåŒ…æ‹¬ Slackã€é‚®ä»¶åˆ—è¡¨å’Œå…¶ä»–è”ç³»ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
## Wrap Up
SIG Node hosted a [KubeCon + CloudNativeCon Europe 2021 talk](https://www.youtube.com/watch?v=z5aY4e2RENA) with an intro and deep dive to their awesome SIG. Join the SIG's meetings to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream Node team as a contributor!
-->
&lt;h2 id="æ€»ç»“">æ€»ç»“&lt;/h2>
&lt;p>SIG Node ä¸¾åŠäº†ä¸€åœº &lt;a href="https://www.youtube.com/watch?v=z5aY4e2RENA">KubeCon + CloudNativeCon Europe 2021 talk&lt;/a>ï¼Œå¯¹ä»–ä»¬å¼ºå¤§çš„ SIG è¿›è¡Œäº†ä»‹ç»å’Œæ·±å…¥æ¢è®¨ã€‚
åŠ å…¥ SIG çš„ä¼šè®®ï¼Œäº†è§£æœ€æ–°çš„ç ”ç©¶æˆæœï¼Œæœªæ¥ä¸€å¹´çš„è®¡åˆ’æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå¦‚ä½•ä½œä¸ºè´¡çŒ®è€…å‚ä¸åˆ°ä¸Šæ¸¸çš„ Node å›¢é˜Ÿä¸­!&lt;/p></description></item><item><title>Blog: Kubernetes 1.20ï¼šCSI é©±åŠ¨ç¨‹åºä¸­çš„ Pod èº«ä»½å‡æ‰®å’ŒçŸ­æ—¶å·</title><link>https://kubernetes.io/zh/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers'
date: 2020-12-18
slug: kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi
-->
&lt;!--
**Author**: Shihang Zhang (Google)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Shihang Zhangï¼ˆè°·æ­Œï¼‰&lt;/p>
&lt;!--
Typically when a [CSI](https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md) driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods' identities rather than the CSI driver's identity. CSI drivers, therefore, need some way to retrieve pod's service account token.
-->
&lt;p>é€šå¸¸ï¼Œå½“ &lt;a href="https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md">CSI&lt;/a> é©±åŠ¨ç¨‹åºæŒ‚è½½
è¯¸å¦‚ Secret å’Œè¯ä¹¦ä¹‹ç±»çš„å‡­æ®æ—¶ï¼Œå®ƒå¿…é¡»é€šè¿‡å­˜å‚¨æä¾›è€…çš„èº«ä»½è®¤è¯æ‰èƒ½è®¿é—®è¿™äº›å‡­æ®ã€‚
ç„¶è€Œï¼Œå¯¹è¿™äº›å‡­æ®çš„è®¿é—®æ˜¯æ ¹æ® Pod çš„èº«ä»½è€Œä¸æ˜¯ CSI é©±åŠ¨ç¨‹åºçš„èº«ä»½æ¥æ§åˆ¶çš„ã€‚
å› æ­¤ï¼ŒCSI é©±åŠ¨ç¨‹åºéœ€è¦æŸç§æ–¹æ³•æ¥å–å¾— Pod çš„æœåŠ¡å¸æˆ·ä»¤ç‰Œã€‚&lt;/p>
&lt;!--
Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.
-->
&lt;p>å½“å‰ï¼Œæœ‰ä¸¤ç§ä¸æ˜¯é‚£ä¹ˆç†æƒ³çš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç›®çš„ï¼Œè¦ä¹ˆé€šè¿‡æˆäºˆ CSI é©±åŠ¨ç¨‹åºä½¿ç”¨ TokenRequest API çš„æƒé™ï¼Œè¦ä¹ˆç›´æ¥ä»ä¸»æœºæ–‡ä»¶ç³»ç»Ÿä¸­è¯»å–ä»¤ç‰Œã€‚&lt;/p>
&lt;!--
Both of them exhibit the following drawbacks:
-->
&lt;p>ä¸¤è€…éƒ½å­˜åœ¨ä»¥ä¸‹ç¼ºç‚¹ï¼š&lt;/p>
&lt;!--
- Violating the principle of least privilege
- Every CSI driver needs to re-implement the logic of getting the podâ€™s service account token
-->
&lt;ul>
&lt;li>è¿åæœ€å°‘ç‰¹æƒåŸåˆ™&lt;/li>
&lt;li>æ¯ä¸ª CSI é©±åŠ¨ç¨‹åºéƒ½éœ€è¦é‡æ–°å®ç°è·å– Pod çš„æœåŠ¡å¸æˆ·ä»¤ç‰Œçš„é€»è¾‘&lt;/li>
&lt;/ul>
&lt;!--
The second approach is more problematic due to:
-->
&lt;p>ç¬¬äºŒç§æ–¹å¼é—®é¢˜æ›´å¤šï¼Œå› ä¸ºï¼š&lt;/p>
&lt;!--
- The audience of the token defaults to the kube-apiserver
- The token is not guaranteed to be available (e.g. `AutomountServiceAccountToken=false`)
- The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See [file permission section for service account token](https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission)
- The token might be legacy Kubernetes service account token which doesnâ€™t expire if `BoundServiceAccountTokenVolume=false`
-->
&lt;ul>
&lt;li>ä»¤ç‰Œçš„å—ä¼—é»˜è®¤ä¸º kube-apiserver&lt;/li>
&lt;li>è¯¥ä»¤ç‰Œä¸èƒ½ä¿è¯å¯ç”¨ï¼ˆä¾‹å¦‚ï¼Œ&lt;code>AutomountServiceAccountToken=false&lt;/code>ï¼‰&lt;/li>
&lt;li>è¯¥æ–¹æ³•ä¸é€‚ç”¨äºä»¥ä¸ Pod ä¸åŒçš„ï¼ˆé root ç”¨æˆ·ï¼‰ç”¨æˆ·èº«ä»½è¿è¡Œçš„ CSI é©±åŠ¨ç¨‹åºã€‚è¯·å‚è§
&lt;a href="https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission">æœåŠ¡å¸æˆ·ä»¤ç‰Œçš„æ–‡ä»¶è®¸å¯æƒéƒ¨åˆ†&lt;/a>&lt;/li>
&lt;li>è¯¥ä»¤ç‰Œå¯èƒ½æ˜¯æ—§çš„ Kubernetes æœåŠ¡å¸æˆ·ä»¤ç‰Œï¼Œå¦‚æœ &lt;code>BoundServiceAccountTokenVolume=false&lt;/code>ï¼Œè¯¥ä»¤ç‰Œä¸ä¼šè¿‡æœŸã€‚&lt;/li>
&lt;/ul>
&lt;!--
Kubernetes 1.20 introduces an alpha feature, `CSIServiceAccountToken`, to improve the security posture. The new feature allows CSI drivers to receive pods' [bound service account tokens](https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md).
-->
&lt;p>Kubernetes 1.20 å¼•å…¥äº†ä¸€ä¸ªå†…æµ‹åŠŸèƒ½ &lt;code>CSIServiceAccountToken&lt;/code> ä»¥æ”¹å–„å®‰å…¨çŠ¶å†µã€‚è¿™é¡¹æ–°åŠŸèƒ½å…è®¸ CSI é©±åŠ¨ç¨‹åºæ¥æ”¶ Pod çš„&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md">ç»‘å®šæœåŠ¡å¸æˆ·ä»¤ç‰Œ&lt;/a>ã€‚&lt;/p>
&lt;!--
This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.
-->
&lt;p>æ­¤åŠŸèƒ½è¿˜æä¾›äº†ä¸€ä¸ªé‡æ–°å‘å¸ƒå·çš„èƒ½åŠ›ï¼Œä»¥ä¾¿å¯ä»¥åˆ·æ–°çŸ­æ—¶å·ã€‚&lt;/p>
&lt;!--
## Pod Impersonation
### Using GCP APIs
-->
&lt;h2 id="pod-èº«ä»½å‡æ‰®">Pod èº«ä»½å‡æ‰®&lt;/h2>
&lt;h3 id="ä½¿ç”¨-gcp-apis">ä½¿ç”¨ GCP APIs&lt;/h3>
&lt;!--
Using [Workload Identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity), a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod's service account token to [exchange for GCP tokens](https://cloud.google.com/iam/docs/reference/sts/rest). The pod's service account token is plumbed through the volume context in `NodePublishVolume` RPC calls when the feature `CSIServiceAccountToken` is enabled. For example: accessing [Google Secret Manager](https://cloud.google.com/secret-manager/) via a [secret store CSI driver](https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp).
-->
&lt;p>ä½¿ç”¨ &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity&lt;/a>ï¼ŒKubernetes æœåŠ¡å¸æˆ·å¯ä»¥åœ¨è®¿é—® Google Cloud API æ—¶éªŒè¯ä¸º Google æœåŠ¡å¸æˆ·ã€‚
å¦‚æœ CSI é©±åŠ¨ç¨‹åºè¦ä»£è¡¨å…¶ä¸ºæŒ‚è½½å·çš„ Pod è®¿é—® GCP APIï¼Œåˆ™å¯ä»¥ä½¿ç”¨ Pod çš„æœåŠ¡å¸æˆ·ä»¤ç‰Œæ¥
&lt;a href="https://cloud.google.com/iam/docs/reference/sts/rest">äº¤æ¢ GCP ä»¤ç‰Œ&lt;/a>ã€‚å¯ç”¨åŠŸèƒ½ &lt;code>CSIServiceAccountToken&lt;/code> åï¼Œ
å¯é€šè¿‡ &lt;code>NodePublishVolume&lt;/code> RPC è°ƒç”¨ä¸­çš„å·ä¸Šä¸‹æ–‡æ¥è®¿é—® Pod çš„æœåŠ¡å¸æˆ·ä»¤ç‰Œã€‚ä¾‹å¦‚ï¼šé€šè¿‡ &lt;a href="https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp">Secret å­˜å‚¨ CSI é©±åŠ¨&lt;/a>
è®¿é—® &lt;a href="https://cloud.google.com/secret-manager/">Google Secret Manager&lt;/a>ã€‚&lt;/p>
&lt;!--
### Using Vault
If users configure [Kubernetes as an auth method](https://www.vaultproject.io/docs/auth/kubernetes), Vault uses the `TokenReview` API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod's service account to Vault. For example, [secrets store CSI driver](https://github.com/hashicorp/secrets-store-csi-driver-provider-vault) and [cert manager CSI driver](https://github.com/jetstack/cert-manager-csi).
-->
&lt;h3 id="ä½¿ç”¨vault">ä½¿ç”¨Vault&lt;/h3>
&lt;p>å¦‚æœç”¨æˆ·å°† &lt;a href="https://www.vaultproject.io/docs/auth/kubernetes">Kubernetes ä½œä¸ºèº«ä»½éªŒè¯æ–¹æ³•&lt;/a>é…ç½®ï¼Œ
åˆ™ Vault ä½¿ç”¨ &lt;code>TokenReview&lt;/code> API æ¥éªŒè¯ Kubernetes æœåŠ¡å¸æˆ·ä»¤ç‰Œã€‚
å¯¹äºä½¿ç”¨ Vault ä½œä¸ºèµ„æºæä¾›è€…çš„ CSI é©±åŠ¨ç¨‹åºï¼Œå®ƒä»¬éœ€è¦å°† Pod çš„æœåŠ¡å¸æˆ·æä¾›ç»™ Vaultã€‚
ä¾‹å¦‚ï¼Œ&lt;a href="https://github.com/hashicorp/secrets-store-csi-driver-provider-vault">Secret å­˜å‚¨ CSI é©±åŠ¨&lt;/a>å’Œ
&lt;a href="https://github.com/jetstack/cert-manager-csi">è¯ä¹¦ç®¡ç†å™¨ CSI é©±åŠ¨&lt;/a>ã€‚&lt;/p>
&lt;!--
## Short-lived Volumes
To keep short-lived volumes such as certificates effective, CSI drivers can specify `RequiresRepublish=true` in their`CSIDriver` object to have the kubelet periodically call `NodePublishVolume` on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.
-->
&lt;h2 id="çŸ­æ—¶å·">çŸ­æ—¶å·&lt;/h2>
&lt;p>ä¸ºäº†ä½¿è¯¸å¦‚è¯ä¹¦ä¹‹ç±»çš„çŸ­æ—¶å·ä¿æŒæœ‰æ•ˆï¼ŒCSI é©±åŠ¨ç¨‹åºå¯ä»¥åœ¨å…¶ &lt;code>CSIDriver&lt;/code> å¯¹è±¡ä¸­æŒ‡å®š &lt;code>RequiresRepublish=true&lt;/code>ï¼Œ
ä»¥ä½¿ kubelet å®šæœŸé’ˆå¯¹å·²æŒ‚è½½çš„å·è°ƒç”¨ &lt;code>NodePublishVolume&lt;/code>ã€‚
è¿™äº›é‡æ–°å‘å¸ƒæ“ä½œä½¿ CSI é©±åŠ¨ç¨‹åºå¯ä»¥ç¡®ä¿å·å†…å®¹æ˜¯æœ€æ–°çš„ã€‚&lt;/p>
&lt;!--
## Next steps
This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:
-->
&lt;h2 id="ä¸‹ä¸€æ­¥">ä¸‹ä¸€æ­¥&lt;/h2>
&lt;p>æ­¤åŠŸèƒ½æ˜¯ Alpha ç‰ˆï¼Œé¢„è®¡å°†åœ¨ 1.21 ç‰ˆä¸­ç§»è‡³ Beta ç‰ˆã€‚ è¯·å‚é˜…ä»¥ä¸‹ KEP å’Œ CSI æ–‡æ¡£ä¸­çš„æ›´å¤šå†…å®¹ï¼š&lt;/p>
&lt;!--
- [KEP-1855: Service Account Token for CSI Driver](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md)
- [Token Requests](https://kubernetes-csi.github.io/docs/token-requests.html)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md">KEP-1855: CSI é©±åŠ¨ç¨‹åºçš„æœåŠ¡å¸æˆ·ä»¤ç‰Œ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/token-requests.html">ä»¤ç‰Œè¯·æ±‚&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Your feedback is always welcome!
- SIG-Auth [meets regularly](https://github.com/kubernetes/community/tree/master/sig-auth#meetings) and can be reached via [Slack and the mailing list](https://github.com/kubernetes/community/tree/master/sig-auth#contact)
- SIG-Storage [meets regularly](https://github.com/kubernetes/community/tree/master/sig-storage#meetings) and can be reached via [Slack and the mailing list](https://github.com/kubernetes/community/tree/master/sig-storage#contact).
-->
&lt;p>éšæ—¶æ¬¢è¿æ‚¨æä¾›åé¦ˆ!&lt;/p>
&lt;ul>
&lt;li>SIG-Auth &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#meetings">å®šæœŸå¼€ä¼š&lt;/a>ï¼Œå¯ä»¥é€šè¿‡ &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#contact">Slack å’Œé‚®ä»¶åˆ—è¡¨&lt;/a>åŠ å…¥&lt;/li>
&lt;li>SIG-Storage &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">å®šæœŸå¼€ä¼š&lt;/a>ï¼Œå¯ä»¥é€šè¿‡ &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack å’Œé‚®ä»¶åˆ—è¡¨&lt;/a>åŠ å…¥&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.20: æœ€æ–°ç‰ˆæœ¬</title><link>https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/</guid><description>
&lt;!-- ---
layout: blog
title: 'Kubernetes 1.20: The Raddest Release'
date: 2020-12-08
slug: kubernetes-1-20-release-announcement
evergreen: true
--- -->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md">Kubernetes 1.20 å‘å¸ƒå›¢é˜Ÿ&lt;/a>&lt;/p>
&lt;!-- **Authors:** [Kubernetes 1.20 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md) -->
&lt;p>æˆ‘ä»¬å¾ˆé«˜å…´åœ°å®£å¸ƒ Kubernetes 1.20 çš„å‘å¸ƒï¼Œè¿™æ˜¯æˆ‘ä»¬ 2020 å¹´çš„ç¬¬ä¸‰ä¸ªä¹Ÿæ˜¯æœ€åä¸€ä¸ªç‰ˆæœ¬ï¼æ­¤ç‰ˆæœ¬åŒ…å« 42 é¡¹å¢å¼ºåŠŸèƒ½ï¼š11 é¡¹å¢å¼ºåŠŸèƒ½å·²å‡çº§åˆ°ç¨³å®šç‰ˆï¼Œ15 é¡¹å¢å¼ºåŠŸèƒ½æ­£åœ¨è¿›å…¥æµ‹è¯•ç‰ˆï¼Œ16 é¡¹å¢å¼ºåŠŸèƒ½æ­£åœ¨è¿›å…¥ Alpha ç‰ˆã€‚&lt;/p>
&lt;!-- Weâ€™re pleased to announce the release of Kubernetes 1.20, our third and final release of 2020! This release consists of 42 enhancements: 11 enhancements have graduated to stable, 15 enhancements are moving to beta, and 16 enhancements are entering alpha. -->
&lt;p>1.20 å‘å¸ƒå‘¨æœŸåœ¨ä¸Šä¸€ä¸ªå»¶é•¿çš„å‘å¸ƒå‘¨æœŸä¹‹åæ¢å¤åˆ° 11 å‘¨çš„æ­£å¸¸èŠ‚å¥ã€‚è¿™æ˜¯ä¸€æ®µæ—¶é—´ä»¥æ¥åŠŸèƒ½æœ€å¯†é›†çš„ç‰ˆæœ¬ä¹‹ä¸€ï¼šKubernetes åˆ›æ–°å‘¨æœŸä»å‘ˆä¸Šå‡è¶‹åŠ¿ã€‚æ­¤ç‰ˆæœ¬å…·æœ‰æ›´å¤šçš„ Alpha è€Œéç¨³å®šçš„å¢å¼ºåŠŸèƒ½ï¼Œè¡¨æ˜äº‘åŸç”Ÿç”Ÿæ€ç³»ç»Ÿä»æœ‰è®¸å¤šéœ€è¦æ¢ç´¢çš„åœ°æ–¹ã€‚&lt;/p>
&lt;!-- The 1.20 release cycle returned to its normal cadence of 11 weeks following the previous extended release cycle. This is one of the most feature dense releases in a while: the Kubernetes innovation cycle is still trending upward. This release has more alpha than stable enhancements, showing that there is still much to explore in the cloud native ecosystem. -->
&lt;h2 id="major-themes">ä¸»é¢˜&lt;/h2>
&lt;!-- ## Major Themes -->
&lt;h3 id="volume-snapshot-operations-goes-stable">Volume å¿«ç…§æ“ä½œå˜å¾—ç¨³å®š&lt;/h3>
&lt;!-- This feature provides a standard way to trigger volume snapshot operations and allows users to incorporate snapshot operations in a portable manner on any Kubernetes environment and supported storage providers. -->
&lt;p>æ­¤åŠŸèƒ½æä¾›äº†è§¦å‘å·å¿«ç…§æ“ä½œçš„æ ‡å‡†æ–¹æ³•ï¼Œå¹¶å…è®¸ç”¨æˆ·ä»¥å¯ç§»æ¤çš„æ–¹å¼åœ¨ä»»ä½• Kubernetes ç¯å¢ƒå’Œæ”¯æŒçš„å­˜å‚¨æä¾›ç¨‹åºä¸Šåˆå¹¶å¿«ç…§æ“ä½œã€‚&lt;/p>
&lt;!-- Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise-grade, storage administration features for Kubernetes, including application or cluster level backup solutions. -->
&lt;p>æ­¤å¤–ï¼Œè¿™äº› Kubernetes å¿«ç…§åŸè¯­å……å½“åŸºæœ¬æ„å»ºå—ï¼Œè§£é”ä¸º Kubernetes å¼€å‘é«˜çº§ä¼ä¸šçº§å­˜å‚¨ç®¡ç†åŠŸèƒ½çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åº”ç”¨ç¨‹åºæˆ–é›†ç¾¤çº§å¤‡ä»½è§£å†³æ–¹æ¡ˆã€‚&lt;/p>
&lt;!-- Note that snapshot support requires Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. A CSI driver supporting the snapshot functionality must also be deployed on the cluster. -->
&lt;p>è¯·æ³¨æ„ï¼Œå¿«ç…§æ”¯æŒè¦æ±‚ Kubernetes åˆ†é”€å•†æ†ç»‘ Snapshot æ§åˆ¶å™¨ã€Snapshot CRD å’ŒéªŒè¯ webhookã€‚è¿˜å¿…é¡»åœ¨é›†ç¾¤ä¸Šéƒ¨ç½²æ”¯æŒå¿«ç…§åŠŸèƒ½çš„ CSI é©±åŠ¨ç¨‹åºã€‚&lt;/p>
&lt;!-- ### Kubectl Debug Graduates to Beta -->
&lt;h3 id="kubectl-debug-graduates-to-beta">Kubectl Debug åŠŸèƒ½å‡çº§åˆ° Beta&lt;/h3>
&lt;!-- The `kubectl alpha debug` features graduates to beta in 1.20, becoming `kubectl debug`. The feature provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of kubectl include: -->
&lt;p>&lt;code>kubectl alpha debug&lt;/code> åŠŸèƒ½åœ¨ 1.20 ä¸­å‡çº§åˆ°æµ‹è¯•ç‰ˆï¼Œæˆä¸º &lt;code>kubectl debug&lt;/code>. è¯¥åŠŸèƒ½ç›´æ¥ä» kubectl æä¾›å¯¹å¸¸è§è°ƒè¯•å·¥ä½œæµçš„æ”¯æŒã€‚æ­¤ç‰ˆæœ¬çš„ kubectl æ”¯æŒçš„æ•…éšœæ’é™¤åœºæ™¯åŒ…æ‹¬ï¼š&lt;/p>
&lt;!-- * Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.
* Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)
* Troubleshoot on a node by creating a container running in the host namespaces and with access to the hostâ€™s filesystem. -->
&lt;ul>
&lt;li>é€šè¿‡åˆ›å»ºä½¿ç”¨ä¸åŒå®¹å™¨æ˜ åƒæˆ–å‘½ä»¤çš„ pod å‰¯æœ¬ï¼Œå¯¹åœ¨å¯åŠ¨æ—¶å´©æºƒçš„å·¥ä½œè´Ÿè½½è¿›è¡Œæ•…éšœæ’é™¤ã€‚&lt;/li>
&lt;li>é€šè¿‡åœ¨ pod çš„æ–°å‰¯æœ¬æˆ–ä½¿ç”¨ä¸´æ—¶å®¹å™¨ä¸­æ·»åŠ å¸¦æœ‰è°ƒè¯•å·¥å…·çš„æ–°å®¹å™¨æ¥å¯¹ distroless å®¹å™¨è¿›è¡Œæ•…éšœæ’é™¤ã€‚ï¼ˆä¸´æ—¶å®¹å™¨æ˜¯é»˜è®¤æœªå¯ç”¨çš„ alpha åŠŸèƒ½ã€‚ï¼‰&lt;/li>
&lt;li>é€šè¿‡åˆ›å»ºåœ¨ä¸»æœºå‘½åç©ºé—´ä¸­è¿è¡Œå¹¶å¯ä»¥è®¿é—®ä¸»æœºæ–‡ä»¶ç³»ç»Ÿçš„å®¹å™¨æ¥å¯¹èŠ‚ç‚¹è¿›è¡Œæ•…éšœæ’é™¤ã€‚&lt;/li>
&lt;/ul>
&lt;!-- Note that as a new built-in command, `kubectl debug` takes priority over any kubectl plugin named â€œdebugâ€. You must rename the affected plugin. -->
&lt;p>è¯·æ³¨æ„ï¼Œä½œä¸ºæ–°çš„å†…ç½®å‘½ä»¤ï¼Œ&lt;code>kubectl debug&lt;/code> ä¼˜å…ˆäºä»»ä½•åä¸º â€œdebugâ€ çš„ kubectl æ’ä»¶ã€‚ä½ å¿…é¡»é‡å‘½åå—å½±å“çš„æ’ä»¶ã€‚&lt;/p>
&lt;!-- Invocations using `kubectl alpha debug` are now deprecated and will be removed in a subsequent release. Update your scripts to use `kubectl debug`. For more information about `kubectl debug`, see [Debugging Running Pods](https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/). -->
&lt;p>&lt;code>kubectl alpha debug&lt;/code> ç°åœ¨ä¸æ¨èä½¿ç”¨ï¼Œå¹¶å°†åœ¨åç»­ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚æ›´æ–°ä½ çš„è„šæœ¬ä»¥ä½¿ç”¨ &lt;code>kubectl debug&lt;/code>ã€‚ æœ‰å…³æ›´å¤šä¿¡æ¯ &lt;code>kubectl debug&lt;/code>ï¼Œè¯·å‚é˜…[è°ƒè¯•æ­£åœ¨è¿è¡Œçš„ Pod]((&lt;a href="https://kubernetes.io/zh/docs/tasks/debug/debug-application/debug-running-pod/">https://kubernetes.io/zh/docs/tasks/debug/debug-application/debug-running-pod/&lt;/a>)ã€‚&lt;/p>
&lt;!-- ### Beta: API Priority and Fairness -->
&lt;h3 id="æµ‹è¯•ç‰ˆ-api-ä¼˜å…ˆçº§å’Œå…¬å¹³æ€§-beta-api-priority-and-fairness">æµ‹è¯•ç‰ˆï¼šAPI ä¼˜å…ˆçº§å’Œå…¬å¹³æ€§ {#beta-api-priority-and-fairness)&lt;/h3>
&lt;!-- Introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows `kube-apiserver` to categorize incoming requests by priority levels. -->
&lt;p>Kubernetes 1.20 ç”± 1.18 å¼•å…¥ï¼Œç°åœ¨é»˜è®¤å¯ç”¨ API ä¼˜å…ˆçº§å’Œå…¬å¹³æ€§ (APF)ã€‚è¿™å…è®¸ &lt;code>kube-apiserver&lt;/code> æŒ‰ä¼˜å…ˆçº§å¯¹ä¼ å…¥è¯·æ±‚è¿›è¡Œåˆ†ç±»ã€‚&lt;/p>
&lt;!-- ### Alpha with updates: IPV4/IPV6 -->
&lt;h3 id="alpha-with-updates-ipv4-ipv6">Alpha æ›´æ–°ï¼šIPV4/IPV6&lt;/h3>
&lt;!-- The IPv4/IPv6 dual stack has been reimplemented to support dual stack services based on user and community feedback. This allows both IPv4 and IPv6 service cluster IP addresses to be assigned to a single service, and also enables a service to be transitioned from single to dual IP stack and vice versa. -->
&lt;p>åŸºäºç”¨æˆ·å’Œç¤¾åŒºåé¦ˆï¼Œé‡æ–°å®ç°äº† IPv4/IPv6 åŒæ ˆä»¥æ”¯æŒåŒæ ˆæœåŠ¡ã€‚
è¿™å…è®¸å°† IPv4 å’Œ IPv6 æœåŠ¡é›†ç¾¤ IP åœ°å€åˆ†é…ç»™å•ä¸ªæœåŠ¡ï¼Œè¿˜å…è®¸æœåŠ¡ä»å• IP å †æ ˆè½¬æ¢ä¸ºåŒ IP å †æ ˆï¼Œåä¹‹äº¦ç„¶ã€‚&lt;/p>
&lt;!-- ### GA: Process PID Limiting for Stability -->
&lt;h3 id="ga-process-pid-limiting-for-stability">GAï¼šè¿›ç¨‹ PID ç¨³å®šæ€§é™åˆ¶&lt;/h3>
&lt;!-- Process IDs (pids) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine. -->
&lt;p>è¿›ç¨‹ ID (pid) æ˜¯ Linux ä¸»æœºä¸Šçš„åŸºæœ¬èµ„æºã€‚è¾¾åˆ°ä»»åŠ¡é™åˆ¶è€Œä¸è¾¾åˆ°ä»»ä½•å…¶ä»–èµ„æºé™åˆ¶å¹¶å¯¼è‡´ä¸»æœºä¸ç¨³å®šæ˜¯å¾ˆå¯èƒ½å‘ç”Ÿçš„ã€‚&lt;/p>
&lt;!-- Administrators require mechanisms to ensure that user pods cannot induce pid exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that pids are limited among pods in order to ensure they have limited impact to other workloads on the node. -->
&lt;!-- After being enabled-by-default for a year, SIG Node graduates PID Limits to GA on both `SupportNodePidsLimit` (node-to-pod PID isolation) and `SupportPodPidsLimit` (ability to limit PIDs per pod). -->
&lt;p>ç®¡ç†å‘˜éœ€è¦æœºåˆ¶æ¥ç¡®ä¿ç”¨æˆ· pod ä¸ä¼šå¯¼è‡´ pid è€—å°½ï¼Œä»è€Œé˜»æ­¢ä¸»æœºå®ˆæŠ¤ç¨‹åºï¼ˆè¿è¡Œæ—¶ã€kubelet ç­‰ï¼‰è¿è¡Œã€‚æ­¤å¤–ï¼Œé‡è¦çš„æ˜¯è¦ç¡®ä¿ pod ä¹‹é—´çš„ pid å—åˆ°é™åˆ¶ï¼Œä»¥ç¡®ä¿å®ƒä»¬å¯¹èŠ‚ç‚¹ä¸Šçš„å…¶ä»–å·¥ä½œè´Ÿè½½çš„å½±å“æœ‰é™ã€‚
é»˜è®¤å¯ç”¨ä¸€å¹´åï¼ŒSIG Node åœ¨ &lt;code>SupportNodePidsLimit&lt;/code>ï¼ˆèŠ‚ç‚¹åˆ° Pod PID éš”ç¦»ï¼‰å’Œ &lt;code>SupportPodPidsLimit&lt;/code>ï¼ˆé™åˆ¶æ¯ä¸ª Pod çš„ PID çš„èƒ½åŠ›ï¼‰ä¸Šéƒ½å°† PID é™åˆ¶å‡çº§ä¸º GAã€‚&lt;/p>
&lt;!-- ### Alpha: Graceful node shutdown -->
&lt;h3 id="alpha-graceful-node-shutdown">Alphaï¼šèŠ‚ç‚¹ä½“é¢åœ°å…³é—­&lt;/h3>
&lt;!-- Users and cluster administrators expect that pods will adhere to expected pod lifecycle including pod termination. Currently, when a node shuts down, pods do not follow the expected pod termination lifecycle and are not terminated gracefully which can cause issues for some workloads.
The `GracefulNodeShutdown` feature is now in Alpha. `GracefulNodeShutdown` makes the kubelet aware of node system shutdowns, enabling graceful termination of pods during a system shutdown. -->
&lt;p>ç”¨æˆ·å’Œé›†ç¾¤ç®¡ç†å‘˜å¸Œæœ› Pod éµå®ˆé¢„æœŸçš„ Pod ç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬ Pod ç»ˆæ­¢ã€‚ç›®å‰ï¼Œå½“ä¸€ä¸ªèŠ‚ç‚¹å…³é—­æ—¶ï¼ŒPod ä¸ä¼šéµå¾ªé¢„æœŸçš„ Pod ç»ˆæ­¢ç”Ÿå‘½å‘¨æœŸï¼Œä¹Ÿä¸ä¼šæ­£å¸¸ç»ˆæ­¢ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æŸäº›å·¥ä½œè´Ÿè½½å‡ºç°é—®é¢˜ã€‚
è¯¥ &lt;code>GracefulNodeShutdown&lt;/code> åŠŸèƒ½ç°åœ¨å¤„äº Alpha é˜¶æ®µã€‚&lt;code>GracefulNodeShutdown&lt;/code> ä½¿ kubelet çŸ¥é“èŠ‚ç‚¹ç³»ç»Ÿå…³é—­ï¼Œä»è€Œåœ¨ç³»ç»Ÿå…³é—­æœŸé—´æ­£å¸¸ç»ˆæ­¢ podã€‚&lt;/p>
&lt;!-- ## Major Changes -->
&lt;h2 id="major-changes">ä¸»è¦å˜åŒ–&lt;/h2>
&lt;!-- ### Dockershim Deprecation -->
&lt;h3 id="dockershim-deprecation">Dockershim å¼ƒç”¨&lt;/h3>
&lt;!-- Dockershim, the container runtime interface (CRI) shim for Docker is being deprecated. Support for Docker is deprecated and will be removed in a future release. Docker-produced images will continue to work in your cluster with all CRI compliant runtimes as Docker images follow the Open Container Initiative (OCI) image specification.
The Kubernetes community has written a [detailed blog post about deprecation](https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/) with [a dedicated FAQ page for it](https://blog.k8s.io/2020/12/02/dockershim-faq/). -->
&lt;p>Dockershimï¼ŒDocker çš„å®¹å™¨è¿è¡Œæ—¶æ¥å£ (CRI) shim å·²è¢«å¼ƒç”¨ã€‚ä¸æ¨èä½¿ç”¨å¯¹ Docker çš„æ”¯æŒï¼Œå¹¶å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚ç”±äº Docker æ˜ åƒéµå¾ªå¼€æ”¾å®¹å™¨è®¡åˆ’ (OCI) æ˜ åƒè§„èŒƒï¼Œå› æ­¤ Docker ç”Ÿæˆçš„æ˜ åƒå°†ç»§ç»­åœ¨å…·æœ‰æ‰€æœ‰ CRI å…¼å®¹è¿è¡Œæ—¶çš„é›†ç¾¤ä¸­å·¥ä½œã€‚
Kubernetes ç¤¾åŒºå†™äº†ä¸€ç¯‡å…³äºå¼ƒç”¨çš„è¯¦ç»†&lt;a href="https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/">åšå®¢æ–‡ç« &lt;/a>ï¼Œå¹¶ä¸ºå…¶æä¾›äº†ä¸€ä¸ªä¸“é—¨çš„å¸¸è§é—®é¢˜&lt;a href="https://blog.k8s.io/2020/12/02/dockershim-faq/">è§£ç­”é¡µé¢&lt;/a>ã€‚&lt;/p>
&lt;!-- ### Exec Probe Timeout Handling -->
&lt;h3 id="exec-probe-timeout-handling">Exec æ¢æµ‹è¶…æ—¶å¤„ç†&lt;/h3>
&lt;!-- A longstanding bug regarding exec probe timeouts that may impact existing pod definitions has been fixed. Prior to this fix, the field `timeoutSeconds` was not respected for exec probes. Instead, probes would run indefinitely, even past their configured deadline, until a result was returned. With this change, the default value of `1 second` will be applied if a value is not specified and existing pod definitions may no longer be sufficient if a probe takes longer than one second. A feature gate, called `ExecProbeTimeout`, has been added with this fix that enables cluster operators to revert to the previous behavior, but this will be locked and removed in subsequent releases. In order to revert to the previous behavior, cluster operators should set this feature gate to `false`. -->
&lt;p>ä¸€ä¸ªå…³äº exec æ¢æµ‹è¶…æ—¶çš„é•¿æœŸé”™è¯¯å¯èƒ½ä¼šå½±å“ç°æœ‰çš„ pod å®šä¹‰ï¼Œå·²å¾—åˆ°ä¿®å¤ã€‚åœ¨æ­¤ä¿®å¤ä¹‹å‰ï¼Œexec æ¢æµ‹å™¨ä¸è€ƒè™‘ &lt;code>timeoutSeconds&lt;/code> å­—æ®µã€‚ç›¸åï¼Œæ¢æµ‹å°†æ— é™æœŸè¿è¡Œï¼Œç”šè‡³è¶…è¿‡å…¶é…ç½®çš„æˆªæ­¢æ—¥æœŸï¼Œç›´åˆ°è¿”å›ç»“æœã€‚
é€šè¿‡æ­¤æ›´æ”¹ï¼Œå¦‚æœæœªæŒ‡å®šå€¼ï¼Œå°†åº”ç”¨é»˜è®¤å€¼ &lt;code>1 second&lt;/code>ï¼Œå¹¶ä¸”å¦‚æœæ¢æµ‹æ—¶é—´è¶…è¿‡ä¸€ç§’ï¼Œç°æœ‰ pod å®šä¹‰å¯èƒ½ä¸å†è¶³å¤Ÿã€‚
æ–°å¼•å…¥çš„ &lt;code>ExecProbeTimeout&lt;/code> ç‰¹æ€§é—¨æ§æ‰€æä¾›çš„ä¿®å¤ä½¿é›†ç¾¤æ“ä½œå‘˜èƒ½å¤Ÿæ¢å¤åˆ°ä»¥å‰çš„è¡Œä¸ºï¼Œä½†è¿™ç§è¡Œä¸ºå°†åœ¨åç»­ç‰ˆæœ¬ä¸­é”å®šå¹¶åˆ é™¤ã€‚ä¸ºäº†æ¢å¤åˆ°ä»¥å‰çš„è¡Œä¸ºï¼Œé›†ç¾¤è¿è¥å•†åº”è¯¥å°†æ­¤ç‰¹æ€§é—¨æ§è®¾ç½®ä¸º &lt;code>false&lt;/code>ã€‚&lt;/p>
&lt;!-- Please review the updated documentation regarding [configuring probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes) for more details. -->
&lt;p>æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æœ‰å…³é…ç½®æ¢é’ˆçš„&lt;a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">æ›´æ–°æ–‡æ¡£&lt;/a>ã€‚&lt;/p>
&lt;!-- ## Other Updates -->
&lt;h2 id="other-updates">å…¶ä»–æ›´æ–°&lt;/h2>
&lt;!-- ### Graduated to Stable -->
&lt;h3 id="graduated-to-stable">ç¨³å®šç‰ˆ&lt;/h3>
&lt;!-- * [RuntimeClass](https://github.com/kubernetes/enhancements/issues/585)
* [Built-in API Types Defaults](https://github.com/kubernetes/enhancements/issues/1929)
* [Add Pod-Startup Liveness-Probe Holdoff](https://github.com/kubernetes/enhancements/issues/950)
* [Support CRI-ContainerD On Windows](https://github.com/kubernetes/enhancements/issues/1001)
* [SCTP Support for Services](https://github.com/kubernetes/enhancements/issues/614)
* [Adding AppProtocol To Services And Endpoints](https://github.com/kubernetes/enhancements/issues/1507) -->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/585">RuntimeClass&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1929">å†…ç½® API ç±»å‹é»˜è®¤å€¼&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/950">æ·»åŠ äº†å¯¹ Pod å±‚é¢å¯åŠ¨æ¢é’ˆå’Œæ´»è·ƒæ€§æ¢é’ˆçš„æ‰¼åˆ¶&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1001">åœ¨ Windows ä¸Šæ”¯æŒ CRI-ContainerD&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/614">SCTP å¯¹ Services çš„æ”¯æŒ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">å°† AppProtocol æ·»åŠ åˆ° Services å’Œ Endpoints ä¸Š&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- ### Notable Feature Updates -->
&lt;h3 id="notable-feature-updates">å€¼å¾—æ³¨æ„çš„åŠŸèƒ½æ›´æ–°&lt;/h3>
&lt;!-- * [CronJobs](https://github.com/kubernetes/enhancements/issues/19) -->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- # Release notes -->
&lt;h1 id="release-notes">å‘è¡Œè¯´æ˜&lt;/h1>
&lt;!-- You can check out the full details of the 1.20 release in the [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md). -->
&lt;p>ä½ å¯ä»¥åœ¨&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">å‘è¡Œè¯´æ˜&lt;/a>ä¸­æŸ¥çœ‹ 1.20 å‘è¡Œç‰ˆçš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ã€‚&lt;/p>
&lt;!-- # Availability of release -->
&lt;h1 id="availability-of-release">å¯ç”¨çš„å‘å¸ƒ&lt;/h1>
&lt;!-- Kubernetes 1.20 is available for [download on GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0). There are some great resources out there for getting started with Kubernetes. You can check out some [interactive tutorials](https://kubernetes.io/docs/tutorials/) on the main Kubernetes site, or run a local cluster on your machine using Docker containers with [kind](https://kind.sigs.k8s.io). If youâ€™d like to try building a cluster from scratch, check out the [Kubernetes the Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way) tutorial by Kelsey Hightower. -->
&lt;p>Kubernetes 1.20 å¯åœ¨ &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0">GitHub&lt;/a> ä¸Šä¸‹è½½ã€‚æœ‰ä¸€äº›å¾ˆæ£’çš„èµ„æºå¯ä»¥å¸®åŠ©ä½ å¼€å§‹ä½¿ç”¨ Kubernetesã€‚ä½ å¯ä»¥åœ¨ Kubernetes ä¸»ç«™ç‚¹ä¸ŠæŸ¥çœ‹ä¸€äº›&lt;a href="https://kubernetes.io/docs/tutorials/">äº¤äº’å¼æ•™ç¨‹&lt;/a>ï¼Œæˆ–è€…ä½¿ç”¨ &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a> çš„ Docker å®¹å™¨åœ¨ä½ çš„æœºå™¨ä¸Šè¿è¡Œæœ¬åœ°é›†ç¾¤ã€‚å¦‚æœä½ æƒ³å°è¯•ä»å¤´å¼€å§‹æ„å»ºé›†ç¾¤ï¼Œè¯·æŸ¥çœ‹ Kelsey Hightower çš„ &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> æ•™ç¨‹ã€‚&lt;/p>
&lt;!-- # Release Team -->
&lt;h1 id="release-team">å‘å¸ƒå›¢é˜Ÿ&lt;/h1>
&lt;!-- This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Jeremy Rickard, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.20 release for the community. -->
&lt;p>è¿™ä¸ªç‰ˆæœ¬æ˜¯ç”±ä¸€ç¾¤éå¸¸æ•¬ä¸šçš„äººä¿ƒæˆçš„ï¼Œä»–ä»¬åœ¨ä¸–ç•Œä¸Šå‘ç”Ÿçš„è®¸å¤šäº‹æƒ…çš„æ—¶æ®µä½œä¸ºä¸€ä¸ªå›¢é˜Ÿèµ°åˆ°äº†ä¸€èµ·ã€‚
éå¸¸æ„Ÿè°¢å‘å¸ƒè´Ÿè´£äºº Jeremy Rickard ä»¥åŠå‘å¸ƒå›¢é˜Ÿä¸­çš„å…¶ä»–æ‰€æœ‰äººï¼Œæ„Ÿè°¢ä»–ä»¬ç›¸äº’æ”¯æŒï¼Œå¹¶åŠªåŠ›ä¸ºç¤¾åŒºå‘å¸ƒ 1.20 ç‰ˆæœ¬ã€‚&lt;/p>
&lt;!-- # Release Logo -->
&lt;h1 id="release-logo">å‘å¸ƒ Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-08-kubernetes-1.20-release-announcement/laser.png" alt="Kubernetes 1.20 Release Logo">&lt;/p>
&lt;p>&lt;a href="https://www.dictionary.com/browse/rad">raddest&lt;/a>: &lt;em>adjective&lt;/em>, Slang. excellent; wonderful; cool:&lt;/p>
&lt;!-- > The Kubernetes 1.20 Release has been the raddest release yet. -->
&lt;blockquote>
&lt;p>Kubernetes 1.20 ç‰ˆæœ¬æ˜¯è¿„ä»Šä¸ºæ­¢æœ€æ¿€åŠ¨äººå¿ƒçš„ç‰ˆæœ¬ã€‚&lt;/p>
&lt;/blockquote>
&lt;!-- 2020 has been a challenging year for many of us, but Kubernetes contributors have delivered a record-breaking number of enhancements in this release. That is a great accomplishment, so the release lead wanted to end the year with a little bit of levity and pay homage to [Kubernetes 1.14 - Caturnetes](https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14) with a "rad" cat named Humphrey. -->
&lt;p>2020 å¹´å¯¹æˆ‘ä»¬ä¸­çš„è®¸å¤šäººæ¥è¯´éƒ½æ˜¯å……æ»¡æŒ‘æˆ˜çš„ä¸€å¹´ï¼Œä½† Kubernetes è´¡çŒ®è€…åœ¨æ­¤ç‰ˆæœ¬ä¸­æä¾›äº†åˆ›çºªå½•çš„å¢å¼ºåŠŸèƒ½ã€‚è¿™æ˜¯ä¸€é¡¹äº†ä¸èµ·çš„æˆå°±ï¼Œå› æ­¤å‘å¸ƒè´Ÿè´£äººå¸Œæœ›ä»¥ä¸€ç‚¹è½»æ¾çš„æ–¹å¼ç»“æŸè¿™ä¸€å¹´ï¼Œå¹¶å‘ &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14">Kubernetes 1.14 - Caturnetes&lt;/a> å’Œä¸€åªåå« Humphrey çš„ â€œradâ€ çŒ«è‡´æ•¬ã€‚&lt;/p>
&lt;!-- Humphrey is the release lead's cat and has a permanent [`blep`](https://www.inverse.com/article/42316-why-do-cats-blep-science-explains). *Rad* was pretty common slang in the 1990s in the United States, and so were laser backgrounds. Humphrey in a 1990s style school picture felt like a fun way to end the year. Hopefully, Humphrey and his *blep* bring you a little joy at the end of 2020! -->
&lt;p>Humphreyæ˜¯å‘å¸ƒè´Ÿè´£äººçš„çŒ«ï¼Œæœ‰ä¸€ä¸ªæ°¸ä¹…çš„ &lt;code>blep&lt;/code>. åœ¨ 1990 å¹´ä»£ï¼Œ&lt;em>Rad&lt;/em> æ˜¯ç¾å›½éå¸¸æ™®éçš„ä¿šè¯­ï¼Œæ¿€å…‰èƒŒæ™¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚Humphrey åœ¨ 1990 å¹´ä»£é£æ ¼çš„å­¦æ ¡ç…§ç‰‡ä¸­æ„Ÿè§‰åƒæ˜¯ç»“æŸè¿™ä¸€å¹´çš„æœ‰è¶£æ–¹å¼ã€‚å¸Œæœ› Humphrey å’Œå®ƒçš„ &lt;em>blep&lt;/em> åœ¨ 2020 å¹´åº•ç»™ä½ å¸¦æ¥ä¸€ç‚¹å¿«ä¹ï¼&lt;/p>
&lt;!-- The release logo was created by [Henry Hsu - @robotdancebattle](https://www.instagram.com/robotdancebattle/). -->
&lt;p>å‘å¸ƒæ ‡å¿—ç”± &lt;a href="https://www.instagram.com/robotdancebattle/">Henry Hsu - @robotdancebattle&lt;/a> åˆ›å»ºã€‚&lt;/p>
&lt;!-- # User Highlights -->
&lt;h1 id="user-highlights">ç”¨æˆ·äº®ç‚¹&lt;/h1>
&lt;!-- - Apple is operating multi-thousand node Kubernetes clusters in data centers all over the world. Watch [Alena Prokharchyk's KubeCon NA Keynote](https://youtu.be/Tx8qXC-U3KM) to learn more about their cloud native journey. -->
&lt;ul>
&lt;li>Apple æ­£åœ¨ä¸–ç•Œå„åœ°çš„æ•°æ®ä¸­å¿ƒè¿è¡Œæ•°åƒä¸ªèŠ‚ç‚¹çš„ Kubernetes é›†ç¾¤ã€‚è§‚çœ‹ &lt;a href="https://youtu.be/Tx8qXC-U3KM">Alena Prokarchyk&lt;/a> çš„ KubeCon NA ä¸»é¢˜æ¼”è®²ï¼Œäº†è§£æœ‰å…³ä»–ä»¬çš„äº‘åŸç”Ÿä¹‹æ—…çš„æ›´å¤šä¿¡æ¯ã€‚&lt;/li>
&lt;/ul>
&lt;!-- # Project Velocity -->
&lt;h1 id="project-velocity">é¡¹ç›®é€Ÿåº¦&lt;/h1>
&lt;!-- The [CNCF K8s DevStats project](https://k8s.devstats.cncf.io/) aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem. -->
&lt;p>&lt;a href="https://k8s.devstats.cncf.io/">CNCF K8S DevStats é¡¹ç›®&lt;/a>èšé›†äº†è®¸å¤šæœ‰å…³Kuberneteså’Œå„åˆ†é¡¹ç›®çš„é€Ÿåº¦æœ‰è¶£çš„æ•°æ®ç‚¹ã€‚è¿™åŒ…æ‹¬ä»ä¸ªäººè´¡çŒ®åˆ°åšå‡ºè´¡çŒ®çš„å…¬å¸æ•°é‡çš„æ‰€æœ‰å†…å®¹ï¼Œå¹¶ä¸”æ¸…æ¥šåœ°è¯´æ˜äº†ä¸ºå‘å±•è¿™ä¸ªç”Ÿæ€ç³»ç»Ÿæ‰€åšçš„åŠªåŠ›çš„æ·±åº¦å’Œå¹¿åº¦ã€‚&lt;/p>
&lt;!-- In the v1.20 release cycle, which ran for 11 weeks (September 25 to December 9), we saw contributions from [967 companies](https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.19.0%20-%20now&amp;var-metric=contributions) and [1335 individuals](https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.19.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All) ([44 of whom](https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;from=1601006400000&amp;to=1607576399000&amp;var-repogroup_name=Kubernetes) made their first Kubernetes contribution) from [26 countries](https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;from=1601006400000&amp;to=1607576399000&amp;var-period_name=Quarter&amp;var-countries=All&amp;var-repogroup_name=Kubernetes&amp;var-metric=rcommitters&amp;var-cum=countries). -->
&lt;p>åœ¨æŒç»­ 11 å‘¨ï¼ˆ9 æœˆ 25 æ—¥è‡³ 12 æœˆ 9 æ—¥ï¼‰çš„ v1.20 å‘å¸ƒå‘¨æœŸä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ¥è‡ª &lt;a href="https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-period_name=Quarter&amp;amp;var-countries=All&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-metric=rcommitters&amp;amp;var-cum=countries">26 ä¸ªå›½å®¶/åœ°åŒº&lt;/a> çš„ &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions">967 å®¶å…¬å¸&lt;/a> å’Œ &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">1335 åä¸ªäºº&lt;/a>ï¼ˆå…¶ä¸­ &lt;a href="https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-repogroup_name=Kubernetes">44 äºº&lt;/a>é¦–æ¬¡ä¸º Kubernetes åšå‡ºè´¡çŒ®ï¼‰çš„è´¡çŒ®ã€‚&lt;/p>
&lt;!-- # Ecosystem Updates -->
&lt;h1 id="ecosystem-updates">ç”Ÿæ€ç³»ç»Ÿæ›´æ–°&lt;/h1>
&lt;!-- - KubeCon North America just wrapped up three weeks ago, the second such event to be virtual! All talks are [now available to all on-demand](https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut) for anyone still needing to catch up!
- In June, the Kubernetes community formed a new working group as a direct response to the Black Lives Matter protests occurring across America. WG Naming's goal is to remove harmful and unclear language in the Kubernetes project as completely as possible and to do so in a way that is portable to other CNCF projects. A great introductory talk on this important work and how it is conducted was given [at KubeCon 2020 North America](https://sched.co/eukp), and the initial impact of this labor [can actually be seen in the v1.20 release](https://github.com/kubernetes/enhancements/issues/2067).
- Previously announced this summer, [The Certified Kubernetes Security Specialist (CKS) Certification](https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/) was released during Kubecon NA for immediate scheduling! Following the model of CKA and CKAD, the CKS is a performance-based exam, focused on security-themed competencies and domains. This exam is targeted at current CKA holders, particularly those who want to round out their baseline knowledge in securing cloud workloads (which is all of us, right?). -->
&lt;ul>
&lt;li>KubeCon North America ä¸‰å‘¨å‰åˆšåˆšç»“æŸï¼Œè¿™æ˜¯ç¬¬äºŒä¸ªè™šæ‹Ÿçš„æ­¤ç±»æ´»åŠ¨ï¼ç°åœ¨æ‰€æœ‰æ¼”è®²éƒ½å¯ä»¥&lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut">ç‚¹æ’­&lt;/a>ï¼Œä¾›ä»»ä½•éœ€è¦èµ¶ä¸Šçš„äººä½¿ç”¨ï¼&lt;/li>
&lt;li>6 æœˆï¼ŒKubernetes ç¤¾åŒºæˆç«‹äº†ä¸€ä¸ªæ–°çš„å·¥ä½œç»„ï¼Œä½œä¸ºå¯¹ç¾å›½å„åœ°å‘ç”Ÿçš„ Black Lives Matter æŠ—è®®æ´»åŠ¨çš„ç›´æ¥å›åº”ã€‚WG Naming çš„ç›®æ ‡æ˜¯å°½å¯èƒ½å½»åº•åœ°åˆ é™¤ Kubernetes é¡¹ç›®ä¸­æœ‰å®³å’Œä¸æ¸…æ¥šçš„è¯­è¨€ï¼Œå¹¶ä»¥å¯ç§»æ¤åˆ°å…¶ä»– CNCF é¡¹ç›®çš„æ–¹å¼è¿›è¡Œã€‚åœ¨ &lt;a href="https://sched.co/eukp">KubeCon 2020 North America&lt;/a> ä¸Šå°±è¿™é¡¹é‡è¦å·¥ä½œåŠå…¶å¦‚ä½•è¿›è¡Œè¿›è¡Œäº†ç²¾å½©çš„ä»‹ç»æ€§æ¼”è®²ï¼Œè¿™é¡¹å·¥ä½œçš„åˆæ­¥å½±å“&lt;a href="https://github.com/kubernetes/enhancements/issues/2067">å®é™…ä¸Šå¯ä»¥åœ¨ v1.20 ç‰ˆæœ¬ä¸­çœ‹åˆ°&lt;/a>ã€‚&lt;/li>
&lt;li>æ­¤å‰äºä»Šå¹´å¤å¤©å®£å¸ƒï¼Œåœ¨ Kubecon NA æœŸé—´å‘å¸ƒäº†ç»è®¤è¯çš„ &lt;a href="https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/">Kubernetes å®‰å…¨ä¸“å®¶ (CKS) è®¤è¯&lt;/a> ï¼Œä»¥ä¾¿ç«‹å³å®‰æ’ï¼éµå¾ª CKA å’Œ CKAD çš„æ¨¡å‹ï¼ŒCKS æ˜¯ä¸€é¡¹åŸºäºæ€§èƒ½çš„è€ƒè¯•ï¼Œä¾§é‡äºä»¥å®‰å…¨ä¸ºä¸»é¢˜çš„èƒ½åŠ›å’Œé¢†åŸŸã€‚è¯¥è€ƒè¯•é¢å‘å½“å‰çš„ CKA æŒæœ‰è€…ï¼Œå°¤å…¶æ˜¯é‚£äº›æƒ³è¦å®Œå–„å…¶åœ¨ä¿æŠ¤äº‘å·¥ä½œè´Ÿè½½æ–¹é¢çš„åŸºç¡€çŸ¥è¯†çš„äººï¼ˆè¿™æ˜¯æˆ‘ä»¬æ‰€æœ‰äººï¼Œå¯¹å§ï¼Ÿï¼‰ã€‚&lt;/li>
&lt;/ul>
&lt;!-- # Event Updates -->
&lt;h1 id="event-updates">æ´»åŠ¨æ›´æ–°&lt;/h1>
&lt;!-- KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! Registration will open on January 11. You can find more information about the conference [here](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/). Remember that [the CFP](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/) closes on Sunday, December 13, 11:59pm PST! -->
&lt;p>KubeCon + CloudNativeCon Europe 2021 å°†äº 2021 å¹´ 5 æœˆ 4 æ—¥è‡³ 7 æ—¥ä¸¾è¡Œï¼æ³¨å†Œå°†äº 1 æœˆ 11 æ—¥å¼€æ”¾ã€‚ä½ å¯ä»¥åœ¨&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">æ­¤å¤„&lt;/a>æ‰¾åˆ°æœ‰å…³ä¼šè®®çš„æ›´å¤šä¿¡æ¯ã€‚
è¯·è®°ä½ï¼Œ&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/">CFP&lt;/a> å°†äºå¤ªå¹³æ´‹æ ‡å‡†æ—¶é—´ 12 æœˆ 13 æ—¥æ˜ŸæœŸæ—¥æ™šä¸Š 11:59 å…³é—­ï¼&lt;/p>
&lt;!-- # Upcoming release webinar -->
&lt;h1 id="upcoming-release-webinar">å³å°†å‘å¸ƒçš„ç½‘ç»œç ”è®¨ä¼š&lt;/h1>
&lt;!-- Stay tuned for the upcoming release webinar happening this January. -->
&lt;p>è¯·ç»§ç»­å…³æ³¨ä»Šå¹´ 1 æœˆå³å°†ä¸¾è¡Œçš„å‘å¸ƒç½‘ç»œç ”è®¨ä¼šã€‚&lt;/p>
&lt;!-- # Get Involved -->
&lt;h1 id="get-involved">å‚ä¸å…¶ä¸­&lt;/h1>
&lt;!-- If youâ€™re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things youâ€™d like to share with the community, you can join the weekly community meeting, or use any of the following channels: -->
&lt;p>å¦‚æœä½ æœ‰å…´è¶£ä¸º Kubernetes ç¤¾åŒºåšå‡ºè´¡çŒ®ï¼Œé‚£ä¹ˆç‰¹åˆ«å…´è¶£å°ç»„ (SIG) æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚å…¶ä¸­è®¸å¤šå¯èƒ½ç¬¦åˆä½ çš„å…´è¶£ï¼å¦‚æœä½ æœ‰ä»€ä¹ˆæƒ³ä¸ç¤¾åŒºåˆ†äº«çš„å†…å®¹ï¼Œä½ å¯ä»¥å‚åŠ æ¯å‘¨çš„ç¤¾åŒºä¼šè®®ï¼Œæˆ–ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€æ¸ é“ï¼š&lt;/p>
&lt;!-- * Find out more about contributing to Kubernetes at the new [Kubernetes Contributor website](https://www.kubernetes.dev/)
* Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
* Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
* Join the community on [Slack](http://slack.k8s.io/)
* Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
* Read more about whatâ€™s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
* Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team) -->
&lt;ul>
&lt;li>åœ¨æ–°çš„ &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor ç½‘ç«™&lt;/a>ä¸Šäº†è§£æ›´å¤šå…³äºä¸ºKubernetes åšå‡ºè´¡çŒ®çš„ä¿¡æ¯&lt;/li>
&lt;li>åœ¨ Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> ä¸Šå…³æ³¨æˆ‘ä»¬ä»¥è·å–æœ€æ–°æ›´æ–°&lt;/li>
&lt;li>åŠ å…¥å…³äºè®¨è®ºçš„&lt;a href="https://discuss.kubernetes.io/">ç¤¾åŒº&lt;/a>è®¨è®º&lt;/li>
&lt;li>åŠ å…¥ &lt;a href="http://slack.k8s.io/">Slack ç¤¾åŒº&lt;/a>&lt;/li>
&lt;li>åˆ†äº«ä½ çš„ &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">Kubernetes æ•…äº‹&lt;/a>&lt;/li>
&lt;li>åœ¨&lt;a href="https://kubernetes.io/blog/">åšå®¢&lt;/a>ä¸Šé˜…è¯»æ›´å¤šå…³äº Kubernetes å‘ç”Ÿçš„äº‹æƒ…&lt;/li>
&lt;li>äº†è§£æœ‰å…³ &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes å‘å¸ƒå›¢é˜Ÿ&lt;/a>çš„æ›´å¤šä¿¡æ¯&lt;/li>
&lt;/ul></description></item><item><title>Blog: åˆ«æ…Œ: Kubernetes å’Œ Docker</title><link>https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/</guid><description>
&lt;!--
layout: blog
title: "Don't Panic: Kubernetes and Docker"
date: 2020-12-02
slug: dont-panic-kubernetes-and-docker
-->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong> Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan â€œPOPâ€ Papandrea, Jeffrey Sica, Davanum â€œDimsâ€ Srinivas&lt;/p>
&lt;!--
_Update: Kubernetes support for Docker via `dockershim` is now deprecated.
For more information, read the [deprecation notice](/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation).
You can also discuss the deprecation via a dedicated [GitHub issue](https://github.com/kubernetes/kubernetes/issues/106917)._
-->
&lt;p>&lt;em>æ›´æ–°ï¼šKubernetes é€šè¿‡ &lt;code>dockershim&lt;/code> å¯¹ Docker çš„æ”¯æŒç°å·²å¼ƒç”¨ã€‚
æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯»&lt;a href="https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">å¼ƒç”¨é€šçŸ¥&lt;/a>ã€‚
ä½ è¿˜å¯ä»¥é€šè¿‡ä¸“é—¨çš„ &lt;a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub issue&lt;/a> è®¨è®ºå¼ƒç”¨ã€‚&lt;/em>&lt;/p>
&lt;!--
Kubernetes is [deprecating
Docker](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation)
as a container runtime after v1.20.
-->
&lt;p>Kubernetes ä»ç‰ˆæœ¬ v1.20 ä¹‹åï¼Œ&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">å¼ƒç”¨ Docker&lt;/a>
è¿™ä¸ªå®¹å™¨è¿è¡Œæ—¶ã€‚&lt;/p>
&lt;!--
**You do not need to panic. Itâ€™s not as dramatic as it sounds.**
-->
&lt;p>&lt;strong>ä¸å¿…æ…Œå¼ ï¼Œè¿™ä»¶äº‹å¹¶æ²¡æœ‰å¬èµ·æ¥é‚£ä¹ˆå“äººã€‚&lt;/strong>&lt;/p>
&lt;!--
TL;DR Docker as an underlying runtime is being deprecated in favor of runtimes
that use the [Container Runtime Interface (CRI)](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)
created for Kubernetes. Docker-produced images will continue to work in your
cluster with all runtimes, as they always have.
-->
&lt;p>å¼ƒç”¨ Docker è¿™ä¸ªåº•å±‚è¿è¡Œæ—¶ï¼Œè½¬è€Œæ”¯æŒç¬¦åˆä¸º Kubernetes åˆ›å»ºçš„å®¹å™¨è¿è¡Œæ¥å£
&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface (CRI)&lt;/a>
çš„è¿è¡Œæ—¶ã€‚
Docker æ„å»ºçš„é•œåƒï¼Œå°†åœ¨ä½ çš„é›†ç¾¤çš„æ‰€æœ‰è¿è¡Œæ—¶ä¸­ç»§ç»­å·¥ä½œï¼Œä¸€å¦‚æ—¢å¾€ã€‚&lt;/p>
&lt;!--
If youâ€™re an end-user of Kubernetes, not a whole lot will be changing for you.
This doesnâ€™t mean the death of Docker, and it doesnâ€™t mean you canâ€™t, or
shouldnâ€™t, use Docker as a development tool anymore. Docker is still a useful
tool for building containers, and the images that result from running `docker
build` can still run in your Kubernetes cluster.
-->
&lt;p>å¦‚æœä½ æ˜¯ Kubernetes çš„ç»ˆç«¯ç”¨æˆ·ï¼Œè¿™å¯¹ä½ ä¸ä¼šæœ‰å¤ªå¤§å½±å“ã€‚
è¿™äº‹å¹¶ä¸æ„å‘³ç€ Docker å·²æ­»ã€ä¹Ÿä¸æ„å‘³ç€ä½ ä¸èƒ½æˆ–ä¸è¯¥ç»§ç»­æŠŠ Docker ç”¨ä½œå¼€å‘å·¥å…·ã€‚
Docker ä»ç„¶æ˜¯æ„å»ºå®¹å™¨çš„åˆ©å™¨ï¼Œä½¿ç”¨å‘½ä»¤ &lt;code>docker build&lt;/code> æ„å»ºçš„é•œåƒåœ¨ Kubernetes é›†ç¾¤ä¸­ä»ç„¶å¯ä»¥è¿è¡Œã€‚&lt;/p>
&lt;!--
If youâ€™re using a managed Kubernetes service like GKE, EKS, or AKS (which [defaults to containerd](https://github.com/Azure/AKS/releases/tag/2020-11-16)) you will need to
make sure your worker nodes are using a supported container runtime before
Docker support is removed in a future version of Kubernetes. If you have node
customizations you may need to update them based on your environment and runtime
requirements. Please work with your service provider to ensure proper upgrade
testing and planning.
-->
&lt;p>å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ GKEã€EKSã€æˆ– AKS
(&lt;a href="https://github.com/Azure/AKS/releases/tag/2020-11-16">é»˜è®¤ä½¿ç”¨ containerd&lt;/a>)&lt;br>
è¿™ç±»æ‰˜ç®¡ Kubernetes æœåŠ¡ï¼Œä½ éœ€è¦åœ¨ Kubernetes åç»­ç‰ˆæœ¬ç§»é™¤å¯¹ Docker æ”¯æŒä¹‹å‰ï¼Œ
ç¡®è®¤å·¥ä½œèŠ‚ç‚¹ä½¿ç”¨äº†è¢«æ”¯æŒçš„å®¹å™¨è¿è¡Œæ—¶ã€‚
å¦‚æœä½ çš„èŠ‚ç‚¹è¢«å®šåˆ¶è¿‡ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®ä½ è‡ªå·±çš„ç¯å¢ƒå’Œè¿è¡Œæ—¶éœ€æ±‚æ›´æ–°å®ƒä»¬ã€‚
è¯·ä¸ä½ çš„æœåŠ¡ä¾›åº”å•†åä½œï¼Œç¡®ä¿åšå‡ºé€‚å½“çš„å‡çº§æµ‹è¯•å’Œè®¡åˆ’ã€‚&lt;/p>
&lt;!--
If youâ€™re rolling your own clusters, you will also need to make changes to avoid
your clusters breaking. At v1.20, you will get a deprecation warning for Docker.
When Docker runtime support is removed in a future release (currently planned
for the 1.22 release in late 2021) of Kubernetes it will no longer be supported
and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports
the docker daemon configurations you currently use (e.g. logging).
-->
&lt;p>å¦‚æœä½ æ­£åœ¨è¿è¥ä½ è‡ªå·±çš„é›†ç¾¤ï¼Œé‚£è¿˜åº”è¯¥åšäº›å·¥ä½œï¼Œä»¥é¿å…é›†ç¾¤ä¸­æ–­ã€‚
åœ¨ v1.20 ç‰ˆä¸­ï¼Œä½ ä»…ä¼šå¾—åˆ°ä¸€ä¸ª Docker çš„å¼ƒç”¨è­¦å‘Šã€‚
å½“å¯¹ Docker è¿è¡Œæ—¶çš„æ”¯æŒåœ¨ Kubernetes æŸä¸ªåç»­å‘è¡Œç‰ˆï¼ˆç›®å‰çš„è®¡åˆ’æ˜¯ 2021 å¹´æ™šäº›æ—¶å€™çš„ 1.22 ç‰ˆï¼‰ä¸­è¢«ç§»é™¤æ—¶ï¼Œ
ä½ éœ€è¦åˆ‡æ¢åˆ° containerd æˆ– CRI-O ç­‰å…¼å®¹çš„å®¹å™¨è¿è¡Œæ—¶ã€‚
åªè¦ç¡®ä¿ä½ é€‰æ‹©çš„è¿è¡Œæ—¶æ”¯æŒä½ å½“å‰ä½¿ç”¨çš„ Docker å®ˆæŠ¤è¿›ç¨‹é…ç½®ï¼ˆä¾‹å¦‚ loggingï¼‰ã€‚&lt;/p>
&lt;!--
## So why the confusion and what is everyone freaking out about?
-->
&lt;h2 id="so-why-the-confusion-and-what-is-everyone-freaking-out-about">é‚£ä¸ºä»€ä¹ˆä¼šæœ‰è¿™æ ·çš„å›°æƒ‘ï¼Œä¸ºä»€ä¹ˆæ¯ä¸ªäººè¦å®³æ€•å‘¢ï¼Ÿ&lt;/h2>
&lt;!--
Weâ€™re talking about two different environments here, and thatâ€™s creating
confusion. Inside of your Kubernetes cluster, thereâ€™s a thing called a container
runtime thatâ€™s responsible for pulling and running your container images. Docker
is a popular choice for that runtime (other common options include containerd
and CRI-O), but Docker was not designed to be embedded inside Kubernetes, and
that causes a problem.
-->
&lt;p>æˆ‘ä»¬åœ¨è¿™é‡Œè®¨è®ºçš„æ˜¯ä¸¤å¥—ä¸åŒçš„ç¯å¢ƒï¼Œè¿™å°±æ˜¯é€ æˆå›°æƒ‘çš„æ ¹æºã€‚
åœ¨ä½ çš„ Kubernetes é›†ç¾¤ä¸­ï¼Œæœ‰ä¸€ä¸ªå«åšå®¹å™¨è¿è¡Œæ—¶çš„ä¸œè¥¿ï¼Œå®ƒè´Ÿè´£æ‹‰å–å¹¶è¿è¡Œå®¹å™¨é•œåƒã€‚
Docker å¯¹äºè¿è¡Œæ—¶æ¥è¯´æ˜¯ä¸€ä¸ªæµè¡Œçš„é€‰æ‹©ï¼ˆå…¶ä»–å¸¸è§çš„é€‰æ‹©åŒ…æ‹¬ containerd å’Œ CRI-Oï¼‰ï¼Œ
ä½† Docker å¹¶éè®¾è®¡ç”¨æ¥åµŒå…¥åˆ° Kubernetesï¼Œè¿™å°±æ˜¯é—®é¢˜æ‰€åœ¨ã€‚&lt;/p>
&lt;!--
You see, the thing we call â€œDockerâ€ isnâ€™t actually one thing&amp;mdash;itâ€™s an entire
tech stack, and one part of it is a thing called â€œcontainerd,â€ which is a
high-level container runtime by itself. Docker is cool and useful because it has
a lot of UX enhancements that make it really easy for humans to interact with
while weâ€™re doing development work, but those UX enhancements arenâ€™t necessary
for Kubernetes, because it isnâ€™t a human.
-->
&lt;p>ä½ çœ‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º â€œDockerâ€ çš„ç‰©ä»¶å®é™…ä¸Šå¹¶ä¸æ˜¯ä¸€ä¸ªç‰©ä»¶â€”â€”å®ƒæ˜¯ä¸€ä¸ªå®Œæ•´çš„æŠ€æœ¯å †æ ˆï¼Œ
å®ƒå…¶ä¸­ä¸€ä¸ªå«åš â€œcontainerdâ€ çš„éƒ¨ä»¶æœ¬èº«ï¼Œæ‰æ˜¯ä¸€ä¸ªé«˜çº§å®¹å™¨è¿è¡Œæ—¶ã€‚
Docker æ—¢é…·ç‚«åˆå®ç”¨ï¼Œå› ä¸ºå®ƒæä¾›äº†å¾ˆå¤šç”¨æˆ·ä½“éªŒå¢å¼ºåŠŸèƒ½ï¼Œè€Œè¿™ç®€åŒ–äº†æˆ‘ä»¬åšå¼€å‘å·¥ä½œæ—¶çš„æ“ä½œï¼Œ
Kubernetes ç”¨ä¸åˆ°è¿™äº›å¢å¼ºçš„ç”¨æˆ·ä½“éªŒï¼Œæ¯•ç«Ÿå®ƒå¹¶éäººç±»ã€‚&lt;/p>
&lt;!--
As a result of this human-friendly abstraction layer, your Kubernetes cluster
has to use another tool called Dockershim to get at what it really needs, which
is containerd. Thatâ€™s not great, because it gives us another thing that has to
be maintained and can possibly break. Whatâ€™s actually happening here is that
Dockershim is being removed from Kubelet as early as v1.23 release, which
removes support for Docker as a container runtime as a result. You might be
thinking to yourself, but if containerd is included in the Docker stack, why
does Kubernetes need the Dockershim?
-->
&lt;p>å› ä¸ºè¿™ä¸ªç”¨æˆ·å‹å¥½çš„æŠ½è±¡å±‚ï¼ŒKubernetes é›†ç¾¤ä¸å¾—ä¸å¼•å…¥ä¸€ä¸ªå«åš Dockershim çš„å·¥å…·æ¥è®¿é—®å®ƒçœŸæ­£éœ€è¦çš„ containerdã€‚
è¿™ä¸æ˜¯ä¸€ä»¶å¥½äº‹ï¼Œå› ä¸ºè¿™å¼•å…¥äº†é¢å¤–çš„è¿ç»´å·¥ä½œé‡ï¼Œè€Œä¸”è¿˜å¯èƒ½å‡ºé”™ã€‚
å®é™…ä¸Šæ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…å°±æ˜¯ï¼šDockershim å°†åœ¨ä¸æ—©äº v1.23 ç‰ˆä¸­ä» kubelet ä¸­è¢«ç§»é™¤ï¼Œä¹Ÿå°±å–æ¶ˆå¯¹ Docker å®¹å™¨è¿è¡Œæ—¶çš„æ”¯æŒã€‚
ä½ å¿ƒé‡Œå¯èƒ½ä¼šæƒ³ï¼Œå¦‚æœ containerd å·²ç»åŒ…å«åœ¨ Docker å †æ ˆä¸­ï¼Œä¸ºä»€ä¹ˆ Kubernetes éœ€è¦ Dockershimã€‚&lt;/p>
&lt;!--
Docker isnâ€™t compliant with CRI, the [Container Runtime Interface](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/).
If it were, we wouldnâ€™t need the shim, and this wouldnâ€™t be a thing. But itâ€™s
not the end of the world, and you donâ€™t need to panic&amp;mdash;you just need to change
your container runtime from Docker to another supported container runtime.
-->
&lt;p>Docker ä¸å…¼å®¹ CRIï¼Œ
&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">å®¹å™¨è¿è¡Œæ—¶æ¥å£&lt;/a>ã€‚
å¦‚æœæ”¯æŒï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦è¿™ä¸ª shim äº†ï¼Œä¹Ÿå°±æ²¡é—®é¢˜äº†ã€‚
ä½†è¿™ä¹Ÿä¸æ˜¯ä¸–ç•Œæœ«æ—¥ï¼Œä½ ä¹Ÿä¸éœ€è¦ææ…Œâ€”â€”ä½ å”¯ä¸€è¦åšçš„å°±æ˜¯æŠŠä½ çš„å®¹å™¨è¿è¡Œæ—¶ä» Docker åˆ‡æ¢åˆ°å…¶ä»–å—æ”¯æŒçš„å®¹å™¨è¿è¡Œæ—¶ã€‚&lt;/p>
&lt;!--
One thing to note: If you are relying on the underlying docker socket
(`/var/run/docker.sock`) as part of a workflow within your cluster today, moving
to a different runtime will break your ability to use it. This pattern is often
called Docker in Docker. There are lots of options out there for this specific
use case including things like
[kaniko](https://github.com/GoogleContainerTools/kaniko),
[img](https://github.com/genuinetools/img), and
[buildah](https://github.com/containers/buildah).
-->
&lt;p>è¦æ³¨æ„ä¸€ç‚¹ï¼šå¦‚æœä½ ä¾èµ–åº•å±‚çš„ Docker å¥—æ¥å­—(&lt;code>/var/run/docker.sock&lt;/code>)ï¼Œä½œä¸ºä½ é›†ç¾¤ä¸­å·¥ä½œæµçš„ä¸€éƒ¨åˆ†ï¼Œ
åˆ‡æ¢åˆ°ä¸åŒçš„è¿è¡Œæ—¶ä¼šå¯¼è‡´ä½ æ— æ³•ä½¿ç”¨å®ƒã€‚
è¿™ç§æ¨¡å¼ç»å¸¸è¢«ç§°ä¹‹ä¸ºåµŒå¥— Dockerï¼ˆDocker in Dockerï¼‰ã€‚
å¯¹äºè¿™ç§ç‰¹æ®Šçš„åœºæ™¯ï¼Œæœ‰å¾ˆå¤šé€‰é¡¹ï¼Œæ¯”å¦‚ï¼š
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>ã€
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>ã€å’Œ
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>ã€‚&lt;/p>
&lt;!--
## What does this change mean for developers, though? Do we still write Dockerfiles? Do we still build things with Docker?
-->
&lt;h2 id="what-does-this-change-mean-for-developers">é‚£ä¹ˆï¼Œè¿™ä¸€æ”¹å˜å¯¹å¼€å‘äººå‘˜æ„å‘³ç€ä»€ä¹ˆï¼Ÿæˆ‘ä»¬è¿˜è¦å†™ Dockerfile å—ï¼Ÿè¿˜èƒ½ç”¨ Docker æ„å»ºé•œåƒå—ï¼Ÿ&lt;/h2>
&lt;!--
This change addresses a different environment than most folks use to interact
with Docker. The Docker installation youâ€™re using in development is unrelated to
the Docker runtime inside your Kubernetes cluster. Itâ€™s confusing, we understand.
As a developer, Docker is still useful to you in all the ways it was before this
change was announced. The image that Docker produces isnâ€™t really a
Docker-specific image&amp;mdash;itâ€™s an OCI ([Open Container Initiative](https://opencontainers.org/)) image.
Any OCI-compliant image, regardless of the tool you use to build it, will look
the same to Kubernetes. Both [containerd](https://containerd.io/) and
[CRI-O](https://cri-o.io/) know how to pull those images and run them. This is
why we have a standard for what containers should look like.
-->
&lt;p>æ­¤æ¬¡æ”¹å˜å¸¦æ¥äº†ä¸€ä¸ªä¸åŒçš„ç¯å¢ƒï¼Œè¿™ä¸åŒäºæˆ‘ä»¬å¸¸ç”¨çš„ Docker äº¤äº’æ–¹å¼ã€‚
ä½ åœ¨å¼€å‘ç¯å¢ƒä¸­ç”¨çš„ Docker å’Œä½  Kubernetes é›†ç¾¤ä¸­çš„ Docker è¿è¡Œæ—¶æ— å…³ã€‚
æˆ‘ä»¬çŸ¥é“è¿™å¬èµ·æ¥è®©äººå›°æƒ‘ã€‚
å¯¹äºå¼€å‘äººå‘˜ï¼ŒDocker ä»æ‰€æœ‰è§’åº¦æ¥çœ‹ä»ç„¶æœ‰ç”¨ï¼Œå°±è·Ÿè¿™æ¬¡æ”¹å˜ä¹‹å‰ä¸€æ ·ã€‚
Docker æ„å»ºçš„é•œåƒå¹¶ä¸æ˜¯ Docker ç‰¹æœ‰çš„é•œåƒâ€”â€”å®ƒæ˜¯ä¸€ä¸ª
OCIï¼ˆ&lt;a href="https://opencontainers.org/">å¼€æ”¾å®¹å™¨æ ‡å‡†&lt;/a>ï¼‰é•œåƒã€‚
ä»»ä¸€ OCI å…¼å®¹çš„é•œåƒï¼Œä¸ç®¡å®ƒæ˜¯ç”¨ä»€ä¹ˆå·¥å…·æ„å»ºçš„ï¼Œåœ¨ Kubernetes çš„è§’åº¦æ¥çœ‹éƒ½æ˜¯ä¸€æ ·çš„ã€‚
&lt;a href="https://containerd.io/">containerd&lt;/a> å’Œ
&lt;a href="https://cri-o.io/">CRI-O&lt;/a>
ä¸¤è€…éƒ½çŸ¥é“æ€ä¹ˆæ‹‰å–å¹¶è¿è¡Œè¿™äº›é•œåƒã€‚
è¿™å°±æ˜¯æˆ‘ä»¬åˆ¶å®šå®¹å™¨æ ‡å‡†çš„åŸå› ã€‚&lt;/p>
&lt;!--
So, this change is coming. Itâ€™s going to cause issues for some, but it isnâ€™t
catastrophic, and generally itâ€™s a good thing. Depending on how you interact
with Kubernetes, this could mean nothing to you, or it could mean a bit of work.
In the long run, itâ€™s going to make things easier. If this is still confusing
for you, thatâ€™s okay&amp;mdash;thereâ€™s a lot going on here; Kubernetes has a lot of
moving parts, and nobody is an expert in 100% of it. We encourage any and all
questions regardless of experience level or complexity! Our goal is to make sure
everyone is educated as much as possible on the upcoming changes. We hope
this has answered most of your questions and soothed some anxieties! â¤ï¸
-->
&lt;p>æ‰€ä»¥ï¼Œæ”¹å˜å·²ç»å‘ç”Ÿã€‚
å®ƒç¡®å®å¸¦æ¥äº†ä¸€äº›é—®é¢˜ï¼Œä½†è¿™ä¸æ˜¯ä¸€ä¸ªç¾éš¾ï¼Œæ€»çš„è¯´æ¥ï¼Œè¿™è¿˜æ˜¯ä¸€ä»¶å¥½äº‹ã€‚
æ ¹æ®ä½ æ“ä½œ Kubernetes çš„æ–¹å¼çš„ä¸åŒï¼Œè¿™å¯èƒ½å¯¹ä½ ä¸æ„æˆä»»ä½•é—®é¢˜ï¼Œæˆ–è€…ä¹Ÿåªæ˜¯æ„å‘³ç€ä¸€ç‚¹ç‚¹çš„å·¥ä½œé‡ã€‚
ä»ä¸€ä¸ªé•¿è¿œçš„è§’åº¦çœ‹ï¼Œå®ƒä½¿å¾—äº‹æƒ…æ›´ç®€å•ã€‚
å¦‚æœä½ è¿˜åœ¨å›°æƒ‘ï¼Œä¹Ÿæ²¡é—®é¢˜â€”â€”è¿™é‡Œè¿˜æœ‰å¾ˆå¤šäº‹æƒ…ï¼›
Kubernetes æœ‰å¾ˆå¤šå˜åŒ–ä¸­çš„åŠŸèƒ½ï¼Œæ²¡æœ‰äººæ˜¯100%çš„ä¸“å®¶ã€‚
æˆ‘ä»¬é¼“åŠ±ä½ æå‡ºä»»ä½•é—®é¢˜ï¼Œæ— è®ºæ°´å¹³é«˜ä½ã€é—®é¢˜éš¾æ˜“ã€‚
æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç¡®ä¿æ‰€æœ‰äººéƒ½èƒ½åœ¨å³å°†åˆ°æ¥çš„æ”¹å˜ä¸­è·å¾—è¶³å¤Ÿçš„äº†è§£ã€‚
æˆ‘ä»¬å¸Œæœ›è¿™å·²ç»å›ç­”äº†ä½ çš„å¤§éƒ¨åˆ†é—®é¢˜ï¼Œå¹¶ç¼“è§£äº†ä¸€äº›ç„¦è™‘ï¼â¤ï¸&lt;/p>
&lt;!--
Looking for more answers? Check out our accompanying [Dockershim Removal FAQ](/blog/2022/02/17/dockershim-faq/) _(updated February 2022)_.
-->
&lt;p>è¿˜åœ¨å¯»æ±‚æ›´å¤šç­”æ¡ˆå—ï¼Ÿè¯·å‚è€ƒæˆ‘ä»¬é™„å¸¦çš„
&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">ç§»é™¤ Dockershim çš„å¸¸è§é—®é¢˜&lt;/a> &lt;em>(2022å¹´2æœˆæ›´æ–°)&lt;/em>ã€‚&lt;/p></description></item><item><title>Blog: å¼ƒç”¨ Dockershim çš„å¸¸è§é—®é¢˜</title><link>https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/</guid><description>
&lt;!--
layout: blog
title: "Dockershim Deprecation FAQ"
date: 2020-12-02
slug: dockershim-faq
aliases: [ '/dockershim' ]
-->
&lt;!--
_**Update**: There is a [newer version](/blog/2022/02/17/dockershim-faq/) of this article available._
-->
&lt;p>&lt;em>&lt;strong>æ›´æ–°&lt;/strong>ï¼šæœ¬æ–‡æœ‰&lt;a href="https://kubernetes.io/zh/blog/2022/02/17/dockershim-faq/">è¾ƒæ–°ç‰ˆæœ¬&lt;/a>ã€‚&lt;/em>&lt;/p>
&lt;!--
This document goes over some frequently asked questions regarding the Dockershim
deprecation announced as a part of the Kubernetes v1.20 release. For more detail
on the deprecation of Docker as a container runtime for Kubernetes kubelets, and
what that means, check out the blog post
[Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/).
Also, you can read [check whether Dockershim deprecation affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/) to check whether it does.
-->
&lt;p>æœ¬æ–‡å›é¡¾äº†è‡ª Kubernetes v1.20 ç‰ˆå®£å¸ƒå¼ƒç”¨ Dockershim ä»¥æ¥æ‰€å¼•å‘çš„ä¸€äº›å¸¸è§é—®é¢˜ã€‚
å…³äº Kubernetes kubelets ä»å®¹å™¨è¿è¡Œæ—¶çš„è§’åº¦å¼ƒç”¨ Docker çš„ç»†èŠ‚ä»¥åŠè¿™äº›ç»†èŠ‚èƒŒåçš„å«ä¹‰ï¼Œè¯·å‚è€ƒåšæ–‡
&lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">åˆ«æ…Œ: Kubernetes å’Œ Docker&lt;/a>ã€‚&lt;/p>
&lt;p>æ­¤å¤–ï¼Œä½ å¯ä»¥é˜…è¯» &lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">æ£€æŸ¥ Dockershim å¼ƒç”¨æ˜¯å¦å½±å“ä½ &lt;/a>
ä»¥æ£€æŸ¥å®ƒæ˜¯å¦ä¼šå½±å“ä½ ã€‚&lt;/p>
&lt;!--
### Why is dockershim being deprecated?
-->
&lt;h3 id="why-is-dockershim-being-deprecated">ä¸ºä»€ä¹ˆå¼ƒç”¨ dockershim&lt;/h3>
&lt;!--
Maintaining dockershim has become a heavy burden on the Kubernetes maintainers.
The CRI standard was created to reduce this burden and allow smooth interoperability
of different container runtimes. Docker itself doesn't currently implement CRI,
thus the problem.
-->
&lt;p>ç»´æŠ¤ dockershim å·²ç»æˆä¸º Kubernetes ç»´æŠ¤è€…è‚©å¤´ä¸€ä¸ªæ²‰é‡çš„è´Ÿæ‹…ã€‚
åˆ›å»º CRI æ ‡å‡†å°±æ˜¯ä¸ºäº†å‡è½»è¿™ä¸ªè´Ÿæ‹…ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å¢åŠ ä¸åŒå®¹å™¨è¿è¡Œæ—¶ä¹‹é—´å¹³æ»‘çš„äº’æ“ä½œæ€§ã€‚
ä½†åè§‚ Docker å´è‡³ä»Šä¹Ÿæ²¡æœ‰å®ç° CRIï¼Œæ‰€ä»¥éº»çƒ¦å°±æ¥äº†ã€‚&lt;/p>
&lt;!--
Dockershim was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
-->
&lt;p>Dockershim å‘æ¥éƒ½æ˜¯ä¸€ä¸ªä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼ˆå› æ­¤å¾—åï¼šshimï¼‰ã€‚
ä½ å¯ä»¥è¿›ä¸€æ­¥é˜…è¯»
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">ç§»é™¤ Kubernetes å¢å¼ºæ–¹æ¡ˆ Dockershim&lt;/a>
ä»¥äº†è§£ç›¸å…³çš„ç¤¾åŒºè®¨è®ºå’Œè®¡åˆ’ã€‚&lt;/p>
&lt;!--
Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.
-->
&lt;p>æ­¤å¤–ï¼Œä¸ dockershim ä¸å…¼å®¹çš„ä¸€äº›ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼šæ§åˆ¶ç»„ï¼ˆcgoupsï¼‰v2 å’Œç”¨æˆ·åå­—ç©ºé—´ï¼ˆuser namespaceï¼‰ï¼Œå·²ç»åœ¨æ–°çš„ CRI è¿è¡Œæ—¶ä¸­è¢«å®ç°ã€‚
ç§»é™¤å¯¹ dockershim çš„æ”¯æŒå°†åŠ é€Ÿè¿™äº›é¢†åŸŸçš„å‘å±•ã€‚&lt;/p>
&lt;!--
### Can I still use Docker in Kubernetes 1.20?
-->
&lt;h3 id="can-I-still-use-docker-in-kubernetes-1.20">åœ¨ Kubernetes 1.20 ç‰ˆæœ¬ä¸­ï¼Œæˆ‘è¿˜å¯ä»¥ç”¨ Docker å—ï¼Ÿ&lt;/h3>
&lt;!--
Yes, the only thing changing in 1.20 is a single warning log printed at [kubelet]
startup if using Docker as the runtime.
-->
&lt;p>å½“ç„¶å¯ä»¥ï¼Œåœ¨ 1.20 ç‰ˆæœ¬ä¸­ä»…æœ‰çš„æ”¹å˜å°±æ˜¯ï¼šå¦‚æœä½¿ç”¨ Docker è¿è¡Œæ—¶ï¼Œå¯åŠ¨
&lt;a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
çš„è¿‡ç¨‹ä¸­å°†æ‰“å°ä¸€æ¡è­¦å‘Šæ—¥å¿—ã€‚&lt;/p>
&lt;!--
### When will dockershim be removed?
-->
&lt;h3 id="when-will-dockershim-be-removed">ä»€ä¹ˆæ—¶å€™ç§»é™¤ dockershim&lt;/h3>
&lt;!--
Given the impact of this change, we are using an extended deprecation timeline.
It will not be removed before Kubernetes 1.22, meaning the earliest release without
dockershim would be 1.23 in late 2021.
_Update_: removal of dockershim is scheduled for Kubernetes v1.24, see
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
We will be working closely with vendors and other ecosystem groups to ensure a smooth transition and will evaluate
things as the situation evolves.
-->
&lt;p>è€ƒè™‘åˆ°æ­¤æ”¹å˜å¸¦æ¥çš„å½±å“ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŠ é•¿çš„åºŸå¼ƒæ—¶é—´è¡¨ã€‚
åœ¨ Kubernetes 1.22 ç‰ˆä¹‹å‰ï¼Œå®ƒä¸ä¼šè¢«å½»åº•ç§»é™¤ï¼›æ¢å¥è¯è¯´ï¼Œdockershim è¢«ç§»é™¤çš„æœ€æ—©ç‰ˆæœ¬ä¼šæ˜¯ 2021 å¹´åº•å‘å¸ƒçš„ 1.23 ç‰ˆã€‚
&lt;em>æ›´æ–°&lt;/em>ï¼šdockershim è®¡åˆ’åœ¨ Kubernetes 1.24 ç‰ˆè¢«ç§»é™¤ï¼Œ
è¯·å‚é˜…&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">ç§»é™¤ Kubernetes å¢å¼ºæ–¹æ¡ˆ Dockershim&lt;/a>ã€‚
æˆ‘ä»¬å°†ä¸ä¾›åº”å•†ä»¥åŠå…¶ä»–ç”Ÿæ€å›¢é˜Ÿç´§å¯†åˆä½œï¼Œç¡®ä¿é¡ºåˆ©è¿‡æ¸¡ï¼Œå¹¶å°†ä¾æ®äº‹æ€çš„å‘å±•è¯„ä¼°åç»­äº‹é¡¹ã€‚&lt;/p>
&lt;!--
### Will my existing Docker images still work?
-->
&lt;h3 id="will-my-existing-docker-image-still-work">æˆ‘ç°æœ‰çš„ Docker é•œåƒè¿˜èƒ½æ­£å¸¸å·¥ä½œå—ï¼Ÿ&lt;/h3>
&lt;!--
Yes, the images produced from `docker build` will work with all CRI implementations.
All your existing images will still work exactly the same.
-->
&lt;p>å½“ç„¶å¯ä»¥ï¼Œ&lt;code>docker build&lt;/code> åˆ›å»ºçš„é•œåƒé€‚ç”¨äºä»»ä½• CRI å®ç°ã€‚
æ‰€æœ‰ä½ çš„ç°æœ‰é•œåƒå°†å’Œå¾€å¸¸ä¸€æ ·å·¥ä½œã€‚&lt;/p>
&lt;!--
### What about private images?
-->
&lt;h3 id="what-about-private-images">ç§æœ‰é•œåƒå‘¢ï¼Ÿ&lt;/h3>
&lt;!--
Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.
-->
&lt;p>å½“ç„¶å¯ä»¥ã€‚æ‰€æœ‰ CRI è¿è¡Œæ—¶å‡æ”¯æŒ Kubernetes ä¸­ç›¸åŒçš„æ‹‰å–ï¼ˆpullï¼‰Secret é…ç½®ï¼Œ
ä¸ç®¡æ˜¯é€šè¿‡ PodSpec è¿˜æ˜¯é€šè¿‡ ServiceAccount å‡å¯ã€‚&lt;/p>
&lt;!--
### Are Docker and containers the same thing?
-->
&lt;h3 id="are-docker-and-containers-the-same-thing">Docker å’Œå®¹å™¨æ˜¯ä¸€å›äº‹å—ï¼Ÿ&lt;/h3>
&lt;!--
Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.
-->
&lt;p>è™½ç„¶ Linux çš„å®¹å™¨æŠ€æœ¯å·²ç»å­˜åœ¨äº†å¾ˆä¹…ï¼Œ
ä½† Docker æ™®åŠäº† Linux å®¹å™¨è¿™ç§æŠ€æœ¯æ¨¡å¼ï¼Œå¹¶åœ¨å¼€å‘åº•å±‚æŠ€æœ¯æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚
å®¹å™¨çš„ç”Ÿæ€ç›¸æ¯”äºå•çº¯çš„ Dockerï¼Œå·²ç»è¿›åŒ–åˆ°äº†ä¸€ä¸ªæ›´å®½å¹¿çš„é¢†åŸŸã€‚
åƒ OCI å’Œ CRI è¿™ç±»æ ‡å‡†å¸®åŠ©è®¸å¤šå·¥å…·åœ¨æˆ‘ä»¬çš„ç”Ÿæ€ä¸­æˆé•¿å’Œç¹è£ï¼Œ
å…¶ä¸­ä¸€äº›å·¥å…·æ›¿ä»£äº† Docker çš„æŸäº›éƒ¨åˆ†ï¼Œå¦ä¸€äº›å¢å¼ºäº†ç°æœ‰åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
### Are there examples of folks using other runtimes in production today?
-->
&lt;h3 id="are-there-example-of-folks-using-other-runtimes-in-production-today">ç°åœ¨æ˜¯å¦æœ‰åœ¨ç”Ÿäº§ç³»ç»Ÿä¸­ä½¿ç”¨å…¶ä»–è¿è¡Œæ—¶çš„ä¾‹å­ï¼Ÿ&lt;/h3>
&lt;!--
All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.
-->
&lt;p>Kubernetes æ‰€æœ‰é¡¹ç›®åœ¨æ‰€æœ‰ç‰ˆæœ¬ä¸­å‡ºäº§çš„å·¥ä»¶ï¼ˆKubernetes äºŒè¿›åˆ¶æ–‡ä»¶ï¼‰éƒ½ç»è¿‡äº†éªŒè¯ã€‚&lt;/p>
&lt;!--
Additionally, the [kind] project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the [CRI-O] runtime in production since June 2019.
-->
&lt;p>æ­¤å¤–ï¼Œ&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> é¡¹ç›®ä½¿ç”¨ containerd å·²ç»æœ‰å¹´å¤´äº†ï¼Œ
å¹¶ä¸”åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œç¨³å®šæ€§è¿˜æ˜æ˜¾å¾—åˆ°æå‡ã€‚
Kind å’Œ containerd æ¯å¤©éƒ½ä¼šåšå¤šæ¬¡åè°ƒï¼Œä»¥éªŒè¯å¯¹ Kubernetes ä»£ç åº“çš„æ‰€æœ‰æ›´æ”¹ã€‚
å…¶ä»–ç›¸å…³é¡¹ç›®ä¹Ÿéµå¾ªåŒæ ·çš„æ¨¡å¼ï¼Œä»è€Œå±•ç¤ºäº†å…¶ä»–å®¹å™¨è¿è¡Œæ—¶çš„ç¨³å®šæ€§å’Œå¯ç”¨æ€§ã€‚
ä¾‹å¦‚ï¼ŒOpenShift 4.x ä» 2019 å¹´ 6 æœˆä»¥æ¥ï¼Œå°±ä¸€ç›´åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ &lt;a href="https://cri-o.io/">CRI-O&lt;/a> è¿è¡Œæ—¶ã€‚&lt;/p>
&lt;!--
For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation ([CNCF]).
- [containerd](https://github.com/containerd/containerd/blob/master/ADOPTERS.md)
- [CRI-O](https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md)
-->
&lt;p>è‡³äºå…¶ä»–ç¤ºä¾‹å’Œå‚è€ƒèµ„æ–™ï¼Œä½ å¯ä»¥æŸ¥çœ‹ containerd å’Œ CRI-O çš„ä½¿ç”¨è€…åˆ—è¡¨ï¼Œ
è¿™ä¸¤ä¸ªå®¹å™¨è¿è¡Œæ—¶æ˜¯äº‘åŸç”ŸåŸºé‡‘ä¼šï¼ˆ[CNCF]ï¼‰ä¸‹çš„é¡¹ç›®ã€‚&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### People keep referencing OCI, what is that?
-->
&lt;h3 id="people-keep-referenceing-oci-what-is-that">äººä»¬æ€»åœ¨è°ˆè®º OCIï¼Œé‚£æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h3>
&lt;!--
OCI stands for the [Open Container Initiative], which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of [runc], which is the underlying default runtime for both
[containerd] and [CRI-O]. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.
-->
&lt;p>OCI ä»£è¡¨&lt;a href="https://opencontainers.org/about/overview/">å¼€æ”¾å®¹å™¨æ ‡å‡†&lt;/a>ï¼Œ
å®ƒæ ‡å‡†åŒ–äº†å®¹å™¨å·¥å…·å’Œåº•å±‚å®ç°ï¼ˆtechnologiesï¼‰ä¹‹é—´çš„å¤§é‡æ¥å£ã€‚
ä»–ä»¬ç»´æŠ¤äº†æ‰“åŒ…å®¹å™¨é•œåƒï¼ˆOCI image-specï¼‰å’Œè¿è¡Œå®¹å™¨ï¼ˆOCI runtime-specï¼‰çš„æ ‡å‡†è§„èŒƒã€‚
ä»–ä»¬è¿˜ä»¥ &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>
çš„å½¢å¼ç»´æŠ¤äº†ä¸€ä¸ª runtime-spec çš„çœŸå®å®ç°ï¼Œ
è¿™ä¹Ÿæ˜¯ &lt;a href="https://containerd.io/">containerd&lt;/a> å’Œ &lt;a href="https://cri-o.io/">CRI-O&lt;/a> ä¾èµ–çš„é»˜è®¤è¿è¡Œæ—¶ã€‚
CRI å»ºç«‹åœ¨è¿™äº›åº•å±‚è§„èŒƒä¹‹ä¸Šï¼Œä¸ºç®¡ç†å®¹å™¨æä¾›ç«¯åˆ°ç«¯çš„æ ‡å‡†ã€‚&lt;/p>
&lt;!--
### Which CRI implementation should I use?
-->
&lt;h3 id="which-cri-implementation-should-I-use">æˆ‘åº”è¯¥ç”¨å“ªä¸ª CRI å®ç°ï¼Ÿ&lt;/h3>
&lt;!--
Thatâ€™s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the [CNCF landscape] in case another would be an
even better fit for your environment.
-->
&lt;p>è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œä¾èµ–äºè®¸å¤šå› ç´ ã€‚
åœ¨ Docker å·¥ä½œè‰¯å¥½çš„æƒ…å†µä¸‹ï¼Œè¿ç§»åˆ° containerd æ˜¯ä¸€ä¸ªç›¸å¯¹å®¹æ˜“çš„è½¬æ¢ï¼Œå¹¶å°†è·å¾—æ›´å¥½çš„æ€§èƒ½å’Œæ›´å°‘çš„å¼€é”€ã€‚
ç„¶è€Œï¼Œæˆ‘ä»¬å»ºè®®ä½ å…ˆæ¢ç´¢ &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF å…¨æ™¯å›¾&lt;/a>
æä¾›çš„æ‰€æœ‰é€‰é¡¹ï¼Œä»¥åšå‡ºæ›´é€‚åˆä½ çš„ç¯å¢ƒçš„é€‰æ‹©ã€‚&lt;/p>
&lt;!--
### What should I look out for when changing CRI implementations?
-->
&lt;h3 id="what-should-I-look-out-for-when-changing-CRI-implementation">å½“åˆ‡æ¢ CRI åº•å±‚å®ç°æ—¶ï¼Œæˆ‘åº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ&lt;/h3>
&lt;!--
While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:
-->
&lt;p>Docker å’Œå¤§å¤šæ•° CRIï¼ˆåŒ…æ‹¬ containerdï¼‰çš„åº•å±‚å®¹å™¨åŒ–ä»£ç æ˜¯ç›¸åŒçš„ï¼Œä½†å…¶å‘¨è¾¹éƒ¨åˆ†å´å­˜åœ¨ä¸€äº›ä¸åŒã€‚
è¿ç§»æ—¶ä¸€äº›å¸¸è§çš„å…³æ³¨ç‚¹æ˜¯ï¼š&lt;/p>
&lt;!--
- Logging configuration
- Runtime resource limitations
- Node provisioning scripts that call docker or use docker via it's control socket
- Kubectl plugins that require docker CLI or the control socket
- Kubernetes tools that require direct access to Docker (e.g. kube-imagepuller)
- Configuration of functionality like `registry-mirrors` and insecure registries
- Other support scripts or daemons that expect Docker to be available and are run
outside of Kubernetes (e.g. monitoring or security agents)
- GPUs or special hardware and how they integrate with your runtime and Kubernetes
-->
&lt;ul>
&lt;li>æ—¥å¿—é…ç½®&lt;/li>
&lt;li>è¿è¡Œæ—¶çš„èµ„æºé™åˆ¶&lt;/li>
&lt;li>ç›´æ¥è®¿é—® docker å‘½ä»¤æˆ–é€šè¿‡æ§åˆ¶å¥—æ¥å­—è°ƒç”¨ Docker çš„èŠ‚ç‚¹ä¾›åº”è„šæœ¬&lt;/li>
&lt;li>éœ€è¦è®¿é—® docker å‘½ä»¤æˆ–æ§åˆ¶å¥—æ¥å­—çš„ kubectl æ’ä»¶&lt;/li>
&lt;li>éœ€è¦ç›´æ¥è®¿é—® Docker çš„ Kubernetes å·¥å…·ï¼ˆä¾‹å¦‚ï¼škube-imagepullerï¼‰&lt;/li>
&lt;li>åƒ &lt;code>registry-mirrors&lt;/code> å’Œä¸å®‰å…¨çš„æ³¨å†Œè¡¨è¿™ç±»åŠŸèƒ½çš„é…ç½®&lt;/li>
&lt;li>éœ€è¦ Docker ä¿æŒå¯ç”¨ã€ä¸”è¿è¡Œåœ¨ Kubernetes ä¹‹å¤–çš„ï¼Œå…¶ä»–æ”¯æŒè„šæœ¬æˆ–å®ˆæŠ¤è¿›ç¨‹ï¼ˆä¾‹å¦‚ï¼šç›‘è§†æˆ–å®‰å…¨ä»£ç†ï¼‰&lt;/li>
&lt;li>GPU æˆ–ç‰¹æ®Šç¡¬ä»¶ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ä¸ä½ çš„è¿è¡Œæ—¶å’Œ Kubernetes é›†æˆ&lt;/li>
&lt;/ul>
&lt;!--
If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if youâ€™ve customized
your dockerd configuration, youâ€™ll need to adapt that for your new container
runtime where possible.
-->
&lt;p>å¦‚æœä½ åªæ˜¯ç”¨äº† Kubernetes èµ„æºè¯·æ±‚/é™åˆ¶æˆ–åŸºäºæ–‡ä»¶çš„æ—¥å¿—æ”¶é›† DaemonSetï¼Œå®ƒä»¬å°†ç»§ç»­ç¨³å®šå·¥ä½œï¼Œ
ä½†æ˜¯å¦‚æœä½ ç”¨äº†è‡ªå®šä¹‰äº† dockerd é…ç½®ï¼Œåˆ™å¯èƒ½éœ€è¦ä¸ºæ–°å®¹å™¨è¿è¡Œæ—¶åšä¸€äº›é€‚é…å·¥ä½œã€‚&lt;/p>
&lt;!--
Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the [`crictl`][cr] tool as a drop-in replacement (see [mapping from docker cli to crictl](https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl)) and for the
latter you can use newer container build options like [img], [buildah],
[kaniko], or [buildkit-cli-for-kubectl] that donâ€™t require Docker.
-->
&lt;p>å¦å¤–è¿˜æœ‰ä¸€ä¸ªéœ€è¦å…³æ³¨çš„ç‚¹ï¼Œé‚£å°±æ˜¯å½“åˆ›å»ºé•œåƒæ—¶ï¼Œç³»ç»Ÿç»´æŠ¤æˆ–åµŒå…¥å®¹å™¨æ–¹é¢çš„ä»»åŠ¡å°†æ— æ³•å·¥ä½œã€‚
å¯¹äºå‰è€…ï¼Œå¯ä»¥ç”¨ &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> å·¥å…·ä½œä¸ºä¸´æ—¶æ›¿ä»£æ–¹æ¡ˆ
(å‚è§ &lt;a href="https://kubernetes.io/zh/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl">ä» docker å‘½ä»¤æ˜ å°„åˆ° crictl&lt;/a>)ï¼›
å¯¹äºåè€…ï¼Œå¯ä»¥ç”¨æ–°çš„å®¹å™¨åˆ›å»ºé€‰é¡¹ï¼Œæ¯”å¦‚
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>ã€
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>ã€
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>ã€æˆ–
&lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a>ï¼Œ
ä»–ä»¬å‡ä¸éœ€è¦è®¿é—® Dockerã€‚&lt;/p>
&lt;!--
For containerd, you can start with their [documentation] to see what configuration
options are available as you migrate things over.
-->
&lt;p>å¯¹äº containerdï¼Œä½ å¯ä»¥ä»å®ƒä»¬çš„
&lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">æ–‡æ¡£&lt;/a>
å¼€å§‹ï¼Œçœ‹çœ‹åœ¨è¿ç§»è¿‡ç¨‹ä¸­æœ‰å“ªäº›é…ç½®é€‰é¡¹å¯ç”¨ã€‚&lt;/p>
&lt;!--
For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on [Container Runtimes]
-->
&lt;p>å¯¹äºå¦‚ä½•ååŒ Kubernetes ä½¿ç”¨ containerd å’Œ CRI-O çš„è¯´æ˜ï¼Œå‚è§ Kubernetes æ–‡æ¡£ä¸­è¿™éƒ¨åˆ†ï¼š
&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes">å®¹å™¨è¿è¡Œæ—¶&lt;/a>ã€‚&lt;/p>
&lt;!--
### What if I have more questions?
-->
&lt;h3 id="what-if-I-have-more-question">æˆ‘è¿˜æœ‰é—®é¢˜æ€ä¹ˆåŠï¼Ÿ&lt;/h3>
&lt;!--
If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: https://discuss.kubernetes.io/.
-->
&lt;p>å¦‚æœä½ ä½¿ç”¨äº†ä¸€ä¸ªæœ‰ä¾›åº”å•†æ”¯æŒçš„ Kubernetes å‘è¡Œç‰ˆï¼Œä½ å¯ä»¥å’¨è¯¢ä¾›åº”å•†ä»–ä»¬äº§å“çš„å‡çº§è®¡åˆ’ã€‚
å¯¹äºæœ€ç»ˆç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·æŠŠé—®é¢˜å‘åˆ°æˆ‘ä»¬çš„æœ€ç»ˆç”¨æˆ·ç¤¾åŒºçš„è®ºå›ï¼šhttps://discuss.kubernetes.io/ã€‚&lt;/p>
&lt;!--
You can also check out the excellent blog post
[Wait, Docker is deprecated in Kubernetes now?][dep] a more in-depth technical
discussion of the changes.
-->
&lt;p>ä½ ä¹Ÿå¯ä»¥çœ‹çœ‹è¿™ç¯‡ä¼˜ç§€çš„åšæ–‡ï¼š
&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">ç­‰ç­‰ï¼ŒDocker åˆšåˆšè¢« Kubernetes åºŸæ‰äº†ï¼Ÿ&lt;/a>
ä¸€ä¸ªå¯¹æ­¤å˜åŒ–æ›´æ·±å…¥çš„æŠ€æœ¯è®¨è®ºã€‚&lt;/p>
&lt;!--
### Can I have a hug?
-->
&lt;h3 id="can-I-have-a-hug">æˆ‘å¯ä»¥åŠ å…¥å—ï¼Ÿ&lt;/h3>
&lt;!--
Always and whenever you want! ğŸ¤—ğŸ¤—
-->
&lt;p>åªè¦ä½ æ„¿æ„ï¼Œéšæ—¶éšåœ°æ¬¢è¿åŠ å…¥ï¼&lt;/p></description></item><item><title>Blog: ä¸ºå¼€å‘æŒ‡å—åšè´¡çŒ®</title><link>https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/</guid><description>
&lt;!--
---
title: "Contributing to the Development Guide"
linkTitle: "Contributing to the Development Guide"
Author: Erik L. Arneson
Description: "A new contributor describes the experience of writing and submitting changes to the Kubernetes Development Guide."
date: 2020-10-01
canonicalUrl: https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/
resources:
- src: "jorge-castro-code-of-conduct.jpg"
title: "Jorge Castro announcing the Kubernetes Code of Conduct during a weekly SIG ContribEx meeting."
---
-->
&lt;!--
When most people think of contributing to an open source project, I suspect they probably think of
contributing code changes, new features, and bug fixes. As a software engineer and a long-time open
source user and contributor, that's certainly what I thought. Although I have written a good quantity
of documentation in different workflows, the massive size of the Kubernetes community was a new kind
of "client." I just didn't know what to expect when Google asked my compatriots and me at
[Lion's Way](https://lionswaycontent.com/) to make much-needed updates to the Kubernetes Development Guide.
*This article originally appeared on the [Kubernetes Contributor Community blog](https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/).*
-->
&lt;p>å½“å¤§å¤šæ•°äººæƒ³åˆ°ä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®åšè´¡çŒ®æ—¶ï¼Œæˆ‘çŒœæƒ³ä»–ä»¬å¯èƒ½æƒ³åˆ°çš„æ˜¯è´¡çŒ®ä»£ç ä¿®æ”¹ã€æ–°åŠŸèƒ½å’Œé”™è¯¯ä¿®å¤ã€‚ä½œä¸ºä¸€ä¸ªè½¯ä»¶å·¥ç¨‹å¸ˆå’Œä¸€ä¸ªé•¿æœŸçš„å¼€æºç”¨æˆ·å’Œè´¡çŒ®è€…ï¼Œè¿™ä¹Ÿæ­£æ˜¯æˆ‘çš„æƒ³æ³•ã€‚
è™½ç„¶æˆ‘å·²ç»åœ¨ä¸åŒçš„å·¥ä½œæµä¸­å†™äº†ä¸å°‘æ–‡æ¡£ï¼Œä½†è§„æ¨¡åºå¤§çš„ Kubernetes ç¤¾åŒºæ˜¯ä¸€ç§æ–°å‹ &amp;quot;å®¢æˆ·&amp;quot;ã€‚æˆ‘åªæ˜¯ä¸çŸ¥é“å½“ Google è¦æ±‚æˆ‘å’Œ &lt;a href="https://lionswaycontent.com/">Lion's Way&lt;/a> çš„åŒèƒä»¬å¯¹ Kubernetes å¼€å‘æŒ‡å—è¿›è¡Œå¿…è¦æ›´æ–°æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚&lt;/p>
&lt;p>&lt;em>æœ¬æ–‡æœ€åˆå‡ºç°åœ¨ &lt;a href="https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/">Kubernetes Contributor Community blog&lt;/a>ã€‚&lt;/em>&lt;/p>
&lt;!--
## The Delights of Working With a Community
As professional writers, we are used to being hired to write very specific pieces. We specialize in
marketing, training, and documentation for technical services and products, which can range anywhere from relatively fluffy marketing emails to deeply technical white papers targeted at IT and developers. With
this kind of professional service, every deliverable tends to have a measurable return on investment.
I knew this metric wouldn't be present when working on open source documentation, but I couldn't
predict how it would change my relationship with the project.
-->
&lt;h2 id="ä¸ç¤¾åŒºåˆä½œçš„ä¹è¶£">ä¸ç¤¾åŒºåˆä½œçš„ä¹è¶£&lt;/h2>
&lt;p>ä½œä¸ºä¸“ä¸šçš„å†™æ‰‹ï¼Œæˆ‘ä»¬ä¹ æƒ¯äº†å—é›‡äºä»–äººå»ä¹¦å†™éå¸¸å…·ä½“çš„é¡¹ç›®ã€‚æˆ‘ä»¬ä¸“æ³¨äºæŠ€æœ¯æœåŠ¡ï¼Œäº§å“è¥é”€ï¼ŒæŠ€æœ¯åŸ¹è®­ä»¥åŠæ–‡æ¡£ç¼–åˆ¶ï¼ŒèŒƒå›´ä»ç›¸å¯¹å®½æ¾çš„è¥é”€é‚®ä»¶åˆ°é’ˆå¯¹ IT å’Œå¼€å‘äººå‘˜çš„æ·±å±‚æŠ€æœ¯ç™½çš®ä¹¦ã€‚
åœ¨è¿™ç§ä¸“ä¸šæœåŠ¡ä¸‹ï¼Œæ¯ä¸€ä¸ªå¯äº¤ä»˜çš„é¡¹ç›®å¾€å¾€éƒ½æœ‰å¯è¡¡é‡çš„æŠ•èµ„å›æŠ¥ã€‚æˆ‘çŸ¥é“åœ¨ä»äº‹å¼€æºæ–‡æ¡£å·¥ä½œæ—¶ä¸ä¼šå‡ºç°è¿™ä¸ªæŒ‡æ ‡ï¼Œä½†æˆ‘ä¸ç¡®å®šå®ƒå°†å¦‚ä½•æ”¹å˜æˆ‘ä¸é¡¹ç›®çš„å…³ç³»ã€‚&lt;/p>
&lt;!--
One of the primary traits of the relationship between our writing and our traditional clients is that we
always have one or two primary points of contact inside a company. These contacts are responsible
for reviewing our writing and making sure it matches the voice of the company and targets the
audience they're looking for. It can be stressful -- which is why I'm so glad that my writing
partner, eagle-eyed reviewer, and bloodthirsty editor [Joel](https://twitter.com/JoelByronBarker)
handles most of the client contact.
-->
&lt;p>æˆ‘ä»¬çš„å†™ä½œå’Œä¼ ç»Ÿå®¢æˆ·ä¹‹é—´çš„å…³ç³»æœ‰ä¸€ä¸ªä¸»è¦çš„ç‰¹ç‚¹ï¼Œå°±æ˜¯æˆ‘ä»¬åœ¨ä¸€ä¸ªå…¬å¸é‡Œé¢æ€»æ˜¯æœ‰ä¸€ä¸¤ä¸ªä¸»è¦çš„å¯¹æ¥äººã€‚ä»–ä»¬è´Ÿè´£å®¡æŸ¥æˆ‘ä»¬çš„æ–‡ç¨¿ï¼Œå¹¶ç¡®ä¿æ–‡ç¨¿å†…å®¹ç¬¦åˆå…¬å¸çš„å£°æ˜ä¸”å¯¹æ ‡äºä»–ä»¬æ­£åœ¨å¯»æ‰¾çš„å—ä¼—ã€‚
è¿™éšä¹‹è€Œæ¥çš„å‹åŠ›--æ­£å¥½è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘å¾ˆé«˜å…´æˆ‘çš„å†™ä½œä¼™ä¼´ã€é¹°çœ¼å®¡ç¨¿äººåŒæ—¶ä¹Ÿæ˜¯å—œè¡€ç¼–è¾‘çš„ &lt;a href="https://twitter.com/JoelByronBarker">Joel&lt;/a> å¤„ç†äº†å¤§éƒ¨åˆ†çš„å®¢æˆ·è”ç³»ã€‚&lt;/p>
&lt;!--
I was surprised and delighted that all of the stress of client contact went out the window when
working with the Kubernetes community.
-->
&lt;p>åœ¨ä¸ Kubernetes ç¤¾åŒºåˆä½œæ—¶ï¼Œæ‰€æœ‰ä¸å®¢æˆ·æ¥è§¦çš„å‹åŠ›éƒ½æ¶ˆå¤±äº†ï¼Œè¿™è®©æˆ‘æ„Ÿåˆ°æƒŠè®¶å’Œé«˜å…´ã€‚&lt;/p>
&lt;!--
"How delicate do I have to be? What if I screw up? What if I make a developer angry? What if I make
enemies?" These were all questions that raced through my mind and made me feel like I was
approaching a field of eggshells when I first joined the `#sig-contribex` channel on the Kubernetes
Slack and announced that I would be working on the
[Development Guide](https://github.com/kubernetes/community/blob/master/contributors/devel/development.md).
-->
&lt;p>&amp;quot;æˆ‘å¿…é¡»å¾—å¤šä»”ç»†ï¼Ÿå¦‚æœæˆ‘æç ¸äº†æ€ä¹ˆåŠï¼Ÿå¦‚æœæˆ‘è®©å¼€å‘å•†ç”Ÿæ°”äº†æ€ä¹ˆåŠï¼Ÿå¦‚æœæˆ‘æ ‘æ•Œäº†æ€ä¹ˆåŠï¼Ÿ&amp;quot;ã€‚
å½“æˆ‘ç¬¬ä¸€æ¬¡åŠ å…¥ Kubernetes Slack ä¸Šçš„ &amp;quot;#sig-contribex &amp;quot; é¢‘é“å¹¶å®£å¸ƒæˆ‘å°†ç¼–å†™ &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/development.md">å¼€å‘æŒ‡å—&lt;/a> æ—¶ï¼Œè¿™äº›é—®é¢˜éƒ½åœ¨æˆ‘è„‘æµ·ä¸­å¥”è…¾ï¼Œè®©æˆ‘æ„Ÿè§‰å¦‚å±¥è–„å†°ã€‚&lt;/p>
&lt;!--
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"The Kubernetes Code of Conduct is in effect, so please be excellent to each other." &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
-->
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"Kubernetes ç¼–ç å‡†åˆ™å·²ç»ç”Ÿæ•ˆï¼Œè®©æˆ‘ä»¬å…±åŒå‹‰åŠ±ã€‚" &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
&lt;!--
My fears were unfounded. Immediately, I felt welcome. I like to think this isn't just because I was
working on a much needed task, but rather because the Kubernetes community is filled
with friendly, welcoming people. During the weekly SIG ContribEx meetings, our reports on progress
with the Development Guide were included immediately. In addition, the leader of the meeting would
always stress that the [Kubernetes Code of Conduct](https://www.kubernetes.dev/resources/code-of-conduct/) was in
effect, and that we should, like Bill and Ted, be excellent to each other.
-->
&lt;p>äº‹å®ä¸Šæˆ‘çš„æ‹…å¿ƒæ˜¯å¤šè™‘çš„ã€‚å¾ˆå¿«ï¼Œæˆ‘å°±æ„Ÿè§‰åˆ°è‡ªå·±æ˜¯è¢«æ¬¢è¿çš„ã€‚æˆ‘å€¾å‘äºè®¤ä¸ºè¿™ä¸ä»…ä»…æ˜¯å› ä¸ºæˆ‘æ­£åœ¨ä»äº‹ä¸€é¡¹æ€¥éœ€çš„ä»»åŠ¡ï¼Œè€Œæ˜¯å› ä¸º Kubernetes ç¤¾åŒºå……æ»¡äº†å‹å¥½ã€çƒ­æƒ…çš„äººä»¬ã€‚
åœ¨æ¯å‘¨çš„ SIG ContribEx ä¼šè®®ä¸Šï¼Œæˆ‘ä»¬å…³äºå¼€å‘æŒ‡å—è¿›å±•æƒ…å†µçš„æŠ¥å‘Šä¼šè¢«ç«‹å³çº³å…¥å…¶ä¸­ã€‚æ­¤å¤–ï¼Œä¼šè®®çš„é¢†å¯¼ä¼šä¸€ç›´å¼ºè°ƒ &lt;a href="https://www.kubernetes.dev/resources/code-of-conduct/">Kubernetes&lt;/a> ç¼–ç å‡†åˆ™ï¼Œæˆ‘ä»¬åº”è¯¥åƒ Bill å’Œ Ted ä¸€æ ·ï¼Œç›¸äº’è¿›æ­¥ã€‚&lt;/p>
&lt;!--
## This Doesn't Mean It's All Easy
The Development Guide needed a pretty serious overhaul. When we got our hands on it, it was already
packed with information and lots of steps for new developers to go through, but it was getting dusty
with age and neglect. Documentation can really require a global look, not just point fixes.
As a result, I ended up submitting a gargantuan pull request to the
[Community repo](https://github.com/kubernetes/community): 267 additions and 88 deletions.
-->
&lt;h2 id="è¿™å¹¶ä¸æ„å‘³ç€è¿™ä¸€åˆ‡éƒ½å¾ˆç®€å•">è¿™å¹¶ä¸æ„å‘³ç€è¿™ä¸€åˆ‡éƒ½å¾ˆç®€å•&lt;/h2>
&lt;p>å¼€å‘æŒ‡å—éœ€è¦ä¸€æ¬¡å…¨é¢æ£€æŸ¥ã€‚å½“æˆ‘ä»¬æ‹¿åˆ°å®ƒçš„æ—¶å€™ï¼Œå®ƒå·²ç»æ†ç»‘äº†å¤§é‡çš„ä¿¡æ¯å’Œå¾ˆå¤šæ–°å¼€å‘è€…éœ€è¦ç»å†çš„æ­¥éª¤ï¼Œä½†éšç€æ—¶é—´çš„æ¨ç§»å’Œè¢«å¿½è§†ï¼Œå®ƒå˜å¾—ç›¸å½“é™ˆæ—§ã€‚
æ–‡æ¡£çš„ç¡®éœ€è¦å…¨å±€è§‚ï¼Œè€Œä¸ä»…ä»…æ˜¯ç‚¹ä¸ç‚¹çš„ä¿®å¤ã€‚ç»“æœï¼Œæœ€ç»ˆæˆ‘å‘è¿™ä¸ªé¡¹ç›®æäº¤äº†ä¸€ä¸ªå·¨å¤§çš„ pull è¯·æ±‚ã€‚&lt;a href="https://github.com/kubernetes/community">ç¤¾åŒºä»“åº“&lt;/a>ï¼šæ–°å¢ 267 è¡Œï¼Œåˆ é™¤ 88 è¡Œã€‚&lt;/p>
&lt;!--
The life cycle of a pull request requires a certain number of Kubernetes organization members to review and approve changes
before they can be merged. This is a great practice, as it keeps both documentation and code in
pretty good shape, but it can be tough to cajole the right people into taking the time for such a hefty
review. As a result, that massive PR took 26 days from my first submission to final merge. But in
the end, [it was successful](https://github.com/kubernetes/community/pull/5003).
-->
&lt;p>pull è¯·æ±‚çš„å‘¨æœŸéœ€è¦ä¸€å®šæ•°é‡çš„ Kubernetes ç»„ç»‡æˆå‘˜å®¡æŸ¥å’Œæ‰¹å‡†æ›´æ”¹åæ‰èƒ½åˆå¹¶ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åšæ³•ï¼Œå› ä¸ºå®ƒä½¿æ–‡æ¡£å’Œä»£ç éƒ½ä¿æŒåœ¨ç›¸å½“ä¸é”™çš„çŠ¶æ€ï¼Œ
ä½†è¦å“„éª—åˆé€‚çš„äººèŠ±æ—¶é—´æ¥åšè¿™æ ·ä¸€ä¸ªèµ«èµ«æœ‰åçš„å®¡æŸ¥æ˜¯å¾ˆéš¾çš„ã€‚
å› æ­¤ï¼Œé‚£æ¬¡å¤§è§„æ¨¡çš„ PR ä»æˆ‘ç¬¬ä¸€æ¬¡æäº¤åˆ°æœ€ååˆå¹¶ï¼Œç”¨äº† 26 å¤©ã€‚ ä½†æœ€ç»ˆï¼Œ&lt;a href="https://github.com/kubernetes/community/pull/5003">å®ƒæ˜¯æˆåŠŸçš„&lt;/a>.&lt;/p>
&lt;!--
Since Kubernetes is a pretty fast-moving project, and since developers typically aren't really
excited about writing documentation, I also ran into the problem that sometimes, the secret jewels
that describe the workings of a Kubernetes subsystem are buried deep within the [labyrinthine mind of
a brilliant engineer](https://github.com/amwat), and not in plain English in a Markdown file. I ran headlong into this issue
when it came time to update the getting started documentation for end-to-end (e2e) testing.
-->
&lt;p>ç”±äº Kubernetes æ˜¯ä¸€ä¸ªå‘å±•ç›¸å½“è¿…é€Ÿçš„é¡¹ç›®ï¼Œè€Œä¸”å¼€å‘äººå‘˜é€šå¸¸å¯¹ç¼–å†™æ–‡æ¡£å¹¶ä¸ååˆ†æ„Ÿå…´è¶£ï¼Œæ‰€ä»¥æˆ‘ä¹Ÿé‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯æœ‰æ—¶å€™ï¼Œ
æè¿° Kubernetes å­ç³»ç»Ÿå·¥ä½œåŸç†çš„ç§˜å¯†çå®è¢«æ·±åŸ‹åœ¨ &lt;a href="https://github.com/amwat">å¤©æ‰å·¥ç¨‹å¸ˆçš„è¿·å®«å¼æ€ç»´&lt;/a> ä¸­ï¼Œè€Œä¸æ˜¯ç”¨å•çº¯çš„è‹±æ–‡å†™åœ¨ Markdown æ–‡ä»¶ä¸­ã€‚
å½“æˆ‘è¦æ›´æ–°ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰æµ‹è¯•çš„å…¥é—¨æ–‡æ¡£æ—¶ï¼Œå°±ä¸€å¤´æ’ä¸Šäº†è¿™ä¸ªé—®é¢˜ã€‚&lt;/p>
&lt;!--
This portion of my journey took me out of documentation-writing territory and into the role of a
brand new user of some unfinished software. I ended up working with one of the developers of the new
[`kubetest2` framework](https://github.com/kubernetes-sigs/kubetest2) to document the latest process of
getting up-and-running for e2e testing, but it required a lot of head scratching on my part. You can
judge the results for yourself by checking out my
[completed pull request](https://github.com/kubernetes/community/pull/5045).
-->
&lt;p>è¿™æ®µæ—…ç¨‹å°†æˆ‘å¸¦å‡ºäº†ç¼–å†™æ–‡æ¡£çš„é¢†åŸŸï¼Œè¿›å…¥åˆ°ä¸€äº›æœªå®Œæˆè½¯ä»¶çš„å…¨æ–°ç”¨æˆ·è§’è‰²ã€‚æœ€ç»ˆæˆ‘èŠ±äº†å¾ˆå¤šå¿ƒæ€ä¸æ–°çš„ &lt;a href="https://github.com/kubernetes-sigs/kubetest2">kubetest2`æ¡†æ¶&lt;/a> çš„å¼€å‘è€…ä¹‹ä¸€åˆä½œï¼Œ
è®°å½•äº†æœ€æ–° e2e æµ‹è¯•çš„å¯åŠ¨å’Œè¿è¡Œè¿‡ç¨‹ã€‚
ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹æˆ‘çš„ &lt;a href="https://github.com/kubernetes/community/pull/5045">å·²å®Œæˆçš„ pull request&lt;/a> æ¥è‡ªå·±åˆ¤æ–­ç»“æœã€‚&lt;/p>
&lt;!--
## Nobody Is the Boss, and Everybody Gives Feedback
But while I secretly expected chaos, the process of contributing to the Kubernetes Development Guide
and interacting with the amazing Kubernetes community went incredibly smoothly. There was no
contention. I made no enemies. Everybody was incredibly friendly and welcoming. It was *enjoyable*.
-->
&lt;h2 id="æ²¡æœ‰äººæ˜¯è€æ¿-æ¯ä¸ªäººéƒ½ç»™å‡ºåé¦ˆ">æ²¡æœ‰äººæ˜¯è€æ¿ï¼Œæ¯ä¸ªäººéƒ½ç»™å‡ºåé¦ˆã€‚&lt;/h2>
&lt;p>ä½†å½“æˆ‘æš—è‡ªæœŸå¾…æ··ä¹±çš„æ—¶å€™ï¼Œä¸º Kubernetes å¼€å‘æŒ‡å—åšè´¡çŒ®ä»¥åŠä¸ç¥å¥‡çš„ Kubernetes ç¤¾åŒºäº’åŠ¨çš„è¿‡ç¨‹å´éå¸¸é¡ºåˆ©ã€‚
æ²¡æœ‰äº‰æ‰§ï¼Œæˆ‘ä¹Ÿæ²¡æœ‰æ ‘æ•Œã€‚æ¯ä¸ªäººéƒ½éå¸¸å‹å¥½å’Œçƒ­æƒ…ã€‚è¿™æ˜¯ä»¤äºº&lt;em>æ„‰å¿«çš„&lt;/em>ã€‚&lt;/p>
&lt;!--
With an open source project, there is no one boss. The Kubernetes project, which approaches being
gargantuan, is split into many different special interest groups (SIGs), working groups, and
communities. Each has its own regularly scheduled meetings, assigned duties, and elected
chairpersons. My work intersected with the efforts of both SIG ContribEx (who watch over and seek to
improve the contributor experience) and SIG Testing (who are in charge of testing). Both of these
SIGs proved easy to work with, eager for contributions, and populated with incredibly friendly and
welcoming people.
-->
&lt;p>å¯¹äºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ²¡äººæ˜¯è€æ¿ã€‚Kubernetes é¡¹ç›®ï¼Œä¸€ä¸ªè¿‘ä¹å·¨å¤§çš„é¡¹ç›®ï¼Œè¢«åˆ†å‰²æˆè®¸å¤šä¸åŒçš„ç‰¹æ®Šå…´è¶£å°ç»„ï¼ˆSIGï¼‰ã€å·¥ä½œç»„å’Œç¤¾åŒºã€‚
æ¯ä¸ªå°ç»„éƒ½æœ‰è‡ªå·±çš„å®šæœŸä¼šè®®ã€èŒè´£åˆ†é…å’Œä¸»å¸­æ¨é€‰ã€‚æˆ‘çš„å·¥ä½œä¸ SIG ContribExï¼ˆè´Ÿè´£ç›‘ç£å¹¶å¯»æ±‚æ”¹å–„è´¡çŒ®è€…ä½“éªŒï¼‰å’Œ SIG Testingï¼ˆè´Ÿè´£æµ‹è¯•ï¼‰çš„å·¥ä½œæœ‰äº¤é›†ã€‚
äº‹å®è¯æ˜ï¼Œè¿™ä¸¤ä¸ª SIG éƒ½å¾ˆå®¹æ˜“åˆä½œï¼Œä»–ä»¬æ¸´æœ›è´¡çŒ®ï¼Œè€Œä¸”éƒ½æ˜¯éå¸¸å‹å¥½å’Œçƒ­æƒ…çš„äººã€‚&lt;/p>
&lt;!--
In an active, living project like Kubernetes, documentation continues to need maintenance, revision,
and testing alongside the code base. The Development Guide will continue to be crucial to onboarding
new contributors to the Kubernetes code base, and as our efforts have shown, it is important that
this guide keeps pace with the evolution of the Kubernetes project.
-->
&lt;p>åœ¨ Kubernetes è¿™æ ·ä¸€ä¸ªæ´»è·ƒçš„ã€æœ‰ç”Ÿå‘½åŠ›çš„é¡¹ç›®ä¸­ï¼Œæ–‡æ¡£ä»ç„¶éœ€è¦ä¸ä»£ç åº“ä¸€èµ·è¿›è¡Œç»´æŠ¤ã€ä¿®è®¢å’Œæµ‹è¯•ã€‚
å¼€å‘æŒ‡å—å°†ç»§ç»­å¯¹ Kubernetes ä»£ç åº“çš„æ–°è´¡çŒ®è€…èµ·åˆ°è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæ­£å¦‚æˆ‘ä»¬çš„åŠªåŠ›æ‰€æ˜¾ç¤ºçš„é‚£æ ·ï¼Œè¯¥æŒ‡å—å¿…é¡»ä¸ Kubernetes é¡¹ç›®çš„å‘å±•ä¿æŒåŒæ­¥ã€‚&lt;/p>
&lt;!--
Joel and I really enjoy interacting with the Kubernetes community and contributing to
the Development Guide. I really look forward to continuing to not only contributing more, but to
continuing to build the new friendships I've made in this vast open source community over the past
few months.
-->
&lt;p>Joel å’Œæˆ‘éå¸¸å–œæ¬¢ä¸ Kubernetes ç¤¾åŒºäº’åŠ¨å¹¶ä¸ºå¼€å‘æŒ‡å—åšå‡ºè´¡çŒ®ã€‚æˆ‘çœŸçš„å¾ˆæœŸå¾…ï¼Œä¸ä»…èƒ½ç»§ç»­åšå‡ºæ›´å¤šè´¡çŒ®ï¼Œè¿˜èƒ½ç»§ç»­ä¸è¿‡å»å‡ ä¸ªæœˆåœ¨è¿™ä¸ªåºå¤§çš„å¼€æºç¤¾åŒºä¸­ç»“è¯†çš„æ–°æœ‹å‹è¿›è¡Œåˆä½œã€‚&lt;/p></description></item><item><title>Blog: ç»“æ„åŒ–æ—¥å¿—ä»‹ç»</title><link>https://kubernetes.io/zh/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</guid><description>
&lt;!--
layout: blog
title: 'Introducing Structured Logs'
date: 2020-09-04
slug: kubernetes-1-19-Introducing-Structured-Logs
-->
&lt;!--
**Authors:** Marek Siarkowicz (Google), Nathan Beach (Google)
-->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong> Marek Siarkowiczï¼ˆè°·æ­Œï¼‰ï¼ŒNathan Beachï¼ˆè°·æ­Œï¼‰&lt;/p>
&lt;!--
Logs are an essential aspect of observability and a critical tool for debugging. But Kubernetes logs have traditionally been unstructured strings, making any automated parsing difficult and any downstream processing, analysis, or querying challenging to do reliably.
-->
&lt;p>æ—¥å¿—æ˜¯å¯è§‚å¯Ÿæ€§çš„ä¸€ä¸ªé‡è¦æ–¹é¢ï¼Œä¹Ÿæ˜¯è°ƒè¯•çš„é‡è¦å·¥å…·ã€‚ ä½†æ˜¯Kubernetesæ—¥å¿—ä¼ ç»Ÿä¸Šæ˜¯éç»“æ„åŒ–çš„å­—ç¬¦ä¸²ï¼Œå› æ­¤å¾ˆéš¾è¿›è¡Œè‡ªåŠ¨è§£æï¼Œä»¥åŠä»»ä½•å¯é çš„åç»­å¤„ç†ã€åˆ†ææˆ–æŸ¥è¯¢ã€‚&lt;/p>
&lt;!--
In Kubernetes 1.19, we are adding support for structured logs, which natively support (key, value) pairs and object references. We have also updated many logging calls such that over 99% of logging volume in a typical deployment are now migrated to the structured format.
-->
&lt;p>åœ¨Kubernetes 1.19ä¸­ï¼Œæˆ‘ä»¬æ·»åŠ ç»“æ„åŒ–æ—¥å¿—çš„æ”¯æŒï¼Œè¯¥æ—¥å¿—æœ¬èº«æ”¯æŒï¼ˆé”®ï¼Œå€¼ï¼‰å¯¹å’Œå¯¹è±¡å¼•ç”¨ã€‚ æˆ‘ä»¬è¿˜æ›´æ–°äº†è®¸å¤šæ—¥å¿—è®°å½•è°ƒç”¨ï¼Œä»¥ä¾¿ç°åœ¨å°†å…¸å‹éƒ¨ç½²ä¸­è¶…è¿‡99ï¼…çš„æ—¥å¿—è®°å½•é‡è¿ç§»ä¸ºç»“æ„åŒ–æ ¼å¼ã€‚&lt;/p>
&lt;!--
To maintain backwards compatibility, structured logs will still be outputted as a string where the string contains representations of those "key"="value" pairs. Starting in alpha in 1.19, logs can also be outputted in JSON format using the `--logging-format=json` flag.
-->
&lt;p>ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§ï¼Œç»“æ„åŒ–æ—¥å¿—ä»å°†ä½œä¸ºå­—ç¬¦ä¸²è¾“å‡ºï¼Œå…¶ä¸­è¯¥å­—ç¬¦ä¸²åŒ…å«è¿™äº›â€œé”®â€ =â€œå€¼â€å¯¹çš„è¡¨ç¤ºã€‚ ä»1.19çš„Alphaç‰ˆæœ¬å¼€å§‹ï¼Œæ—¥å¿—ä¹Ÿå¯ä»¥ä½¿ç”¨&lt;code>--logging-format = json&lt;/code>æ ‡å¿—ä»¥JSONæ ¼å¼è¾“å‡ºã€‚&lt;/p>
&lt;h2 id="ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—">ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—&lt;/h2>
&lt;!--
We've added two new methods to the klog library: InfoS and ErrorS. For example, this invocation of InfoS:
-->
&lt;p>æˆ‘ä»¬åœ¨klogåº“ä¸­æ·»åŠ äº†ä¸¤ä¸ªæ–°æ–¹æ³•ï¼šInfoSå’ŒErrorSã€‚ ä¾‹å¦‚ï¼ŒInfoSçš„æ­¤è°ƒç”¨ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">klog.&lt;span style="color:#00a000">InfoS&lt;/span>(&lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, klog.&lt;span style="color:#00a000">KObj&lt;/span>(pod), &lt;span style="color:#b44">&amp;#34;status&amp;#34;&lt;/span>, status)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
will result in this log:
-->
&lt;p>å°†å¾—åˆ°ä¸‹é¢çš„æ—¥å¿—è¾“å‡ºï¼š&lt;/p>
&lt;pre>&lt;code>I1025 00:15:15.525108 1 controller_utils.go:116] &amp;quot;Pod status updated&amp;quot; pod=&amp;quot;kube-system/kubedns&amp;quot; status=&amp;quot;ready&amp;quot;
&lt;/code>&lt;/pre>&lt;!--
Or, if the --logging-format=json flag is set, it will result in this output:
-->
&lt;p>æˆ–è€…, å¦‚æœ --logging-format=json æ¨¡å¼è¢«è®¾ç½®, å°†ä¼šäº§ç”Ÿå¦‚ä¸‹ç»“æœ:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;ts&amp;#34;&lt;/span>: &lt;span style="color:#666">1580306777.04728&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;msg&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;pod&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;coredns&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kube-system&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ready&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
This means downstream logging tools can easily ingest structured logging data and instead of using regular expressions to parse unstructured strings. This also makes processing logs easier, querying logs more robust, and analyzing logs much faster.
-->
&lt;p>è¿™æ„å‘³ç€ä¸‹æ¸¸æ—¥å¿—è®°å½•å·¥å…·å¯ä»¥è½»æ¾åœ°è·å–ç»“æ„åŒ–æ—¥å¿—æ•°æ®ï¼Œè€Œæ— éœ€ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥è§£æéç»“æ„åŒ–å­—ç¬¦ä¸²ã€‚è¿™ä¹Ÿä½¿å¤„ç†æ—¥å¿—æ›´å®¹æ˜“ï¼ŒæŸ¥è¯¢æ—¥å¿—æ›´å¥å£®ï¼Œå¹¶ä¸”åˆ†ææ—¥å¿—æ›´å¿«ã€‚&lt;/p>
&lt;!--
With structured logs, all references to Kubernetes objects are structured the same way, so you can filter the output and only log entries referencing the particular pod. You can also find logs indicating how the scheduler was scheduling the pod, how the pod was created, the health probes of the pod, and all other changes in the lifecycle of the pod.
-->
&lt;p>ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—ï¼Œæ‰€æœ‰å¯¹Kuberneteså¯¹è±¡çš„å¼•ç”¨éƒ½ä»¥ç›¸åŒçš„æ–¹å¼è¿›è¡Œç»“æ„åŒ–ï¼Œå› æ­¤æ‚¨å¯ä»¥è¿‡æ»¤è¾“å‡ºå¹¶ä¸”ä»…å¼•ç”¨ç‰¹å®šPodçš„æ—¥å¿—æ¡ç›®ã€‚æ‚¨è¿˜å¯ä»¥å‘ç°æŒ‡ç¤ºè°ƒåº¦ç¨‹åºå¦‚ä½•è°ƒåº¦Podï¼Œå¦‚ä½•åˆ›å»ºPodï¼Œç›‘æµ‹Podçš„è¿è¡ŒçŠ¶å†µä»¥åŠPodç”Ÿå‘½å‘¨æœŸä¸­çš„æ‰€æœ‰å…¶ä»–æ›´æ”¹çš„æ—¥å¿—ã€‚&lt;/p>
&lt;!--
Suppose you are debugging an issue with a pod. With structured logs, you can filter to only those log entries referencing the pod of interest, rather than needing to scan through potentially thousands of log lines to find the relevant ones.
-->
&lt;p>å‡è®¾æ‚¨æ­£åœ¨è°ƒè¯•Podçš„é—®é¢˜ã€‚ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—ï¼Œæ‚¨å¯ä»¥åªè¿‡æ»¤æŸ¥çœ‹æ„Ÿå…´è¶£çš„Podçš„æ—¥å¿—æ¡ç›®ï¼Œè€Œæ— éœ€æ‰«æå¯èƒ½æˆåƒä¸Šä¸‡æ¡æ—¥å¿—è¡Œä»¥æ‰¾åˆ°ç›¸å…³çš„æ—¥å¿—è¡Œã€‚&lt;/p>
&lt;!--
Not only are structured logs more useful when manual debugging of issues, they also enable richer features like automated pattern recognition within logs or tighter correlation of log and trace data.
-->
&lt;p>ç»“æ„åŒ–æ—¥å¿—ä¸ä»…åœ¨æ‰‹åŠ¨è°ƒè¯•é—®é¢˜æ—¶æ›´æœ‰ç”¨ï¼Œè€Œä¸”è¿˜å¯ç”¨äº†æ›´ä¸°å¯Œçš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ—¥å¿—çš„è‡ªåŠ¨æ¨¡å¼è¯†åˆ«æˆ–æ—¥å¿—å’Œæ‰€è·Ÿè¸ªæ•°æ®çš„æ›´ç´§å¯†å…³è”æ€§ï¼ˆåˆ†æï¼‰ã€‚&lt;/p>
&lt;!--
Finally, structured logs can help reduce storage costs for logs because most storage systems are more efficiently able to compress structured key=value data than unstructured strings.
-->
&lt;p>æœ€åï¼Œç»“æ„åŒ–æ—¥å¿—å¯ä»¥å¸®åŠ©é™ä½æ—¥å¿—çš„å­˜å‚¨æˆæœ¬ï¼Œå› ä¸ºå¤§å¤šæ•°å­˜å‚¨ç³»ç»Ÿæ¯”éç»“æ„åŒ–å­—ç¬¦ä¸²æ›´æœ‰æ•ˆåœ°å‹ç¼©ç»“æ„åŒ–é”®å€¼æ•°æ®ã€‚&lt;/p>
&lt;h2 id="å‚ä¸å…¶ä¸­">å‚ä¸å…¶ä¸­&lt;/h2>
&lt;!--
While we have updated over 99% of the log entries by log volume in a typical deployment, there are still thousands of logs to be updated. Pick a file or directory that you would like to improve and [migrate existing log calls to use structured logs](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md). It's a great and easy way to make your first contribution to Kubernetes!
-->
&lt;p>è™½ç„¶åœ¨å…¸å‹éƒ¨ç½²ä¸­ï¼Œæˆ‘ä»¬å·²æŒ‰æ—¥å¿—é‡æ›´æ–°äº†99ï¼…ä»¥ä¸Šçš„æ—¥å¿—æ¡ç›®ï¼Œä½†ä»æœ‰æ•°åƒä¸ªæ—¥å¿—éœ€è¦æ›´æ–°ã€‚ é€‰æ‹©ä¸€ä¸ªæ‚¨è¦æ”¹è¿›çš„æ–‡ä»¶æˆ–ç›®å½•ï¼Œç„¶å[è¿ç§»ç°æœ‰çš„æ—¥å¿—è°ƒç”¨ä»¥ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—]ï¼ˆhttps://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.mdï¼‰ã€‚è¿™æ˜¯å¯¹Kubernetesåšå‡ºç¬¬ä¸€ç¬”è´¡çŒ®çš„å¥½æ–¹æ³•!&lt;/p></description></item><item><title>Blog: Kubernetes 1.18: Fit &amp; Finish</title><link>https://kubernetes.io/zh/blog/2020/03/25/kubernetes-1-18-release-announcement/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/03/25/kubernetes-1-18-release-announcement/</guid><description>
&lt;!--
**Authors:** [Kubernetes 1.18 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">Kubernetes 1.18 å‘å¸ƒå›¢é˜Ÿ&lt;/a>&lt;/p>
&lt;!--
We're pleased to announce the delivery of Kubernetes 1.18, our first release of 2020! Kubernetes 1.18 consists of 38 enhancements: 15 enhancements are moving to stable, 11 enhancements in beta, and 12 enhancements in alpha.
-->
&lt;p>æˆ‘ä»¬å¾ˆé«˜å…´å®£å¸ƒ Kubernetes 1.18 ç‰ˆæœ¬çš„äº¤ä»˜ï¼Œè¿™æ˜¯æˆ‘ä»¬ 2020 å¹´çš„ç¬¬ä¸€ç‰ˆï¼ Kubernetes 1.18 åŒ…å« 38 ä¸ªå¢å¼ºåŠŸèƒ½ï¼š15 é¡¹å¢å¼ºåŠŸèƒ½å·²è½¬ä¸ºç¨³å®šç‰ˆï¼Œ11 é¡¹å¢å¼ºåŠŸèƒ½å¤„äº beta é˜¶æ®µï¼Œ12 é¡¹å¢å¼ºåŠŸèƒ½å¤„äº alpha é˜¶æ®µã€‚&lt;/p>
&lt;!--
Kubernetes 1.18 is a "fit and finish" release. Significant work has gone into improving beta and stable features to ensure users have a better experience. An equal effort has gone into adding new developments and exciting new features that promise to enhance the user experience even more.
-->
&lt;p>Kubernetes 1.18 æ˜¯ä¸€ä¸ªè¿‘ä¹ â€œå®Œç¾â€ çš„ç‰ˆæœ¬ã€‚ ä¸ºäº†æ”¹å–„ beta å’Œç¨³å®šçš„ç‰¹æ€§ï¼Œå·²è¿›è¡Œäº†å¤§é‡å·¥ä½œï¼Œä»¥ç¡®ä¿ç”¨æˆ·è·å¾—æ›´å¥½çš„ä½“éªŒã€‚ æˆ‘ä»¬åœ¨å¢å¼ºç°æœ‰åŠŸèƒ½çš„åŒæ—¶ä¹Ÿå¢åŠ äº†ä»¤äººå…´å¥‹çš„æ–°ç‰¹æ€§ï¼Œè¿™äº›æœ‰æœ›è¿›ä¸€æ­¥å¢å¼ºç”¨æˆ·ä½“éªŒã€‚&lt;/p>
&lt;!--
Having almost as many enhancements in alpha, beta, and stable is a great achievement. It shows the tremendous effort made by the community on improving the reliability of Kubernetes as well as continuing to expand its existing functionality.
-->
&lt;p>å¯¹ alphaï¼Œbeta å’Œç¨³å®šç‰ˆè¿›è¡Œå‡ ä¹åŒç­‰ç¨‹åº¦çš„å¢å¼ºæ˜¯ä¸€é¡¹ä¼Ÿå¤§çš„æˆå°±ã€‚ å®ƒå±•ç°äº†ç¤¾åŒºåœ¨æé«˜ Kubernetes çš„å¯é æ€§ä»¥åŠç»§ç»­æ‰©å±•å…¶ç°æœ‰åŠŸèƒ½æ–¹é¢æ‰€åšçš„å·¨å¤§åŠªåŠ›ã€‚&lt;/p>
&lt;!--
## Major Themes
-->
&lt;h2 id="ä¸»è¦å†…å®¹">ä¸»è¦å†…å®¹&lt;/h2>
&lt;!--
### Kubernetes Topology Manager Moves to Beta - Align Up!
-->
&lt;h3 id="kubernetes-æ‹“æ‰‘ç®¡ç†å™¨-topology-manager-è¿›å…¥-beta-é˜¶æ®µ-å¯¹é½">Kubernetes æ‹“æ‰‘ç®¡ç†å™¨ï¼ˆTopology Managerï¼‰è¿›å…¥ Beta é˜¶æ®µ - å¯¹é½ï¼&lt;/h3>
&lt;!--
A beta feature of Kubernetes in release 1.18, the [Topology Manager feature](https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md) enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.
-->
&lt;p>Kubernetes åœ¨ 1.18 ç‰ˆä¸­çš„ Beta é˜¶æ®µåŠŸèƒ½ &lt;a href="https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md">æ‹“æ‰‘ç®¡ç†å™¨ç‰¹æ€§&lt;/a> å¯ç”¨ CPU å’Œè®¾å¤‡ï¼ˆä¾‹å¦‚ SR-IOV VFï¼‰çš„ NUMA å¯¹é½ï¼Œè¿™å°†ä½¿æ‚¨çš„å·¥ä½œè´Ÿè½½åœ¨é’ˆå¯¹ä½å»¶è¿Ÿè€Œä¼˜åŒ–çš„ç¯å¢ƒä¸­è¿è¡Œã€‚åœ¨å¼•å…¥æ‹“æ‰‘ç®¡ç†å™¨ä¹‹å‰ï¼ŒCPU å’Œè®¾å¤‡ç®¡ç†å™¨å°†åšå‡ºå½¼æ­¤ç‹¬ç«‹çš„èµ„æºåˆ†é…å†³ç­–ã€‚ è¿™å¯èƒ½ä¼šå¯¼è‡´åœ¨å¤šå¤„ç†å™¨ç³»ç»Ÿä¸Šéé¢„æœŸçš„èµ„æºåˆ†é…ç»“æœï¼Œä»è€Œå¯¼è‡´å¯¹å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨ç¨‹åºçš„æ€§èƒ½ä¸‹é™ã€‚&lt;/p>
&lt;!--
### Serverside Apply Introduces Beta 2
-->
&lt;h3 id="serverside-apply-æ¨å‡ºbeta-2">Serverside Apply æ¨å‡ºBeta 2&lt;/h3>
&lt;!--
Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.
-->
&lt;p>Serverside Apply åœ¨1.16 ä¸­è¿›å…¥ Beta é˜¶æ®µï¼Œä½†ç°åœ¨åœ¨ 1.18 ä¸­è¿›å…¥äº†ç¬¬äºŒä¸ª Beta é˜¶æ®µã€‚ è¿™ä¸ªæ–°ç‰ˆæœ¬å°†è·Ÿè¸ªå’Œç®¡ç†æ‰€æœ‰æ–° Kubernetes å¯¹è±¡çš„å­—æ®µæ›´æ”¹ï¼Œä»è€Œä½¿æ‚¨çŸ¥é“ä»€ä¹ˆæ›´æ”¹äº†èµ„æºä»¥åŠä½•æ—¶å‘ç”Ÿäº†æ›´æ”¹ã€‚&lt;/p>
&lt;!--
### Extending Ingress with and replacing a deprecated annotation with IngressClass
-->
&lt;h3 id="ä½¿ç”¨-ingressclass-æ‰©å±•-ingress-å¹¶ç”¨-ingressclass-æ›¿æ¢å·²å¼ƒç”¨çš„æ³¨é‡Š">ä½¿ç”¨ IngressClass æ‰©å±• Ingress å¹¶ç”¨ IngressClass æ›¿æ¢å·²å¼ƒç”¨çš„æ³¨é‡Š&lt;/h3>
&lt;!--
In Kubernetes 1.18, there are two significant additions to Ingress: A new `pathType` field and a new `IngressClass` resource. The `pathType` field allows specifying how paths should be matched. In addition to the default `ImplementationSpecific` type, there are new `Exact` and `Prefix` path types.
-->
&lt;p>åœ¨ Kubernetes 1.18 ä¸­ï¼ŒIngress æœ‰ä¸¤ä¸ªé‡è¦çš„è¡¥å……ï¼šä¸€ä¸ªæ–°çš„ &lt;code>pathType&lt;/code> å­—æ®µå’Œä¸€ä¸ªæ–°çš„ &lt;code>IngressClass&lt;/code> èµ„æºã€‚&lt;code>pathType&lt;/code> å­—æ®µå…è®¸æŒ‡å®šè·¯å¾„çš„åŒ¹é…æ–¹å¼ã€‚ é™¤äº†é»˜è®¤çš„&lt;code>ImplementationSpecific&lt;/code>ç±»å‹å¤–ï¼Œè¿˜æœ‰æ–°çš„ &lt;code>Exact&lt;/code>å’Œ&lt;code>Prefix&lt;/code> è·¯å¾„ç±»å‹ã€‚&lt;/p>
&lt;!--
The `IngressClass` resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new `ingressClassName` field on Ingresses. This new resource and field replace the deprecated `kubernetes.io/ingress.class` annotation.
-->
&lt;p>&lt;code>IngressClass&lt;/code> èµ„æºç”¨äºæè¿° Kubernetes é›†ç¾¤ä¸­ Ingress çš„ç±»å‹ã€‚ Ingress å¯¹è±¡å¯ä»¥é€šè¿‡åœ¨Ingress èµ„æºç±»å‹ä¸Šä½¿ç”¨æ–°çš„&lt;code>ingressClassName&lt;/code> å­—æ®µæ¥æŒ‡å®šä¸å®ƒä»¬å…³è”çš„ç±»ã€‚ è¿™ä¸ªæ–°çš„èµ„æºå’Œå­—æ®µæ›¿æ¢äº†ä¸å†å»ºè®®ä½¿ç”¨çš„ &lt;code>kubernetes.io/ingress.class&lt;/code> æ³¨è§£ã€‚&lt;/p>
&lt;!--
### SIG-CLI introduces kubectl alpha debug
-->
&lt;h3 id="sig-cli-å¼•å…¥äº†-kubectl-alpha-debug">SIG-CLI å¼•å…¥äº† kubectl alpha debug&lt;/h3>
&lt;!--
SIG-CLI was debating the need for a debug utility for quite some time already. With the development of [ephemeral containers](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/), it became more obvious how we can support developers with tooling built on top of `kubectl exec`. The addition of the [`kubectl alpha debug` command](https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md) (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.
-->
&lt;p>SIG-CLI ä¸€ç›´åœ¨äº‰è®ºç€è°ƒè¯•å·¥å…·çš„å¿…è¦æ€§ã€‚éšç€ &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">ä¸´æ—¶å®¹å™¨&lt;/a> çš„å‘å±•ï¼Œæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨åŸºäº &lt;code>kubectl exec&lt;/code> çš„å·¥å…·æ¥æ”¯æŒå¼€å‘äººå‘˜çš„å¿…è¦æ€§å˜å¾—è¶Šæ¥è¶Šæ˜æ˜¾ã€‚ &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">&lt;code>kubectl alpha debug&lt;/code> å‘½ä»¤&lt;/a> çš„å¢åŠ ï¼Œï¼ˆç”±äºæ˜¯ alpha é˜¶æ®µï¼Œéå¸¸æ¬¢è¿æ‚¨åé¦ˆæ„è§ï¼‰ï¼Œä½¿å¼€å‘äººå‘˜å¯ä»¥è½»æ¾åœ°åœ¨é›†ç¾¤ä¸­è°ƒè¯• Podã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªåŠŸèƒ½çš„ä»·å€¼éå¸¸é«˜ã€‚ æ­¤å‘½ä»¤å…è®¸åˆ›å»ºä¸€ä¸ªä¸´æ—¶å®¹å™¨ï¼Œè¯¥å®¹å™¨åœ¨è¦å°è¯•æ£€æŸ¥çš„ Pod æ—è¾¹è¿è¡Œï¼Œå¹¶ä¸”è¿˜é™„åŠ åˆ°æ§åˆ¶å°ä»¥è¿›è¡Œäº¤äº’å¼æ•…éšœæ’é™¤ã€‚&lt;/p>
&lt;!--
### Introducing Windows CSI support alpha for Kubernetes
-->
&lt;h3 id="ä¸º-kubernetes-å¼•å…¥-windows-csi-æ”¯æŒ-alpha">ä¸º Kubernetes å¼•å…¥ Windows CSI æ”¯æŒï¼ˆAlphaï¼‰&lt;/h3>
&lt;!--
The alpha version of CSI Proxy for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.
-->
&lt;p>ç”¨äº Windows çš„ CSI ä»£ç†çš„ Alpha ç‰ˆæœ¬éš Kubernetes 1.18 ä¸€èµ·å‘å¸ƒã€‚ CSI ä»£ç†é€šè¿‡å…è®¸Windows ä¸­çš„å®¹å™¨æ‰§è¡Œç‰¹æƒå­˜å‚¨æ“ä½œæ¥å¯ç”¨ Windows ä¸Šçš„ CSI é©±åŠ¨ç¨‹åºã€‚&lt;/p>
&lt;!--
## Other Updates
-->
&lt;h2 id="å…¶å®ƒæ›´æ–°">å…¶å®ƒæ›´æ–°&lt;/h2>
&lt;!--
### Graduated to Stable ğŸ’¯
-->
&lt;h3 id="æ¯•ä¸šè½¬ä¸ºç¨³å®šç‰ˆ">æ¯•ä¸šè½¬ä¸ºç¨³å®šç‰ˆ&lt;/h3>
&lt;!--
- [Taint Based Eviction](https://github.com/kubernetes/enhancements/issues/166)
- [`kubectl diff`](https://github.com/kubernetes/enhancements/issues/491)
- [CSI Block storage support](https://github.com/kubernetes/enhancements/issues/565)
- [API Server dry run](https://github.com/kubernetes/enhancements/issues/576)
- [Pass Pod information in CSI calls](https://github.com/kubernetes/enhancements/issues/603)
- [Support Out-of-Tree vSphere Cloud Provider](https://github.com/kubernetes/enhancements/issues/670)
- [Support GMSA for Windows workloads](https://github.com/kubernetes/enhancements/issues/689)
- [Skip attach for non-attachable CSI volumes](https://github.com/kubernetes/enhancements/issues/770)
- [PVC cloning](https://github.com/kubernetes/enhancements/issues/989)
- [Moving kubectl package code to staging](https://github.com/kubernetes/enhancements/issues/1020)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
- [AppProtocol for Services and Endpoints](https://github.com/kubernetes/enhancements/issues/1507)
- [Extending Hugepage Feature](https://github.com/kubernetes/enhancements/issues/1539)
- [client-go signature refactor to standardize options and context handling](https://github.com/kubernetes/enhancements/issues/1601)
- [Node-local DNS cache](https://github.com/kubernetes/enhancements/issues/1024)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/166">åŸºäºæ±¡ç‚¹çš„é€å‡ºæ“ä½œ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/491">&lt;code>kubectl diff&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/565">CSI å—å­˜å‚¨æ”¯æŒ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/576">API æœåŠ¡å™¨ dry run&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/603">åœ¨ CSI è°ƒç”¨ä¸­ä¼ é€’ Pod ä¿¡æ¯&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/670">æ”¯æŒæ ‘å¤– vSphere äº‘é©±åŠ¨&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/689">å¯¹ Windows è´Ÿè½½æ”¯æŒ GMSA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/770">å¯¹ä¸å¯æŒ‚è½½çš„CSIå·è·³è¿‡æŒ‚è½½&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/989">PVC å…‹éš†&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">ç§»åŠ¨ kubectl åŒ…ä»£ç åˆ° staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">Windows çš„ RunAsUserName&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">æœåŠ¡å’Œç«¯ç‚¹çš„ AppProtocol&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">æ‰©å±• Hugepage ç‰¹æ€§&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1024">Node-local DNS cache&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Major Changes
-->
&lt;h3 id="ä¸»è¦å˜åŒ–">ä¸»è¦å˜åŒ–&lt;/h3>
&lt;!--
- [EndpointSlice API](https://github.com/kubernetes/enhancements/issues/752)
- [Moving kubectl package code to staging](https://github.com/kubernetes/enhancements/issues/1020)
- [CertificateSigningRequest API](https://github.com/kubernetes/enhancements/issues/1513)
- [Extending Hugepage Feature](https://github.com/kubernetes/enhancements/issues/1539)
- [client-go signature refactor to standardize options and context handling](https://github.com/kubernetes/enhancements/issues/1601)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go çš„è°ƒç”¨è§„èŒƒé‡æ„æ¥æ ‡å‡†åŒ–é€‰é¡¹å’Œç®¡ç†ä¸Šä¸‹æ–‡&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Release Notes
-->
&lt;h3 id="å‘å¸ƒè¯´æ˜">å‘å¸ƒè¯´æ˜&lt;/h3>
&lt;!--
Check out the full details of the Kubernetes 1.18 release in our [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md).
-->
&lt;p>åœ¨æˆ‘ä»¬çš„ &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md">å‘å¸ƒæ–‡æ¡£&lt;/a>ä¸­æŸ¥çœ‹ Kubernetes 1.18 å‘è¡Œç‰ˆçš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
### Availability
-->
&lt;h3 id="ä¸‹è½½å®‰è£…">ä¸‹è½½å®‰è£…&lt;/h3>
&lt;!--
Kubernetes 1.18 is available for download on [GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/) or run local Kubernetes clusters using Docker container â€œnodesâ€ with [kind](https://kind.sigs.k8s.io/). You can also easily install 1.18 using [kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;p>Kubernetes 1.18 å¯ä»¥åœ¨ &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0">GitHub&lt;/a> ä¸Šä¸‹è½½ã€‚ è¦å¼€å§‹ä½¿ç”¨Kubernetesï¼Œè¯·æŸ¥çœ‹è¿™äº› &lt;a href="https://kubernetes.io/docs/tutorials/">äº¤äº’æ•™ç¨‹&lt;/a> æˆ–é€šè¿‡&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> ä½¿ç”¨ Docker å®¹å™¨è¿è¡Œæœ¬åœ° kubernetes é›†ç¾¤ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨&lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>è½»æ¾å®‰è£… 1.18ã€‚&lt;/p>
&lt;!--
### Release Team
-->
&lt;h3 id="å‘å¸ƒå›¢é˜Ÿ">å‘å¸ƒå›¢é˜Ÿ&lt;/h3>
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md) led by Jorge Alarcon Ochoa, Site Reliability Engineer at Searchable AI. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
-->
&lt;p>é€šè¿‡æ•°ç™¾ä½è´¡çŒ®äº†æŠ€æœ¯å’ŒéæŠ€æœ¯å†…å®¹çš„ä¸ªäººçš„åŠªåŠ›ï¼Œä½¿æœ¬æ¬¡å‘è¡Œæˆä¸ºå¯èƒ½ã€‚ ç‰¹åˆ«æ„Ÿè°¢ç”± Searchable AI çš„ç½‘ç«™å¯é æ€§å·¥ç¨‹å¸ˆ Jorge Alarcon Ochoa é¢†å¯¼çš„&lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">å‘å¸ƒå›¢é˜Ÿ&lt;/a>ã€‚ 34 ä½å‘å¸ƒå›¢é˜Ÿæˆå‘˜åè°ƒäº†å‘å¸ƒçš„å„ä¸ªæ–¹é¢ï¼Œä»æ–‡æ¡£åˆ°æµ‹è¯•ã€éªŒè¯å’ŒåŠŸèƒ½å®Œæ•´æ€§ã€‚&lt;/p>
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [40,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 3,000 people.
-->
&lt;p>éšç€ Kubernetes ç¤¾åŒºçš„å‘å±•å£®å¤§ï¼Œæˆ‘ä»¬çš„å‘å¸ƒè¿‡ç¨‹å¾ˆå¥½åœ°å±•ç¤ºäº†å¼€æºè½¯ä»¶å¼€å‘ä¸­çš„åä½œã€‚ Kubernetes ç»§ç»­å¿«é€Ÿè·å–æ–°ç”¨æˆ·ã€‚ è¿™ç§å¢é•¿åˆ›é€ äº†ä¸€ä¸ªç§¯æçš„åé¦ˆå›è·¯ï¼Œå…¶ä¸­æœ‰æ›´å¤šçš„è´¡çŒ®è€…æäº¤äº†ä»£ç ï¼Œä»è€Œåˆ›å»ºäº†æ›´åŠ æ´»è·ƒçš„ç”Ÿæ€ç³»ç»Ÿã€‚ è¿„ä»Šä¸ºæ­¢ï¼ŒKubernetes å·²æœ‰ &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">40,000 ç‹¬ç«‹è´¡çŒ®è€…&lt;/a> å’Œä¸€ä¸ªè¶…è¿‡3000äººçš„æ´»è·ƒç¤¾åŒºã€‚&lt;/p>
&lt;!--
### Release Logo
-->
&lt;h3 id="å‘å¸ƒ-logo">å‘å¸ƒ logo&lt;/h3>
&lt;!--
![Kubernetes 1.18 Release Logo](/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png)
-->
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png" alt="Kubernetes 1.18 å‘å¸ƒå›¾æ ‡">&lt;/p>
&lt;!--
#### Why the LHC?
-->
&lt;h4 id="ä¸ºä»€ä¹ˆæ˜¯-lhc">ä¸ºä»€ä¹ˆæ˜¯ LHC&lt;/h4>
&lt;!--
The LHC is the worldâ€™s largest and most powerful particle accelerator. It is the result of the collaboration of thousands of scientists from around the world, all for the advancement of science. In a similar manner, Kubernetes has been a project that has united thousands of contributors from hundreds of organizations â€“ all to work towards the same goal of improving cloud computing in all aspects! "A Bit Quarky" as the release name is meant to remind us that unconventional ideas can bring about great change and keeping an open mind to diversity will lead help us innovate.
-->
&lt;p>LHC æ˜¯ä¸–ç•Œä¸Šæœ€å¤§ï¼ŒåŠŸèƒ½æœ€å¼ºå¤§çš„ç²’å­åŠ é€Ÿå™¨ã€‚å®ƒæ˜¯ç”±æ¥è‡ªä¸–ç•Œå„åœ°æˆåƒä¸Šä¸‡ç§‘å­¦å®¶åˆä½œçš„ç»“æœï¼Œæ‰€æœ‰è¿™äº›åˆä½œéƒ½æ˜¯ä¸ºäº†ä¿ƒè¿›ç§‘å­¦çš„å‘å±•ã€‚ä»¥ç±»ä¼¼çš„æ–¹å¼ï¼ŒKubernetes å·²ç»æˆä¸ºä¸€ä¸ªèšé›†äº†æ¥è‡ªæ•°ç™¾ä¸ªç»„ç»‡çš„æ•°åƒåè´¡çŒ®è€…â€“æ‰€æœ‰äººéƒ½æœç€åœ¨å„ä¸ªæ–¹é¢æ”¹å–„äº‘è®¡ç®—çš„ç›¸åŒç›®æ ‡åŠªåŠ›çš„é¡¹ç›®ï¼ å‘å¸ƒåç§°â€œ A Bit Quarkyâ€ çš„æ„æ€æ˜¯æé†’æˆ‘ä»¬ï¼Œéå¸¸è§„çš„æƒ³æ³•å¯ä»¥å¸¦æ¥å·¨å¤§çš„å˜åŒ–ï¼Œå¯¹å¼€æ”¾æ€§ä¿æŒå¼€æ”¾æ€åº¦å°†æœ‰åŠ©äºæˆ‘ä»¬è¿›è¡Œåˆ›æ–°ã€‚&lt;/p>
&lt;!--
#### About the designer
-->
&lt;h4 id="å…³äºè®¾è®¡è€…">å…³äºè®¾è®¡è€…&lt;/h4>
&lt;!--
Maru Lango is a designer currently based in Mexico City. While her area of expertise is Product Design, she also enjoys branding, illustration and visual experiments using CSS + JS and contributing to diversity efforts within the tech and design communities. You may find her in most social media as @marulango or check her website: https://marulango.com
-->
&lt;p>Maru Lango æ˜¯ç›®å‰å±…ä½åœ¨å¢¨è¥¿å“¥åŸçš„è®¾è®¡å¸ˆã€‚å¥¹çš„ä¸“é•¿æ˜¯äº§å“è®¾è®¡ï¼Œå¥¹è¿˜å–œæ¬¢ä½¿ç”¨ CSS + JS è¿›è¡Œå“ç‰Œã€æ’å›¾å’Œè§†è§‰å®éªŒï¼Œä¸ºæŠ€æœ¯å’Œè®¾è®¡ç¤¾åŒºçš„å¤šæ ·æ€§åšè´¡çŒ®ã€‚æ‚¨å¯èƒ½ä¼šåœ¨å¤§å¤šæ•°ç¤¾äº¤åª’ä½“ä¸Šä»¥ @marulango çš„èº«ä»½æ‰¾åˆ°å¥¹ï¼Œæˆ–æŸ¥çœ‹å¥¹çš„ç½‘ç«™ï¼š &lt;a href="https://marulango.com">https://marulango.com&lt;/a>&lt;/p>
&lt;!--
### User Highlights
-->
&lt;h3 id="é«˜å…‰ç”¨æˆ·">é«˜å…‰ç”¨æˆ·&lt;/h3>
&lt;!--
- Ericsson is using Kubernetes and other cloud native technology to deliver a [highly demanding 5G network](https://www.cncf.io/case-study/ericsson/) that resulted in up to 90 percent CI/CD savings.
- Zendesk is using Kubernetes to [run around 70% of its existing applications](https://www.cncf.io/case-study/zendesk/). Itâ€™s also building all new applications to also run on Kubernetes, which has brought time savings, greater flexibility, and increased velocity to its application development.
- LifeMiles has [reduced infrastructure spending by 50%](https://www.cncf.io/case-study/lifemiles/) because of its move to Kubernetes. It has also allowed them to double its available resource capacity.
-->
&lt;ul>
&lt;li>çˆ±ç«‹ä¿¡æ­£åœ¨ä½¿ç”¨ Kubernetes å’Œå…¶ä»–äº‘åŸç”ŸæŠ€æœ¯æ¥äº¤ä»˜&lt;a href="https://www.cncf.io/case-study/ericsson/">é«˜æ ‡å‡†çš„ 5G ç½‘ç»œ&lt;/a>ï¼Œè¿™å¯ä»¥åœ¨ CI/CD ä¸ŠèŠ‚çœå¤šè¾¾ 90ï¼… çš„æ”¯å‡ºã€‚&lt;/li>
&lt;li>Zendesk æ­£åœ¨ä½¿ç”¨ Kubernetes &lt;a href="https://www.cncf.io/case-study/zendesk/">è¿è¡Œå…¶ç°æœ‰åº”ç”¨ç¨‹åºçš„çº¦ 70ï¼…&lt;/a>ã€‚å®ƒè¿˜æ­£åœ¨ä½¿æ‰€æ„å»ºçš„æ‰€æœ‰æ–°åº”ç”¨éƒ½å¯ä»¥åœ¨ Kubernetes ä¸Šè¿è¡Œï¼Œä»è€ŒèŠ‚çœæ—¶é—´ã€æé«˜çµæ´»æ€§å¹¶åŠ å¿«å…¶åº”ç”¨ç¨‹åºå¼€å‘çš„é€Ÿåº¦ã€‚&lt;/li>
&lt;li>LifeMiles å› è¿ç§»åˆ° Kubernetes è€Œ&lt;a href="https://www.cncf.io/case-study/lifemiles/">é™ä½äº† 50% çš„åŸºç¡€è®¾æ–½å¼€æ”¯&lt;/a>ã€‚Kubernetes è¿˜ä½¿ä»–ä»¬å¯ä»¥å°†å…¶å¯ç”¨èµ„æºå®¹é‡å¢åŠ ä¸€å€ã€‚&lt;/li>
&lt;/ul>
&lt;!--
### Ecosystem Updates
-->
&lt;h3 id="ç”Ÿæ€ç³»ç»Ÿæ›´æ–°">ç”Ÿæ€ç³»ç»Ÿæ›´æ–°&lt;/h3>
&lt;!--
- The CNCF published the results of its [annual survey](https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/) showing that Kubernetes usage in production is skyrocketing. The survey found that 78% of respondents are using Kubernetes in production compared to 58% last year.
- The â€œIntroduction to Kubernetesâ€ course hosted by the CNCF [surpassed 100,000 registrations](https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/).
-->
&lt;ul>
&lt;li>CNCFå‘å¸ƒäº†&lt;a href="https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/">å¹´åº¦è°ƒæŸ¥&lt;/a> çš„ç»“æœï¼Œè¡¨æ˜ Kubernetes åœ¨ç”Ÿäº§ä¸­çš„ä½¿ç”¨æ­£åœ¨é£é€Ÿå¢é•¿ã€‚è°ƒæŸ¥å‘ç°ï¼Œæœ‰78ï¼…çš„å—è®¿è€…åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨Kubernetesï¼Œè€Œå»å¹´è¿™ä¸€æ¯”ä¾‹ä¸º 58ï¼…ã€‚&lt;/li>
&lt;li>CNCF ä¸¾åŠçš„ â€œKuberneteså…¥é—¨â€ è¯¾ç¨‹æœ‰&lt;a href="https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/">è¶…è¿‡ 100,000 äººæ³¨å†Œ&lt;/a>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
### Project Velocity
-->
&lt;h3 id="é¡¹ç›®é€Ÿåº¦">é¡¹ç›®é€Ÿåº¦&lt;/h3>
&lt;!--
The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. [K8s DevStats](https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1) illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times.
-->
&lt;p>CNCF ç»§ç»­å®Œå–„ DevStatsã€‚è¿™æ˜¯ä¸€ä¸ªé›„å¿ƒå‹ƒå‹ƒçš„é¡¹ç›®ï¼Œæ—¨åœ¨å¯¹é¡¹ç›®ä¸­çš„æ— æ•°è´¡çŒ®æ•°æ®è¿›è¡Œå¯è§†åŒ–å±•ç¤ºã€‚&lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">K8s DevStats&lt;/a> å±•ç¤ºäº†ä¸»è¦å…¬å¸è´¡çŒ®è€…çš„è´¡çŒ®ç»†ç›®ï¼Œä»¥åŠä¸€ç³»åˆ—ä»¤äººå°è±¡æ·±åˆ»çš„é¢„å®šä¹‰çš„æŠ¥å‘Šï¼Œæ¶‰åŠä»è´¡çŒ®è€…ä¸ªäººçš„å„æ–¹é¢åˆ° PR ç”Ÿå‘½å‘¨æœŸçš„å„ä¸ªæ–¹é¢ã€‚&lt;/p>
&lt;!--
This past quarter, 641 different companies and over 6,409 individuals contributed to Kubernetes. [Check out DevStats](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;var-period=m&amp;var-repogroup_name=All) to learn more about the overall velocity of the Kubernetes project and community.
-->
&lt;p>åœ¨è¿‡å»çš„ä¸€ä¸ªå­£åº¦ä¸­ï¼Œ641 å®¶ä¸åŒçš„å…¬å¸å’Œè¶…è¿‡ 6,409 ä¸ªä¸ªäººä¸º Kubernetes ä½œå‡ºè´¡çŒ®ã€‚ &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All">æŸ¥çœ‹ DevStats&lt;/a> ä»¥äº†è§£æœ‰å…³ Kubernetes é¡¹ç›®å’Œç¤¾åŒºå‘å±•é€Ÿåº¦çš„ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
### Event Update
-->
&lt;h3 id="æ´»åŠ¨ä¿¡æ¯">æ´»åŠ¨ä¿¡æ¯&lt;/h3>
&lt;!--
Kubecon + CloudNativeCon EU 2020 is being pushed back â€“ for the more most up-to-date information, please check the [Novel Coronavirus Update page](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/).
-->
&lt;p>Kubecon + CloudNativeCon EU 2020 å·²ç»æ¨è¿Ÿ - æœ‰å…³æœ€æ–°ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">æ–°å‹è‚ºç‚å‘å¸ƒé¡µé¢&lt;/a>ã€‚&lt;/p>
&lt;!--
### Upcoming Release Webinar
-->
&lt;h3 id="å³å°†åˆ°æ¥çš„å‘å¸ƒçš„çº¿ä¸Šä¼šè®®">å³å°†åˆ°æ¥çš„å‘å¸ƒçš„çº¿ä¸Šä¼šè®®&lt;/h3>
&lt;!--
Join members of the Kubernetes 1.18 release team on April 23rd, 2020 to learn about the major features in this release including kubectl debug, Topography Manager, Ingress to V1 graduation, and client-go. Register here: https://www.cncf.io/webinars/kubernetes-1-18/.
-->
&lt;p>åœ¨ 2020 å¹´ 4 æœˆ 23 æ—¥ï¼Œå’Œ Kubernetes 1.18 ç‰ˆæœ¬å›¢é˜Ÿä¸€èµ·äº†è§£æ­¤ç‰ˆæœ¬çš„ä¸»è¦åŠŸèƒ½ï¼ŒåŒ…æ‹¬ kubectl debugã€æ‹“æ‰‘ç®¡ç†å™¨ã€Ingress æ¯•ä¸šä¸º V1 ç‰ˆæœ¬ä»¥åŠ client-goã€‚ åœ¨æ­¤å¤„æ³¨å†Œï¼šhttps://www.cncf.io/webinars/kubernetes-1-18/ ã€‚&lt;/p>
&lt;!--
### Get Involved
-->
&lt;h3 id="å¦‚ä½•å‚ä¸">å¦‚ä½•å‚ä¸&lt;/h3>
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something youâ€™d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
-->
&lt;p>å‚ä¸ Kubernetes çš„æœ€ç®€å•æ–¹æ³•æ˜¯åŠ å…¥ä¼—å¤šä¸æ‚¨çš„å…´è¶£ç›¸å…³çš„ &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">ç‰¹åˆ«å…´è¶£å°ç»„&lt;/a> (SIGs) ä¹‹ä¸€ã€‚ æ‚¨æœ‰ä»€ä¹ˆæƒ³å‘ Kubernetes ç¤¾åŒºå‘å¸ƒçš„å†…å®¹å—ï¼Ÿ å‚ä¸æˆ‘ä»¬çš„æ¯å‘¨ &lt;a href="https://github.com/kubernetes/community/tree/master/communication">ç¤¾åŒºä¼šè®®&lt;/a>ï¼Œå¹¶é€šè¿‡ä»¥ä¸‹æ¸ é“åˆ†äº«æ‚¨çš„å£°éŸ³ã€‚ æ„Ÿè°¢æ‚¨ä¸€ç›´ä»¥æ¥çš„åé¦ˆå’Œæ”¯æŒã€‚&lt;/p>
&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
- Read more about whatâ€™s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)
-->
&lt;ul>
&lt;li>åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ä»¬ &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>ï¼Œäº†è§£æœ€æ–°åŠ¨æ€&lt;/li>
&lt;li>åœ¨ &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a> ä¸Šå‚ä¸ç¤¾åŒºè®¨è®º&lt;/li>
&lt;li>åŠ å…¥ &lt;a href="http://slack.k8s.io/">Slack&lt;/a> ä¸Šçš„ç¤¾åŒº&lt;/li>
&lt;li>åœ¨&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>æé—®ï¼ˆæˆ–å›ç­”ï¼‰&lt;/li>
&lt;li>åˆ†äº«æ‚¨çš„ Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">æ•…äº‹&lt;/a>&lt;/li>
&lt;li>é€šè¿‡ &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>äº†è§£æ›´å¤šå…³äº Kubernetes çš„æ–°é²œäº‹&lt;/li>
&lt;li>äº†è§£æ›´å¤šå…³äº &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes å‘å¸ƒå›¢é˜Ÿ&lt;/a> çš„ä¿¡æ¯&lt;/li>
&lt;/ul></description></item><item><title>Blog: åŸºäº MIPS æ¶æ„çš„ Kubernetes æ–¹æ¡ˆ</title><link>https://kubernetes.io/zh/blog/2020/01/15/kubernetes-on-mips/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/01/15/kubernetes-on-mips/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes on MIPS"
date: 2020-01-15
slug: Kubernetes-on-MIPS
-->
&lt;!--
**Authors:** TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> çŸ³å…‰é“¶ï¼Œå°¹ä¸œè¶…ï¼Œå±•æœ›ï¼Œæ±Ÿç‡•ï¼Œè”¡å«å«ï¼Œé«˜ä¼ é›†ï¼Œå­™æ€æ¸…ï¼ˆæµªæ½®ï¼‰&lt;/p>
&lt;!--
## Background
-->
&lt;h2 id="èƒŒæ™¯">èƒŒæ™¯&lt;/h2>
&lt;!--
[MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.
-->
&lt;p>&lt;a href="https://zh.wikipedia.org/wiki/MIPS%E6%9E%B6%E6%A7%8B">MIPS&lt;/a> (Microprocessor without Interlocked Pipelined Stages) æ˜¯ä¸€ç§é‡‡å–ç²¾ç®€æŒ‡ä»¤é›†ï¼ˆRISCï¼‰çš„å¤„ç†å™¨æ¶æ„ (ISA)ï¼Œå‡ºç°äº 1981 å¹´ï¼Œç”± MIPS ç§‘æŠ€å…¬å¸å¼€å‘ã€‚å¦‚ä»Š MIPS æ¶æ„è¢«å¹¿æ³›åº”ç”¨äºè®¸å¤šç”µå­äº§å“ä¸Šã€‚&lt;/p>
&lt;!--
[Kubernetes](https://kubernetes.io) has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it's a pity that Kubernetes doesn't support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.
-->
&lt;p>&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> å®˜æ–¹ç›®å‰æ”¯æŒä¼—å¤š CPU æ¶æ„è¯¸å¦‚ x86, arm/arm64, ppc64le, s390x ç­‰ã€‚ç„¶è€Œç›®å‰è¿˜ä¸æ”¯æŒ MIPS æ¶æ„ï¼Œå§‹ç»ˆæ˜¯ä¸€ä¸ªé—æ†¾ã€‚éšç€äº‘åŸç”ŸæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ï¼ŒMIPS æ¶æ„ä¸‹çš„ç”¨æˆ·å§‹ç»ˆå¯¹ Kubernetes on MIPS æœ‰ç€è¿«åˆ‡çš„éœ€æ±‚ã€‚&lt;/p>
&lt;!--
## Achievements
-->
&lt;h2 id="æˆæœ">æˆæœ&lt;/h2>
&lt;!--
For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.
-->
&lt;p>å¤šå¹´æ¥ï¼Œä¸ºäº†ä¸°å¯Œå¼€æºç¤¾åŒºçš„ç”Ÿæ€ï¼Œæˆ‘ä»¬ä¸€ç›´è‡´åŠ›äºåœ¨ MIPS æ¶æ„ä¸‹é€‚é… Kubernetesã€‚éšç€ MIPS CPU çš„ä¸æ–­è¿­ä»£ä¼˜åŒ–å’Œæ€§èƒ½çš„æå‡ï¼Œæˆ‘ä»¬åœ¨ mips64el å¹³å°ä¸Šå–å¾—äº†ä¸€äº›çªç ´æ€§çš„è¿›å±•ã€‚&lt;/p>
&lt;!--
Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.
-->
&lt;p>å¤šå¹´æ¥ï¼Œæˆ‘ä»¬ä¸€ç›´ç§¯ææŠ•å…¥ Kubernetes ç¤¾åŒºï¼Œåœ¨ Kubernetes æŠ€æœ¯åº”ç”¨å’Œä¼˜åŒ–æ–¹é¢å…·å¤‡äº†ä¸°å¯Œçš„ç»éªŒã€‚æœ€è¿‘ï¼Œæˆ‘ä»¬åœ¨ç ”å‘è¿‡ç¨‹ä¸­å°è¯•å°† Kubernetes é€‚é…åˆ° MIPS æ¶æ„å¹³å°ï¼Œå¹¶å–å¾—äº†é˜¶æ®µæ€§æˆæœã€‚æˆåŠŸå®Œæˆäº† Kubernetes ä»¥åŠç›¸å…³ç»„ä»¶çš„è¿ç§»é€‚é…ï¼Œä¸ä»…æ­å»ºå‡ºç¨³å®šé«˜å¯ç”¨çš„ MIPS é›†ç¾¤ï¼ŒåŒæ—¶å®Œæˆäº† Kubernetes v1.16.2 ç‰ˆæœ¬çš„ä¸€è‡´æ€§æµ‹è¯•ã€‚&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png" alt="Kubernetes on MIPS">&lt;/p>
&lt;!--
_Figure 1 Kubernetes on MIPS_
-->
&lt;p>&lt;em>å›¾ä¸€ Kubernetes on MIPS&lt;/em>&lt;/p>
&lt;!--
## K8S-MIPS component build
-->
&lt;h2 id="k8s-mips-ç»„ä»¶æ„å»º">K8S-MIPS ç»„ä»¶æ„å»º&lt;/h2>
&lt;!--
Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:
-->
&lt;p>å‡ ä¹æ‰€æœ‰çš„ Kubernetes ç›¸å…³çš„äº‘åŸç”Ÿç»„ä»¶éƒ½æ²¡æœ‰æä¾› MIPS ç‰ˆæœ¬çš„å®‰è£…åŒ…æˆ–é•œåƒï¼Œåœ¨ MIPS å¹³å°ä¸Šéƒ¨ç½² Kubernetes çš„å‰ææ˜¯è‡ªè¡Œç¼–è¯‘æ„å»ºå‡ºå…¨éƒ¨æ‰€éœ€ç»„ä»¶ã€‚è¿™äº›ç»„ä»¶ä¸»è¦åŒ…æ‹¬ï¼š&lt;/p>
&lt;ul>
&lt;li>golang&lt;/li>
&lt;li>docker-ce&lt;/li>
&lt;li>hyperkube&lt;/li>
&lt;li>pause&lt;/li>
&lt;li>etcd&lt;/li>
&lt;li>calico&lt;/li>
&lt;li>coredns&lt;/li>
&lt;li>metrics-server&lt;/li>
&lt;/ul>
&lt;!--
Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.
-->
&lt;p>å¾—ç›Šäº Golang ä¼˜ç§€çš„è®¾è®¡ä»¥åŠå¯¹äº MIPS å¹³å°çš„è‰¯å¥½æ”¯æŒï¼Œæå¤§åœ°ç®€åŒ–äº†ä¸Šè¿°äº‘åŸç”Ÿç»„ä»¶çš„ç¼–è¯‘è¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ mips64el å¹³å°ç¼–è¯‘å‡ºäº†æœ€æ–°ç¨³å®šçš„ golang, ç„¶åé€šè¿‡æºç æ„å»ºçš„æ–¹å¼ç¼–è¯‘å®Œæˆäº†ä¸Šè¿°å¤§éƒ¨åˆ†ç»„ä»¶ã€‚&lt;/p>
&lt;!--
During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.
-->
&lt;p>åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸å¯é¿å…åœ°é‡åˆ°äº†å¾ˆå¤šå¹³å°å…¼å®¹æ€§çš„é—®é¢˜ï¼Œæ¯”å¦‚å…³äº golang ç³»ç»Ÿè°ƒç”¨ (syscall) çš„å…¼å®¹æ€§é—®é¢˜, syscall.Stat_t 32 ä½ ä¸ 64 ä½ç±»å‹è½¬æ¢ï¼ŒEpollEvent ä¿®æ­£ä½ç¼ºå¤±ç­‰ç­‰ã€‚&lt;/p>
&lt;!--
To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.
-->
&lt;p>æ„å»º K8S-MIPS ç»„ä»¶ä¸»è¦ä½¿ç”¨äº†äº¤å‰ç¼–è¯‘æŠ€æœ¯ã€‚æ„å»ºè¿‡ç¨‹åŒ…æ‹¬é›†æˆ QEMU å·¥å…·æ¥å®ç° MIPS CPU æŒ‡ä»¤çš„è½¬æ¢ã€‚åŒæ—¶ä¿®æ”¹ Kubernetes å’Œ E2E é•œåƒçš„æ„å»ºè„šæœ¬ï¼Œæ„å»ºäº† Hyperkube å’Œ MIPS æ¶æ„çš„ E2E æµ‹è¯•é•œåƒã€‚&lt;/p>
&lt;!--
After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.
-->
&lt;p>æˆåŠŸæ„å»ºå‡ºä»¥ä¸Šç»„ä»¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å·¥å…·å®Œæˆ Kubernetes é›†ç¾¤çš„æ­å»ºï¼Œæ¯”å¦‚ kubesprayã€kubeadm ç­‰ã€‚&lt;/p>
&lt;!--
| Name | Version | MIPS Repository |
|--------------------------------|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| golang on MIPS | 1.12.5 | - |
| docker-ce on MIPS | 18.09.8 | - |
| metrics-server for CKE on MIPS | 0.3.2 | `registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2` |
| etcd for CKE on MIPS | 3.2.26 | `registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26` |
| pause for CKE on MIPS | 3.1 | `registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1` |
| hyperkube for CKE on MIPS | 1.14.3 | `registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3` |
| coredns for CKE on MIPS | 1.6.5 | `registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5` |
| calico for CKE on MIPS | 3.8.0 | `registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0` |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>åç§°&lt;/th>
&lt;th>ç‰ˆæœ¬&lt;/th>
&lt;th>MIPS é•œåƒä»“åº“&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ golang&lt;/td>
&lt;td>1.12.5&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ docker-ce&lt;/td>
&lt;td>18.09.8&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æ„å»º metrics-server&lt;/td>
&lt;td>0.3.2&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æ„å»º etcd&lt;/td>
&lt;td>3.2.26&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æ„å»º pause&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æ„å»º hyperkube&lt;/td>
&lt;td>1.14.3&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æ„å»º coredns&lt;/td>
&lt;td>1.6.5&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æ„å»º calico&lt;/td>
&lt;td>3.8.0&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!--
**Note**: CKE is a Kubernetes-based cloud container engine launched by Inspur
-->
&lt;p>&lt;strong>æ³¨&lt;/strong>: CKE æ˜¯æµªæ½®æ¨å‡ºçš„ä¸€æ¬¾åŸºäº Kubernetes çš„å®¹å™¨äº‘æœåŠ¡å¼•æ“&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png" alt="K8S-MIPS Cluster Components">&lt;/p>
&lt;!--
_Figure 2 K8S-MIPS Cluster Components_
-->
&lt;p>&lt;em>å›¾äºŒ K8S-MIPS é›†ç¾¤ç»„ä»¶&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png" alt="CPU Architecture">&lt;/p>
&lt;!--
_Figure 3 CPU Architecture_
-->
&lt;p>&lt;em>å›¾ä¸‰ CPU æ¶æ„&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png" alt="Cluster Node Information">&lt;/p>
&lt;!--
_Figure 4 Cluster Node Information_
-->
&lt;p>&lt;em>å›¾å›› é›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯&lt;/em>&lt;/p>
&lt;!--
## Run K8S Conformance Test
-->
&lt;h2 id="è¿è¡Œ-k8s-ä¸€è‡´æ€§æµ‹è¯•">è¿è¡Œ K8S ä¸€è‡´æ€§æµ‹è¯•&lt;/h2>
&lt;!--
The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes [conformance test](https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md).
-->
&lt;p>éªŒè¯ K8S-MIP é›†ç¾¤ç¨³å®šæ€§å’Œå¯ç”¨æ€§æœ€ç®€å•ç›´æ¥çš„æ–¹å¼æ˜¯è¿è¡Œ Kubernetes çš„ &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md">ä¸€è‡´æ€§æµ‹è¯•&lt;/a>ã€‚&lt;/p>
&lt;!--
Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.
-->
&lt;p>ä¸€è‡´æ€§æµ‹è¯•æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å®¹å™¨ï¼Œç”¨äºå¯åŠ¨ Kubernetes ç«¯åˆ°ç«¯çš„ä¸€è‡´æ€§æµ‹è¯•ã€‚&lt;/p>
&lt;!--
Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from `kubernetes/test/images`, and the built images are at `gcr.io/kubernetes-e2e-test-images`. Since there are no MIPS images in the repository, we must first build all needed images to run the test.
-->
&lt;p>å½“æ‰§è¡Œä¸€è‡´æ€§æµ‹è¯•æ—¶ï¼Œæµ‹è¯•ç¨‹åºä¼šå¯åŠ¨è®¸å¤š Pod è¿›è¡Œå„ç§ç«¯åˆ°ç«¯çš„è¡Œä¸ºæµ‹è¯•ï¼Œè¿™äº› Pod ä½¿ç”¨çš„é•œåƒæºç å¤§éƒ¨åˆ†æ¥è‡ªäº &lt;code>kubernetes/test/images&lt;/code> ç›®å½•ä¸‹ï¼Œæ„å»ºçš„é•œåƒä½äº &lt;code>gcr.io/kubernetes-e2e-test-images/&lt;/code>ã€‚ç”±äºé•œåƒä»“åº“ä¸­ç›®å‰å¹¶ä¸å­˜åœ¨ MIPS æ¶æ„çš„é•œåƒï¼Œæˆ‘ä»¬è¦æƒ³è¿è¡Œ E2E æµ‹è¯•ï¼Œå¿…é¡»é¦–å…ˆæ„å»ºå‡ºæµ‹è¯•æ‰€éœ€çš„å…¨éƒ¨é•œåƒã€‚&lt;/p>
&lt;!--
### Build needed images for test
-->
&lt;h3 id="æ„å»ºæµ‹è¯•æ‰€éœ€é•œåƒ">æ„å»ºæµ‹è¯•æ‰€éœ€é•œåƒ&lt;/h3>
&lt;!--
The first step is to find all needed images for the test. We can run `sonobuoy images-p e2e` command to list all images, or we can find those images in [/test/utils/image/manifest.go](https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go). Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.
-->
&lt;p>ç¬¬ä¸€æ­¥æ˜¯æ‰¾åˆ°æµ‹è¯•æ‰€éœ€çš„æ‰€æœ‰é•œåƒã€‚æˆ‘ä»¬å¯ä»¥æ‰§è¡Œ &lt;code>sonobuoy images-p e2e&lt;/code> å‘½ä»¤æ¥åˆ—å‡ºæ‰€æœ‰é•œåƒï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥åœ¨ &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go">/test/utils/image/manifest.go&lt;/a> ä¸­æ‰¾åˆ°è¿™äº›é•œåƒã€‚å°½ç®¡ Kubernetes å®˜æ–¹æä¾›äº†å®Œæ•´çš„ Makefile å’Œ shell è„šæœ¬ï¼Œä¸ºæ„å»ºæµ‹è¯•æ˜ åƒæä¾›äº†å‘½ä»¤ï¼Œä½†æ˜¯ä»ç„¶æœ‰è®¸å¤šä¸ä½“ç³»ç»“æ„ç›¸å…³çš„é—®é¢˜æœªèƒ½è§£å†³ï¼Œæ¯”å¦‚åŸºç¡€æ˜ åƒå’Œä¾èµ–åŒ…çš„ä¸å…¼å®¹é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•é€šè¿‡ç›´æ¥æ‰§è¡Œè¿™äº›æ„å»ºå‘½ä»¤æ¥åˆ¶ä½œ mips64el æ¶æ„é•œåƒã€‚&lt;/p>
&lt;!--
Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of [alpine](https://www.alpinelinux.org/), so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.
-->
&lt;p>å¤šæ•°æµ‹è¯•é•œåƒéƒ½æ˜¯ä½¿ç”¨ golang ç¼–å†™ï¼Œç„¶åç¼–è¯‘å‡ºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå¹¶åŸºäºç›¸åº”çš„ Dockerfile åˆ¶ä½œå‡ºé•œåƒã€‚è¿™äº›é•œåƒå¯¹æˆ‘ä»¬æ¥è¯´å¯ä»¥è½»æ¾åœ°åˆ¶ä½œå‡ºæ¥ã€‚ä½†æ˜¯éœ€è¦æ³¨æ„ä¸€ç‚¹ï¼šæµ‹è¯•é•œåƒé»˜è®¤ä½¿ç”¨çš„åŸºç¡€é•œåƒå¤§å¤šæ˜¯ alpine, ç›®å‰ &lt;a href="https://www.alpinelinux.org/">Alpine&lt;/a> å®˜æ–¹å¹¶ä¸æ”¯æŒ mips64el æ¶æ„ï¼Œæˆ‘ä»¬æš‚æ—¶æœªèƒ½è‡ªå·±åˆ¶ä½œå‡º mips64el ç‰ˆæœ¬çš„ alpine ç¡€é•œåƒï¼Œåªèƒ½å°†åŸºç¡€é•œåƒæ›¿æ¢ä¸ºæˆ‘ä»¬ç›®å‰å·²æœ‰çš„ mips64el åŸºç¡€é•œåƒï¼Œæ¯”å¦‚ debian-stretch,fedora, ubuntu ç­‰ã€‚æ›¿æ¢åŸºç¡€é•œåƒçš„åŒæ—¶ä¹Ÿéœ€è¦æ›¿æ¢å®‰è£…ä¾èµ–åŒ…çš„å‘½ä»¤ï¼Œç”šè‡³ä¾èµ–åŒ…çš„ç‰ˆæœ¬ç­‰ã€‚&lt;/p>
&lt;!--
Some images are not in `kubernetes/test/images`, such as `gcr.io/google-samples/gb-frontend:v6`. There is no clear documentation explaining where these images are locaated, though we found the source code in repository [github.com/GoogleCloudPlatform/kubernetes-engine-samples](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples). We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as `php:5-apache`, `redis`, and `perl`.
-->
&lt;p>æœ‰äº›æµ‹è¯•æ‰€éœ€é•œåƒçš„æºç å¹¶ä¸åœ¨ &lt;code>kubernetes/test/images&lt;/code> ä¸‹,æ¯”å¦‚ &lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code> ç­‰ï¼Œæ²¡æœ‰æ˜ç¡®çš„æ–‡æ¡£è¯´æ˜è¿™ç±»é•œåƒæ¥è‡ªäºä½•æ–¹ï¼Œæœ€ç»ˆè¿˜æ˜¯åœ¨ &lt;a href="github.com/GoogleCloudPlatform/kubernetes-engine-samples">github.com/GoogleCloudPlatform/kubernetes-engine-samples&lt;/a> è¿™ä¸ªä»“åº“æ‰¾åˆ°äº†åŸå§‹çš„é•œåƒæºä»£ç ã€‚ä½†æ˜¯å¾ˆå¿«æˆ‘ä»¬é‡åˆ°äº†æ–°çš„é—®é¢˜ï¼Œä¸ºäº†åˆ¶ä½œè¿™äº›é•œåƒï¼Œè¿˜è¦åˆ¶ä½œå®ƒä¾èµ–çš„åŸºç¡€é•œåƒï¼Œç”šè‡³åŸºç¡€é•œåƒçš„åŸºç¡€é•œåƒï¼Œæ¯”å¦‚ &lt;code>php:5-apache&lt;/code>ã€&lt;code>redis&lt;/code>ã€&lt;code>perl&lt;/code> ç­‰ç­‰ã€‚&lt;/p>
&lt;!--
After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is `imagePullPolicy: ifNotPresent`.
-->
&lt;p>ç»è¿‡æ¼«é•¿åºæ‚çš„çš„é•œåƒé‡åˆ¶å·¥ä½œï¼Œæˆ‘ä»¬å®Œæˆäº†æ€»è®¡çº¦ 40 ä¸ªé•œåƒçš„åˆ¶ä½œ ï¼ŒåŒ…æ‹¬æµ‹è¯•é•œåƒä»¥åŠç›´æ¥å’Œé—´æ¥ä¾èµ–çš„åŸºç¡€é•œåƒã€‚
æœ€ç»ˆæˆ‘ä»¬å°†æ‰€æœ‰é•œåƒåœ¨é›†ç¾¤å†…å‡†å¤‡å¦¥å½“ï¼Œå¹¶ç¡®ä¿æµ‹è¯•ç”¨ä¾‹å†…æ‰€æœ‰ Pod çš„é•œåƒæ‹‰å–ç­–ç•¥è®¾ç½®ä¸º &lt;code>imagePullPolicy: ifNotPresent&lt;/code>ã€‚&lt;/p>
&lt;!--
Here are some of the images we built
-->
&lt;p>è¿™æ˜¯æˆ‘ä»¬æ„å»ºå‡ºçš„éƒ¨åˆ†é•œåƒåˆ—è¡¨ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;code>docker.io/library/busybox:1.29&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.14-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.15-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/perl:5.26&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/httpd:2.4.38-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/redis:5.0.5-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/conformance:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/hyperkube:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/agnhost:2.6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/dnsutils:1.1&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/echoserver:2.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/kitten:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nautilus:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonroot:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/test-webserver:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/etcd:3.3.15&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/pause:3.1&lt;/code>&lt;/li>
&lt;/ul>
&lt;!--
Finally, we ran the tests and got the test result, include `e2e.log`, which showed that all test cases passed. Additionally, we submitted our test result to [k8s-conformance](https://github.com/cncf/k8s-conformance) as a [pull request](https://github.com/cncf/k8s-conformance/pull/779).
-->
&lt;p>æœ€ç»ˆæˆ‘ä»¬æ‰§è¡Œä¸€è‡´æ€§æµ‹è¯•å¹¶ä¸”å¾—åˆ°äº†æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…æ‹¬ &lt;code>e2e.log&lt;/code>ï¼Œæ˜¾ç¤ºæˆ‘ä»¬é€šè¿‡äº†å…¨éƒ¨çš„æµ‹è¯•ç”¨ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æµ‹è¯•ç»“æœä»¥ &lt;a href="https://github.com/cncf/k8s-conformance/pull/779">pull request&lt;/a> çš„å½¢å¼æäº¤ç»™äº† &lt;a href="https://github.com/cncf/k8s-conformance">k8s-conformance&lt;/a> ã€‚&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png" alt="Pull request for conformance test results">&lt;/p>
&lt;!--
_Figure 5 Pull request for conformance test results_
-->
&lt;p>&lt;em>å›¾äº” ä¸€è‡´æ€§æµ‹è¯•ç»“æœçš„ PR&lt;/em>&lt;/p>
&lt;!--
## What's next
-->
&lt;h2 id="åç»­è®¡åˆ’">åç»­è®¡åˆ’&lt;/h2>
&lt;!--
We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.
-->
&lt;p>æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºäº† K8S-MIPS ç»„ä»¶ä»¥åŠæ‰§è¡Œäº† E2E æµ‹è¯•ï¼ŒéªŒè¯äº† Kubernetes on MIPS çš„å¯è¡Œæ€§ï¼Œæå¤§çš„å¢å¼ºäº†æˆ‘ä»¬å¯¹äºæ¨è¿› Kubernetes æ”¯æŒ MIPS æ¶æ„çš„ä¿¡å¿ƒã€‚&lt;/p>
&lt;!--
In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.
-->
&lt;p>åç»­ï¼Œæˆ‘ä»¬å°†ç§¯æåœ°å‘ç¤¾åŒºè´¡çŒ®æˆ‘ä»¬çš„å·¥ä½œç»éªŒä»¥åŠæˆæœï¼Œæäº¤ PR ä»¥åŠ Patch For MIPS ç­‰ï¼Œ å¸Œæœ›èƒ½å¤Ÿæœ‰æ›´å¤šçš„æ¥è‡ªç¤¾åŒºçš„åŠ›é‡åŠ å…¥è¿›æ¥ï¼Œå…±åŒæ¨è¿› Kubernetes for MIPS çš„è¿›ç¨‹ã€‚&lt;/p>
&lt;!--
Contribution planï¼š
-->
&lt;p>åç»­å¼€æºè´¡çŒ®è®¡åˆ’ï¼š&lt;/p>
&lt;!--
- contribute the source of e2e test images for MIPS
- contribute the source of hyperkube for MIPS
- contribute the source of deploy tools like kubeadm for MIPS
-->
&lt;ul>
&lt;li>è´¡çŒ®æ„å»º E2E æµ‹è¯•é•œåƒä»£ç &lt;/li>
&lt;li>è´¡çŒ®æ„å»º MIPS ç‰ˆæœ¬ hyperkube ä»£ç &lt;/li>
&lt;li>è´¡çŒ®æ„å»º MIPS ç‰ˆæœ¬ kubeadm ç­‰é›†ç¾¤éƒ¨ç½²å·¥å…·&lt;/li>
&lt;/ul>
&lt;hr></description></item><item><title>Blog: Kubernetes 1.17ï¼šç¨³å®š</title><link>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</link><pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</guid><description>
&lt;!-- ---
layout: blog
title: "Kubernetes 1.17: Stability"
date: 2019-12-09T13:00:00-08:00
slug: kubernetes-1-17-release-announcement
evergreen: true
--- -->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">Kubernetes 1.17å‘å¸ƒå›¢é˜Ÿ&lt;/a>&lt;/p>
&lt;!--
**Authors:** [Kubernetes 1.17 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md)
-->
&lt;p>æˆ‘ä»¬é«˜å…´çš„å®£å¸ƒKubernetes 1.17ç‰ˆæœ¬çš„äº¤ä»˜ï¼Œå®ƒæ˜¯æˆ‘ä»¬2019å¹´çš„ç¬¬å››ä¸ªä¹Ÿæ˜¯æœ€åä¸€ä¸ªå‘å¸ƒç‰ˆæœ¬ã€‚Kubernetes v1.17åŒ…å«22ä¸ªå¢å¼ºåŠŸèƒ½ï¼šæœ‰14ä¸ªå¢å¼ºå·²ç»é€æ­¥ç¨³å®š(stable)ï¼Œ4ä¸ªå¢å¼ºåŠŸèƒ½å·²ç»è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ(beta)ï¼Œ4ä¸ªå¢å¼ºåŠŸèƒ½åˆšåˆšè¿›å…¥å†…éƒ¨æµ‹è¯•ç‰ˆ(alpha)ã€‚&lt;/p>
&lt;!--
Weâ€™re pleased to announce the delivery of Kubernetes 1.17, our fourth and final release of 2019! Kubernetes v1.17 consists of 22 enhancements: 14 enhancements have graduated to stable, 4 enhancements are moving to beta, and 4 enhancements are entering alpha.
-->
&lt;h2 id="ä¸»è¦çš„ä¸»é¢˜">ä¸»è¦çš„ä¸»é¢˜&lt;/h2>
&lt;!--
## Major Themes
-->
&lt;h3 id="äº‘æœåŠ¡æä¾›å•†æ ‡ç­¾åŸºæœ¬å¯ç”¨">äº‘æœåŠ¡æä¾›å•†æ ‡ç­¾åŸºæœ¬å¯ç”¨&lt;/h3>
&lt;!--
### Cloud Provider Labels reach General Availability
-->
&lt;p>ä½œä¸ºå…¬å¼€æµ‹è¯•ç‰ˆç‰¹æ€§æ·»åŠ åˆ° v1.2 ï¼Œv1.17 ä¸­å¯ä»¥çœ‹åˆ°äº‘æä¾›å•†æ ‡ç­¾è¾¾åˆ°åŸºæœ¬å¯ç”¨ã€‚&lt;/p>
&lt;!--
Added as a beta feature way back in v1.2, v1.17 sees the general availability of cloud provider labels.
-->
&lt;h3 id="å·å¿«ç…§è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ">å·å¿«ç…§è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ&lt;/h3>
&lt;!--
### Volume Snapshot Moves to Beta
-->
&lt;p>åœ¨ v1.17 ä¸­ï¼ŒKuberneteså·å¿«ç…§ç‰¹æ€§æ˜¯å…¬å¼€æµ‹è¯•ç‰ˆã€‚è¿™ä¸ªç‰¹æ€§æ˜¯åœ¨ v1.12 ä¸­ä»¥å†…éƒ¨æµ‹è¯•ç‰ˆå¼•å…¥çš„ï¼Œç¬¬äºŒä¸ªæœ‰é‡å¤§å˜åŒ–çš„å†…éƒ¨æµ‹è¯•ç‰ˆæ˜¯ v1.13 ã€‚&lt;/p>
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.
-->
&lt;h2 id="å®¹å™¨å­˜å‚¨æ¥å£è¿ç§»å…¬å¼€æµ‹è¯•ç‰ˆ">å®¹å™¨å­˜å‚¨æ¥å£è¿ç§»å…¬å¼€æµ‹è¯•ç‰ˆ&lt;/h2>
&lt;!--
### CSI Migration Beta
-->
&lt;p>åœ¨ v1.17 ä¸­ï¼ŒKubernetesæ ‘å†…å­˜å‚¨æ’ä»¶åˆ°å®¹å™¨å­˜å‚¨æ¥å£(CSI)çš„è¿ç§»åŸºç¡€æ¶æ„æ˜¯å…¬å¼€æµ‹è¯•ç‰ˆã€‚å®¹å™¨å­˜å‚¨æ¥å£è¿ç§»æœ€åˆæ˜¯åœ¨Kubernetes v1.14 ä¸­ä»¥å†…éƒ¨æµ‹è¯•ç‰ˆå¼•å…¥çš„ã€‚&lt;/p>
&lt;!--
The Kubernetes in-tree storage plugin to Container Storage Interface (CSI) migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
-->
&lt;h2 id="äº‘æœåŠ¡æä¾›å•†æ ‡ç­¾åŸºæœ¬å¯ç”¨-1">äº‘æœåŠ¡æä¾›å•†æ ‡ç­¾åŸºæœ¬å¯ç”¨&lt;/h2>
&lt;!--
## Cloud Provider Labels reach General Availability
-->
&lt;p>å½“èŠ‚ç‚¹å’Œå·è¢«åˆ›å»ºï¼Œä¼šåŸºäºåŸºç¡€äº‘æä¾›å•†çš„Kubernetesé›†ç¾¤æ‰“ä¸Šä¸€ç³»åˆ—æ ‡å‡†æ ‡ç­¾ã€‚èŠ‚ç‚¹ä¼šè·å¾—ä¸€ä¸ªå®ä¾‹ç±»å‹æ ‡ç­¾ã€‚èŠ‚ç‚¹å’Œå·éƒ½ä¼šå¾—åˆ°ä¸¤ä¸ªæè¿°èµ„æºåœ¨äº‘æä¾›å•†æ‹“æ‰‘çš„ä½ç½®æ ‡ç­¾,é€šå¸¸æ˜¯ä»¥åŒºåŸŸå’Œåœ°åŒºçš„æ–¹å¼ç»„ç»‡ã€‚&lt;/p>
&lt;!--
When nodes and volumes are created, a set of standard labels are applied based on the underlying cloud provider of the Kubernetes cluster. Nodes get a label for the instance type. Both nodes and volumes get two labels describing the location of the resource in the cloud provider topology, usually organized in zones and regions.
-->
&lt;p>Kubernetesç»„ä»¶ä½¿ç”¨æ ‡å‡†æ ‡ç­¾æ¥æ”¯æŒä¸€äº›ç‰¹æ€§ã€‚ä¾‹å¦‚ï¼Œè°ƒåº¦è€…ä¼šä¿è¯podså’Œå®ƒä»¬æ‰€å£°æ˜çš„å·æ”¾ç½®åœ¨ç›¸åŒçš„åŒºåŸŸï¼›å½“è°ƒåº¦éƒ¨ç½²çš„podsæ—¶ï¼Œè°ƒåº¦å™¨ä¼šä¼˜å…ˆå°†å®ƒä»¬åˆ†å¸ƒåœ¨ä¸åŒçš„åŒºåŸŸã€‚ä½ è¿˜å¯ä»¥åœ¨è‡ªå·±çš„podsæ ‡å‡†ä¸­åˆ©ç”¨æ ‡ç­¾æ¥é…ç½®ï¼Œå¦‚èŠ‚ç‚¹äº²å’Œæ€§ï¼Œä¹‹ç±»çš„äº‹ã€‚æ ‡å‡†æ ‡ç­¾ä½¿å¾—ä½ å†™çš„podè§„èŒƒåœ¨ä¸åŒçš„äº‘æä¾›å•†ä¹‹é—´æ˜¯å¯ç§»æ¤çš„ã€‚&lt;/p>
&lt;!--
Standard labels are used by Kubernetes components to support some features. For example, the scheduler would ensure that pods are placed on the same zone as the volumes they claim; and when scheduling pods belonging to a deployment, the scheduler would prioritize spreading them across zones. You can also use the labels in your pod specs to configure things as such node affinity. Standard labels allow you to write pod specs that are portable among different cloud providers.
-->
&lt;p>åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œæ ‡ç­¾å·²ç»è¾¾åˆ°åŸºæœ¬å¯ç”¨ã€‚Kubernetesç»„ä»¶éƒ½å·²ç»æ›´æ–°ï¼Œå¯ä»¥å¡«å……åŸºæœ¬å¯ç”¨å’Œå…¬å¼€æµ‹è¯•ç‰ˆæ ‡ç­¾ï¼Œå¹¶å¯¹ä¸¤è€…åšå‡ºååº”ã€‚ç„¶è€Œï¼Œå¦‚æœä½ çš„podè§„èŒƒæˆ–è‡ªå®šä¹‰çš„æ§åˆ¶å™¨æ­£åœ¨ä½¿ç”¨å…¬å¼€æµ‹è¯•ç‰ˆæ ‡ç­¾ï¼Œå¦‚èŠ‚ç‚¹äº²å’Œæ€§ï¼Œæˆ‘ä»¬å»ºè®®ä½ å¯ä»¥å°†å®ƒä»¬è¿ç§»åˆ°æ–°çš„åŸºæœ¬å¯ç”¨æ ‡ç­¾ä¸­ã€‚ä½ å¯ä»¥ä»å¦‚ä¸‹åœ°æ–¹æ‰¾åˆ°æ–°æ ‡ç­¾çš„æ–‡æ¡£ï¼š&lt;/p>
&lt;!--
The labels are reaching general availability in this release. Kubernetes components have been updated to populate the GA and beta labels and to react to both. However, if you are using the beta labels in your pod specs for features such as node affinity, or in your custom controllers, we recommend that you start migrating them to the new GA labels. You can find the documentation for the new labels here:
-->
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/zh/docs/reference/labels-annotations-taints/#nodekubernetesioinstance-type">å®ä¾‹ç±»å‹&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/zh/docs/reference/labels-annotations-taints/#topologykubernetesioregion">åœ°åŒº&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/zh/docs/reference/labels-annotations-taints/#topologykubernetesiozone">åŒºåŸŸ&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [node.kubernetes.io/instance-type](/docs/reference/labels-annotations-taints/#nodekubernetesioinstance-type)
- [topology.kubernetes.io/region](/docs/reference/labels-annotations-taints/#topologykubernetesioregion)
- [topology.kubernetes.io/zone](/docs/reference/labels-annotations-taints/#topologykubernetesiozone)
-->
&lt;h2 id="å·å¿«ç…§è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ-1">å·å¿«ç…§è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ&lt;/h2>
&lt;!--
## Volume Snapshot Moves to Beta
-->
&lt;p>åœ¨ v1.17 ä¸­ï¼ŒKuberneteså·å¿«ç…§æ˜¯æ˜¯å…¬å¼€æµ‹è¯•ç‰ˆã€‚æœ€åˆæ˜¯åœ¨ v1.12 ä¸­ä»¥å†…éƒ¨æµ‹è¯•ç‰ˆå¼•å…¥çš„ï¼Œç¬¬äºŒä¸ªæœ‰é‡å¤§å˜åŒ–çš„å†…éƒ¨æµ‹è¯•ç‰ˆæ˜¯ v1.13 ã€‚è¿™ç¯‡æ–‡ç« æ€»ç»“å®ƒåœ¨å…¬å¼€ç‰ˆæœ¬ä¸­çš„å˜åŒ–ã€‚&lt;/p>
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13. This post summarizes the changes in the beta release.
-->
&lt;h3 id="å·å¿«ç…§æ˜¯ä»€ä¹ˆ">å·å¿«ç…§æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h3>
&lt;!-- ### What is a Volume Snapshot? -->
&lt;p>è®¸å¤šçš„å­˜å‚¨ç³»ç»Ÿ(å¦‚è°·æ­Œäº‘æŒä¹…åŒ–ç£ç›˜ï¼Œäºšé©¬é€Šå¼¹æ€§å—å­˜å‚¨å’Œè®¸å¤šçš„å†…éƒ¨å­˜å‚¨ç³»ç»Ÿ)æ”¯æŒä¸ºæŒä¹…å·åˆ›å»ºå¿«ç…§ã€‚å¿«ç…§ä»£è¡¨å·åœ¨ä¸€ä¸ªæ—¶é—´ç‚¹çš„å¤åˆ¶ã€‚å®ƒå¯ç”¨äºé…ç½®æ–°å·(ä½¿ç”¨å¿«ç…§æ•°æ®æå‰å¡«å……)æˆ–æ¢å¤å·åˆ°ä¸€ä¸ªä¹‹å‰çš„çŠ¶æ€(ç”¨å¿«ç…§è¡¨ç¤º)ã€‚&lt;/p>
&lt;!--
Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a â€œsnapshotâ€ of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).
-->
&lt;h3 id="ä¸ºä»€ä¹ˆç»™kubernetesåŠ å…¥å·å¿«ç…§">ä¸ºä»€ä¹ˆç»™KubernetesåŠ å…¥å·å¿«ç…§ï¼Ÿ&lt;/h3>
&lt;!--
### Why add Volume Snapshots to Kubernetes?
-->
&lt;p>Kuberneteså·æ’ä»¶ç³»ç»Ÿå·²ç»æä¾›äº†åŠŸèƒ½å¼ºå¤§çš„æŠ½è±¡ç”¨äºè‡ªåŠ¨é…ç½®ã€é™„åŠ å’ŒæŒ‚è½½å—æ–‡ä»¶ç³»ç»Ÿã€‚&lt;/p>
&lt;!--
The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.
-->
&lt;p>æ”¯æŒæ‰€æœ‰è¿™äº›ç‰¹æ€§æ˜¯Kubernetsè´Ÿè½½å¯ç§»æ¤çš„ç›®æ ‡ï¼šKubernetesæ—¨åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿåº”ç”¨å’Œåº•å±‚é›†ç¾¤ä¹‹é—´åˆ›å»ºä¸€ä¸ªæŠ½è±¡å±‚,ä½¿å¾—åº”ç”¨å¯ä»¥ä¸æ„ŸçŸ¥å…¶è¿è¡Œé›†ç¾¤çš„å…·ä½“ä¿¡æ¯å¹¶ä¸”éƒ¨ç½²ä¹Ÿä¸éœ€ç‰¹å®šé›†ç¾¤çš„çŸ¥è¯†ã€‚&lt;/p>
&lt;!--
Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no â€œcluster specificâ€ knowledge.
-->
&lt;p>Kuberneteså­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„(SIG)å°†å¿«ç…§æ“ä½œç¡®å®šä¸ºå¯¹å¾ˆå¤šæœ‰çŠ¶æ€è´Ÿè½½çš„å…³é”®åŠŸèƒ½ã€‚å¦‚æ•°æ®åº“ç®¡ç†å‘˜å¸Œæœ›åœ¨æ“ä½œæ•°æ®åº“å‰ä¿å­˜æ•°æ®åº“å·å¿«ç…§ã€‚&lt;/p>
&lt;!--
The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.
-->
&lt;p>åœ¨Kubernetesæ¥å£ä¸­æä¾›ä¸€ç§æ ‡å‡†çš„æ–¹å¼è§¦å‘å¿«ç…§æ“ä½œï¼ŒKubernetesç”¨æˆ·å¯ä»¥å¤„ç†è¿™ç§ç”¨æˆ·åœºæ™¯ï¼Œè€Œä¸å¿…ä½¿ç”¨Kubernetes API(å¹¶æ‰‹åŠ¨æ‰§è¡Œå­˜å‚¨ç³»ç»Ÿçš„å…·ä½“æ“ä½œ)ã€‚&lt;/p>
&lt;!--
By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).
-->
&lt;p>å–è€Œä»£ä¹‹çš„æ˜¯ï¼ŒKubernetesç”¨æˆ·ç°åœ¨è¢«æˆæƒä»¥ä¸é›†ç¾¤æ— å…³çš„æ–¹å¼å°†å¿«ç…§æ“ä½œæ”¾è¿›ä»–ä»¬çš„å·¥å…·å’Œç­–ç•¥ä¸­ï¼Œå¹¶ä¸”ç¡®ä¿¡å®ƒå°†å¯¹ä»»æ„çš„Kubernetesé›†ç¾¤æœ‰æ•ˆï¼Œè€Œä¸åº•å±‚å­˜å‚¨æ— å…³ã€‚&lt;/p>
&lt;!--
Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.
-->
&lt;p>æ­¤å¤–ï¼ŒKubernetes å¿«ç…§åŸè¯­ä½œä¸ºåŸºç¡€æ„å»ºèƒ½åŠ›è§£é”äº†ä¸ºKuberneteså¼€å‘é«˜çº§ã€ä¼ä¸šçº§ã€å­˜å‚¨ç®¡ç†ç‰¹æ€§çš„èƒ½åŠ›:åŒ…æ‹¬åº”ç”¨æˆ–é›†ç¾¤çº§åˆ«çš„å¤‡ä»½æ–¹æ¡ˆã€‚&lt;/p>
&lt;!--
Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.
-->
&lt;p>ä½ å¯ä»¥é˜…è¯»æ›´å¤šå…³äº&lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/">å‘å¸ƒå®¹å™¨å­˜å‚¨æ¥å£å·å¿«ç…§å…¬å¼€æµ‹è¯•ç‰ˆ&lt;/a>&lt;/p>
&lt;!--
You can read more in the blog entry about [releasing CSI volume snapshots to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/).
-->
&lt;h2 id="å®¹å™¨å­˜å‚¨æ¥å£è¿ç§»å…¬æµ‹ç‰ˆ">å®¹å™¨å­˜å‚¨æ¥å£è¿ç§»å…¬æµ‹ç‰ˆ&lt;/h2>
&lt;!--
## CSI Migration Beta
-->
&lt;h3 id="ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿ç§»å†…å»ºæ ‘æ’ä»¶åˆ°å®¹å™¨å­˜å‚¨æ¥å£">ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿ç§»å†…å»ºæ ‘æ’ä»¶åˆ°å®¹å™¨å­˜å‚¨æ¥å£ï¼Ÿ&lt;/h3>
&lt;!--
### Why are we migrating in-tree plugins to CSI?
-->
&lt;p>åœ¨å®¹å™¨å­˜å‚¨æ¥å£ä¹‹å‰ï¼ŒKubernetesæä¾›åŠŸèƒ½å¼ºå¤§çš„å·æ’ä»¶ç³»ç»Ÿã€‚è¿™äº›å·æ’ä»¶æ˜¯æ ‘å†…çš„æ„å‘³ç€å®ƒä»¬çš„ä»£ç æ˜¯æ ¸å¿ƒKubernetesä»£ç çš„ä¸€éƒ¨åˆ†å¹¶é™„å¸¦åœ¨æ ¸å¿ƒKubernetesäºŒè¿›åˆ¶ä¸­ã€‚ç„¶è€Œï¼Œä¸ºKubernetesæ·»åŠ æ’ä»¶æ”¯æŒæ–°å·æ˜¯éå¸¸æœ‰æŒ‘æˆ˜çš„ã€‚å¸Œæœ›åœ¨Kubernetesä¸Šä¸ºè‡ªå·±å­˜å‚¨ç³»ç»Ÿæ·»åŠ æ”¯æŒ(æˆ–ä¿®å¤ç°æœ‰å·æ’ä»¶çš„bug)çš„ä¾›åº”å•†è¢«è¿«ä¸Kuberneteså‘è¡Œè¿›ç¨‹å¯¹é½ã€‚æ­¤å¤–ï¼Œç¬¬ä¸‰æ–¹å­˜å‚¨ä»£ç åœ¨æ ¸å¿ƒKubernetesäºŒè¿›åˆ¶ä¸­ä¼šé€ æˆå¯é æ€§å’Œå®‰å…¨é—®é¢˜ï¼Œå¹¶ä¸”è¿™äº›ä»£ç å¯¹äºKubernetesçš„ç»´æŠ¤è€…æ¥è¯´æ˜¯éš¾ä»¥(ä¸€äº›åœºæ™¯æ˜¯ä¸å¯èƒ½)æµ‹è¯•å’Œç»´æŠ¤çš„ã€‚åœ¨Kubernetesä¸Šé‡‡ç”¨å®¹å™¨å­˜å‚¨æ¥å£å¯ä»¥è§£å†³å¤§éƒ¨åˆ†é—®é¢˜ã€‚&lt;/p>
&lt;!--
Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were â€œin-treeâ€ meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.
-->
&lt;p>éšç€æ›´å¤šå®¹å™¨å­˜å‚¨æ¥å£é©±åŠ¨å˜æˆç”Ÿäº§ç¯å¢ƒå¯ç”¨ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„Kubernetesç”¨æˆ·ä»å®¹å™¨å­˜å‚¨æ¥å£æ¨¡å‹ä¸­è·ç›Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸å¸Œæœ›å¼ºåˆ¶ç”¨æˆ·ä»¥ç ´åç°æœ‰åŸºæœ¬å¯ç”¨çš„å­˜å‚¨æ¥å£çš„æ–¹å¼å»æ”¹å˜è´Ÿè½½å’Œé…ç½®ã€‚é“è·¯å¾ˆæ˜ç¡®ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ç”¨CSIæ›¿æ¢æ ‘å†…æ’ä»¶æ¥å£ã€‚ä»€ä¹ˆæ˜¯å®¹å™¨å­˜å‚¨æ¥å£è¿ç§»ï¼Ÿ&lt;/p>
&lt;!--
As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the â€œin-tree pluginâ€ APIs with CSI.What is CSI migration?
-->
&lt;p>åœ¨å®¹å™¨å­˜å‚¨æ¥å£è¿ç§»ä¸Šæ‰€åšçš„åŠªåŠ›ä½¿å¾—æ›¿æ¢ç°æœ‰çš„æ ‘å†…å­˜å‚¨æ’ä»¶ï¼Œå¦‚&lt;code>kubernetes.io/gce-pd&lt;/code>æˆ–&lt;code>kubernetes.io/aws-ebs&lt;/code>ï¼Œä¸ºç›¸åº”çš„å®¹å™¨å­˜å‚¨æ¥å£é©±åŠ¨æˆä¸ºå¯èƒ½ã€‚å¦‚æœå®¹å™¨å­˜å‚¨æ¥å£è¿ç§»æ­£å¸¸å·¥ä½œï¼ŒKubernetesç»ˆç«¯ç”¨æˆ·ä¸ä¼šæ³¨æ„åˆ°ä»»ä½•å·®åˆ«ã€‚è¿ç§»è¿‡åï¼ŒKubernetesç”¨æˆ·å¯ä»¥ç»§ç»­ä½¿ç”¨ç°æœ‰æ¥å£æ¥ä¾èµ–æ ‘å†…å­˜å‚¨æ’ä»¶çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
The CSI migration effort enables the replacement of existing in-tree storage plugins such as `kubernetes.io/gce-pd` or `kubernetes.io/aws-ebs` with a corresponding CSI driver. If CSI Migration is working properly, Kubernetes end users shouldnâ€™t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.
-->
&lt;p>å½“Kubernetesé›†ç¾¤ç®¡ç†è€…æ›´æ–°é›†ç¾¤ä½¿å¾—CSIè¿ç§»å¯ç”¨ï¼Œç°æœ‰çš„æœ‰çŠ¶æ€éƒ¨ç½²å’Œå·¥ä½œè´Ÿè½½ç…§å¸¸å·¥ä½œï¼›ç„¶è€Œï¼Œåœ¨å¹•åKuberneteså°†å­˜å‚¨ç®¡ç†æ“ä½œäº¤ç»™äº†(ä»¥å‰æ˜¯äº¤ç»™æ ‘å†…é©±åŠ¨)CSIé©±åŠ¨ã€‚&lt;/p>
&lt;!--
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.
-->
&lt;p>Kubernetesç»„éå¸¸åŠªåŠ›åœ°ä¿è¯å­˜å‚¨æ¥å£çš„ç¨³å®šæ€§å’Œå¹³æ»‘å‡çº§ä½“éªŒçš„æ‰¿è¯ºã€‚è¿™éœ€è¦ç»†è‡´çš„è€ƒè™‘ç°æœ‰ç‰¹æ€§å’Œè¡Œä¸ºæ¥ç¡®ä¿åå‘å…¼å®¹å’Œæ¥å£ç¨³å®šæ€§ã€‚ä½ å¯ä»¥æƒ³åƒæˆåœ¨åŠ é€Ÿè¡Œé©¶çš„ç›´çº¿ä¸Šç»™èµ›è½¦æ¢è½®èƒã€‚&lt;/p>
&lt;!--
The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while itâ€™s speeding down the straightaway.
-->
&lt;p>ä½ å¯ä»¥åœ¨è¿™ç¯‡åšå®¢ä¸­é˜…è¯»æ›´å¤šå…³äº&lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">å®¹å™¨å­˜å‚¨æ¥å£è¿ç§»æˆä¸ºå…¬å¼€æµ‹è¯•ç‰ˆ&lt;/a>.&lt;/p>
&lt;!--
You can read more in the blog entry about [CSI migration going to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/). -->
&lt;h2 id="å…¶å®ƒæ›´æ–°">å…¶å®ƒæ›´æ–°&lt;/h2>
&lt;!--
## Other Updates
-->
&lt;h3 id="ç¨³å®š">ç¨³å®šğŸ’¯&lt;/h3>
&lt;!--
### Graduated to Stable ğŸ’¯
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/382">æŒ‰æ¡ä»¶æ±¡æŸ“èŠ‚ç‚¹&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/495">å¯é…ç½®çš„Podè¿›ç¨‹å…±äº«å‘½åç©ºé—´&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/548">é‡‡ç”¨kube-schedulerè°ƒåº¦DaemonSet Pods&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/554">åŠ¨æ€å·æœ€å¤§å€¼&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/557">Kuberneteså®¹å™¨å­˜å‚¨æ¥å£æ”¯æŒæ‹“æ‰‘&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/559">åœ¨SubPathæŒ‚è½½æä¾›ç¯å¢ƒå˜é‡æ‰©å±•&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/575">ä¸ºCustom Resourcesæä¾›é»˜è®¤å€¼&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/589">ä»é¢‘ç¹çš„Kubletå¿ƒè·³åˆ°ç§Ÿçº¦æ¥å£&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/714">æ‹†åˆ†Kubernetesæµ‹è¯•Tarball&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/956">æ·»åŠ Watchä¹¦ç­¾æ”¯æŒ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/960">è¡Œä¸ºé©±åŠ¨ä¸€è‡´æ€§æµ‹è¯•&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/980">æœåŠ¡è´Ÿè½½å‡è¡¡ç»ˆç»“ä¿æŠ¤&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1152">é¿å…æ¯ä¸€ä¸ªWatcherç‹¬ç«‹åºåˆ—åŒ–ç›¸åŒçš„å¯¹è±¡&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Taint Node by Condition](https://github.com/kubernetes/enhancements/issues/382)
- [Configurable Pod Process Namespace Sharing](https://github.com/kubernetes/enhancements/issues/495)
- [Schedule DaemonSet Pods by kube-scheduler](https://github.com/kubernetes/enhancements/issues/548)
- [Dynamic Maximum Volume Count](https://github.com/kubernetes/enhancements/issues/554)
- [Kubernetes CSI Topology Support](https://github.com/kubernetes/enhancements/issues/557)
- [Provide Environment Variables Expansion in SubPath Mount](https://github.com/kubernetes/enhancements/issues/559)
- [Defaulting of Custom Resources](https://github.com/kubernetes/enhancements/issues/575)
- [Move Frequent Kubelet Heartbeats To Lease Api](https://github.com/kubernetes/enhancements/issues/589)
- [Break Apart The Kubernetes Test Tarball](https://github.com/kubernetes/enhancements/issues/714)
- [Add Watch Bookmarks Support](https://github.com/kubernetes/enhancements/issues/956)
- [Behavior-Driven Conformance Testing](https://github.com/kubernetes/enhancements/issues/960)
- [Finalizer Protection For Service Loadbalancers](https://github.com/kubernetes/enhancements/issues/980)
- [Avoid Serializing The Same Object Independently For Every Watcher](https://github.com/kubernetes/enhancements/issues/1152)
-->
&lt;h3 id="ä¸»è¦å˜åŒ–">ä¸»è¦å˜åŒ–&lt;/h3>
&lt;!--
### Major Changes
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/563">æ·»åŠ IPv4/IPv6åŒæ ˆæ”¯æŒ&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Add IPv4/IPv6 Dual Stack Support](https://github.com/kubernetes/enhancements/issues/563)
-->
&lt;h3 id="å…¶å®ƒæ˜¾è‘—ç‰¹æ€§">å…¶å®ƒæ˜¾è‘—ç‰¹æ€§&lt;/h3>
&lt;!--
### Other Notable Features
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/536">æ‹“æ‰‘æ„ŸçŸ¥è·¯ç”±æœåŠ¡(å†…éƒ¨æµ‹è¯•ç‰ˆ)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">ä¸ºWindowsæ·»åŠ RunAsUserName&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Topology Aware Routing of Services (Alpha)](https://github.com/kubernetes/enhancements/issues/536)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
-->
&lt;h3 id="å¯ç”¨æ€§">å¯ç”¨æ€§&lt;/h3>
&lt;!--
### Availability
-->
&lt;p>Kubernetes 1.17 å¯ä»¥&lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0">åœ¨GitHubä¸‹è½½&lt;/a>ã€‚å¼€å§‹ä½¿ç”¨Kubernetesï¼Œçœ‹çœ‹è¿™äº›&lt;a href="https://kubernetes.io/docs/tutorials/">äº¤äº’æ•™å­¦&lt;/a>ã€‚ä½ å¯ä»¥éå¸¸å®¹æ˜“ä½¿ç”¨&lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>å®‰è£…1.17ã€‚&lt;/p>
&lt;!--
Kubernetes 1.17 is available for [download on GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/). You can also easily install 1.17 using
[kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;h3 id="å‘å¸ƒå›¢é˜Ÿ">å‘å¸ƒå›¢é˜Ÿ&lt;/h3>
&lt;!--
### Release Team
-->
&lt;p>æ­£æ˜¯å› ä¸ºæœ‰ä¸Šåƒäººå‚ä¸æŠ€æœ¯æˆ–éæŠ€æœ¯å†…å®¹çš„è´¡çŒ®æ‰ä½¿è¿™ä¸ªç‰ˆæœ¬æˆä¸ºå¯èƒ½ã€‚ç‰¹åˆ«æ„Ÿè°¢ç”±Guinevere Saengeré¢†å¯¼çš„&lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">å‘å¸ƒå›¢é˜Ÿ&lt;/a>ã€‚å‘å¸ƒå›¢é˜Ÿçš„35åæˆå‘˜åœ¨å‘å¸ƒç‰ˆæœ¬çš„å¤šæ–¹é¢è¿›è¡Œäº†åè°ƒï¼Œä»æ–‡æ¡£åˆ°æµ‹è¯•ï¼Œæ ¡éªŒå’Œç‰¹æ€§çš„å®Œå–„ã€‚&lt;/p>
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md) led by Guinevere Saenger. The 35 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
-->
&lt;p>éšç€Kubernetesç¤¾åŒºçš„æˆé•¿ï¼Œæˆ‘ä»¬çš„å‘å¸ƒæµç¨‹æ˜¯åœ¨å¼€æºè½¯ä»¶åä½œæ–¹é¢æƒŠäººçš„ç¤ºä¾‹ã€‚Kuberneteså¿«é€Ÿå¹¶æŒç»­è·å¾—æ–°ç”¨æˆ·ã€‚è¿™ä¸€æˆé•¿äº§ç”Ÿäº†è‰¯æ€§çš„åé¦ˆå¾ªç¯ï¼Œæ›´å¤šçš„è´¡çŒ®è€…è´¡çŒ®ä»£ç åˆ›é€ äº†æ›´åŠ æ´»è·ƒçš„ç”Ÿæ€ã€‚Kuberneteså·²ç»æœ‰è¶…è¿‡&lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">39000ä½è´¡çŒ®è€…&lt;/a>å’Œä¸€ä¸ªè¶…è¿‡66000äººçš„æ´»è·ƒç¤¾åŒºã€‚&lt;/p>
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [39,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 66,000 people.
-->
&lt;h3 id="ç½‘ç»œç ”è®¨ä¼š">ç½‘ç»œç ”è®¨ä¼š&lt;/h3>
&lt;!--
### Webinar
-->
&lt;p>2020å¹´1æœˆ7å·ï¼ŒåŠ å…¥Kubernetes 1.17å‘å¸ƒå›¢é˜Ÿï¼Œå­¦ä¹ å…³äºè¿™æ¬¡å‘å¸ƒçš„ä¸»è¦ç‰¹æ€§ã€‚&lt;a href="https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A">è¿™é‡Œ&lt;/a>æ³¨å†Œã€‚&lt;/p>
&lt;!--
Join members of the Kubernetes 1.17 release team on Jan 7th, 2020 to learn about the major features in this release. Register [here](https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A).
-->
&lt;h3 id="å‚ä¸å…¶ä¸­">å‚ä¸å…¶ä¸­&lt;/h3>
&lt;!--
### Get Involved
-->
&lt;p>æœ€ç®€å•çš„å‚ä¸Kubernetesçš„æ–¹å¼æ˜¯åŠ å…¥å…¶ä¸­ä¸€ä¸ªä¸ä½ å…´è¶£ç›¸åŒçš„&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">ç‰¹åˆ«å…´è¶£ç»„&lt;/a>ï¼ˆSIGs)ã€‚æœ‰ä»€ä¹ˆæƒ³è¦å¹¿æ’­åˆ°Kubernetesç¤¾åŒºå—ï¼Ÿé€šè¿‡å¦‚ä¸‹çš„é¢‘é“ï¼Œåœ¨æ¯å‘¨çš„&lt;a href="https://github.com/kubernetes/community/tree/master/communication">ç¤¾åŒºä¼šè®®&lt;/a>åˆ†äº«ä½ çš„å£°éŸ³ã€‚æ„Ÿè°¢ä½ çš„è´¡çŒ®å’Œæ”¯æŒã€‚&lt;/p>
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something youâ€™d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
-->
&lt;ul>
&lt;li>åœ¨Twitterä¸Šå…³æ³¨æˆ‘ä»¬&lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>è·å–æœ€æ–°çš„æ›´æ–°&lt;/li>
&lt;li>åœ¨&lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>å‚ä¸ç¤¾åŒºçš„è®¨è®º&lt;/li>
&lt;li>åœ¨&lt;a href="http://slack.k8s.io/">Slack&lt;/a>åŠ å…¥ç¤¾åŒº&lt;/li>
&lt;li>åœ¨&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>å‘å¸ƒé—®é¢˜(æˆ–å›ç­”é—®é¢˜)&lt;/li>
&lt;li>åˆ†äº«ä½ çš„Kubernetes&lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">æ•…äº‹&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
--></description></item><item><title>Blog: ä½¿ç”¨ Java å¼€å‘ä¸€ä¸ª Kubernetes controller</title><link>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</guid><description>
&lt;!--
---
layout: blog
title: "Develop a Kubernetes controller in Java"
date: 2019-11-26
slug: Develop-A-Kubernetes-Controller-in-Java
---
-->
&lt;!--
**Authors:** Min Kim (Ant Financial), Tony Ado (Ant Financial)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> Min Kim (èš‚èšé‡‘æœ), Tony Ado (èš‚èšé‡‘æœ)&lt;/p>
&lt;!--
The official [Kubernetes Java SDK](https://github.com/kubernetes-client/java) project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.
-->
&lt;p>&lt;a href="https://github.com/kubernetes-client/java">Kubernetes Java SDK&lt;/a> å®˜æ–¹é¡¹ç›®æœ€è¿‘å‘å¸ƒäº†ä»–ä»¬çš„æœ€æ–°å·¥ä½œï¼Œä¸º Java Kubernetes å¼€å‘äººå‘˜æä¾›ä¸€ä¸ªä¾¿æ·çš„ Kubernetes æ§åˆ¶å™¨-æ„å»ºå™¨ SDKï¼Œå®ƒæœ‰åŠ©äºè½»æ¾å¼€å‘é«˜çº§å·¥ä½œè´Ÿè½½æˆ–ç³»ç»Ÿã€‚&lt;/p>
&lt;!--
## Overall
Java is no doubt one of the most popular programming languages in the world but
it's been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there're already some excellent controller
frameworks, for example, [controller runtime](https://github.com/kubernetes-sigs/controller-runtime),
[operator SDK](https://github.com/operator-framework/operator-sdk). These
existing Golang frameworks are relying on the various utilities from the
[Kubernetes Golang SDK](https://github.com/kubernetes/client-go) proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.
-->
&lt;h2 id="ç»¼è¿°">ç»¼è¿°&lt;/h2>
&lt;p>Java æ— ç–‘æ˜¯ä¸–ç•Œä¸Šæœ€æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ä¹‹ä¸€ï¼Œä½†ç”±äºç¤¾åŒºä¸­ç¼ºå°‘åº“èµ„æºï¼Œä¸€æ®µæ—¶é—´ä»¥æ¥ï¼Œé‚£äº›é Golang å¼€å‘äººå‘˜å¾ˆéš¾æ„å»ºä»–ä»¬å®šåˆ¶çš„ controller/operatorã€‚åœ¨ Golang çš„ä¸–ç•Œé‡Œï¼Œå·²ç»æœ‰ä¸€äº›å¾ˆå¥½çš„ controller æ¡†æ¶äº†ï¼Œä¾‹å¦‚ï¼Œ&lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller runtime&lt;/a>ï¼Œ&lt;a href="https://github.com/operator-framework/operator-sdk">operator SDK&lt;/a>ã€‚è¿™äº›ç°æœ‰çš„ Golang æ¡†æ¶ä¾èµ–äº &lt;a href="https://github.com/kubernetes/client-go">Kubernetes Golang SDK&lt;/a> æä¾›çš„å„ç§å®ç”¨å·¥å…·ï¼Œè¿™äº›å·¥å…·ç»è¿‡å¤šå¹´è¯æ˜æ˜¯ç¨³å®šçš„ã€‚å—è¿›ä¸€æ­¥é›†æˆåˆ° Kubernetes å¹³å°çš„éœ€æ±‚é©±åŠ¨ï¼Œæˆ‘ä»¬ä¸ä»…å°† Golang SDK ä¸­çš„è®¸å¤šåŸºæœ¬å·¥å…·ç§»æ¤åˆ° kubernetes Java SDK ä¸­ï¼ŒåŒ…æ‹¬ informersã€work-queuesã€leader-elections ç­‰ï¼Œä¹Ÿå¼€å‘äº†ä¸€ä¸ªæ§åˆ¶å™¨æ„å»º SDKï¼Œå®ƒå¯ä»¥å°†æ‰€æœ‰ä¸œè¥¿è¿æ¥åˆ°ä¸€ä¸ªå¯è¿è¡Œçš„æ§åˆ¶å™¨ä¸­ï¼Œè€Œä¸ä¼šäº§ç”Ÿä»»ä½•é—®é¢˜ã€‚&lt;/p>
&lt;!--
## Backgrounds
Why use Java to implement Kubernetes tooling? You might pick Java for:
- __Integrating legacy enterprise Java systems__: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.
- __More open-source community resources__: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.
-->
&lt;h2 id="èƒŒæ™¯">èƒŒæ™¯&lt;/h2>
&lt;p>ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ Java å®ç° kubernetes å·¥å…·ï¼Ÿé€‰æ‹© Java çš„åŸå› å¯èƒ½æ˜¯ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>é›†æˆé—ç•™çš„ä¼ä¸šçº§ Java ç³»ç»Ÿ&lt;/strong>ï¼šè®¸å¤šå…¬å¸çš„é—ç•™ç³»ç»Ÿæˆ–æ¡†æ¶éƒ½æ˜¯ç”¨ Java ç¼–å†™çš„ï¼Œç”¨ä»¥æ”¯æŒç¨³å®šæ€§ã€‚æˆ‘ä»¬ä¸èƒ½è½»æ˜“æŠŠæ‰€æœ‰ä¸œè¥¿æ¬åˆ° Golangã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ›´å¤šå¼€æºç¤¾åŒºçš„èµ„æº&lt;/strong>ï¼šJava æ˜¯æˆç†Ÿçš„ï¼Œå¹¶ä¸”åœ¨è¿‡å»å‡ åå¹´ä¸­ç´¯è®¡äº†ä¸°å¯Œçš„å¼€æºåº“ï¼Œå°½ç®¡ Golang å¯¹äºå¼€å‘äººå‘˜æ¥è¯´è¶Šæ¥è¶Šå…·æœ‰å¸å¼•åŠ›ï¼Œè¶Šæ¥è¶Šæµè¡Œã€‚æ­¤å¤–ï¼Œç°åœ¨å¼€å‘äººå‘˜èƒ½å¤Ÿåœ¨ SQL å­˜å‚¨ä¸Šå¼€å‘ä»–ä»¬çš„èšåˆ-apiserverï¼Œè€Œ Java åœ¨ SQL ä¸Šæœ‰æ›´å¥½çš„æ”¯æŒã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
## How to use?
Take maven project as example, adding the following dependencies into your dependencies:
-->
&lt;h2 id="å¦‚ä½•å»ä½¿ç”¨">å¦‚ä½•å»ä½¿ç”¨&lt;/h2>
&lt;p>ä»¥ maven é¡¹ç›®ä¸ºä¾‹ï¼Œå°†ä»¥ä¸‹ä¾èµ–é¡¹æ·»åŠ åˆ°æ‚¨çš„ä¾èµ–ä¸­ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-xml" data-lang="xml">&lt;span style="color:#008000;font-weight:bold">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;groupId&amp;gt;&lt;/span>io.kubernetes&lt;span style="color:#008000;font-weight:bold">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;artifactId&amp;gt;&lt;/span>client-java-extended&lt;span style="color:#008000;font-weight:bold">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;version&amp;gt;&lt;/span>6.0.1&lt;span style="color:#008000;font-weight:bold">&amp;lt;/version&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example [here](https://github.com/kubernetes-client/java/blob/master/examples/examples-release-13/src/main/java/io/kubernetes/client/examples/ControllerExample.java):
-->
&lt;p>ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æä¾›çš„ç”Ÿæˆå™¨åº“æ¥ç¼–å†™è‡ªå·±çš„æ§åˆ¶å™¨ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„æ§åˆ¶ï¼Œå®ƒæ‰“å°å‡ºå…³äºç›‘è§†é€šçŸ¥çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œ
åœ¨&lt;a href="https://github.com/kubernetes-client/java/blob/master/examples/examples-release-13/src/main/java/io/kubernetes/client/examples/ControllerExample.java">æ­¤å¤„&lt;/a>
æŸ¥çœ‹å®Œæ•´çš„ä¾‹å­ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-java" data-lang="java">&lt;span style="color:#666">...&lt;/span>
Reconciler reconciler &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Reconciler&lt;span style="color:#666">()&lt;/span> &lt;span style="color:#666">{&lt;/span>
&lt;span style="color:#a2f">@Override&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">public&lt;/span> Result &lt;span style="color:#00a000">reconcile&lt;/span>&lt;span style="color:#666">(&lt;/span>Request request&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">{&lt;/span>
V1Node node &lt;span style="color:#666">=&lt;/span> nodeLister&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">get&lt;/span>&lt;span style="color:#666">(&lt;/span>request&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
System&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">out&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">println&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;triggered reconciling &amp;#34;&lt;/span> &lt;span style="color:#666">+&lt;/span> node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getMetadata&lt;/span>&lt;span style="color:#666">().&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Result&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#666">);&lt;/span>
&lt;span style="color:#666">}&lt;/span>
&lt;span style="color:#666">};&lt;/span>
Controller controller &lt;span style="color:#666">=&lt;/span>
ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">defaultBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>informerFactory&lt;span style="color:#666">)&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">watch&lt;/span>&lt;span style="color:#666">(&lt;/span>
&lt;span style="color:#666">(&lt;/span>workQueue&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">-&amp;gt;&lt;/span> ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">controllerWatchBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>V1Node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">class&lt;/span>&lt;span style="color:#666">,&lt;/span> workQueue&lt;span style="color:#666">).&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">())&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReconciler&lt;/span>&lt;span style="color:#666">(&lt;/span>nodeReconciler&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// required, set the actual reconciler
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withName&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;node-printing-controller&amp;#34;&lt;/span>&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set name for controller for logging, thread-tracing
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withWorkerCount&lt;/span>&lt;span style="color:#666">(&lt;/span>4&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set worker thread count
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReadyFunc&lt;/span>&lt;span style="color:#666">(&lt;/span> nodeInformer&lt;span style="color:#666">::&lt;/span>hasSynced&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, only starts controller when the cache has synced up
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">();&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
If you notice, the new Java controller framework learnt a lot from the design of
[controller-runtime](https://github.com/kubernetes-sigs/controller-runtime) which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.
-->
&lt;p>å¦‚æœæ‚¨ç•™æ„ï¼Œæ–°çš„ Java æ§åˆ¶å™¨æ¡†æ¶å¾ˆå¤šåœ°æ–¹å€Ÿé‰´äº &lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime&lt;/a> çš„è®¾è®¡ï¼Œå®ƒæˆåŠŸåœ°å°†æ§åˆ¶å™¨å†…éƒ¨çš„å¤æ‚ç»„ä»¶å°è£…åˆ°å‡ ä¸ªå¹²å‡€çš„æ¥å£ä¸­ã€‚åœ¨ Java æ³›å‹çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬ç”šè‡³æ›´è¿›ä¸€æ­¥ï¼Œä»¥æ›´å¥½çš„æ–¹å¼ç®€åŒ–äº†å°è£…ã€‚&lt;/p>
&lt;!--
As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.
-->
&lt;p>æˆ‘ä»¬å¯ä»¥å°†å¤šä¸ªæ§åˆ¶å™¨å°è£…åˆ°ä¸€ä¸ª controller-manager æˆ– leader-electing controller ä¸­ï¼Œè¿™æœ‰åŠ©äºåœ¨ HA è®¾ç½®ä¸­è¿›è¡Œéƒ¨ç½²ã€‚&lt;/p>
&lt;!--
## Future steps
The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo [kubernetes-client/java](https://github.com/kubernetes-client/java).
Feel free to share also your feedback with us, through Issues or [Slack](http://kubernetes.slack.com/messages/kubernetes-client/).
-->
&lt;h2 id="æœªæ¥è®¡åˆ’">æœªæ¥è®¡åˆ’&lt;/h2>
&lt;p>Kubernetes Java SDK é¡¹ç›®èƒŒåçš„ç¤¾åŒºå°†ä¸“æ³¨äºä¸ºå¸Œæœ›ç¼–å†™äº‘åŸç”Ÿ Java åº”ç”¨ç¨‹åºæ¥æ‰©å±• Kubernetes çš„å¼€å‘äººå‘˜æä¾›æ›´æœ‰ç”¨çš„å®ç”¨ç¨‹åºã€‚å¦‚æœæ‚¨å¯¹æ›´è¯¦ç»†çš„ä¿¡æ¯æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ä»“åº“ &lt;a href="https://github.com/kubernetes-client/java">kubernetes-client/java&lt;/a>ã€‚è¯·é€šè¿‡é—®é¢˜æˆ– &lt;a href="http://kubernetes.slack.com/messages/kubernetes-client/">Slack&lt;/a> ä¸æˆ‘ä»¬åˆ†äº«æ‚¨çš„åé¦ˆã€‚&lt;/p></description></item><item><title>Blog: ä½¿ç”¨ Microk8s åœ¨ Linux ä¸Šæœ¬åœ°è¿è¡Œ Kubernetes</title><link>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</guid><description>
&lt;!--
---
title: 'Running Kubernetes locally on Linux with Microk8s'
date: 2019-11-26
---
-->
&lt;!--
**Authors**: [Ihor Dvoretskyi](https://twitter.com/idvoretskyi), Developer Advocate, Cloud Native Computing Foundation; [Carmine Rimi](https://twitter.com/carminerimi)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: &lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>ï¼Œå¼€å‘æ”¯æŒè€…ï¼Œäº‘åŸç”Ÿè®¡ç®—åŸºé‡‘ä¼šï¼›&lt;a href="https://twitter.com/carminerimi">Carmine Rimi&lt;/a>&lt;/p>
&lt;!--
This article, the second in a [series](/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/) about local deployment options on Linux, and covers [MicroK8s](https://microk8s.io/). Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.
-->
&lt;p>æœ¬æ–‡æ˜¯å…³äº Linux ä¸Šçš„æœ¬åœ°éƒ¨ç½²é€‰é¡¹&lt;a href="https://twitter.com/idvoretskyi">ç³»åˆ—&lt;/a>çš„ç¬¬äºŒç¯‡ï¼Œæ¶µç›–äº† &lt;a href="https://microk8s.io/">MicroK8s&lt;/a>ã€‚Microk8s æ˜¯æœ¬åœ°éƒ¨ç½² Kubernetes é›†ç¾¤çš„ 'click-and-run' æ–¹æ¡ˆï¼Œæœ€åˆç”± Ubuntu çš„å‘å¸ƒè€… Canonical å¼€å‘ã€‚&lt;/p>
&lt;!--
While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesnâ€™t require a VM. It uses [snap](https://snapcraft.io/) packages, an application packaging and isolation technology.
-->
&lt;p>è™½ç„¶ Minikube é€šå¸¸ä¸º Kubernetes é›†ç¾¤åˆ›å»ºä¸€ä¸ªæœ¬åœ°è™šæ‹Ÿæœºï¼ˆVMï¼‰ï¼Œä½†æ˜¯ MicroK8s ä¸éœ€è¦ VMã€‚å®ƒä½¿ç”¨&lt;a href="https://snapcraft.io/">snap&lt;/a> åŒ…ï¼Œè¿™æ˜¯ä¸€ç§åº”ç”¨ç¨‹åºæ‰“åŒ…å’Œéš”ç¦»æŠ€æœ¯ã€‚&lt;/p>
&lt;!--
This difference has its pros and cons. Here weâ€™ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions [that support snaps](https://snapcraft.io/docs/installing-snapd). Most popular Linux distributions are supported.
-->
&lt;p>è¿™ç§å·®å¼‚æœ‰å…¶ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›æœ‰è¶£çš„åŒºåˆ«ï¼Œå¹¶ä¸”åŸºäº VM çš„æ–¹æ³•å’Œé VM æ–¹æ³•çš„å¥½å¤„ã€‚ç¬¬ä¸€ä¸ªå› ç´ æ˜¯è·¨å¹³å°çš„ç§»æ¤æ€§ã€‚è™½ç„¶ Minikube VM å¯ä»¥è·¨æ“ä½œç³»ç»Ÿç§»æ¤â€”â€”å®ƒä¸ä»…æ”¯æŒ Linuxï¼Œè¿˜æ”¯æŒ Windowsã€macOSã€ç”šè‡³ FreeBSDï¼Œä½† Microk8s éœ€è¦ Linuxï¼Œè€Œä¸”åªåœ¨&lt;a href="https://snapcraft.io/docs/installing-snapd">é‚£äº›æ”¯æŒ snaps&lt;/a> çš„å‘è¡Œç‰ˆä¸Šã€‚æ”¯æŒå¤§å¤šæ•°æµè¡Œçš„ Linux å‘è¡Œç‰ˆã€‚&lt;/p>
&lt;!--
Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean youâ€™ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. Youâ€™ll consume more disk space when the VM is dormant. Youâ€™ll consume more RAM and CPU while it is running. Since Microk8s doesnâ€™t require spinning up a virtual machine youâ€™ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!
-->
&lt;p>å¦ä¸€ä¸ªè€ƒè™‘åˆ°çš„å› ç´ æ˜¯èµ„æºæ¶ˆè€—ã€‚è™½ç„¶ VM è®¾å¤‡ä¸ºæ‚¨æä¾›äº†æ›´å¥½çš„å¯ç§»æ¤æ€§ï¼Œä½†å®ƒç¡®å®æ„å‘³ç€æ‚¨å°†æ¶ˆè€—æ›´å¤šèµ„æºæ¥è¿è¡Œ VMï¼Œè¿™ä¸»è¦æ˜¯å› ä¸º VM æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æ“ä½œç³»ç»Ÿï¼Œå¹¶ä¸”è¿è¡Œåœ¨ç®¡ç†ç¨‹åºä¹‹ä¸Šã€‚å½“ VM å¤„äºä¼‘çœ æ—¶ä½ å°†æ¶ˆè€—æ›´å¤šçš„ç£ç›˜ç©ºé—´ã€‚å½“å®ƒè¿è¡Œæ—¶ï¼Œä½ å°†ä¼šæ¶ˆè€—æ›´å¤šçš„ RAM å’Œ CPUã€‚å› ä¸º Microk8s ä¸éœ€è¦åˆ›å»ºè™šæ‹Ÿæœºï¼Œä½ å°†ä¼šæœ‰æ›´å¤šçš„èµ„æºå»è¿è¡Œä½ çš„å·¥ä½œè´Ÿè½½å’Œå…¶ä»–è®¾å¤‡ã€‚è€ƒè™‘åˆ°æ‰€å ç”¨çš„ç©ºé—´æ›´å°ï¼ŒMicroK8s æ˜¯ç‰©è”ç½‘è®¾å¤‡çš„ç†æƒ³é€‰æ‹©-ä½ ç”šè‡³å¯ä»¥åœ¨ Paspberry Pi å’Œè®¾å¤‡ä¸Šä½¿ç”¨å®ƒï¼&lt;/p>
&lt;!--
Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide [channels](https://snapcraft.io/docs/channels) that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.
-->
&lt;p>æœ€åï¼Œé¡¹ç›®ä¼¼ä¹éµå¾ªäº†ä¸åŒçš„å‘å¸ƒèŠ‚å¥å’Œç­–ç•¥ã€‚Microk8s å’Œ snaps é€šå¸¸æä¾›&lt;a href="https://snapcraft.io/docs/channels">æ¸ é“&lt;/a>å…è®¸ä½ ä½¿ç”¨æµ‹è¯•ç‰ˆå’Œå‘å¸ƒ KUbernetes æ–°ç‰ˆæœ¬çš„å€™é€‰ç‰ˆæœ¬ï¼ŒåŒæ ·ä¹Ÿæä¾›å…ˆå‰ç¨³å®šç‰ˆæœ¬ã€‚Microk8s é€šå¸¸å‡ ä¹ç«‹åˆ»å‘å¸ƒ Kubernetes ä¸Šæ¸¸çš„ç¨³å®šç‰ˆæœ¬ã€‚&lt;/p>
&lt;!--
But wait, thereâ€™s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - thereâ€™s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. Weâ€™ll write more about this kind of architecture in a future article.
-->
&lt;p>ä½†æ˜¯ç­‰ç­‰ï¼Œè¿˜æœ‰æ›´å¤šï¼Minikube å’Œ Microk8s éƒ½æ˜¯ä½œä¸ºå•èŠ‚ç‚¹é›†ç¾¤å¯åŠ¨çš„ã€‚æœ¬è´¨ä¸Šæ¥è¯´ï¼Œå®ƒä»¬å…è®¸ä½ ç”¨å•ä¸ªå·¥ä½œèŠ‚ç‚¹åˆ›å»º Kubernetes é›†ç¾¤ã€‚è¿™ç§æƒ…å†µå³å°†æ”¹å˜ - MicroK8s æ—©æœŸçš„ alpha ç‰ˆæœ¬åŒ…æ‹¬é›†ç¾¤ã€‚æœ‰äº†è¿™ä¸ªèƒ½åŠ›ï¼Œä½ å¯ä»¥åˆ›å»ºæ­£å¦‚ä½ å¸Œæœ›å¤šçš„å·¥ä½œèŠ‚ç‚¹çš„ KUbernetes é›†ç¾¤ã€‚å¯¹äºåˆ›å»ºé›†ç¾¤æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªæ²¡æœ‰ä¸»è§çš„é€‰é¡¹ - å¼€å‘è€…åœ¨èŠ‚ç‚¹ä¹‹é—´åˆ›å»ºç½‘ç»œè¿æ¥å’Œé›†æˆäº†å…¶ä»–æ‰€éœ€è¦çš„åŸºç¡€è®¾æ–½ï¼Œæ¯”å¦‚ä¸€ä¸ªå¤–éƒ¨çš„è´Ÿè½½å‡è¡¡ã€‚æ€»çš„æ¥è¯´ï¼ŒMicroK8s æä¾›äº†ä¸€ç§å¿«é€Ÿç®€æ˜“çš„æ–¹æ³•ï¼Œä½¿å¾—å°‘é‡çš„è®¡ç®—æœºå’Œè™šæ‹Ÿæœºå˜æˆä¸€ä¸ªå¤šèŠ‚ç‚¹çš„ Kubernetes é›†ç¾¤ã€‚ä»¥åæˆ‘ä»¬å°†æ’°å†™æ›´å¤šè¿™ç§ä½“ç³»ç»“æ„çš„æ–‡ç« ã€‚&lt;/p>
&lt;!--
## Disclaimer
This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it's official [webpage](https://microk8s.io/docs/), where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.
-->
&lt;h2 id="å…è´£å£°æ˜">å…è´£å£°æ˜&lt;/h2>
&lt;p>è¿™ä¸æ˜¯ MicroK8s å®˜æ–¹ä»‹ç»æ–‡æ¡£ã€‚ä½ å¯ä»¥åœ¨å®ƒçš„å®˜æ–¹&lt;a href="https://microk8s.io/docs/">ç½‘é¡µ&lt;/a>æŸ¥è¯¢è¿è¡Œå’Œä½¿ç”¨ MicroK8s çš„è¯¦æƒ…ä¿¡æ¯ï¼Œå…¶ä¸­è¦†ç›–äº†ä¸åŒçš„ç”¨ä¾‹ï¼Œæ“ä½œç³»ç»Ÿï¼Œç¯å¢ƒç­‰ã€‚ç›¸åï¼Œè¿™ç¯‡æ–‡ç« çš„æ„å›¾æ˜¯æä¾›åœ¨ Linux ä¸Šè¿è¡Œ MicroK8s æ¸…æ™°æ˜“æ‡‚çš„æŒ‡å—ã€‚&lt;/p>
&lt;!--
## Prerequisites
A Linux distribution that [supports snaps](https://snapcraft.io/docs/installing-snapd), is required. In this guide, weâ€™ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out [Multipass](https://multipass.run) to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.
-->
&lt;h2 id="å‰ææ¡ä»¶">å‰ææ¡ä»¶&lt;/h2>
&lt;p>ä¸€ä¸ª&lt;a href="https://snapcraft.io/docs/installing-snapd">æ”¯æŒ snaps&lt;/a> çš„ Linux å‘è¡Œç‰ˆæ˜¯è¢«éœ€è¦çš„ã€‚è¿™ç¯‡æŒ‡å—ï¼Œæˆ‘ä»¬å°†ä¼šç”¨æ”¯æŒ snaps ä¸”å³å¼€å³ç”¨çš„ Ubuntu 18.04 LTSã€‚å¦‚æœä½ å¯¹è¿è¡Œåœ¨ Windows æˆ–è€… Mac ä¸Šçš„ MicroK8s æ„Ÿå…´è¶£ï¼Œä½ åº”è¯¥æ£€æŸ¥&lt;a href="https://multipass.run">å¤šé€šé“&lt;/a>ï¼Œå®‰è£…ä¸€ä¸ªå¿«é€Ÿçš„ Ubuntu VMï¼Œä½œä¸ºåœ¨ä½ çš„ç³»ç»Ÿä¸Šè¿è¡Œè™šæ‹Ÿæœº Ubuntu çš„å®˜æ–¹æ–¹å¼ã€‚&lt;/p>
&lt;!--
## MicroK8s installation
MicroK8s installation is straightforward:
-->
&lt;h2 id="microk8s-å®‰è£…">MicroK8s å®‰è£…&lt;/h2>
&lt;p>ç®€æ´çš„ MicroK8s å®‰è£…ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap install microk8s --classic
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.
&lt;p>You may verify the MicroK8s status with the following command:
--&amp;gt;
ä»¥ä¸Šçš„å‘½ä»¤å°†ä¼šåœ¨å‡ ç§’å†…å®‰è£…ä¸€ä¸ªæœ¬åœ°å•èŠ‚ç‚¹çš„ Kubernetes é›†ç¾¤ã€‚ä¸€æ—¦å‘½ä»¤æ‰§è¡Œç»“æŸï¼Œä½ çš„ Kubernetes é›†ç¾¤å°†ä¼šå¯åŠ¨å¹¶è¿è¡Œã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo microk8s.status
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Using microk8s
Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a `kubectl` binary, which can be accessed by running the `microk8s.kubectl` command. As an example:
-->
&lt;h2 id="ä½¿ç”¨-microk8s">ä½¿ç”¨ microk8s&lt;/h2>
&lt;p>ä½¿ç”¨ MicrosK8s å°±åƒå’Œå®‰è£…å®ƒä¸€æ ·ä¾¿æ·ã€‚MicroK8s æœ¬èº«åŒ…æ‹¬ä¸€ä¸ª &lt;code>kubectl&lt;/code> åº“ï¼Œè¯¥åº“å¯ä»¥é€šè¿‡æ‰§è¡Œ &lt;code>microk8s.kubectl&lt;/code> å‘½ä»¤å»è®¿é—®ã€‚ä¾‹å¦‚ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">microk8s.kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
While using the prefix `microk8s.kubectl` allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the `snap alias` command:
-->
&lt;p>å½“ä½¿ç”¨å‰ç¼€ &lt;code>microk8s.kubectl&lt;/code> æ—¶ï¼Œå…è®¸åœ¨æ²¡æœ‰å½±å“çš„æƒ…å†µä¸‹å¹¶è¡Œåœ°å®‰è£…å¦ä¸€ä¸ªç³»ç»Ÿçº§çš„ kubectlï¼Œä½ å¯ä»¥ä¾¿æ·åœ°ä½¿ç”¨ &lt;code>snap alias&lt;/code> å‘½ä»¤æ‘†è„±å®ƒï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap &lt;span style="color:#a2f">alias&lt;/span> microk8s.kubectl kubectl
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
This will allow you to simply use `kubectl` after. You can revert this change using the `snap unalias` command.
-->
&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;p>è¿™å°†å…è®¸ä½ ä»¥åä¾¿æ·åœ°ä½¿ç”¨ &lt;code>kubectl&lt;/code>ï¼Œä½ å¯ä»¥ç”¨ &lt;code>snap unalias&lt;/code>å‘½ä»¤æ¢å¤è¿™ä¸ªæ”¹å˜ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## MicroK8s addons
One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.
The full list of extensions can be checked by running the `microk8s.status` command:
-->
&lt;h2 id="microk8s-æ’ä»¶">MicroK8s æ’ä»¶&lt;/h2>
&lt;p>ä½¿ç”¨ MicroK8s å…¶ä¸­æœ€å¤§çš„å¥½å¤„ä¹‹ä¸€äº‹å®ä¸Šæ˜¯ä¹Ÿæ”¯æŒå„ç§å„æ ·çš„æ’ä»¶å’Œæ‰©å±•ã€‚æ›´é‡è¦çš„æ˜¯å®ƒä»¬æ˜¯å¼€ç®±å³ç”¨çš„ï¼Œç”¨æˆ·ä»…ä»…éœ€è¦å¯åŠ¨å®ƒä»¬ã€‚é€šè¿‡è¿è¡Œ &lt;code>microk8s.status&lt;/code> å‘½ä»¤æ£€æŸ¥å‡ºæ‰©å±•çš„å®Œæ•´åˆ—è¡¨ã€‚&lt;/p>
&lt;pre>&lt;code>sudo microk8s.status
&lt;/code>&lt;/pre>&lt;!--
As of the time of writing this article, the following add-ons are supported:
-->
&lt;p>æˆªè‡³åˆ°å†™è¿™ç¯‡æ–‡ç« ä¸ºæ­¢ï¼ŒMicroK8s å·²æ”¯æŒä»¥ä¸‹æ’ä»¶ï¼š&lt;/p>
&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
More add-ons are being created and contributed by the community all the time, it definitely helps to check often!
-->
&lt;p>ç¤¾åŒºåˆ›å»ºå’Œè´¡çŒ®äº†è¶Šæ¥è¶Šå¤šçš„æ’ä»¶ï¼Œç»å¸¸æ£€æŸ¥ä»–ä»¬æ˜¯ååˆ†æœ‰å¸®åŠ©çš„ã€‚&lt;/p>
&lt;!--
## Release channels
-->
&lt;h2 id="å‘å¸ƒæ¸ é“">å‘å¸ƒæ¸ é“&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap info microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Installing the sample application
In this tutorial weâ€™ll use NGINX as a sample application ([the official Docker Hub image](https://hub.docker.com/_/nginx)).
It will be installed as a Kubernetes deployment:
-->
&lt;h2 id="å®‰è£…ç®€å•çš„åº”ç”¨">å®‰è£…ç®€å•çš„åº”ç”¨&lt;/h2>
&lt;p>åœ¨è¿™ç¯‡æŒ‡å—ä¸­æˆ‘å°†ä¼šç”¨ NGINX ä½œä¸ºä¸€ä¸ªç¤ºä¾‹åº”ç”¨ç¨‹åºï¼ˆ&lt;a href="https://hub.docker.com/_/nginx">å®˜æ–¹ Docker Hub é•œåƒ&lt;/a>ï¼‰ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create deployment nginx --image&lt;span style="color:#666">=&lt;/span>nginx
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
To verify the installation, letâ€™s run the following:
-->
&lt;p>ä¸ºäº†æ£€æŸ¥å®‰è£…ï¼Œè®©æˆ‘ä»¬è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get deployments
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get pods
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
Also, we can retrieve the full output of all available objects within our Kubernetes cluster:
-->
&lt;p>æˆ‘ä»¬ä¹Ÿå¯ä»¥æ£€ç´¢å‡º Kubernetes é›†ç¾¤ä¸­æ‰€æœ‰å¯ç”¨å¯¹è±¡çš„å®Œæ•´è¾“å‡ºã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Uninstalling MicroK8s
Uninstalling your microk8s cluster is so easy as uninstalling the snap:
-->
&lt;h2 id="å¸è½½-mircrok8s">å¸è½½ MircroK8s&lt;/h2>
&lt;p>å¸è½½æ‚¨çš„ microk8s é›†ç¾¤ä¸å¸è½½ Snap åŒæ ·ä¾¿æ·ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap remove microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Screencast
-->
&lt;h2 id="æˆªå±è§†é¢‘">æˆªå±è§†é¢‘&lt;/h2>
&lt;p>&lt;a href="https://asciinema.org/a/263394">&lt;img src="https://asciinema.org/a/263394.svg" alt="asciicast">&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes æ–‡æ¡£æœ€ç»ˆç”¨æˆ·è°ƒç ”</title><link>https://kubernetes.io/zh/blog/2019/10/29/kubernetes-documentation-end-user-survey/</link><pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/29/kubernetes-documentation-end-user-survey/</guid><description>
&lt;!--
---
layout: blog
title: "Kubernetes Documentation Survey"
date: 2019-10-29
slug: kubernetes-documentation-end-user-survey
---
-->
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://www.linkedin.com/in/aimee-ukasick/">Aimee Ukasick&lt;/a> and SIG Docs&lt;/p>
&lt;!--
In September, SIG Docs conducted its first survey about the [Kubernetes
documentation](https://kubernetes.io/docs/). We'd like to thank the CNCF's Kim
McMahon for helping us create the survey and access the results.
-->
&lt;p>9æœˆï¼ŒSIG Docs è¿›è¡Œäº†ç¬¬ä¸€æ¬¡å…³äº &lt;a href="https://kubernetes.io/docs/">Kubernetes æ–‡æ¡£&lt;/a>çš„ç”¨æˆ·è°ƒç ”ã€‚æˆ‘ä»¬è¦æ„Ÿè°¢ CNCF
çš„ Kim McMahon å¸®åŠ©æˆ‘ä»¬åˆ›å»ºè°ƒæŸ¥å¹¶è·å–ç»“æœã€‚&lt;/p>
&lt;!--
# Key takeaways
-->
&lt;h1 id="ä¸»è¦æ”¶è·">ä¸»è¦æ”¶è·&lt;/h1>
&lt;!--
Respondents would like more example code, more detailed content, and more
diagrams in the Concepts, Tasks, and Reference sections.
-->
&lt;p>å—è®¿è€…å¸Œæœ›èƒ½åœ¨æ¦‚å¿µã€ä»»åŠ¡å’Œå‚è€ƒéƒ¨åˆ†å¾—åˆ°æ›´å¤šç¤ºä¾‹ä»£ç ã€æ›´è¯¦ç»†çš„å†…å®¹å’Œæ›´å¤šå›¾è¡¨ã€‚&lt;/p>
&lt;!--
74% of respondents would like the Tutorials section to contain advanced content.
-->
&lt;p>74% çš„å—è®¿è€…å¸Œæœ›æ•™ç¨‹éƒ¨åˆ†åŒ…å«é«˜çº§å†…å®¹ã€‚&lt;/p>
&lt;!--
69.70% said the Kubernetes documentation is the first place they look for
information about Kubernetes.
-->
&lt;p>69.70% çš„å—è®¿è€…è®¤ä¸º Kubernetes æ–‡æ¡£æ˜¯ä»–ä»¬é¦–è¦å¯»æ‰¾å…³äº Kubernetes èµ„æ–™çš„åœ°æ–¹ã€‚&lt;/p>
&lt;!--
# Survey methodology and respondents
-->
&lt;h1 id="è°ƒæŸ¥æ–¹æ³•å’Œå—è®¿è€…">è°ƒæŸ¥æ–¹æ³•å’Œå—è®¿è€…&lt;/h1>
&lt;!--
We conducted the survey in English. The survey was only available for 4 days due
to time constraints. We announced the survey on Kubernetes mailing lists, in
Kubernetes Slack channels, on Twitter, and in Kube Weekly. There were 23
questions, and respondents took an average of 4 minutes to complete the survey.
-->
&lt;p>æˆ‘ä»¬ç”¨è‹±è¯­è¿›è¡Œäº†è°ƒæŸ¥ã€‚ç”±äºæ—¶é—´é™åˆ¶ï¼Œè°ƒæŸ¥çš„æœ‰æ•ˆæœŸåªæœ‰ 4 å¤©ã€‚
æˆ‘ä»¬åœ¨ Kubernetes é‚®ä»¶åˆ—è¡¨ã€Kubernetes Slack é¢‘é“ã€Twitterã€Kube Weekly ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„è°ƒæŸ¥é—®å·ã€‚
è¿™ä»½è°ƒæŸ¥æœ‰ 23 ä¸ªé—®é¢˜ï¼Œ å—è®¿è€…å¹³å‡ç”¨ 4 åˆ†é’Ÿå®Œæˆè¿™ä¸ªè°ƒæŸ¥ã€‚&lt;/p>
&lt;!--
## Quick facts about respondents:
-->
&lt;h2 id="å…³äºå—è®¿è€…çš„ç®€è¦æƒ…å†µ">å…³äºå—è®¿è€…çš„ç®€è¦æƒ…å†µ&lt;/h2>
&lt;!--
- 48.48% are experienced Kubernetes users, 26.26% expert, and 25.25% beginner
- 57.58% use Kubernetes in both administrator and developer roles
- 64.65% have been using the Kubernetes documentation for more than 12 months
- 95.96% read the documentation in English
-->
&lt;ul>
&lt;li>48.48% æ˜¯ç»éªŒä¸°å¯Œçš„ Kubernetes ç”¨æˆ·ï¼Œ26.26% æ˜¯ä¸“å®¶ï¼Œ25.25% æ˜¯åˆå­¦è€…&lt;/li>
&lt;li>57.58% çš„äººåŒæ—¶ä½¿ç”¨ Kubernetes ä½œä¸ºç®¡ç†å‘˜å’Œå¼€å‘äººå‘˜&lt;/li>
&lt;li>64.65% çš„äººä½¿ç”¨ Kubernetes æ–‡æ¡£è¶…è¿‡ 12 ä¸ªæœˆ&lt;/li>
&lt;li>95.96% çš„äººé˜…è¯»è‹±æ–‡æ–‡æ¡£&lt;/li>
&lt;/ul>
&lt;!--
# Question and response highlights
-->
&lt;h1 id="é—®é¢˜å’Œå›ç­”è¦ç‚¹">é—®é¢˜å’Œå›ç­”è¦ç‚¹&lt;/h1>
&lt;!--
## Why people access the Kubernetes documentation
-->
&lt;h2 id="äººä»¬ä¸ºä»€ä¹ˆè®¿é—®-kubernetes-æ–‡æ¡£">äººä»¬ä¸ºä»€ä¹ˆè®¿é—® Kubernetes æ–‡æ¡£&lt;/h2>
&lt;!--
The majority of respondents stated that they access the documentation for the Concepts.
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png"
alt="Why respondents access the Kubernetes documentation"/>
&lt;/figure>
-->
&lt;p>å¤§å¤šæ•°å—è®¿è€…è¡¨ç¤ºï¼Œä»–ä»¬è®¿é—®æ–‡æ¡£æ˜¯ä¸ºäº†äº†è§£æ¦‚å¿µã€‚&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png"
alt="å—è®¿è€…ä¸ºä»€ä¹ˆè®¿é—® Kubernetes æ–‡æ¡£"/>
&lt;/figure>
&lt;!--
This deviates only slightly from what we see in Google Analytics: of the top 10
most viewed pages this year, #1 is the kubectl cheatsheet in the Reference section,
followed overwhelmingly by pages in the Concepts section.
-->
&lt;p>è¿™ä¸æˆ‘ä»¬åœ¨ Google Analytics ä¸Šçœ‹åˆ°çš„ç•¥æœ‰ä¸åŒï¼šåœ¨ä»Šå¹´æµè§ˆé‡æœ€å¤šçš„10ä¸ªé¡µé¢ä¸­ï¼Œç¬¬ä¸€æ˜¯åœ¨å‚è€ƒéƒ¨åˆ†çš„ kubectl
çš„å¤‡å¿˜å•ï¼Œå…¶æ¬¡æ˜¯æ¦‚å¿µéƒ¨åˆ†çš„é¡µé¢ã€‚&lt;/p>
&lt;!--
## Satisfaction with the documentation
-->
&lt;h2 id="å¯¹æ–‡æ¡£çš„æ»¡æ„ç¨‹åº¦">å¯¹æ–‡æ¡£çš„æ»¡æ„ç¨‹åº¦&lt;/h2>
&lt;!--
We asked respondents to record their level of satisfaction with the detail in
the Concepts, Tasks, Reference, and Tutorials sections:
-->
&lt;p>æˆ‘ä»¬è¦æ±‚å—è®¿è€…ä»æ¦‚å¿µã€ä»»åŠ¡ã€å‚è€ƒå’Œæ•™ç¨‹éƒ¨åˆ†è®°å½•ä»–ä»¬å¯¹ç»†èŠ‚çš„æ»¡æ„åº¦ï¼š&lt;/p>
&lt;!--
- Concepts: 47.96% Moderately Satisfied
- Tasks: 50.54% Moderately Satisfied
- Reference: 40.86% Very Satisfied
- Tutorial: 47.25% Moderately Satisfied
-->
&lt;ul>
&lt;li>æ¦‚å¿µï¼š47.96% ä¸­ç­‰æ»¡æ„&lt;/li>
&lt;li>ä»»åŠ¡ï¼š50.54% ä¸­ç­‰æ»¡æ„&lt;/li>
&lt;li>å‚è€ƒï¼š40.86% éå¸¸æ»¡æ„&lt;/li>
&lt;li>æ•™ç¨‹ï¼š47.25% ä¸­ç­‰æ»¡æ„&lt;/li>
&lt;/ul>
&lt;!--
## How SIG Docs can improve each documentation section
-->
&lt;h2 id="sig-docs-å¦‚ä½•æ”¹è¿›æ–‡æ¡£çš„å„ä¸ªéƒ¨åˆ†">SIG Docs å¦‚ä½•æ”¹è¿›æ–‡æ¡£çš„å„ä¸ªéƒ¨åˆ†&lt;/h2>
&lt;!--
We asked how we could improve each section, providing respondents with
selectable answers as well as a text field. The clear majority would like more
example code, more detailed content, more diagrams, and advanced tutorials:
-->
&lt;p>æˆ‘ä»¬è¯¢é—®å¦‚ä½•æ”¹è¿›æ¯ä¸ªéƒ¨åˆ†ï¼Œä¸ºå—è®¿è€…æä¾›å¯é€‰ç­”æ¡ˆä»¥åŠæ–‡æœ¬è¾“å…¥æ¡†ã€‚ç»å¤§å¤šæ•°äººæƒ³è¦æ›´å¤š
ç¤ºä¾‹ä»£ç ã€æ›´è¯¦ç»†çš„å†…å®¹ã€æ›´å¤šå›¾è¡¨å’Œæ›´é«˜çº§çš„æ•™ç¨‹ï¼š&lt;/p>
&lt;!--
```text
- Personally, would like to see more analogies to help further understanding.
- Would be great if corresponding sections of code were explained too
- Expand on the concepts to bring them together - they're a bucket of separate eels moving in different directions right now
- More diagrams, and more example code
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- å°±ä¸ªäººè€Œè¨€ï¼Œå¸Œæœ›çœ‹åˆ°æ›´å¤šçš„ç±»æ¯”ï¼Œä»¥å¸®åŠ©è¿›ä¸€æ­¥ç†è§£ã€‚
- å¦‚æœä»£ç çš„ç›¸åº”éƒ¨åˆ†ä¹Ÿèƒ½è§£é‡Šä¸€ä¸‹å°±å¥½äº†
- é€šè¿‡æ‰©å±•æ¦‚å¿µæŠŠå®ƒä»¬èåˆåœ¨ä¸€èµ· - å®ƒä»¬ç°åœ¨å®›å¦‚åœ¨ä¸€æ¡¶æ°´å†…æœå„ä¸ªæ–¹å‘æ¸¸åŠ¨çš„ä¸€æ¡æ¡é³—é±¼ã€‚
- æ›´å¤šçš„å›¾è¡¨ï¼Œæ›´å¤šçš„ç¤ºä¾‹ä»£ç 
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Respondents used the "Other" text box to record areas causing frustration:
-->
&lt;p>å—è®¿è€…ä½¿ç”¨â€œå…¶ä»–â€æ–‡æœ¬æ¡†è®°å½•å¼•å‘é˜»ç¢çš„åŒºåŸŸï¼š&lt;/p>
&lt;!--
```text
- Keep concepts up to date and accurate
- Keep task topics up to date and accurate. Human testing.
- Overhaul the examples. Many times the output of commands shown is not actual.
- I've never understood how to navigate or interpret the reference section
- Keep the tutorials up to date, or remove them
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- ä½¿æ¦‚å¿µä¿æŒæœ€æ–°å’Œå‡†ç¡®
- ä¿æŒä»»åŠ¡ä¸»é¢˜çš„æœ€æ–°æ€§å’Œå‡†ç¡®æ€§ã€‚äº²èº«è¯•éªŒã€‚
- å½»åº•æ£€æŸ¥ç¤ºä¾‹ã€‚å¾ˆå¤šæ—¶å€™æ˜¾ç¤ºçš„å‘½ä»¤è¾“å‡ºä¸æ˜¯å®é™…æƒ…å†µã€‚
- æˆ‘ä»æ¥éƒ½ä¸çŸ¥é“å¦‚ä½•å¯¼èˆªæˆ–è§£é‡Šå‚è€ƒéƒ¨åˆ†
- ä½¿æ•™ç¨‹ä¿æŒæœ€æ–°ï¼Œæˆ–å°†å…¶åˆ é™¤
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## How SIG Docs can improve the documentation overall
-->
&lt;h2 id="sig-docs-å¦‚ä½•å…¨é¢æ”¹è¿›æ–‡æ¡£">SIG Docs å¦‚ä½•å…¨é¢æ”¹è¿›æ–‡æ¡£&lt;/h2>
&lt;!--
We asked respondents how we can improve the Kubernetes documentation
overall. Some took the opportunity to tell us we are doing a good job:
-->
&lt;p>æˆ‘ä»¬è¯¢é—®å—è®¿è€…å¦‚ä½•ä»æ•´ä½“ä¸Šæ”¹è¿› Kubernetes æ–‡æ¡£ã€‚ä¸€äº›äººæŠ“ä½è¿™æ¬¡æœºä¼šå‘Šè¯‰æˆ‘ä»¬æˆ‘ä»¬æ­£åœ¨åšä¸€ä¸ªå¾ˆæ£’çš„
å·¥ä½œï¼š&lt;/p>
&lt;!--
```text
- For me, it is the best documented open source project.
- Keep going!
- I find the documentation to be excellent.
- You [are] doing a great job. For real.
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- å¯¹æˆ‘è€Œè¨€ï¼Œè¿™æ˜¯æˆ‘è§è¿‡çš„æ–‡æ¡£æœ€å¥½çš„å¼€æºé¡¹ç›®ã€‚
- ç»§ç»­åŠªåŠ›ï¼
- æˆ‘è§‰å¾—æ–‡æ¡£å¾ˆå¥½ã€‚
- ä½ ä»¬åšå¾—çœŸå¥½ã€‚çœŸçš„ã€‚
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Other respondents provided feedback on the content:
-->
&lt;p>å…¶å®ƒå—è®¿è€…æä¾›å…³äºå†…å®¹çš„åé¦ˆï¼š&lt;/p>
&lt;!--
```text
- ...But since we're talking about docs, more is always better. More
advanced configuration examples would be, to me, the way to go. Like a Use Case page for each configuration topic with beginner to advanced example scenarios. Something like that would be
awesome....
-->
&lt;!--
- More in-depth examples and use cases would be great. I often feel that the Kubernetes documentation scratches the surface of a topic, which might be great for new users, but it leaves more experienced users without much "official" guidance on how to implement certain things.
-->
&lt;!--
- More production like examples in the resource sections (notably secrets) or links to production like examples
-->
&lt;!--
- It would be great to see a very clear "Quick Start" A->Z up and running like many other tech projects. There are a handful of almost-quick-starts, but no single guidance. The result is information overkill.
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- ...ä½†æ—¢ç„¶æˆ‘ä»¬è°ˆè®ºçš„æ˜¯æ–‡æ¡£ï¼Œå¤šå¤šç›Šå–„ã€‚æ›´å¤šçš„é«˜çº§é…ç½®ç¤ºä¾‹å¯¹æˆ‘æ¥è¯´å°†æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚æ¯”å¦‚æ¯ä¸ªé…ç½®ä¸»é¢˜çš„ç”¨ä¾‹é¡µé¢ï¼Œ
ä»åˆå­¦è€…åˆ°é«˜çº§ç¤ºä¾‹åœºæ™¯ã€‚åƒè¿™æ ·çš„ä¸œè¥¿çœŸçš„æ˜¯ä»¤äººæƒŠå¹......
- æ›´æ·±å…¥çš„ä¾‹å­å’Œç”¨ä¾‹å°†æ˜¯å¾ˆå¥½çš„ã€‚æˆ‘ç»å¸¸æ„Ÿè§‰ Kubernetes æ–‡æ¡£åªæ˜¯è§¦åŠäº†ä¸€ä¸ªä¸»é¢˜çš„è¡¨é¢ï¼Œè¿™å¯èƒ½å¯¹æ–°ç”¨æˆ·å¾ˆå¥½ï¼Œ
ä½†æ˜¯å®ƒæ²¡æœ‰è®©æ›´æœ‰ç»éªŒçš„ç”¨æˆ·è·å–å¤šå°‘å…³äºå¦‚ä½•å®ç°æŸäº›ä¸œè¥¿çš„â€œå®˜æ–¹â€æŒ‡å¯¼ã€‚
- èµ„æºèŠ‚ï¼ˆç‰¹åˆ«æ˜¯ secretsï¼‰å¸Œæœ›æœ‰æ›´å¤šç±»ä¼¼äºäº§å“çš„ç¤ºä¾‹æˆ–æŒ‡å‘ç±»ä¼¼äº§å“çš„ç¤ºä¾‹çš„é“¾æ¥
- å¦‚æœèƒ½åƒå¾ˆå¤šå…¶å®ƒæŠ€æœ¯é¡¹ç›®é‚£æ ·æœ‰éå¸¸æ¸…æ™°çš„â€œå¿«é€Ÿå¯åŠ¨â€ é€æ­¥æ•™å­¦å®Œæˆæ­å»ºå°±æ›´å¥½äº†ã€‚ç°æœ‰çš„å¿«é€Ÿå…¥é—¨å†…å®¹å±ˆæŒ‡å¯æ•°ï¼Œ
ä¹Ÿæ²¡æœ‰ç»Ÿä¸€çš„æŒ‡å—ã€‚ç»“æœæ˜¯ä¿¡æ¯æ³›æ»¥ã€‚
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
A few respondents provided technical suggestions:
```text
- Make table columns sortable and filterable using a ReactJS or Angular component.
-->
&lt;!--
- For most, I think creating documentation with Hugo - a system for static site generation - is not appropriate. There are better systems for documenting large software project.
-->
&lt;!--
Specifically, I would like to see k8s switch to Sphinx for documentation. It has an excellent built-in search, it is easy tolearn if you know markdown, it is widely adopted by other projects (e.g. every software project in readthedocs.io, linux kernel, docs.python.org etc).
```
-->
&lt;p>å°‘æ•°å—è®¿è€…æä¾›çš„æŠ€æœ¯å»ºè®®ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- ä½¿ç”¨ ReactJS æˆ–è€… Angular component ä½¿è¡¨çš„åˆ—å¯æ’åºå’Œå¯ç­›é€‰ã€‚
- å¯¹äºå¤§å¤šæ•°äººæ¥è¯´ï¼Œæˆ‘è®¤ä¸ºç”¨ Hugo - ä¸€ä¸ªé™æ€ç«™ç‚¹ç”Ÿæˆç³»ç»Ÿ - åˆ›å»ºæ–‡æ¡£æ˜¯ä¸åˆé€‚çš„ã€‚æœ‰æ›´å¥½çš„ç³»ç»Ÿæ¥è®°å½•å¤§å‹è½¯ä»¶é¡¹ç›®ã€‚
å…·ä½“æ¥è¯´ï¼Œæˆ‘å¸Œæœ›çœ‹åˆ° k8s åˆ‡æ¢åˆ° Sphinx æ¥è·å–æ–‡æ¡£ã€‚Sphinx æœ‰ä¸€ä¸ªå¾ˆå¥½çš„å†…ç½®æœç´¢ã€‚å¦‚æœä½ äº†è§£ markdownï¼Œå­¦ä¹ èµ·æ¥ä¹Ÿå¾ˆå®¹æ˜“ã€‚
Sphinx è¢«å…¶ä»–é¡¹ç›®å¹¿æ³›é‡‡ç”¨ï¼ˆä¾‹å¦‚ï¼Œåœ¨ readthedocs.ioã€linux kernelã€docs.python.org ç­‰ç­‰ï¼‰ã€‚
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Overall, respondents provided constructive criticism focusing on the need for
advanced use cases as well as more in-depth examples, guides, and walkthroughs.
-->
&lt;p>æ€»ä½“è€Œè¨€ï¼Œå—è®¿è€…æä¾›äº†å»ºè®¾æ€§çš„æ‰¹è¯„ï¼Œå…¶å…³æ³¨ç‚¹æ˜¯é«˜çº§ç”¨ä¾‹ä»¥åŠæ›´æ·±å…¥çš„ç¤ºä¾‹ã€æŒ‡å—å’Œæ¼”ç»ƒã€‚&lt;/p>
&lt;!--
# Where to see more
-->
&lt;h1 id="å“ªé‡Œå¯ä»¥çœ‹åˆ°æ›´å¤š">å“ªé‡Œå¯ä»¥çœ‹åˆ°æ›´å¤š&lt;/h1>
&lt;!--
Survey results summary, charts, and raw data are available in `kubernetes/community` sig-docs [survey](https://github.com/kubernetes/community/tree/master/sig-docs/survey) directory.
-->
&lt;p>è°ƒæŸ¥ç»“æœæ‘˜è¦ã€å›¾è¡¨å’ŒåŸå§‹æ•°æ®å¯åœ¨ &lt;code>kubernetes/community&lt;/code> sig-docs
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs/survey">survey&lt;/a>
ç›®å½•ä¸‹ã€‚&lt;/p></description></item><item><title>Blog: åœ£è¿­æˆˆè´¡çŒ®è€…å³°ä¼šæ—¥ç¨‹å…¬å¸ƒï¼</title><link>https://kubernetes.io/zh/blog/2019/10/10/contributor-summit-san-diego-schedule/</link><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/10/contributor-summit-san-diego-schedule/</guid><description>
&lt;!--
layout: blog
title: "Contributor Summit San Diego Schedule Announced!"
date: 2019-10-10
slug: contributor-summit-san-diego-schedule
-->
&lt;!--
Authors: Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)
-->
&lt;p>ä½œè€…ï¼šJosh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)&lt;/p>
&lt;!--
tl;dr A week ago we announced that [registration is open][reg] for the contributor
summit , and we're now live with [the full Contributor Summit schedule!][schedule]
Grab your spot while tickets are still available. There is currently a waitlist
for new contributor workshop. ([Register here!][reg])
-->
&lt;p>ä¸€å‘¨å‰ï¼Œæˆ‘ä»¬å®£å¸ƒè´¡çŒ®è€…å³°ä¼š&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">å¼€æ”¾æ³¨å†Œ&lt;/a>ï¼Œç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†æ•´ä¸ª&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/program/schedule/">è´¡çŒ®è€…å³°ä¼šçš„æ—¥ç¨‹å®‰æ’&lt;/a>ï¼è¶ç°åœ¨è¿˜æœ‰ç¥¨ï¼Œé©¬ä¸ŠæŠ¢å ä½ çš„ä½ç½®ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæ–°è´¡çŒ®è€…ç ”è®¨ä¼šçš„ç­‰å¾…åå•ã€‚ (&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">ç‚¹å‡»è¿™é‡Œæ³¨å†Œ!&lt;/a>)&lt;/p>
&lt;!--
There are many great sessions planned for the Contributor Summit, spread across
five rooms of current contributor content in addition to the new contributor
workshops. Since this is an upstream contributor summit and we don't often meet,
being a globally distributed team, most of these sessions are discussions or
hands-on labs, not just presentations. We want folks to learn and have a
good time meeting their OSS teammates.
-->
&lt;p>é™¤äº†æ–°è´¡çŒ®è€…ç ”è®¨ä¼šä¹‹å¤–ï¼Œè´¡çŒ®è€…å³°ä¼šè¿˜å®‰æ’äº†è®¸å¤šç²¾å½©çš„ä¼šè®®ï¼Œè¿™äº›ä¼šè®®åˆ†å¸ƒåœ¨å½“å‰äº”ä¸ªè´¡çŒ®è€…å†…å®¹çš„ä¼šè®®å®¤ä¸­ã€‚ç”±äºè¿™æ˜¯ä¸€ä¸ªä¸Šæ¸¸è´¡çŒ®è€…å³°ä¼šï¼Œå¹¶ä¸”æˆ‘ä»¬ä¸ç»å¸¸è§é¢ï¼Œæ‰€ä»¥ä½œä¸ºä¸€ä¸ªå…¨çƒåˆ†å¸ƒçš„å›¢é˜Ÿï¼Œè¿™äº›ä¼šè®®å¤§å¤šæ˜¯è®¨è®ºæˆ–åŠ¨æ‰‹å®è·µï¼Œè€Œä¸ä»…ä»…æ˜¯æ¼”ç¤ºã€‚æˆ‘ä»¬å¸Œæœ›å¤§å®¶äº’ç›¸å­¦ä¹ ï¼Œå¹¶äºä»–ä»¬çš„å¼€æºä»£ç é˜Ÿå‹ç©çš„å¼€å¿ƒã€‚&lt;/p>
&lt;!--
Unconference tracks are returning from last year with sessions to be chosen
Monday morning. These are ideal for the latest hot topics and specific
discussions that contributors want to have. In previous years, we've covered
flaky tests, cluster lifecycle, KEPs (Kubernetes Enhancement Proposals), mentoring,
security, and more.
-->
&lt;p>åƒå»å¹´ä¸€æ ·ï¼Œéç»„ç»‡ä¼šè®®å°†é‡æ–°å¼€å§‹ï¼Œä¼šè®®å°†åœ¨å‘¨ä¸€ä¸Šåˆè¿›è¡Œé€‰æ‹©ã€‚å¯¹äºæœ€æ–°çš„çƒ­é—¨è¯é¢˜å’Œè´¡çŒ®è€…æƒ³è¦è¿›è¡Œçš„ç‰¹å®šè®¨è®ºï¼Œè¿™æ˜¯ç†æƒ³çš„é€‰æ‹©ã€‚åœ¨è¿‡å»çš„å‡ å¹´ä¸­ï¼Œæˆ‘ä»¬æ¶µç›–äº†ä¸ç¨³å®šçš„æµ‹è¯•ï¼Œé›†ç¾¤ç”Ÿå‘½å‘¨æœŸï¼ŒKEPï¼ˆKuberneteså¢å¼ºå»ºè®®ï¼‰ï¼ŒæŒ‡å¯¼ï¼Œå®‰å…¨æ€§ç­‰ç­‰ã€‚&lt;/p>
&lt;!--
![Unconference](/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg)
-->
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg" alt="éç»„ç»‡ä¼šè®®">&lt;/p>
&lt;!--
While the schedule contains difficult decisions in every timeslot, we've picked
a few below to give you a taste of what you'll hear, see, and participate in, at
the summit:
-->
&lt;p>å°½ç®¡åœ¨æ¯ä¸ªæ—¶é—´é—´éš™æ—¥ç¨‹å®‰æ’éƒ½åŒ…å«å›°éš¾çš„å†³å®šï¼Œä½†æˆ‘ä»¬é€‰æ‹©äº†ä»¥ä¸‹å‡ ç‚¹ï¼Œè®©æ‚¨ä½“éªŒä¸€ä¸‹æ‚¨å°†åœ¨å³°ä¼šä¸Šå¬åˆ°ã€çœ‹åˆ°å’Œå‚ä¸çš„å†…å®¹ï¼š&lt;/p>
&lt;!--
* **[Vision]**: SIG-Architecture will be sharing their vision of where we're going
with Kubernetes development for the next year and beyond.
* **[Security]**: Tim Allclair and CJ Cullen will present on the current state of
Kubernetes security. In another security talk, Vallery Lancey will lead a
discussion about making our platform secure by default.
* **[Prow]**: Interested in working with Prow and contributing to Test-Infra, but
not sure where to start? Rob Keilty will help you get a Prow test environment
running on your laptop.
* **[Git]**: Staff from GitHub will be collaborating with Christoph Blecker to share
practical Git tips for Kubernetes contributors.
* **[Reviewing]**: Tim Hockin will share the secrets of becoming a great code
reviewer, and Jordan Liggitt will conduct a live API review so that you can do
one, or at least pass one.
* **[End Users]**: Several end users from the CNCF partner ecosystem, invited by
Cheryl Hung, will hold a Q&amp;A with contributors to strengthen our feedback loop.
* **[Docs]**: As always, SIG-Docs will run a three-hour contributing-to-documentation
workshop.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMc">é¢„è§&lt;/a>&lt;/strong>: SIGç»„ç»‡å°†åˆ†äº«ä»–ä»¬å¯¹äºæ˜å¹´å’Œä»¥åKuberneteså¼€å‘å‘å±•æ–¹å‘çš„è®¤è¯†ã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMj">å®‰å…¨&lt;/a>&lt;/strong>: Tim Allclairå’ŒCJ Cullenå°†ä»‹ç»Kuberneteså®‰å…¨çš„å½“å‰æƒ…å†µã€‚åœ¨å¦ä¸€ä¸ªå®‰å…¨æ€§æ¼”è®²ä¸­ï¼ŒVallery Lanceyå°†ä¸»æŒæœ‰å…³ä½¿æˆ‘ä»¬çš„å¹³å°é»˜è®¤æƒ…å†µä¸‹å®‰å…¨çš„è®¨è®ºã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vv6Z">Prow&lt;/a>&lt;/strong>: æœ‰å…´è¶£ä¸Prowåˆä½œå¹¶ä¸ºTest-Infraåšè´¡çŒ®ï¼Œä½†ä¸ç¡®å®šä»å“ªé‡Œå¼€å§‹ï¼Ÿ Rob Keiltyå°†å¸®åŠ©æ‚¨åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡ŒProwæµ‹è¯•ç¯å¢ƒ&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNa">Git&lt;/a>&lt;/strong>: GitHubçš„å‘˜å·¥å°†ä¸Christoph Bleckeråˆä½œï¼Œä¸ºKubernetesè´¡çŒ®è€…åˆ†äº«å®ç”¨çš„GitæŠ€å·§ã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VutA">å®¡é˜…&lt;/a>&lt;/strong>: è’‚å§†Â·éœé‡‘ï¼ˆTim Hockinï¼‰å°†åˆ†äº«æˆä¸ºä¸€åå‡ºè‰²çš„ä»£ç å®¡é˜…è€…çš„ç§˜å¯†ï¼Œè€Œä¹”ä¸¹Â·åˆ©å‰ç‰¹ï¼ˆJordan Liggittï¼‰å°†è¿›è¡Œå®æ—¶APIå®¡é˜…ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è¿›è¡Œä¸€æ¬¡æˆ–è‡³å°‘äº†è§£ä¸€æ¬¡å®¡é˜…ã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNJ">ç»ˆç«¯ç”¨æˆ·&lt;/a>&lt;/strong>: åº”Cheryl Hungé‚€è¯·ï¼Œæ¥è‡ªCNCFåˆä½œä¼™ä¼´ç”Ÿæ€çš„æ•°ä¸ªç»ˆç«¯ç”¨æˆ·ï¼Œå°†å›ç­”è´¡çŒ®è€…çš„é—®é¢˜ï¼Œä»¥åŠ å¼ºæˆ‘ä»¬çš„åé¦ˆå¾ªç¯ã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vux2">æ–‡æ¡£&lt;/a>&lt;/strong>: ä¸å¾€å¸¸ä¸€æ ·ï¼ŒSIG-Docså°†ä¸¾åŠä¸€ä¸ªä¸ºæ—¶ä¸‰ä¸ªå°æ—¶çš„æ–‡æ¡£æ’°å†™ç ”è®¨ä¼šã€‚&lt;/li>
&lt;/ul>
&lt;!--
We're also giving out awards to contributors who distinguished themselves in 2019,
and there will be a huge Meet &amp; Greet for new contributors to find their SIG
(and for existing contributors to ask about their PRs) at the end of the day on
Monday.
-->
&lt;p>æˆ‘ä»¬è¿˜å°†å‘åœ¨2019å¹´æ°å‡ºçš„è´¡çŒ®è€…é¢å‘å¥–é¡¹ï¼Œå‘¨ä¸€æ˜ŸæœŸä¸€ç»“æŸæ—¶å°†æœ‰ä¸€ä¸ªå·¨å¤§çš„è§é¢ä¼šï¼Œä¾›æ–°çš„è´¡çŒ®è€…æ‰¾åˆ°ä»–ä»¬çš„SIGï¼ˆä»¥åŠç°æœ‰çš„è´¡çŒ®è€…è¯¢é—®ä»–ä»¬çš„PRï¼‰ã€‚&lt;/p>
&lt;!--
Hope to see you all there, and [make sure you register!][reg]
-->
&lt;p>å¸Œæœ›èƒ½å¤Ÿåœ¨å³°ä¼šä¸Šè§åˆ°æ‚¨ï¼Œå¹¶ä¸”ç¡®ä¿æ‚¨å·²ç»æå‰&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">æ³¨å†Œ&lt;/a>ï¼&lt;/p>
&lt;!--
[San Diego team][team]
-->
&lt;p>&lt;a href="http://git.k8s.io/community/events/events-team">åœ£è¿­æˆˆå›¢é˜Ÿ&lt;/a>&lt;/p></description></item><item><title>Blog: 2019 æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾ç»“æœ</title><link>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</guid><description>
&lt;!--
---
layout: blog
title: "2019 Steering Committee Election Results"
date: 2019-10-03
slug: 2019-steering-committee-election-results
---
-->
&lt;!--
**Authors**: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šBob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)&lt;/p>
&lt;!--
The [2019 Steering Committee Election] is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.
-->
&lt;p>&lt;a href="https://git.k8s.io/community/events/elections/2021">2019 æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾&lt;/a> æ˜¯ Kubernetes é¡¹ç›®çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚æœ€åˆçš„è‡ªåŠ©å§”å‘˜ä¼šæ­£é€æ­¥é€€ä¼‘ï¼Œç°åœ¨è¯¥å§”å‘˜ä¼šå·²ç¼©å‡åˆ°æœ€ååˆ†é…çš„ 7 ä¸ªå¸­ä½ã€‚æŒ‡å¯¼å§”å‘˜ä¼šçš„æ‰€æœ‰æˆå‘˜ç°åœ¨éƒ½ç”± Kubernetes ç¤¾åŒºé€‰ä¸¾äº§ç”Ÿã€‚&lt;/p>
&lt;!--
Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.
-->
&lt;p>æ¥ä¸‹æ¥çš„é€‰ä¸¾å°†é€‰å‡º 3 åˆ° 4 åå§”å‘˜ï¼Œä»»æœŸä¸¤å¹´ã€‚&lt;/p>
&lt;!--
## **Results**
The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):
-->
&lt;h2 id="é€‰ä¸¾ç»“æœ">é€‰ä¸¾ç»“æœ&lt;/h2>
&lt;p>Kubernetes æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾ç°å·²å®Œæˆï¼Œä»¥ä¸‹å€™é€‰äººæå‰è·å¾—ç«‹å³å¼€å§‹çš„ä¸¤å¹´ä»»æœŸ (æŒ‰ GitHub handle çš„å­—æ¯é¡ºåºæ’åˆ—) ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>), Loodse&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>)&lt;/strong>, &lt;strong>Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
They join Aaron Crickenberger ([@spiffxp]), Google; Davanum Srinivas ([@dims]),
VMware; and Timothy St. Clair ([@timothysc]), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.
-->
&lt;p>ä»–ä»¬åŠ å…¥äº† Aaron Crickenberger (&lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>)ï¼Œ Googleï¼›Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>)ï¼ŒVMware; and Timothy St. Clair (&lt;a href="https://github.com/timothysc">@timothysc&lt;/a>), VMwareï¼Œä½¿å¾—å§”å‘˜ä¼šæ›´åœ†æ»¡ã€‚Aaronã€Davanum å’Œ Timothy å æ®çš„è¿™äº›å¸­ä½å°†ä¼šåœ¨æ˜å¹´çš„è¿™ä¸ªæ—¶å€™è¿›è¡Œé€‰ä¸¾ã€‚&lt;/p>
&lt;!--
## Big Thanks!
* Thanks to the initial bootstrap committee for establishing the initial
project governance and overseeing a multi-year transition period:
-->
&lt;h2 id="è¯šæŒšçš„æ„Ÿè°¢">è¯šæŒšçš„æ„Ÿè°¢ï¼&lt;/h2>
&lt;ul>
&lt;li>æ„Ÿè°¢æœ€åˆçš„å¼•å¯¼å§”å‘˜ä¼šåˆ›ç«‹äº†æœ€åˆé¡¹ç›®çš„ç®¡ç†å¹¶ç›‘ç£äº†å¤šå¹´çš„è¿‡æ¸¡æœŸï¼š
&lt;ul>
&lt;li>Joe Beda (&lt;a href="https://github.com/jbeda">@jbeda&lt;/a>), VMware&lt;/li>
&lt;li>Brendan Burns (&lt;a href="https://github.com/brendandburns">@brendandburns&lt;/a>), Microsoft&lt;/li>
&lt;li>Clayton Coleman (&lt;a href="https://github.com/smarterclayton">@smarterclayton&lt;/a>), Red Hat&lt;/li>
&lt;li>Brian Grant (&lt;a href="https://github.com/bgrant0607">@bgrant0607&lt;/a>), Google&lt;/li>
&lt;li>Tim Hockin (&lt;a href="https://github.com/thockin">@thockin&lt;/a>), Google&lt;/li>
&lt;li>Sarah Novotny (&lt;a href="https://github.com/sarahnovotny">@sarahnovotny&lt;/a>), Microsoft&lt;/li>
&lt;li>Brandon Philips (&lt;a href="https://github.com/philips">@philips&lt;/a>), Red Hat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:
-->
&lt;ul>
&lt;li>åŒæ ·æ„Ÿè°¢å…¶ä»–çš„å·²é€€ä¼‘æŒ‡å¯¼å§”å‘˜ä¼šæˆå‘˜ã€‚ç¤¾åŒºå¯¹ä½ ä»¬å…ˆå‰çš„æœåŠ¡è¡¨ç¤ºèµèµï¼š
&lt;ul>
&lt;li>Quinton Hoole (&lt;a href="https://github.com/quinton-hoole">@quinton-hoole&lt;/a>), Huawei&lt;/li>
&lt;li>Michelle Noorali (&lt;a href="https://github.com/michelleN">@michelleN&lt;/a>), Microsoft&lt;/li>
&lt;li>Phillip Wittrock (&lt;a href="https://github.com/pwittrock">@pwittrock&lt;/a>), Google&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.
* Thanks to all 377 voters who cast a ballot.
* And last but not leastâ€¦Thanks to Cornell University for hosting [CIVS]!
-->
&lt;ul>
&lt;li>æ„Ÿè°¢å‚é€‰çš„å€™é€‰äººã€‚ æ„¿åœ¨æ¯æ¬¡é€‰ä¸¾ä¸­ï¼Œæˆ‘ä»¬éƒ½èƒ½æ‹¥æœ‰ä¸€ç¾¤åƒæ‚¨ä¸€æ ·æ¨åŠ¨ç¤¾åŒºå‘å‰å‘å±•çš„äººã€‚&lt;/li>
&lt;li>æ„Ÿè°¢æ‰€æœ‰æŠ•ç¥¨çš„377ä½é€‰æ°‘ã€‚&lt;/li>
&lt;li>æœ€åï¼Œæ„Ÿè°¢åº·å¥ˆå°”å¤§å­¦ä¸¾åŠçš„ &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>!&lt;/li>
&lt;/ul>
&lt;!--
## Get Involved with the Steering Committee
You can follow along with Steering Committee [backlog items] and weigh in by
filing an issue or creating a PR against their [repo]. They meet bi-weekly on
[Wednesdays at 8pm UTC] and regularly attend Meet Our Contributors. They can
also be contacted at their public mailing list [steering@kubernetes.io].
Steering Committee Meetings:
-->
&lt;h2 id="å‚ä¸æŒ‡å¯¼å§”å‘˜ä¼š">å‚ä¸æŒ‡å¯¼å§”å‘˜ä¼š&lt;/h2>
&lt;p>ä½ å¯ä»¥è·Ÿè¿›æŒ‡å¯¼å§”å‘˜ä¼šçš„ &lt;a href="https://github.com/kubernetes/steering/projects/1">ä»£åŠäº‹é¡¹&lt;/a>ï¼Œé€šè¿‡æå‡ºé—®é¢˜æˆ–è€…å‘ &lt;a href="https://github.com/kubernetes/steering">ä»“åº“&lt;/a> æäº¤ä¸€ä¸ª pr ã€‚ä»–ä»¬æ¯ä¸¤å‘¨ä¸€æ¬¡ï¼Œåœ¨ &lt;a href="https://github.com/kubernetes/steering">UTC æ—¶é—´å‘¨ä¸‰æ™šä¸Š 8 ç‚¹&lt;/a> ä¼šé¢ï¼Œå¹¶å®šæœŸä¸æˆ‘ä»¬çš„è´¡çŒ®è€…è§é¢ã€‚ä¹Ÿå¯ä»¥é€šè¿‡ä»–ä»¬çš„å…¬å…±é‚®ä»¶åˆ—è¡¨ &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a> è”ç³»ä»–ä»¬ã€‚&lt;/p>
&lt;p>æŒ‡å¯¼å§”å‘˜ä¼šä¼šè®®ï¼š&lt;/p>
&lt;!--
* [YouTube Playlist]
-->
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube æ’­æ”¾åˆ—è¡¨&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: San Diego è´¡çŒ®è€…å³°ä¼šå¼€æ”¾æ³¨å†Œï¼</title><link>https://kubernetes.io/zh/blog/2019/09/24/san-diego-contributor-summit/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/09/24/san-diego-contributor-summit/</guid><description>
&lt;!--
---
layout: blog
title: "Contributor Summit San Diego Registration Open!"
date: 2019-09-24
slug: san-diego-contributor-summit
---
--->
&lt;!--
**Authors: Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)**
--->
&lt;p>&lt;strong>ä½œè€…ï¼šParis Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)&lt;/strong>&lt;/p>
&lt;!--
[Contributor Summit San Diego 2019 Event Page]
Registration is now open and in record time, weâ€™ve hit capacity for the
*new contributor workshop* session of the event! Waitlist is now available.
--->
&lt;p>&lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">2019 San Diego è´¡çŒ®è€…å³°ä¼šæ´»åŠ¨é¡µé¢&lt;/a>
æ³¨å†Œå·²ç»å¼€æ”¾ï¼Œå¹¶ä¸”åœ¨åˆ›çºªå½•çš„æ—¶é—´å†…ï¼Œ&lt;em>æ–°è´¡çŒ®è€…ç ”è®¨ä¼š&lt;/em> æ´»åŠ¨å·²æ»¡å‘˜ï¼å€™è¡¥åå•å·²ç»å¼€æ”¾ã€‚&lt;/p>
&lt;!--
**Sunday, November 17**
Evening Contributor Celebration:
[QuartYard]*
Address: 1301 Market Street, San Diego, CA 92101
Time: 6:00PM - 9:00PM
--->
&lt;p>&lt;strong>11æœˆ17æ—¥ï¼Œæ˜ŸæœŸæ—¥&lt;/strong>&lt;br>
æ™šé—´è´¡çŒ®è€…åº†å…¸ï¼š&lt;br>
&lt;a href="https://quartyardsd.com/">QuartYard&lt;/a>*&lt;br>
åœ°å€: 1301 Market Street, San Diego, CA 92101&lt;br>
æ—¶é—´: ä¸‹åˆ6:00 - ä¸‹åˆ9:00&lt;/p>
&lt;!--
**Monday, November 18**
All Day Contributor Summit:
[Marriott Marquis San Diego Marina]
Address: 333 W Harbor Dr, San Diego, CA 92101
Time: 9:00AM - 5:00PM
--->
&lt;p>&lt;strong>11æœˆ18æ—¥ï¼Œæ˜ŸæœŸä¸€&lt;/strong>&lt;br>
å…¨å¤©è´¡çŒ®è€…å³°ä¼šï¼š&lt;br>
&lt;a href="https://www.marriott.com/hotels/travel/sandt-marriott-marquis-san-diego-marina/?scid=bb1a189a-fec3-4d19-a255-54ba596febe2">Marriott Marquis San Diego Marina&lt;/a>&lt;br>
åœ°å€: 333 W Harbor Dr, San Diego, CA 92101&lt;br>
æ—¶é—´: ä¸Šåˆ9:00 - ä¸‹åˆ5:00&lt;/p>
&lt;!--
While the Kubernetes project is only five years old, weâ€™re already going into our
9th Contributor Summit this November in San Diego before KubeCon + CloudNativeCon.
The rapid increase is thanks to adding European and Asian Contributor Summits to
the North American events weâ€™ve done previously. We will continue to run Contributor
Summits across the globe, as it is important that our contributor base grows in
all forms of diversity.
--->
&lt;p>è™½ç„¶ Kubernetes é¡¹ç›®åªæœ‰äº”å¹´çš„å†å²ï¼Œä½†æ˜¯åœ¨ KubeCon + CloudNativeCon ä¹‹å‰ï¼Œä»Šå¹´11æœˆåœ¨åœ£è¿­æˆˆæ—¶æˆ‘ä»¬ä¸¾åŠçš„ç¬¬ä¹å±Šè´¡çŒ®è€…å³°ä¼šäº†ã€‚å¿«é€Ÿå¢é•¿çš„åŸå› æ˜¯åœ¨æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„åŒ—ç¾æ´»åŠ¨ä¸­å¢åŠ äº†æ¬§æ´²å’Œäºšæ´²è´¡çŒ®è€…å³°ä¼šã€‚æˆ‘ä»¬å°†ç»§ç»­åœ¨å…¨çƒä¸¾åŠè´¡çŒ®è€…å³°ä¼šï¼Œå› ä¸ºé‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è´¡çŒ®è€…è¦ä»¥å„ç§å½¢å¼çš„å¤šæ ·æ€§åœ°æˆé•¿ã€‚&lt;/p>
&lt;!--
Kubernetes has a large distributed remote contributing team, from [individuals and
organizations] all over the world. The Contributor Summits give the community three
chances a year to get together, work on community topics, and have hallway track
time. The upcoming San Diego summit is expected to bring over 450 attendees, and
will contain multiple tracks with something for everyone. The focus will be around
contributor growth and sustainability. We're going to stop here with capacity for
future summits; we want this event to offer value to individuals and the project.
We've heard from past summit attendee feedback that getting work done, learning,
and meeting folks face to face is a priority. By capping attendance and offering
the contributor gatherings in more locations, it will help us achieve those goals.
--->
&lt;p>Kubernetes æ‹¥æœ‰ä¸€ä¸ªåºå¤§çš„åˆ†å¸ƒå¼è¿œç¨‹è´¡çŒ®å›¢é˜Ÿï¼Œç”±æ¥è‡ªä¸–ç•Œå„åœ°çš„ &lt;a href="https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&amp;amp;var-period=y&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&amp;amp;var-companies=All">ä¸ªäººå’Œç»„ç»‡&lt;/a> ç»„æˆã€‚è´¡çŒ®è€…å³°ä¼šæ¯å¹´ä¸ºç¤¾åŒºæä¾›ä¸‰æ¬¡èšä¼šçš„æœºä¼šï¼Œå›´ç»•ç¤¾åŒºä¸»é¢˜å¼€å±•å·¥ä½œï¼Œå¹¶æœ‰äº’ç›¸äº†è§£çš„æ—¶é—´ã€‚å³å°†ä¸¾è¡Œçš„ San Diego å³°ä¼šé¢„è®¡å°†å¸å¼• 450 å¤šåä¸ä¼šè€…ï¼Œå¹¶å°†åŒ…å«å¤šä¸ªæ–¹å‘ï¼Œé€‚åˆæ‰€æœ‰äººã€‚é‡ç‚¹å°†å›´ç»•è´¡çŒ®è€…çš„å¢é•¿å’Œå¯æŒç»­æ€§ã€‚æˆ‘ä»¬å°†åœ¨è¿™é‡Œåœç•™ï¼Œä¸ºä¸¾è¡Œæœªæ¥å³°ä¼šåšå‡†å¤‡ï¼›æˆ‘ä»¬å¸Œæœ›è¿™æ¬¡æ´»åŠ¨ä¸ºä¸ªäººå’Œé¡¹ç›®æä¾›ä»·å€¼ã€‚æˆ‘ä»¬å·²ç»ä»å³°ä¼šä¸ä¼šè€…çš„åé¦ˆä¸­å¾—çŸ¥ï¼Œå®Œæˆå·¥ä½œã€å­¦ä¹ å’Œä¸äººä»¬é¢å¯¹é¢äº¤æµæ˜¯å½“åŠ¡ä¹‹æ€¥ã€‚é€šè¿‡é™åˆ¶å‚åŠ äººæ•°å¹¶åœ¨æ›´å¤šåœ°æ–¹æä¾›è´¡çŒ®è€…èšä¼šï¼Œå°†æœ‰åŠ©äºæˆ‘ä»¬å®ç°è¿™äº›ç›®æ ‡ã€‚&lt;/p>
&lt;!--
This summit is unique as weâ€™ve taken big moves on sustaining ourselves, the
contributor experience events team. Taking a page from the release teamâ€™s playbook,
we have added additional core team and shadow roles making it a natural mentoring
(watching+doing) relationship. The shadows are expected to fill another role at
one of the three events in 2020, and core team members to take the lead.
In preparation for this team, weâ€™ve open sourced our [rolebooks, guidelines,
best practices] and opened up our [meetings] and [project board]. Our team makes up
many parts of the Kubernetes project and takes care of making sure all voices
are represented.
--->
&lt;p>è¿™æ¬¡å³°ä¼šæ˜¯ç‹¬ä¸€æ— äºŒçš„ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è´¡çŒ®è€…ä½“éªŒæ´»åŠ¨å›¢é˜Ÿçš„åšæŒè‡ªæˆ‘æ–¹é¢å·²é‡‡å–äº†é‡å¤§ä¸¾æªã€‚ä»å‘è¡Œå›¢é˜Ÿçš„æ‰‹å†Œä¸­æ‘˜å½•ï¼Œæˆ‘ä»¬æ·»åŠ äº†å…¶ä»–æ ¸å¿ƒå›¢é˜Ÿå’Œè·Ÿéšå­¦ä¹ è§’è‰²ï¼Œä½¿å…¶æˆä¸ºå¤©ç„¶çš„è¾…å¯¼å…³ç³»ï¼ˆåŒ…æ‹¬ç›‘ç£å’Œå®æ–½ï¼‰ã€‚é¢„è®¡è·Ÿéšå­¦ä¹ è€…å°†åœ¨ 2020 å¹´çš„ä¸‰é¡¹æ´»åŠ¨ä¸­æ‰®æ¼”å¦ä¸€ä¸ªè§’è‰²ï¼Œæ ¸å¿ƒå›¢é˜Ÿæˆå‘˜å°†å¸¦å¤´å®Œæˆã€‚ä¸ºè¿™ä¸ªå›¢é˜Ÿåšå‡†å¤‡ï¼Œæˆ‘ä»¬å¼€æºäº† &lt;a href="https://github.com/kubernetes/community/tree/master/events/events-team">æŠ€æœ¯æ‰‹å†Œï¼ŒæŒ‡å—ï¼Œæœ€ä½³åšæ³•&lt;/a>ï¼Œå¹¶å¼€æ”¾äº† &lt;a href="https://docs.google.com/document/d/1oLXv5_rM4f645jlXym_Vd7AUq7x6DV-O87E6tcW1sjU/edit?usp=sharing">ä¼šè®®&lt;/a> å’Œ &lt;a href="https://github.com/orgs/kubernetes/projects/21">é¡¹ç›®å§”å‘˜ä¼š&lt;/a>ã€‚æˆ‘ä»¬çš„å›¢é˜Ÿç»„æˆäº† Kubernetes é¡¹ç›®çš„è®¸å¤šéƒ¨åˆ†ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰å£°éŸ³éƒ½å¾—åˆ°ä½“ç°ã€‚&lt;/p>
&lt;!--
Are you at KubeCon + CloudNativeCon but canâ€™t make it to the summit? Check out
the [SIG Intro and Deep Dive sessions] during KubeCon + CloudNativeCon to
participate in Q&amp;A and hear whatâ€™s up with each Special interest Group (SIG).
Weâ€™ll also record all of Contributor Summitâ€™s presentation sessions, take notes
in discussions, and share it back with you, after the event is complete.
--->
&lt;p>æ‚¨æ˜¯å¦å·²ç»åœ¨ KubeCon + CloudNativeCon ä¸Šï¼Œä½†æ— æ³•å‚ä¸ä¼šè®®ï¼Ÿ åœ¨ KubeCon + CloudNativeCon æœŸé—´æŸ¥çœ‹ &lt;a href="https://kccncna19.sched.com/overview/type/Maintainer+Track+Sessions?iframe=yes">SIGå…¥é—¨å’Œæ·±æ½œè¯¾ç¨‹&lt;/a> å‚ä¸é—®ç­”ï¼Œå¹¶å¬å–æ¯ä¸ªç‰¹æ®Šå…´è¶£å°ç»„ï¼ˆSIGï¼‰çš„æœ€æ–°æ¶ˆæ¯ã€‚æ´»åŠ¨ç»“æŸåï¼Œæˆ‘ä»¬è¿˜å°†è®°å½•æ‰€æœ‰è´¡çŒ®è€…å³°ä¼šçš„è¯¾é¢˜ï¼Œåœ¨è®¨è®ºä¸­åšç¬”è®°ï¼Œå¹¶ä¸æ‚¨åˆ†äº«ã€‚&lt;/p>
&lt;!--
We hope to see you all at Kubernetes Contributor Summit San Diego, make sure you
head over and [register right now]! This event will sell out - hereâ€™s your warning.
:smiley:
--->
&lt;p>æˆ‘ä»¬å¸Œæœ›èƒ½åœ¨ San Diego çš„ Kubernetes è´¡çŒ®è€…å³°ä¼šä¸Šä¸å¤§å®¶è§é¢ï¼Œç¡®ä¿æ‚¨ç›´æ¥è¿›å…¥å¹¶ç‚¹å‡» &lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">ç«‹å³æ³¨å†Œ&lt;/a>ï¼ æ­¤æ´»åŠ¨å°†å…³é—­ - ç‰¹æ­¤æé†’ã€‚ ï¼šç¬‘è„¸ï¼š&lt;/p>
&lt;!--
Check out past blogs on [persona building around our events] and the [Barcelona summit story].
![Group Picture in 2018](/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG)
--->
&lt;p>æŸ¥çœ‹å¾€æœŸåšå®¢æœ‰å…³ &lt;a href="https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/">å›´ç»•æˆ‘ä»¬çš„æ´»åŠ¨æ„å»ºè§’è‰²&lt;/a> å’Œ &lt;a href="https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/">å·´å¡ç½—é‚£å³°ä¼šæ•…äº‹&lt;/a>ã€‚&lt;/p>
&lt;p>ï¼&lt;a href="https://kubernetes.io/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG">2018å¹´é›†ä½“ç…§&lt;/a>&lt;/p>
&lt;!--
*=QuartYard has a huge stage! Want to perform something in front of your contributor peers? Reach out to us! community@kubernetes.io
--->
&lt;p>*=QuartYard æœ‰ä¸€ä¸ªå·¨å¤§çš„èˆå°ï¼æƒ³è¦åœ¨æ‚¨çš„è´¡çŒ®è€…åŒè¡Œé¢å‰åšç‚¹ä»€ä¹ˆï¼ŸåŠ å…¥æˆ‘ä»¬å§ï¼ &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a>&lt;/p></description></item><item><title>Blog: OPA Gatekeeperï¼šKubernetes çš„ç­–ç•¥å’Œç®¡ç†</title><link>https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</guid><description>
&lt;!--
---
layout: blog
title: "OPA Gatekeeper: Policy and Governance for Kubernetes"
date: 2019-08-06
slug: OPA-Gatekeeper-Policy-and-Governance-for-Kubernetes
---
--->
&lt;!--
**Authors:** Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)
--->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong> Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)&lt;/p>
&lt;!--
The [Open Policy Agent Gatekeeper](https://github.com/open-policy-agent/gatekeeper) project can be leveraged to help enforce policies and strengthen governance in your Kubernetes environment. In this post, we will walk through the goals, history, and current state of the project.
--->
&lt;p>å¯ä»¥ä»é¡¹ç›® &lt;a href="https://github.com/open-policy-agent/gatekeeper">Open Policy Agent Gatekeeper&lt;/a> ä¸­è·å¾—å¸®åŠ©ï¼Œåœ¨ Kubernetes ç¯å¢ƒä¸‹å®æ–½ç­–ç•¥å¹¶åŠ å¼ºæ²»ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é€æ­¥ä»‹ç»è¯¥é¡¹ç›®çš„ç›®æ ‡ï¼Œå†å²å’Œå½“å‰çŠ¶æ€ã€‚&lt;/p>
&lt;!--
The following recordings from the Kubecon EU 2019 sessions are a great starting place in working with Gatekeeper:
* [Intro: Open Policy Agent Gatekeeper](https://youtu.be/Yup1FUc2Qn0)
* [Deep Dive: Open Policy Agent](https://youtu.be/n94_FNhuzy4)
--->
&lt;p>ä»¥ä¸‹æ˜¯ Kubecon EU 2019 ä¼šè®®çš„å½•éŸ³ï¼Œå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°å¼€å±•ä¸ Gatekeeper åˆä½œï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/Yup1FUc2Qn0">ç®€ä»‹ï¼šå¼€æ”¾ç­–ç•¥ä»£ç† Gatekeeper&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/n94_FNhuzy4">æ·±å…¥ç ”ç©¶ï¼šå¼€æ”¾ç­–ç•¥ä»£ç†&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Motivations
If your organization has been operating Kubernetes, you probably have been looking for ways to control what end-users can do on the cluster and ways to ensure that clusters are in compliance with company policies. These policies may be there to meet governance and legal requirements or to enforce best practices and organizational conventions. With Kubernetes, how do you ensure compliance without sacrificing development agility and operational independence?
--->
&lt;h2 id="å‡ºå‘ç‚¹">å‡ºå‘ç‚¹&lt;/h2>
&lt;p>å¦‚æœæ‚¨æ‰€åœ¨çš„ç»„ç»‡ä¸€ç›´åœ¨ä½¿ç”¨ Kubernetesï¼Œæ‚¨å¯èƒ½ä¸€ç›´åœ¨å¯»æ‰¾å¦‚ä½•æ§åˆ¶ç»ˆç«¯ç”¨æˆ·åœ¨é›†ç¾¤ä¸Šçš„è¡Œä¸ºï¼Œä»¥åŠå¦‚ä½•ç¡®ä¿é›†ç¾¤ç¬¦åˆå…¬å¸æ”¿ç­–ã€‚è¿™äº›ç­–ç•¥å¯èƒ½éœ€è¦æ»¡è¶³ç®¡ç†å’Œæ³•å¾‹è¦æ±‚ï¼Œæˆ–è€…ç¬¦åˆæœ€ä½³æ‰§è¡Œæ–¹æ³•å’Œç»„ç»‡æƒ¯ä¾‹ã€‚ä½¿ç”¨ Kubernetesï¼Œå¦‚ä½•åœ¨ä¸ç‰ºç‰²å¼€å‘æ•æ·æ€§å’Œè¿è¥ç‹¬ç«‹æ€§çš„å‰æä¸‹ç¡®ä¿åˆè§„æ€§ï¼Ÿ&lt;/p>
&lt;!--
For example, you can enforce policies like:
* All images must be from approved repositories
* All ingress hostnames must be globally unique
* All pods must have resource limits
* All namespaces must have a label that lists a point-of-contact
--->
&lt;p>ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹ç­–ç•¥ï¼š&lt;/p>
&lt;ul>
&lt;li>æ‰€æœ‰é•œåƒå¿…é¡»æ¥è‡ªè·å¾—æ‰¹å‡†çš„å­˜å‚¨åº“&lt;/li>
&lt;li>æ‰€æœ‰å…¥å£ä¸»æœºåå¿…é¡»æ˜¯å…¨å±€å”¯ä¸€çš„&lt;/li>
&lt;li>æ‰€æœ‰ Pod å¿…é¡»æœ‰èµ„æºé™åˆ¶&lt;/li>
&lt;li>æ‰€æœ‰å‘½åç©ºé—´éƒ½å¿…é¡»å…·æœ‰åˆ—å‡ºè”ç³»çš„æ ‡ç­¾&lt;/li>
&lt;/ul>
&lt;!--
Kubernetes allows decoupling policy decisions from the API server by means of [admission controller webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) to intercept admission requests before they are persisted as objects in Kubernetes. [Gatekeeper](https://github.com/open-policy-agent/gatekeeper) was created to enable users to customize admission control via configuration, not code and to bring awareness of the clusterâ€™s state, not just the single object under evaluation at admission time. Gatekeeper is a customizable admission webhook for Kubernetes that enforces policies executed by the [Open Policy Agent (OPA)](https://www.openpolicyagent.org), a policy engine for Cloud Native environments hosted by CNCF.
--->
&lt;p>åœ¨æ¥æ”¶è¯·æ±‚è¢«æŒä¹…åŒ–ä¸º Kubernetes ä¸­çš„å¯¹è±¡ä¹‹å‰ï¼ŒKubernetes å…è®¸é€šè¿‡ &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission controller webhooks&lt;/a> å°†ç­–ç•¥å†³ç­–ä¸ API æœåŠ¡å™¨åˆ†ç¦»ï¼Œä»è€Œæ‹¦æˆªè¿™äº›è¯·æ±‚ã€‚&lt;a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper&lt;/a> åˆ›å»ºçš„ç›®çš„æ˜¯ä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡é…ç½®ï¼ˆè€Œä¸æ˜¯ä»£ç ï¼‰è‡ªå®šä¹‰æ§åˆ¶è®¸å¯ï¼Œå¹¶ä½¿ç”¨æˆ·äº†è§£ç¾¤é›†çš„çŠ¶æ€ï¼Œè€Œä¸ä»…ä»…æ˜¯é’ˆå¯¹è¯„ä¼°çŠ¶æ€çš„å•ä¸ªå¯¹è±¡ï¼Œåœ¨è¿™äº›å¯¹è±¡å‡†è®¸åŠ å…¥çš„æ—¶å€™ã€‚Gatekeeper æ˜¯ Kubernetes çš„ä¸€ä¸ªå¯å®šåˆ¶çš„è®¸å¯ webhook ï¼Œå®ƒç”± &lt;a href="https://www.openpolicyagent.org">Open Policy Agent (OPA)&lt;/a> å¼ºåˆ¶æ‰§è¡Œï¼Œ OPA æ˜¯ Cloud Native ç¯å¢ƒä¸‹çš„ç­–ç•¥å¼•æ“ï¼Œç”± CNCF ä¸»åŠã€‚&lt;/p>
&lt;!--
## Evolution
Before we dive into the current state of Gatekeeper, letâ€™s take a look at how the Gatekeeper project has evolved.
--->
&lt;h2 id="å‘å±•">å‘å±•&lt;/h2>
&lt;p>åœ¨æ·±å…¥äº†è§£ Gatekeeper çš„å½“å‰æƒ…å†µä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ Gatekeeper é¡¹ç›®æ˜¯å¦‚ä½•å‘å±•çš„ã€‚&lt;/p>
&lt;!--
* Gatekeeper v1.0 - Uses OPA as the admission controller with the kube-mgmt sidecar enforcing configmap-based policies. It provides validating and mutating admission control. Donated by Styra.
* Gatekeeper v2.0 - Uses Kubernetes policy controller as the admission controller with OPA and kube-mgmt sidecars enforcing configmap-based policies. It provides validating and mutating admission control and audit functionality. Donated by Microsoft.
* Gatekeeper v3.0 - The admission controller is integrated with the [OPA Constraint Framework](https://github.com/open-policy-agent/frameworks/tree/master/constraint) to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable. Built with kubebuilder, it provides validating and, eventually, mutating (to be implemented) admission control and audit functionality. This enables the creation of policy templates for [Rego](https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/) policies, creation of policies as CRDs, and storage of audit results on policy CRDs. This project is a collaboration between Google, Microsoft, Red Hat, and Styra.
--->
&lt;ul>
&lt;li>Gatekeeper v1.0 - ä½¿ç”¨ OPA ä½œä¸ºå¸¦æœ‰ kube-mgmt sidecar çš„è®¸å¯æ§åˆ¶å™¨ï¼Œç”¨æ¥å¼ºåˆ¶æ‰§è¡ŒåŸºäº configmap çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å®ç°äº†éªŒè¯å’Œè½¬æ¢è®¸å¯æ§åˆ¶ã€‚è´¡çŒ®æ–¹ï¼šStyra&lt;/li>
&lt;li>Gatekeeper v2.0 - ä½¿ç”¨ Kubernetes ç­–ç•¥æ§åˆ¶å™¨ä½œä¸ºè®¸å¯æ§åˆ¶å™¨ï¼ŒOPA å’Œ kube-mgmt sidecar å®æ–½åŸºäº configmap çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å®ç°äº†éªŒè¯å’Œè½¬æ¢å‡†å…¥æ§åˆ¶å’Œå®¡æ ¸åŠŸèƒ½ã€‚è´¡çŒ®æ–¹ï¼šMicrosoft&lt;/li>
&lt;li>Gatekeeper v3.0 - å‡†å…¥æ§åˆ¶å™¨ä¸ &lt;a href="https://github.com/open-policy-agent/frameworks/tree/master/constraint">OPA Constraint Framework&lt;/a> é›†æˆåœ¨ä¸€èµ·ï¼Œç”¨æ¥å®æ–½åŸºäº CRD çš„ç­–ç•¥ï¼Œå¹¶å¯ä»¥å¯é åœ°å…±äº«å·²å®Œæˆå£°æ˜é…ç½®çš„ç­–ç•¥ã€‚ä½¿ç”¨ kubebuilder è¿›è¡Œæ„å»ºï¼Œå®ç°äº†éªŒè¯ä»¥åŠæœ€ç»ˆè½¬æ¢ï¼ˆå¾…å®Œæˆï¼‰ä¸ºè®¸å¯æ§åˆ¶å’Œå®¡æ ¸åŠŸèƒ½ã€‚è¿™æ ·å°±å¯ä»¥ä¸º &lt;a href="https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/">Rego&lt;/a> ç­–ç•¥åˆ›å»ºç­–ç•¥æ¨¡æ¿ï¼Œå°†ç­–ç•¥åˆ›å»ºä¸º CRD å¹¶å­˜å‚¨å®¡æ ¸ç»“æœåˆ°ç­–ç•¥ CRD ä¸Šã€‚è¯¥é¡¹ç›®æ˜¯ Googleï¼ŒMicrosoftï¼ŒRed Hat å’Œ Styra åˆä½œå®Œæˆçš„ã€‚&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-08-06-opa-gatekeeper/v3.png" alt="">&lt;/p>
&lt;!--
## Gatekeeper v3.0 Features
Now letâ€™s take a closer look at the current state of Gatekeeper and how you can leverage all the latest features. Consider an organization that wants to ensure all objects in a cluster have departmental information provided as part of the objectâ€™s labels. How can you do this with Gatekeeper?
--->
&lt;h2 id="gatekeeper-v3-0-çš„åŠŸèƒ½">Gatekeeper v3.0 çš„åŠŸèƒ½&lt;/h2>
&lt;p>ç°åœ¨æˆ‘ä»¬è¯¦ç»†çœ‹ä¸€ä¸‹ Gatekeeper å½“å‰çš„çŠ¶æ€ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨æ‰€æœ‰æœ€æ–°çš„åŠŸèƒ½ã€‚å‡è®¾ä¸€ä¸ªç»„ç»‡å¸Œæœ›ç¡®ä¿é›†ç¾¤ä¸­çš„æ‰€æœ‰å¯¹è±¡éƒ½æœ‰ department ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯æ˜¯å¯¹è±¡æ ‡ç­¾çš„ä¸€éƒ¨åˆ†ã€‚å¦‚ä½•åˆ©ç”¨ Gatekeeper å®Œæˆè¿™é¡¹éœ€æ±‚ï¼Ÿ&lt;/p>
&lt;!--
### Validating Admission Control
Once all the Gatekeeper components have been [installed](https://github.com/open-policy-agent/gatekeeper) in your cluster, the API server will trigger the Gatekeeper admission webhook to process the admission request whenever a resource in the cluster is created, updated, or deleted.
During the validation process, Gatekeeper acts as a bridge between the API server and OPA. The API server will enforce all policies executed by OPA.
--->
&lt;h3 id="éªŒè¯è®¸å¯æ§åˆ¶">éªŒè¯è®¸å¯æ§åˆ¶&lt;/h3>
&lt;p>åœ¨é›†ç¾¤ä¸­æ‰€æœ‰ Gatekeeper ç»„ä»¶éƒ½ &lt;a href="https://github.com/open-policy-agent/gatekeeper">å®‰è£…&lt;/a> å®Œæˆä¹‹åï¼Œåªè¦é›†ç¾¤ä¸­çš„èµ„æºè¿›è¡Œåˆ›å»ºã€æ›´æ–°æˆ–åˆ é™¤ï¼ŒAPI æœåŠ¡å™¨å°†è§¦å‘ Gatekeeper å‡†å…¥ webhook æ¥å¤„ç†å‡†å…¥è¯·æ±‚ã€‚&lt;/p>
&lt;p>åœ¨éªŒè¯è¿‡ç¨‹ä¸­ï¼ŒGatekeeper å……å½“ API æœåŠ¡å™¨å’Œ OPA ä¹‹é—´çš„æ¡¥æ¢ã€‚API æœåŠ¡å™¨å°†å¼ºåˆ¶å®æ–½ OPA æ‰§è¡Œçš„æ‰€æœ‰ç­–ç•¥ã€‚&lt;/p>
&lt;!--
### Policies and Constraints
With the integration of the OPA Constraint Framework, a Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.
--->
&lt;h3 id="ç­–ç•¥ä¸-constraint">ç­–ç•¥ä¸ Constraint&lt;/h3>
&lt;p>ç»“åˆ OPA Constraint Frameworkï¼ŒConstraint æ˜¯ä¸€ä¸ªå£°æ˜ï¼Œè¡¨ç¤ºä½œè€…å¸Œæœ›ç³»ç»Ÿæ»¡è¶³ç»™å®šçš„ä¸€ç³»åˆ—è¦æ±‚ã€‚Constraint éƒ½ä½¿ç”¨ Rego ç¼–å†™ï¼ŒRego æ˜¯å£°æ˜æ€§æŸ¥è¯¢è¯­è¨€ï¼ŒOPA ç”¨ Rego æ¥æšä¸¾è¿èƒŒç³»ç»Ÿé¢„æœŸçŠ¶æ€çš„æ•°æ®å®ä¾‹ã€‚æ‰€æœ‰ Constraint éƒ½éµå¾ªé€»è¾‘ ANDã€‚å‡ä½¿æœ‰ä¸€ä¸ª Constraint ä¸æ»¡è¶³ï¼Œé‚£ä¹ˆæ•´ä¸ªè¯·æ±‚éƒ½å°†è¢«æ‹’ç»ã€‚&lt;/p>
&lt;!--
Before defining a Constraint, you need to create a Constraint Template that allows people to declare new Constraints. Each template describes both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint, much like arguments to a function.
For example, here is a Constraint template CRD that requires certain labels to be present on an arbitrary object.
--->
&lt;p>åœ¨å®šä¹‰ Constraint ä¹‹å‰ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª Constraint Templateï¼Œå…è®¸å¤§å®¶å£°æ˜æ–°çš„ Constraintã€‚æ¯ä¸ªæ¨¡æ¿éƒ½æè¿°äº†å¼ºåˆ¶æ‰§è¡Œ Constraint çš„ Rego é€»è¾‘å’Œ Constraint çš„æ¨¡å¼ï¼Œå…¶ä¸­åŒ…æ‹¬ CRD çš„æ¨¡å¼å’Œä¼ é€’åˆ° enforces ä¸­çš„å‚æ•°ï¼Œå°±åƒå‡½æ•°çš„å‚æ•°ä¸€æ ·ã€‚&lt;/p>
&lt;p>ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ª Constraint æ¨¡æ¿ CRDï¼Œå®ƒçš„è¯·æ±‚æ˜¯åœ¨ä»»æ„å¯¹è±¡ä¸Šæ˜¾ç¤ºæŸäº›æ ‡ç­¾ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>templates.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ConstraintTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">crd&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">names&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">listKind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabelsList&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">plural&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">singular&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">validation&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Schema for the `parameters` field&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>array&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">items&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targets&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">target&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admission.k8s.gatekeeper.sh&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rego&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>|&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;span style="color:#b44;font-style:italic"> package k8srequiredlabels
&lt;/span>&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;span style="color:#b44;font-style:italic"> deny[{&amp;#34;msg&amp;#34;: msg, &amp;#34;details&amp;#34;: {&amp;#34;missing_labels&amp;#34;: missing}}] {
&lt;/span>&lt;span style="color:#b44;font-style:italic"> provided := {label | input.review.object.metadata.labels[label]}
&lt;/span>&lt;span style="color:#b44;font-style:italic"> required := {label | label := input.parameters.labels[_]}
&lt;/span>&lt;span style="color:#b44;font-style:italic"> missing := required - provided
&lt;/span>&lt;span style="color:#b44;font-style:italic"> count(missing) &amp;gt; 0
&lt;/span>&lt;span style="color:#b44;font-style:italic"> msg := sprintf(&amp;#34;you must provide labels: %v&amp;#34;, [missing])
&lt;/span>&lt;span style="color:#b44;font-style:italic"> }&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Once a Constraint template has been deployed in the cluster, an admin can now create individual Constraint CRDs as defined by the Constraint template. For example, here is a Constraint CRD that requires the label `hr` to be present on all namespaces.
--->
&lt;p>åœ¨é›†ç¾¤ä¸­éƒ¨ç½²äº† Constraint æ¨¡æ¿åï¼Œç®¡ç†å‘˜ç°åœ¨å¯ä»¥åˆ›å»ºç”± Constraint æ¨¡æ¿å®šä¹‰çš„å•ä¸ª Constraint CRDã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œä»¥ä¸‹æ˜¯ä¸€ä¸ª Constraint CRDï¼Œè¦æ±‚æ ‡ç­¾ &lt;code>hr&lt;/code> å‡ºç°åœ¨æ‰€æœ‰å‘½åç©ºé—´ä¸Šã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-hr&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Similarly, another Constraint CRD that requires the label `finance` to be present on all namespaces can easily be created from the same Constraint template.
--->
&lt;p>ç±»ä¼¼åœ°ï¼Œå¯ä»¥ä»åŒä¸€ä¸ª Constraint æ¨¡æ¿è½»æ¾åœ°åˆ›å»ºå¦ä¸€ä¸ª Constraint CRDï¼Œè¯¥ Constraint CRD è¦æ±‚æ‰€æœ‰å‘½åç©ºé—´ä¸Šéƒ½æœ‰ &lt;code>finance&lt;/code> æ ‡ç­¾ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-finance&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;finance&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
As you can see, with the Constraint framework, we can reliably share Regos via the Constraint templates, define the scope of enforcement with the match field, and provide user-defined parameters to the Constraints to create customized behavior for each Constraint.
--->
&lt;p>å¦‚æ‚¨æ‰€è§ï¼Œä½¿ç”¨ Constraint frameworkï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ Constraint æ¨¡æ¿å¯é åœ°å…±äº« regoï¼Œä½¿ç”¨åŒ¹é…å­—æ®µå®šä¹‰æ‰§è¡ŒèŒƒå›´ï¼Œå¹¶ä¸º Constraint æä¾›ç”¨æˆ·å®šä¹‰çš„å‚æ•°ï¼Œä»è€Œä¸ºæ¯ä¸ª Constraint åˆ›å»ºè‡ªå®šä¹‰è¡Œä¸ºã€‚&lt;/p>
&lt;!--
### Audit
The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as `violations` listed in the `status` field of the relevant Constraint. --->
&lt;h3 id="å®¡æ ¸">å®¡æ ¸&lt;/h3>
&lt;p>æ ¹æ®ç¾¤é›†ä¸­å¼ºåˆ¶æ‰§è¡Œçš„ Constraintï¼Œå®¡æ ¸åŠŸèƒ½å¯å®šæœŸè¯„ä¼°å¤åˆ¶çš„èµ„æºï¼Œå¹¶æ£€æµ‹å…ˆå‰å­˜åœ¨çš„é”™è¯¯é…ç½®ã€‚Gatekeeper å°†å®¡æ ¸ç»“æœå­˜å‚¨ä¸º &lt;code>violations&lt;/code>ï¼Œåœ¨ç›¸å…³ Constraint çš„ &lt;code>status&lt;/code> å­—æ®µä¸­åˆ—å‡ºã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-hr&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">auditTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2019-08-06T01:46:13Z&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">byPod&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">enforced&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">id&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gatekeeper-controller-manager-0&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">violations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">enforcementAction&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>deny&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Namespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">message: &amp;#39;you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: default
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: gatekeeper-system
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: kube-public
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&amp;#39;&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Data Replication
Audit requires replication of Kubernetes resources into OPA before they can be evaluated against the enforced Constraints. Data replication is also required by Constraints that need access to objects in the cluster other than the object under evaluation. For example, a Constraint that enforces uniqueness of ingress hostname must have access to all other ingresses in the cluster.
--->
&lt;h3 id="æ•°æ®å¤åˆ¶">æ•°æ®å¤åˆ¶&lt;/h3>
&lt;p>å®¡æ ¸è¦æ±‚å°† Kubernetes å¤åˆ¶åˆ° OPA ä¸­ï¼Œç„¶åæ‰èƒ½æ ¹æ®å¼ºåˆ¶çš„ Constraint å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æ•°æ®å¤åˆ¶åŒæ ·ä¹Ÿéœ€è¦ Constraintï¼Œè¿™äº› Constraint éœ€è¦è®¿é—®é›†ç¾¤ä¸­é™¤è¯„ä¼°å¯¹è±¡ä¹‹å¤–çš„å¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª Constraint è¦å¼ºåˆ¶ç¡®å®šå…¥å£ä¸»æœºåçš„å”¯ä¸€æ€§ï¼Œå°±å¿…é¡»æœ‰æƒè®¿é—®é›†ç¾¤ä¸­çš„æ‰€æœ‰å…¶ä»–å…¥å£ã€‚&lt;/p>
&lt;!--
To configure Kubernetes data to be replicated, create a sync config resource with the resources to be replicated into OPA. For example, the below configuration replicates all namespace and pod resources to OPA.
--->
&lt;p>å¯¹ Kubernetes æ•°æ®è¿›è¡Œå¤åˆ¶ï¼Œè¯·ä½¿ç”¨å¤åˆ¶åˆ° OPA ä¸­çš„èµ„æºåˆ›å»º sync config èµ„æºã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„é…ç½®å°†æ‰€æœ‰å‘½åç©ºé—´å’Œ Pod èµ„æºå¤åˆ¶åˆ° OPAã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>config.gatekeeper.sh/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;gatekeeper-system&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">sync&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">syncOnly&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Pod&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## Planned for Future
The community behind the Gatekeeper project will be focusing on providing mutating admission control to support mutation scenarios (for example: annotate objects automatically with departmental information when creating a new resource), support external data to inject context external to the cluster into the admission decisions, support dry run to see impact of a policy on existing resources in the cluster before enforcing it, and more audit functionalities.
--->
&lt;h2 id="æœªæ¥è®¡åˆ’">æœªæ¥è®¡åˆ’&lt;/h2>
&lt;p>Gatekeeper é¡¹ç›®èƒŒåçš„ç¤¾åŒºå°†ä¸“æ³¨äºæä¾›è½¬æ¢è®¸å¯æ§åˆ¶ï¼Œå¯ä»¥ç”¨æ¥æ”¯æŒè½¬æ¢æ–¹æ¡ˆï¼ˆä¾‹å¦‚ï¼šåœ¨åˆ›å»ºæ–°èµ„æºæ—¶ä½¿ç”¨ department ä¿¡æ¯è‡ªåŠ¨æ³¨é‡Šå¯¹è±¡ï¼‰ï¼Œæ”¯æŒå¤–éƒ¨æ•°æ®ä»¥å°†é›†ç¾¤å¤–éƒ¨ç¯å¢ƒåŠ å…¥åˆ°è®¸å¯å†³ç­–ä¸­ï¼Œæ”¯æŒè¯•è¿è¡Œä»¥ä¾¿åœ¨æ‰§è¡Œç­–ç•¥ä¹‹å‰äº†è§£ç­–ç•¥å¯¹é›†ç¾¤ä¸­ç°æœ‰èµ„æºçš„å½±å“ï¼Œè¿˜æœ‰æ›´å¤šçš„å®¡æ ¸åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
If you are interested in learning more about the project, check out the [Gatekeeper](https://github.com/open-policy-agent/gatekeeper) repo. If you are interested in helping define the direction of Gatekeeper, join the [#kubernetes-policy](https://openpolicyagent.slack.com/messages/CDTN970AX) channel on OPA Slack, and join our [weekly meetings](https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit) to discuss development, issues, use cases, etc.
--->
&lt;p>å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šæœ‰å…³è¯¥é¡¹ç›®çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ &lt;a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper&lt;/a> å­˜å‚¨åº“ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£å¸®åŠ©ç¡®å®š Gatekeeper çš„æ–¹å‘ï¼Œè¯·åŠ å…¥ &lt;a href="https://openpolicyagent.slack.com/messages/CDTN970AX">#kubernetes-policy&lt;/a> OPA Slack é¢‘é“ï¼Œå¹¶åŠ å…¥æˆ‘ä»¬çš„ &lt;a href="https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit">å‘¨ä¼š&lt;/a> ä¸€åŒè®¨è®ºå¼€å‘ã€ä»»åŠ¡ã€ç”¨ä¾‹ç­‰ã€‚&lt;/p></description></item><item><title>Blog: æ¬¢è¿å‚åŠ åœ¨ä¸Šæµ·ä¸¾è¡Œçš„è´¡çŒ®è€…å³°ä¼š</title><link>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid><description>
&lt;!-- ---
layout: blog
title: 'Join us at the Contributor Summit in Shanghai'
date: 2019-06-11
--- -->
&lt;p>&lt;strong>Author&lt;/strong>: Josh Berkus (Red Hat)&lt;/p>
&lt;!-- ![Picture of contributor panel at 2018 Shanghai contributor summit. Photo by Josh Berkus, licensed CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png) -->
&lt;p>![è´¡çŒ®è€…å°ç»„è®¨è®ºæ å½±ï¼Œæ‘„äº 2018 å¹´ä¸Šæµ·è´¡çŒ®è€…å³°ä¼šï¼Œä½œè€… Josh Berkus, è®¸å¯è¯ CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png)&lt;/p>
&lt;!-- For the second year, we will have [a Contributor Summit event](https://www.lfasiallc.com/events/contributors-summit-china-2019/) the day before [KubeCon China](https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/) in Shanghai. If you already contribute to Kubernetes or would like to contribute, please consider attending and [register](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/). The Summit will be held June 24th, at the Shanghai Expo Center (the same location where KubeCon will take place), and will include a Current Contributor Day as well as the New Contributor Workshop and the Documentation Sprints. -->
&lt;p>è¿ç»­ç¬¬äºŒå¹´ï¼Œæˆ‘ä»¬å°†åœ¨ &lt;a href="https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/">KubeCon China&lt;/a> ä¹‹å‰ä¸¾è¡Œä¸€å¤©çš„ &lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/">è´¡çŒ®è€…å³°ä¼š&lt;/a>ã€‚
ä¸ç®¡æ‚¨æ˜¯å¦å·²ç»æ˜¯ä¸€å Kubernetes è´¡çŒ®è€…ï¼Œè¿˜æ˜¯æƒ³è¦åŠ å…¥ç¤¾åŒºé˜Ÿä¼ï¼Œè´¡çŒ®ä¸€ä»½åŠ›é‡ï¼Œéƒ½è¯·è€ƒè™‘&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">æ³¨å†Œ&lt;/a>å‚åŠ è¿™æ¬¡æ´»åŠ¨ã€‚
è¿™æ¬¡å³°ä¼šå°†äºå…­æœˆ 24 å·ï¼Œåœ¨ä¸Šæµ·ä¸–åšä¸­å¿ƒï¼ˆå’Œ KubeCon çš„ä¸¾åŠåœ°ç‚¹ç›¸åŒï¼‰ä¸¾è¡Œï¼Œ
ä¸€å¤©çš„æ´»åŠ¨å°†åŒ…å«â€œç°æœ‰è´¡çŒ®è€…æ´»åŠ¨â€ï¼Œä»¥åŠâ€œæ–°è´¡çŒ®è€…å·¥ä½œåŠâ€å’Œâ€œæ–‡æ¡£å°ç»„æ´»åŠ¨â€ã€‚&lt;/p>
&lt;!-- ### Current Contributor Day -->
&lt;h3 id="ç°æœ‰è´¡çŒ®è€…æ´»åŠ¨">ç°æœ‰è´¡çŒ®è€…æ´»åŠ¨&lt;/h3>
&lt;!-- After last year's Contributor Day, our team received feedback that many of our contributors in Asia and Oceania would like content for current contributors as well. As such, we have added a Current Contributor track to the schedule. -->
&lt;p>å»å¹´çš„è´¡çŒ®è€…èŠ‚ä¹‹åï¼Œæˆ‘ä»¬çš„å›¢é˜Ÿæ”¶åˆ°äº†å¾ˆå¤šåé¦ˆæ„è§ï¼Œå¾ˆå¤šäºšæ´²å’Œå¤§æ´‹æ´²çš„è´¡çŒ®è€…ä¹Ÿæƒ³è¦é’ˆå¯¹å½“å‰è´¡çŒ®è€…çš„å³°ä¼šå†…å®¹ã€‚
æœ‰é‰´äºæ­¤ï¼Œæˆ‘ä»¬åœ¨ä»Šå¹´çš„å®‰æ’ä¸­åŠ å…¥äº†å½“å‰è´¡çŒ®è€…çš„ä¸»é¢˜ã€‚&lt;/p>
&lt;!-- While we do not yet have a full schedule up, the topics covered in the current contributor track will include: -->
&lt;p>å°½ç®¡æˆ‘ä»¬è¿˜æ²¡æœ‰ä¸€ä¸ªå®Œæ•´çš„æ—¶é—´å®‰æ’ï¼Œä¸‹é¢æ˜¯å½“å‰è´¡çŒ®è€…ä¸»é¢˜æ‰€ä¼šåŒ…å«çš„è¯é¢˜ï¼š&lt;/p>
&lt;!-- * How to write a KEP (Kubernetes Enhancement Proposal)
* Codebase and repository review
* Local Build &amp; Test troubleshooting session
* Guide to Non-Code Contribution opportunities
* SIG-Azure face-to-face meeting
* SIG-Scheduling face-to-face meeting
* Other SIG face-to-face meetings as we confirm them -->
&lt;ul>
&lt;li>å¦‚ä½•æ’°å†™ Kubernetes æ”¹è¿›è®®æ¡ˆ (KEP)&lt;/li>
&lt;li>ä»£ç åº“ç ”ä¹ &lt;/li>
&lt;li>æœ¬åœ°æ„å»ºä»¥åŠæµ‹è¯•è°ƒè¯•&lt;/li>
&lt;li>ä¸å†™ä»£ç çš„è´¡çŒ®æœºä¼š&lt;/li>
&lt;li>SIG-Azure é¢å¯¹é¢äº¤æµ&lt;/li>
&lt;li>SIG-Scheduling é¢å¯¹é¢äº¤æµ&lt;/li>
&lt;li>å…¶ä»–å…´è¶£å°ç»„çš„é¢å¯¹é¢æœºä¼š&lt;/li>
&lt;/ul>
&lt;!-- The schedule will be on [the Community page](https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit) once it is complete. -->
&lt;p>æ•´ä¸ªè®¡åˆ’å®‰æ’å°†ä¼šåœ¨å®Œå…¨ç¡®å®šä¹‹åï¼Œæ•´ç†æ”¾åœ¨&lt;a href="https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit">ç¤¾åŒºé¡µé¢&lt;/a>ä¸Šã€‚&lt;/p>
&lt;!-- If your SIG wants to have a face-to-face meeting at Kubecon Shanghai, please contact [Josh Berkus](mailto:jberkus@redhat.com). -->
&lt;p>å¦‚æœæ‚¨çš„ SIG æƒ³è¦åœ¨ Kubecon Shanghai ä¸Šè¿›è¡Œé¢å¯¹é¢çš„äº¤æµï¼Œè¯·è”ç³» &lt;a href="mailto:jberkus@redhat.com">Josh Berkus&lt;/a>ã€‚&lt;/p>
&lt;!-- ### New Contributor Workshop -->
&lt;h3 id="æ–°è´¡çŒ®è€…å·¥ä½œåŠ">æ–°è´¡çŒ®è€…å·¥ä½œåŠ&lt;/h3>
&lt;!-- Students at [last year's New Contributor Workshop](/blog/2018/12/05/new-contributor-workshop-shanghai/) (NCW) found it to be extremely valuable, and the event helped to orient a few of the many Asian and Pacific developers looking to participate in the Kubernetes community. -->
&lt;p>å‚ä¸è¿‡&lt;a href="https://kubernetes.io/blog/2018/12/05/new-contributor-workshop-shanghai/">å»å¹´æ–°è´¡çŒ®è€…å·¥ä½œåŠï¼ˆNCWï¼‰&lt;/a>çš„å­¦ç”Ÿè§‰å¾—è¿™é¡¹æ´»åŠ¨éå¸¸çš„æœ‰ä»·å€¼ï¼Œ
è¿™é¡¹æ´»åŠ¨ä¹Ÿå¸®åŠ©ã€å¼•å¯¼äº†å¾ˆå¤šäºšæ´²å’Œå¤§æ´‹æ´²çš„å¼€å‘è€…æ›´å¤šåœ°å‚ä¸åˆ° Kubernetes ç¤¾åŒºä¹‹ä¸­ã€‚&lt;/p>
&lt;!-- > "It's a one-stop-shop for becoming familiar with the community." said one participant. -->
&lt;blockquote>
&lt;p>â€œè¿™æ¬¡æ´»åŠ¨å¯ä»¥è®©äººä¸€æ¬¡å¿«é€Ÿç†Ÿæ‚‰ç¤¾åŒºã€‚â€å…¶ä¸­çš„ä¸€ä½å‚ä¸è€…æåˆ°ã€‚&lt;/p>
&lt;/blockquote>
&lt;!-- If you have not contributed to Kubernetes before, or have only done one or two things, please consider [enrolling](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) in the NCW. -->
&lt;p>å¦‚æœæ‚¨ä¹‹å‰ä»æ²¡æœ‰å‚ä¸è¿‡ Kubernetes çš„è´¡çŒ®ï¼Œæˆ–è€…åªæ˜¯åšè¿‡ä¸€æ¬¡æˆ–ä¸¤æ¬¡è´¡çŒ®ï¼Œéƒ½è¯·è€ƒè™‘&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">æ³¨å†Œå‚åŠ &lt;/a>æ–°è´¡çŒ®è€…å·¥ä½œåŠã€‚&lt;/p>
&lt;!-- > "Got to know the process from signing CLA to PR and made friends with other contributors." said another. -->
&lt;blockquote>
&lt;p>â€œç†Ÿæ‚‰äº†ä» CLA åˆ° PR çš„æ•´ä¸ªæµç¨‹ï¼Œä¹Ÿè®¤è¯†ç»“äº¤äº†å¾ˆå¤šè´¡çŒ®è€…ã€‚â€å¦ä¸€ä½å¼€å‘è€…æåˆ°ã€‚&lt;/p>
&lt;/blockquote>
&lt;!-- ### Documentation Sprints -->
&lt;h3 id="æ–‡æ¡£å°ç»„æ´»åŠ¨">æ–‡æ¡£å°ç»„æ´»åŠ¨&lt;/h3>
&lt;!-- Both old and new contributors on our Docs Team will spend a day both improving our documentation and translating it into other languages. If you are interested in having better documentation, fully localized into Chinese and other languages, please [sign up](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) to help with the Doc Sprints. -->
&lt;p>æ–‡æ¡£å°ç»„çš„æ–°è€è´¡çŒ®è€…éƒ½ä¼šèšé¦–ä¸€å¤©ï¼Œè®¨è®ºå¦‚ä½•æå‡æ–‡æ¡£è´¨é‡ï¼Œä»¥åŠå°†æ–‡æ¡£ç¿»è¯‘æˆæ›´å¤šçš„è¯­è¨€ã€‚
å¦‚æœæ‚¨å¯¹ç¿»è¯‘æ–‡æ¡£ï¼Œå°†è¿™äº›çŸ¥è¯†å’Œä¿¡æ¯ç¿»è¯‘æˆä¸­æ–‡å’Œå…¶ä»–è¯­è¨€æ„Ÿå…´è¶£çš„è¯ï¼Œè¯·åœ¨è¿™é‡Œ&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">æ³¨å†Œ&lt;/a>ï¼ŒæŠ¥åå‚åŠ æ–‡æ¡£å°ç»„æ´»åŠ¨ã€‚&lt;/p>
&lt;!-- ### Before you attend -->
&lt;h3 id="å‚ä¸ä¹‹å‰">å‚ä¸ä¹‹å‰&lt;/h3>
&lt;!-- Regardless of where you participate, everyone at the Contributor Summit should [sign the Kubernetes Contributor License Agreement](https://git.k8s.io/community/CLA.md#the-contributor-license-agreement) (CLA) before coming to the conference. You should also bring a laptop suitable for working on documentation or code development. -->
&lt;p>ä¸è®ºæ‚¨å‚ä¸çš„æ˜¯å“ªä¸€é¡¹æ´»åŠ¨ï¼Œæ‰€æœ‰äººéƒ½éœ€è¦åœ¨åˆ°è¾¾è´¡çŒ®è€…å³°ä¼šå‰ç­¾ç½² &lt;a href="https://git.k8s.io/community/CLA.md#the-contributor-license-agreement">Kubernetes CLA&lt;/a>ã€‚
æ‚¨ä¹ŸåŒæ—¶éœ€è¦è€ƒè™‘å¸¦ä¸€ä¸ªåˆé€‚çš„ç¬”è®°æœ¬ç”µè„‘ï¼Œå¸®åŠ©æ–‡æ¡£å†™ä½œæˆ–æ˜¯ç¼–ç¨‹å¼€å‘ã€‚&lt;/p></description></item><item><title>Blog: å£®å¤§æˆ‘ä»¬çš„è´¡çŒ®è€…ç ”è®¨ä¼š</title><link>https://kubernetes.io/zh/blog/2019/05/14/expanding-our-contributor-workshops/</link><pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/05/14/expanding-our-contributor-workshops/</guid><description>
&lt;!--
---
layout: blog
title: "Expanding our Contributor Workshops"
date: 2019-05-14
slug: expanding-our-contributor-workshops
---
-->
&lt;!--
**Authors:** Guinevere Saenger (GitHub) and Paris Pittman (Google)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> Guinevere Saenger (GitHub) å’Œ Paris Pittman (Google)ï¼ˆè°·æ­Œï¼‰&lt;/p>
&lt;!--
**tl;dr** - learn about the contributor community with us and land your first PR! We have spots available in [Barcelona][eu] (registration **closes** on Wednesday May 15, so grab your spot!) and the upcoming [Shanghai][cn] Summit.
-->
&lt;p>&lt;strong>tl;dr&lt;/strong> - ä¸æˆ‘ä»¬ä¸€èµ·äº†è§£è´¡çŒ®è€…ç¤¾åŒºï¼Œå¹¶è·å¾—ä½ çš„ç¬¬ä¸€ä»½ PR ! æˆ‘ä»¬åœ¨[å·´å¡ç½—é‚£][æ¬§æ´²]æœ‰ç©ºä½ï¼ˆç™»è®° åœ¨5æœˆ15å·å‘¨ä¸‰&lt;strong>ç»“æŸ&lt;/strong>ï¼Œæ‰€ä»¥æŠ“ä½è¿™æ¬¡æœºä¼šï¼ï¼‰å¹¶ä¸”åœ¨[ä¸Šæµ·][ä¸­å›½]æœ‰å³å°†åˆ°æ¥çš„å³°ä¼šã€‚&lt;/p>
&lt;!--
The Barcelona event is poised to be our biggest one yet, with more registered attendees than ever before!
-->
&lt;p>å·´å¡ç½—é‚£çš„æ´»åŠ¨å°†æ˜¯æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¸€æ¬¡ï¼Œç™»è®°çš„å‚ä¸è€…æ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½å¤šï¼&lt;/p>
&lt;!--
Have you always wanted to contribute to Kubernetes, but not sure where to begin?
Have you seen our communityâ€™s many code bases and seen places to improve? We have a workshop for you!
-->
&lt;p>ä½ æ˜¯å¦æ›¾ç»æƒ³ä¸º Kubernetes åšè´¡çŒ®ï¼Œä½†ä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹ï¼Ÿä½ æœ‰æ²¡æœ‰æ›¾ç»çœ‹è¿‡æˆ‘ä»¬ç¤¾åŒºçš„è®¸å¤šä»£ç åº“å¹¶è®¤ä¸ºéœ€è¦æ”¹è¿›çš„åœ°æ–¹ï¼Ÿæˆ‘ä»¬ä¸ºä½ å‡†å¤‡äº†ä¸€ä¸ªå·¥ä½œå®¤ï¼&lt;/p>
&lt;!--
KubeCon + CloudNativeCon Barcelonaâ€™s [new contributor workshop][ncw] will be the fourth one of its kind, and weâ€™re really looking forward to it!
-->
&lt;p>KubeCon + CloudNativeCon å·´å¡ç½—é‚£çš„ç ”è®¨ä¼š&lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/">æ–°è´¡çŒ®è€…ç ”è®¨ä¼š&lt;/a>å°†æ˜¯ç¬¬å››ä¸ªè¿™æ ·çš„ç ”è®¨ä¼šï¼Œæˆ‘ä»¬çœŸçš„å¾ˆæœŸå¾…ï¼&lt;/p>
&lt;!--
The workshop was kickstarted last year at KubeConEU in Copenhagen, and so far we have taken it to Shanghai and Seattle, and now Barcelona, as well as some non-KubeCon locations.
-->
&lt;p>è¿™ä¸ªç ”è®¨ä¼šå»å¹´åœ¨å“¥æœ¬å“ˆæ ¹çš„ KubeConEU å¯åŠ¨ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»æŠŠå®ƒå¸¦åˆ°äº†ä¸Šæµ·å’Œè¥¿é›…å›¾ï¼Œç°åœ¨æ˜¯å·´å¡ç½—é‚£ï¼Œä»¥åŠä¸€äº›é KubeConEU çš„åœ°æ–¹ã€‚&lt;/p>
&lt;!--
We are constantly updating and improving the workshop content based on feedback from past sessions.
-->
&lt;p>æˆ‘ä»¬æ ¹æ®ä»¥å¾€è¯¾ç¨‹çš„åé¦ˆï¼Œä¸æ–­æ›´æ–°å’Œæ”¹è¿›ç ”è®¨ä¼šçš„å†…å®¹ã€‚&lt;/p>
&lt;!--
This time, weâ€™re breaking up the participants by their experience and comfort level with open source and Kubernetes.
-->
&lt;p>è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬å°†æ ¹æ®å‚ä¸è€…å¯¹å¼€æºå’Œ Kubernetes çš„ç»éªŒå’Œé€‚åº”ç¨‹åº¦æ¥è¿›è¡Œåˆ’åˆ†ã€‚&lt;/p>
&lt;!--
Weâ€™ll have developer setup and project workflow support for folks entirely new to open
source and Kubernetes as part of the 101 track, and hope to set up each
participant with their very own first issue to work on.
-->
&lt;p>ä½œä¸º101è¯¾ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä¸ºå®Œå…¨ä¸ç†Ÿæ‚‰å¼€æºå’Œ Kubernetes çš„äººæä¾›å¼€å‘äººå‘˜è®¾ç½®å’Œé¡¹ç›®å·¥ä½œæµæ”¯æŒï¼Œå¹¶å¸Œæœ›ä¸ºæ¯ä¸ªå‚ä¸è€…å¼€å±•ä»–ä»¬è‡ªå·±çš„ç¬¬ä¸€æœŸå·¥ä½œã€‚&lt;/p>
&lt;!--
In the 201 track, we will have a codebase walkthrough and local development and test demonstration for folks who have a bit more experience in open source but may be unfamiliar with our communityâ€™s development tools.
-->
&lt;p>åœ¨201è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä¸ºé‚£äº›åœ¨å¼€æºæ–¹é¢æœ‰æ›´å¤šç»éªŒä½†å¯èƒ½ä¸ç†Ÿæ‚‰æˆ‘ä»¬ç¤¾åŒºå¼€å‘å·¥å…·çš„äººè¿›è¡Œä»£ç åº“æ¼”ç»ƒå’Œæœ¬åœ°å¼€å‘å’Œæµ‹è¯•æ¼”ç¤ºã€‚&lt;/p>
&lt;!--
For both tracks, you will have a chance to get your hands dirty and have some fun. Because not every contributor works with code, and not every contribution is technical, we will spend the beginning of the workshop learning how our project is structured and organized, where to
find the right people, and where to get help when stuck.
-->
&lt;p>å¯¹äºè¿™ä¸¤é—¨è¯¾ç¨‹ï¼Œä½ å°†æœ‰æœºä¼šäº²è‡ªåŠ¨æ‰‹å¹¶ä½“ä¼šåˆ°å…¶ä¸­çš„ä¹è¶£ã€‚å› ä¸ºä¸æ˜¯æ¯ä¸ªè´¡çŒ®è€…éƒ½ä½¿ç”¨ä»£ç ï¼Œä¹Ÿä¸æ˜¯æ¯é¡¹è´¡çŒ®éƒ½æ˜¯æŠ€æœ¯æ€§çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†åœ¨ç ”è®¨ä¼šå¼€å§‹æ—¶å­¦ä¹ å¦‚ä½•æ„å»ºå’Œç»„ç»‡é¡¹ç›®ï¼Œä»¥åŠå¦‚ä½•è¿›è¡Œæ‰¾åˆ°åˆé€‚çš„äººï¼Œä»¥åŠé‡åˆ°å›°éš¾æ—¶åœ¨å“ªé‡Œå¯»æ±‚å¸®åŠ©ã€‚&lt;/p>
&lt;!--
## Mentoring Opportunities
-->
&lt;h2 id="è¾…å¯¼æœºä¼š">è¾…å¯¼æœºä¼š&lt;/h2>
&lt;!--
We will also bring back the SIG Meet-and-Greet where new contributors will have a chance to mingle with current contributors, perhaps find their dream SIG, learn what exciting areas they can help with, gain mentors, and make friends.
-->
&lt;p>æˆ‘ä»¬è¿˜å°†å›å½’ SIG Meet-and-Greetï¼Œåœ¨è¿™é‡Œæ–°å…¥é—¨çš„èœé¸Ÿè´¡çŒ®è€…å°†æœ‰æœºä¼šä¸å½“å€¼çš„è´¡çŒ®è€…äº¤æµï¼Œè¿™ä¹Ÿè®¸ä¼šè®©ä»–ä»¬æ‰¾åˆ°ä»–ä»¬æ¢¦æƒ³çš„ SIGï¼Œäº†è§£ä»–ä»¬å¯ä»¥å¸®åŠ©å“ªäº›æ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸï¼Œè·å¾—å¯¼å¸ˆï¼Œç»“äº¤æœ‹å‹ã€‚&lt;/p>
&lt;!--
PS - there are also two mentoring sessions DURING KubeCon + CloudNativeCon on Thursday, May 23. [Sign up here][mentor]. 60% of the attendees during the Seattle event asked contributor questions.
-->
&lt;p>PS - 5æœˆ23æ—¥æ˜ŸæœŸå››ï¼Œåœ¨ KubeCon+CloudNativeCon ä¼šè®®æœŸé—´ä¼šæœ‰ä¸¤æ¬¡å¯¼å¸ˆä¼šè®®ã€‚[åœ¨è¿™é‡Œæ³¨å†Œ][å¯¼å¸ˆ]ã€‚åœ¨è¥¿é›…å›¾æ´»åŠ¨æœŸé—´ï¼Œ60% çš„ä¸ä¼šè€…ä¼šå‘è´¡çŒ®è€…æé—®ã€‚&lt;/p>
&lt;!--
## Past Attendee Story - Vallery Lancy, Engineer at Lyft
-->
&lt;h2 id="æ›¾ç»ä¸ä¼šè€…çš„æ•…äº‹-vallery-lancy-lyft-çš„å·¥ç¨‹å¸ˆ">æ›¾ç»ä¸ä¼šè€…çš„æ•…äº‹ - Vallery Lancyï¼ŒLyft çš„å·¥ç¨‹å¸ˆ&lt;/h2>
&lt;!--
We talked to a few of our past participants in a series of interviews that we will publish throughout the course of the year.
-->
&lt;p>åœ¨ä¸€ç³»åˆ—é‡‡è®¿ä¸­ï¼Œæˆ‘ä»¬ä¸ä¸€äº›è¿‡å»çš„å‚ä¸è€…è¿›è¡Œäº†äº¤è°ˆï¼Œè¿™äº›é‡‡è®¿å°†åœ¨ä»Šå¹´å…¨å¹´å…¬å¸ƒã€‚&lt;/p>
&lt;!--
In our first two clips, we meet Vallery Lancy, an Engineer at Lyft and one of 75 attendees at our recent Seattle edition of the workshop. She was poking around in the community for a while to see where she could jump in.
-->
&lt;p>åœ¨æˆ‘ä»¬çš„å‰ä¸¤ä¸ªç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬ä¼šæåˆ° Vallery Lancyï¼ŒLyft å…¬å¸çš„å·¥ç¨‹å¸ˆï¼Œä¹Ÿæ˜¯æˆ‘ä»¬æœ€è¿‘è¥¿é›…å›¾ç‰ˆç ”è®¨ä¼šçš„75åä¸ä¼šè€…ä¹‹ä¸€ã€‚å¥¹åœ¨ç¤¾åŒºé‡Œé—²é€›äº†ä¸€æ®µæ—¶é—´ï¼Œæƒ³çœ‹çœ‹èƒ½ä¸èƒ½æŠ•èº«åˆ°æŸä¸ªé¢†åŸŸå½“ä¸­ã€‚&lt;/p>
&lt;!--
Watch Vallery talk about her experience here:
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/uKg5WUcl6WU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
-->
&lt;p>åœ¨è¿™é‡Œè§‚çœ‹ Vallery è®²è¿°å¥¹çš„ç»å†ï¼š&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/uKg5WUcl6WU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>&lt;/p>
&lt;!--
What does Vallery say to folks curious about the workshops, or those attending the Barcelona edition?
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/niHiem7JmPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
-->
&lt;p>Vallery å’Œé‚£äº›å¯¹ç ”è®¨ä¼šæ„Ÿå…´è¶£çš„äººæˆ–è€…å‚åŠ å·´å¡ç½—é‚£ä¼šè®®çš„äººè¯´äº†äº›ä»€ä¹ˆï¼Ÿ&lt;/p>
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/niHiem7JmPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
&lt;!--
Be like Vallery and hundreds of previous New Contributor Workshop attendees: join us in Barcelona (or Shanghai - or San Diego!) for a unique experience without digging into our documentation!
[eu]: https://events.linuxfoundation.org/events/contributor-summit-europe-2019/
[cn]: https://www.lfasiallc.com/events/contributors-summit-china-2019/
[ncw]: https://events.linuxfoundation.org/events/contributor-summit-europe-2019/
[mentor]: http://bit.ly/mentor-bcn
-->
&lt;p>æƒ³å˜å¾—å’Œ Vallery è¿˜æœ‰æ•°ç™¾åå‚ä¸ä¹‹å‰çš„æ–°è´¡çŒ®è€…ç ”è®¨ä¼šçš„ä¸ä¼šè€…ä¸€æ ·çš„è¯ï¼šåœ¨å·´å¡ç½—é‚£ï¼ˆæˆ–è€…ä¸Šæµ·-æˆ–è€…åœ£åœ°äºšå“¥ï¼ï¼‰åŠ å…¥æˆ‘ä»¬ï¼ä½ ä¼šæœ‰ä¸€ä¸ªä¸ä¼—ä¸åŒçš„ä½“éªŒï¼Œè€Œä¸å†æ˜¯æ­»è¯»æˆ‘ä»¬çš„æ–‡æ¡£ï¼&lt;/p>
&lt;p>Have the opportunity to meet with the experts and go step by step into your journey with your peers around you. Weâ€™re looking forward to seeing you there! &lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/">Register here&lt;/a>
--&amp;gt;
ä½ å°†æœ‰æœºä¼šä¸ä¸“å®¶è§é¢ï¼Œå¹¶ä¸å‘¨å›´çš„åŒé¾„äººä¸€æ­¥æ­¥èµ°ä¸Šå±äºä½ çš„é“è·¯ã€‚æˆ‘ä»¬æœŸå¾…ç€åœ¨é‚£é‡Œä¸ä½ è§é¢ï¼&lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/"> åœ¨è¿™é‡Œæ³¨å†Œ&lt;/a>&lt;/p></description></item><item><title>Blog: å¦‚ä½•å‚ä¸ Kubernetes æ–‡æ¡£çš„æœ¬åœ°åŒ–å·¥ä½œ</title><link>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</link><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</guid><description>
&lt;p>&lt;strong>ä½œè€…: Zach Corleissenï¼ˆLinux åŸºé‡‘ä¼šï¼‰&lt;/strong>&lt;/p>
&lt;p>å»å¹´æˆ‘ä»¬å¯¹ Kubernetes ç½‘ç«™è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒåŠ å…¥äº†&lt;a href="https://kubernetes.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/">å¤šè¯­è¨€å†…å®¹çš„æ”¯æŒ&lt;/a>ã€‚è´¡çŒ®è€…ä»¬è¸Šè·ƒå“åº”ï¼ŒåŠ å…¥äº†å¤šç§æ–°çš„æœ¬åœ°åŒ–å†…å®¹ï¼šæˆªè‡³ 2019 å¹´ 4 æœˆï¼ŒKubernetes æ–‡æ¡£æœ‰äº† 9 ä¸ªä¸åŒè¯­è¨€çš„æœªå®Œæˆç‰ˆæœ¬ï¼Œå…¶ä¸­æœ‰ 6 ä¸ªæ˜¯ 2019 å¹´åŠ å…¥çš„ã€‚åœ¨æ¯ä¸ª Kubernetes æ–‡æ¡£é¡µé¢çš„ä¸Šæ–¹ï¼Œè¯»è€…éƒ½å¯ä»¥çœ‹åˆ°ä¸€ä¸ªè¯­è¨€é€‰æ‹©å™¨ï¼Œå…¶ä¸­åˆ—å‡ºäº†æ‰€æœ‰å¯ç”¨è¯­è¨€ã€‚&lt;/p>
&lt;p>ä¸è®ºæ˜¯å®Œæˆåº¦æœ€é«˜çš„&lt;a href="https://v1-12.docs.kubernetes.io/zh/">ä¸­æ–‡ç‰ˆ v1.12&lt;/a>ï¼Œè¿˜æ˜¯æœ€æ–°åŠ å…¥çš„&lt;a href="https://kubernetes.io/pt/">è‘¡è„ç‰™æ–‡ç‰ˆ v1.14&lt;/a>ï¼Œå„è¯­è¨€çš„æœ¬åœ°åŒ–å†…å®¹è¿˜æœªå®Œæˆï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›è¡Œä¸­çš„é¡¹ç›®ã€‚å¦‚æœè¯»è€…æœ‰å…´è¶£å¯¹ç°æœ‰æœ¬åœ°åŒ–å·¥ä½œæä¾›æ”¯æŒï¼Œè¯·ç»§ç»­é˜…è¯»ã€‚&lt;/p>
&lt;h2 id="ä»€ä¹ˆæ˜¯æœ¬åœ°åŒ–">ä»€ä¹ˆæ˜¯æœ¬åœ°åŒ–&lt;/h2>
&lt;p>ç¿»è¯‘æ˜¯ä»¥è¯è¡¨æ„çš„é—®é¢˜ã€‚è€Œæœ¬åœ°åŒ–åœ¨æ­¤åŸºç¡€ä¹‹ä¸Šï¼Œè¿˜åŒ…å«äº†è¿‡ç¨‹å’Œè®¾è®¡æ–¹é¢çš„å·¥ä½œã€‚&lt;/p>
&lt;p>æœ¬åœ°åŒ–å’Œç¿»è¯‘å¾ˆåƒï¼Œä½†æ˜¯åŒ…å«æ›´å¤šå†…å®¹ã€‚é™¤äº†è¿›è¡Œç¿»è¯‘ä¹‹å¤–ï¼Œæœ¬åœ°åŒ–è¿˜è¦ä¸ºç¼–å†™å’Œå‘å¸ƒè¿‡ç¨‹çš„æ¡†æ¶è¿›è¡Œä¼˜åŒ–ã€‚ä¾‹å¦‚ï¼ŒKubernetes.io å¤šæ•°çš„ç«™ç‚¹æµè§ˆåŠŸèƒ½ï¼ˆæŒ‰é’®æ–‡å­—ï¼‰éƒ½ä¿å­˜åœ¨&lt;a href="https://github.com/kubernetes/website/tree/master/i18n">å•ç‹¬çš„æ–‡ä»¶&lt;/a>ä¹‹ä¸­ã€‚æ‰€ä»¥å¯åŠ¨æ–°æœ¬åœ°åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åŒ…å«åŠ å…¥å¯¹ç‰¹å®šæ–‡ä»¶ä¸­å­—ç¬¦ä¸²è¿›è¡Œç¿»è¯‘çš„å·¥ä½œã€‚&lt;/p>
&lt;p>æœ¬åœ°åŒ–å¾ˆé‡è¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆçš„é™ä½ Kubernetes çš„é‡‡çº³å’Œæ”¯æŒé—¨æ§›ã€‚å¦‚æœèƒ½ç”¨æ¯è¯­é˜…è¯» Kubernetes æ–‡æ¡£ï¼Œå°±èƒ½æ›´è½»æ¾çš„å¼€å§‹ä½¿ç”¨ Kubernetesï¼Œå¹¶å¯¹å…¶å‘å±•ä½œå‡ºè´¡çŒ®ã€‚&lt;/p>
&lt;h2 id="å¦‚ä½•å¯åŠ¨æœ¬åœ°åŒ–å·¥ä½œ">å¦‚ä½•å¯åŠ¨æœ¬åœ°åŒ–å·¥ä½œ&lt;/h2>
&lt;p>ä¸åŒè¯­è¨€çš„æœ¬åœ°åŒ–å·¥ä½œéƒ½æ˜¯å•ç‹¬çš„åŠŸèƒ½â€”â€”å’Œå…¶å®ƒ Kubernetes åŠŸèƒ½ä¸€è‡´ï¼Œè´¡çŒ®è€…ä»¬åœ¨ä¸€ä¸ª SIG ä¸­è¿›è¡Œæœ¬åœ°åŒ–å·¥ä½œï¼Œåˆ†äº«å‡ºæ¥è¿›è¡Œè¯„å®¡ï¼Œå¹¶åŠ å…¥é¡¹ç›®ã€‚&lt;/p>
&lt;p>è´¡çŒ®è€…ä»¬åœ¨å›¢é˜Ÿä¸­è¿›è¡Œå†…å®¹çš„æœ¬åœ°åŒ–å·¥ä½œã€‚å› ä¸ºè‡ªå·±ä¸èƒ½æ‰¹å‡†è‡ªå·±çš„ PRï¼Œæ‰€ä»¥ä¸€ä¸ªæœ¬åœ°åŒ–å›¢é˜Ÿè‡³å°‘åº”è¯¥æœ‰ä¸¤ä¸ªäººâ€”â€”ä¾‹å¦‚æ„å¤§åˆ©æ–‡çš„æœ¬åœ°åŒ–å›¢é˜Ÿæœ‰ä¸¤ä¸ªäººã€‚è¿™ä¸ªå›¢é˜Ÿè§„æ¨¡å¯èƒ½å¾ˆå¤§ï¼šä¸­æ–‡å›¢é˜Ÿæœ‰å‡ åä¸ªæˆå‘˜ã€‚&lt;/p>
&lt;p>æ¯ä¸ªå›¢é˜Ÿéƒ½æœ‰è‡ªå·±çš„å·¥ä½œæµã€‚æœ‰äº›å›¢é˜Ÿæ‰‹å·¥å®Œæˆæ‰€æœ‰çš„å†…å®¹ç¿»è¯‘ï¼›æœ‰äº›ä¼šä½¿ç”¨å¸¦æœ‰ç¿»è¯‘æ’ä»¶çš„ç¼–è¯‘å™¨ï¼Œå¹¶ä½¿ç”¨è¯„å®¡æœºæ¥æä¾›æ­£ç¡®æ€§çš„ä¿éšœã€‚SIG Docs ä¸“æ³¨äºè¾“å‡ºçš„æ ‡å‡†ï¼›è¿™å°±ç»™äº†æœ¬åœ°åŒ–å›¢é˜Ÿé‡‡ç”¨é€‚åˆè‡ªå·±å·¥ä½œæƒ…å†µçš„å·¥ä½œæµã€‚è¿™æ ·ä¸€æ¥ï¼Œå›¢é˜Ÿå¯ä»¥æ ¹æ®æœ€ä½³å®è·µè¿›è¡Œåä½œï¼Œå¹¶ä»¥ Kubernetes çš„ç¤¾åŒºç²¾ç¥è¿›è¡Œåˆ†äº«ã€‚&lt;/p>
&lt;h2 id="ä¸ºæœ¬åœ°åŒ–å·¥ä½œæ·»ç –åŠ ç“¦">ä¸ºæœ¬åœ°åŒ–å·¥ä½œæ·»ç –åŠ ç“¦&lt;/h2>
&lt;p>å¦‚æœä½ æœ‰å…´è¶£ä¸º Kubernetes æ–‡æ¡£åŠ å…¥æ–°è¯­ç§çš„æœ¬åœ°åŒ–å†…å®¹ï¼Œ&lt;a href="https://kubernetes.io/docs/contribute/localization/">Kubernetes contribution guide&lt;/a> ä¸­åŒ…å«äº†è¿™æ–¹é¢çš„ç›¸å…³å†…å®¹ã€‚&lt;/p>
&lt;p>å·²ç»å¯åŠ¨çš„çš„æœ¬åœ°åŒ–å·¥ä½œåŒæ ·éœ€è¦æ”¯æŒã€‚å¦‚æœæœ‰å…´è¶£ä¸ºç°å­˜é¡¹ç›®åšå‡ºè´¡çŒ®ï¼Œå¯ä»¥åŠ å…¥æœ¬åœ°åŒ–å›¢é˜Ÿçš„ Slack é¢‘é“ï¼Œå»åšä¸ªè‡ªæˆ‘ä»‹ç»ã€‚å„å›¢é˜Ÿçš„æˆå‘˜ä¼šå¸®åŠ©ä½ å¼€å§‹å·¥ä½œã€‚&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>è¯­ç§&lt;/th>
&lt;th>Slack é¢‘é“&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ä¸­æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CE3LNFYJ1/">#kubernetes-docs-zh&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>è‹±æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/C1J0BPD2M/">#sig-docs&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>æ³•æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CG838BFT9/">#kubernetes-docs-fr&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>å¾·æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CH4UJ2BAL/">#kubernetes-docs-de&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>å°åœ°&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ14B9BDJ/">#kubernetes-docs-hi&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>å°åº¦å°¼è¥¿äºšæ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ1LUCUHM/">#kubernetes-docs-id&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>æ„å¤§åˆ©æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CGB1MCK7X/">#kubernetes-docs-it&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>æ—¥æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CAG2M83S8/">#kubernetes-docs-ja&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>éŸ©æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CA1MMR86S/">#kubernetes-docs-ko&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>è‘¡è„ç‰™æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ21AS0NA/">#kubernetes-docs-pt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>è¥¿ç­ç‰™æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CH7GB2E3B/">#kubernetes-docs-es&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="ä¸‹ä¸€æ­¥">ä¸‹ä¸€æ­¥ï¼Ÿ&lt;/h2>
&lt;p>æœ€æ–°çš„&lt;a href="https://kubernetes.slack.com/messages/CJ14B9BDJ/">å°åœ°æ–‡æœ¬åœ°åŒ–&lt;/a>å·¥ä½œæ­£åœ¨å¯åŠ¨ã€‚ä¸ºä»€ä¹ˆä¸åŠ å…¥ä½ çš„è¯­è¨€ï¼Ÿ&lt;/p>
&lt;p>èº«ä¸º SIG Docs çš„ä¸»å¸­ï¼Œæˆ‘ç”šè‡³å¸Œæœ›æœ¬åœ°åŒ–å·¥ä½œè·³å‡ºæ–‡æ¡£èŒƒç•´ï¼Œç›´æ¥ä¸º Kubernetes ç»„ä»¶æä¾›æœ¬åœ°åŒ–æ”¯æŒã€‚æœ‰ä»€ä¹ˆç»„ä»¶æ˜¯ä½ å¸Œæœ›æ”¯æŒä¸åŒè¯­è¨€çš„ä¹ˆï¼Ÿå¯ä»¥æäº¤ä¸€ä¸ª &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">Kubernetes Enhancement Proposal&lt;/a> æ¥ä¿ƒæˆè¿™ä¸€è¿›æ­¥ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes 1.14 ç¨³å®šæ€§æ”¹è¿›ä¸­çš„è¿›ç¨‹IDé™åˆ¶</title><link>https://kubernetes.io/zh/blog/2019/04/15/kubernetes-1.14-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%94%B9%E8%BF%9B%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8Bid%E9%99%90%E5%88%B6/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/04/15/kubernetes-1.14-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%94%B9%E8%BF%9B%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8Bid%E9%99%90%E5%88%B6/</guid><description>
&lt;!--
---
title: 'Process ID Limiting for Stability Improvements in Kubernetes 1.14'
date: 2019-04-15
---
-->
&lt;!--
**Author: Derek Carr**
Have you ever seen someone take more than their fair share of the cookies? The one person who reaches in and grabs a half dozen fresh baked chocolate chip chunk morsels and skitters off like Cookie Monster exclaiming â€œOm nom nom nom.â€
In some rare workloads, a similar occurrence was taking place inside Kubernetes clusters. With each Pod and Node, there comes a finite number of possible process IDs (PIDs) for all applications to share. While it is rare for any one process or pod to reach in and grab all the PIDs, some users were experiencing resource starvation due to this type of behavior. So in Kubernetes 1.14, we introduced an enhancement to mitigate the risk of a single pod monopolizing all of the PIDs available.
-->
&lt;p>&lt;strong>ä½œè€…: Derek Carr&lt;/strong>&lt;/p>
&lt;p>ä½ æ˜¯å¦è§è¿‡æœ‰äººæ‹¿èµ°äº†æ¯”å±äºä»–ä»¬é‚£ä¸€ä»½æ›´å¤šçš„é¥¼å¹²ï¼Ÿ ä¸€ä¸ªäººèµ°è¿‡æ¥ï¼ŒæŠ“èµ·åŠæ‰“æ–°é²œçƒ¤åˆ¶çš„å¤§å—å·§å…‹åŠ›é¥¼å¹²ç„¶ååŒ†åŒ†ç¦»å»ï¼Œå°±åƒé¥¼å¹²æ€ªå…½å¤§å–Š â€œOm nom nom nomâ€ã€‚&lt;/p>
&lt;p>åœ¨ä¸€äº›ç½•è§çš„å·¥ä½œè´Ÿè½½ä¸­ï¼ŒKubernetes é›†ç¾¤å†…éƒ¨ä¹Ÿå‘ç”Ÿäº†ç±»ä¼¼çš„æƒ…å†µã€‚æ¯ä¸ª Pod å’Œ Node éƒ½æœ‰æœ‰é™æ•°é‡çš„å¯èƒ½çš„è¿›ç¨‹ IDï¼ˆPIDï¼‰ï¼Œä¾›æ‰€æœ‰åº”ç”¨ç¨‹åºå…±äº«ã€‚å°½ç®¡å¾ˆå°‘æœ‰è¿›ç¨‹æˆ– Pod èƒ½å¤Ÿè¿›å…¥å¹¶è·å–æ‰€æœ‰ PIDï¼Œä½†ç”±äºè¿™ç§è¡Œä¸ºï¼Œä¸€äº›ç”¨æˆ·ä¼šé‡åˆ°èµ„æºåŒ®ä¹çš„æƒ…å†µã€‚ å› æ­¤ï¼Œåœ¨ Kubernetes 1.14 ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹å¢å¼ºåŠŸèƒ½ï¼Œä»¥é™ä½å•ä¸ª Pod å„æ–­æ‰€æœ‰å¯ç”¨ PID çš„é£é™©ã€‚&lt;/p>
&lt;!--
## Can You Spare Some PIDs?
Here, weâ€™re talking about the greed of certain containers. Outside the ideal, runaway processes occur from time to time, particularly in clusters where testing is taking place. Thus, some wildly non-production-ready activity is happening.
In such a scenario, itâ€™s possible for something akin to a fork bomb taking place inside a node. As resources slowly erode, being taken over by some zombie-like process that continually spawns children, other legitimate workloads begin to get bumped in favor of this inflating balloon of wasted processing power. This could result in other processes on the same pod being starved of their needed PIDs. It could also lead to interesting side effects as a node could fail and a replica of that pod is scheduled to a new machine where the process repeats across your entire cluster.
-->
&lt;h2 id="ä½ èƒ½é¢„ç•™ä¸€äº›-pids-å—">ä½ èƒ½é¢„ç•™ä¸€äº› PIDs å—ï¼Ÿ&lt;/h2>
&lt;p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è°ˆè®ºçš„æ˜¯æŸäº›å®¹å™¨çš„è´ªå©ªæ€§ã€‚ åœ¨ç†æƒ³æƒ…å†µä¹‹å¤–ï¼Œå¤±æ§è¿›ç¨‹æœ‰æ—¶ä¼šå‘ç”Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨æµ‹è¯•é›†ç¾¤ä¸­ã€‚ å› æ­¤ï¼Œåœ¨è¿™äº›é›†ç¾¤ä¸­ä¼šå‘ç”Ÿä¸€äº›æ··ä¹±çš„éç”Ÿäº§ç¯å¢ƒå‡†å¤‡å°±ç»ªçš„äº‹æƒ…ã€‚&lt;/p>
&lt;p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šåœ¨èŠ‚ç‚¹å†…éƒ¨å‘ç”Ÿç±»ä¼¼äº fork ç‚¸å¼¹è€—å°½ PID çš„æ”»å‡»ã€‚éšç€èµ„æºçš„ç¼“æ…¢è…èš€ï¼Œè¢«ä¸€äº›ä¸æ–­äº§ç”Ÿå­è¿›ç¨‹çš„åƒµå°¸èˆ¬çš„è¿›ç¨‹æ‰€æ¥ç®¡ï¼Œå…¶ä»–æ­£å¸¸çš„å·¥ä½œè´Ÿè½½ä¼šå› ä¸ºè¿™äº›åƒæ°”çƒèˆ¬ä¸æ–­è†¨èƒ€çš„æµªè´¹çš„å¤„ç†èƒ½åŠ›è€Œå¼€å§‹å—åˆ°å†²å‡»ã€‚è¿™å¯èƒ½å¯¼è‡´åŒä¸€ Pod ä¸Šçš„å…¶ä»–è¿›ç¨‹ç¼ºå°‘æ‰€éœ€çš„ PIDã€‚è¿™ä¹Ÿå¯èƒ½å¯¼è‡´æœ‰è¶£çš„å‰¯ä½œç”¨ï¼Œå› ä¸ºèŠ‚ç‚¹å¯èƒ½ä¼šå‘ç”Ÿæ•…éšœï¼Œå¹¶ä¸”è¯¥Podçš„å‰¯æœ¬å°†å®‰æ’åˆ°æ–°çš„æœºå™¨ä¸Šï¼Œè‡³æ­¤ï¼Œè¯¥è¿‡ç¨‹å°†åœ¨æ•´ä¸ªç¾¤é›†ä¸­é‡å¤è¿›è¡Œã€‚&lt;/p>
&lt;!--
## Fixing the Problem
Thus, in Kubernetes 1.14, we have added a feature that allows for the configuration of a kubelet to limit the number of PIDs a given pod can consume. If that machine supports 32,768 PIDs and 100 pods, one can give each pod a budget of 300 PIDs to prevent total exhaustion of PIDs. If the admin wants to overcommit PIDs similar to cpu or memory, they may do so as well with some additional risks. Either way, no one pod can bring the whole machine down. This will generally prevent against simple fork bombs from taking over your cluster.
This change allows administrators to protect one pod from another, but does not ensure if all pods on the machine can protect the node, and the node agents themselves from falling over. Thus, weâ€™ve introduced a feature in this release in alpha form that provides isolation of PIDs from end user workloads on a pod from the node agents (kubelet, runtime, etc.). The admin is able to reserve a specific number of PIDs--similar to how one reserves CPU or memory today--and ensure they are never consumed by pods on that machine. Once that graduates from alpha, to beta, then stable in future releases of Kubernetes, weâ€™ll have protection against an easily starved Linux resource.
Get started with [Kubernetes 1.14](https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0).
-->
&lt;h2 id="è§£å†³é—®é¢˜">è§£å†³é—®é¢˜&lt;/h2>
&lt;p>å› æ­¤ï¼Œåœ¨ Kubernetes 1.14 ä¸­ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªç‰¹æ€§ï¼Œå…è®¸é€šè¿‡é…ç½® kubeletï¼Œé™åˆ¶ç»™å®š Pod å¯ä»¥æ¶ˆè€—çš„ PID æ•°é‡ã€‚å¦‚æœè¯¥æœºå™¨æ”¯æŒ 32768 ä¸ª PIDs å’Œ 100 ä¸ª Podï¼Œåˆ™å¯ä»¥ä¸ºæ¯ä¸ª Pod æä¾› 300 ä¸ª PIDs çš„é¢„ç®—ï¼Œä»¥é˜²æ­¢ PIDs å®Œå…¨è€—å°½ã€‚å¦‚æœç®¡ç†å‘˜æƒ³è¦åƒ CPU æˆ–å†…å­˜é‚£æ ·è¿‡åº¦ä½¿ç”¨ PIDsï¼Œé‚£ä¹ˆä»–ä»¬ä¹Ÿå¯ä»¥é…ç½®è¶…é¢ä½¿ç”¨ï¼Œä½†æ˜¯è¿™æ ·ä¼šæœ‰ä¸€äº›é¢å¤–é£é™©ã€‚ä¸ç®¡æ€æ ·ï¼Œæ²¡æœ‰ä¸€ä¸ªPodèƒ½æåæ•´ä¸ªæœºå™¨ã€‚è¿™é€šå¸¸ä¼šé˜²æ­¢ç®€å•çš„åˆ†å‰å‡½æ•°ç‚¸å¼¹æ¥ç®¡ä½ çš„é›†ç¾¤ã€‚&lt;/p>
&lt;p>æ­¤æ›´æ”¹å…è®¸ç®¡ç†å‘˜ä¿æŠ¤ä¸€ä¸ª Pod ä¸å—å¦ä¸€ä¸ª Pod çš„å½±å“ï¼Œä½†ä¸èƒ½ç¡®ä¿è®¡ç®—æœºä¸Šçš„æ‰€æœ‰ Pod éƒ½èƒ½ä¿æŠ¤èŠ‚ç‚¹å’ŒèŠ‚ç‚¹ä»£ç†æœ¬èº«ä¸å—å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ä»¥ Alpha çš„å½¢å¼å¼•å…¥äº†è¿™ä¸ªä¸€ä¸ªç‰¹æ€§ï¼Œå®ƒæä¾›äº† PIDs åœ¨èŠ‚ç‚¹ä»£ç†ï¼ˆ kubeletã€runtime ç­‰ï¼‰ä¸ Pod ä¸Šçš„æœ€ç»ˆç”¨æˆ·å·¥ä½œè´Ÿè½½çš„åˆ†ç¦»ã€‚ç®¡ç†å‘˜å¯ä»¥é¢„å®šç‰¹å®šæ•°é‡çš„ pidï¼ˆç±»ä¼¼äºä»Šå¤©å¦‚ä½•é¢„å®š CPU æˆ–å†…å­˜ï¼‰ï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä¸ä¼šè¢«è¯¥è®¡ç®—æœºä¸Šçš„ pod æ¶ˆè€—ã€‚ä¸€æ—¦ä» Alpha è¿›å…¥åˆ° Betaï¼Œç„¶ååœ¨å°†æ¥çš„ Kubernetes ç‰ˆæœ¬ä¸­ç¨³å®šä¸‹æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨è¿™ä¸ªç‰¹æ€§é˜²æ­¢ Linux èµ„æºè€—å°½ã€‚&lt;/p>
&lt;p>å¼€å§‹ä½¿ç”¨ &lt;a href="https://github.com/Kubernetes/Kubernetes/releases/tag/v1.14.0">Kubernetes 1.14&lt;/a>ã€‚&lt;/p>
&lt;!--
## Get Involved
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Node Special Interest Group](https://github.com/kubernetes/community/tree/master/sig-node).
### About the author:
Derek Carr is Senior Principal Software Engineer at Red Hat. He is a Kubernetes contributor and member of the Kubernetes Community Steering Committee.
-->
&lt;p>##å‚ä¸å…¶ä¸­&lt;/p>
&lt;p>å¦‚æœæ‚¨å¯¹æ­¤ç‰¹æ€§æœ‰åé¦ˆæˆ–æœ‰å…´è¶£å‚ä¸å…¶è®¾è®¡ä¸å¼€å‘ï¼Œè¯·åŠ å…¥[èŠ‚ç‚¹ç‰¹åˆ«å…´è¶£å°ç»„](&lt;a href="https://github.com/kubernetes/community/tree/master/sig">https://github.com/kubernetes/community/tree/master/sig&lt;/a> Node)ã€‚&lt;/p>
&lt;p>###å…³äºä½œè€…ï¼š
Derek Carr æ˜¯ Red Hat é«˜çº§é¦–å¸­è½¯ä»¶å·¥ç¨‹å¸ˆã€‚ä»–ä¹Ÿæ˜¯ Kubernetes çš„è´¡çŒ®è€…å’Œ Kubernetes ç¤¾åŒºæŒ‡å¯¼å§”å‘˜ä¼šçš„æˆå‘˜ã€‚&lt;/p></description></item><item><title>Blog: Raw Block Volume æ”¯æŒè¿›å…¥ Beta</title><link>https://kubernetes.io/zh/blog/2019/03/07/raw-block-volume-%E6%94%AF%E6%8C%81%E8%BF%9B%E5%85%A5-beta/</link><pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/03/07/raw-block-volume-%E6%94%AF%E6%8C%81%E8%BF%9B%E5%85%A5-beta/</guid><description>
&lt;!--
---
title: Raw Block Volume support to Beta
date: 2019-03-07
---
--->
&lt;!--
**Authors:**
Ben Swartzlander (NetApp), Saad Ali (Google)
Kubernetes v1.13 moves raw block volume support to beta. This feature allows persistent volumes to be exposed inside containers as a block device instead of as a mounted file system.
--->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong>
Ben Swartzlander (NetApp), Saad Ali (Google)&lt;/p>
&lt;p>Kubernetes v1.13 ä¸­å¯¹åŸç”Ÿæ•°æ®å—å·ï¼ˆRaw Block Volumeï¼‰çš„æ”¯æŒè¿›å…¥ Beta é˜¶æ®µã€‚æ­¤åŠŸèƒ½å…è®¸å°†æŒä¹…å·ä½œä¸ºå—è®¾å¤‡è€Œä¸æ˜¯ä½œä¸ºå·²æŒ‚è½½çš„æ–‡ä»¶ç³»ç»Ÿæš´éœ²åœ¨å®¹å™¨å†…éƒ¨ã€‚&lt;/p>
&lt;!--
## What are block devices?
Block devices enable random access to data in fixed-size blocks. Hard drives, SSDs, and CD-ROMs drives are all examples of block devices.
Typically persistent storage is implemented in a layered maner with a file system (like ext4) on top of a block device (like a spinning disk or SSD). Applications then read and write files instead of operating on blocks. The operating systems take care of reading and writing files, using the specified filesystem, to the underlying device as blocks.
It's worth noting that while whole disks are block devices, so are disk partitions, and so are LUNs from a storage area network (SAN) device.
--->
&lt;h2 id="ä»€ä¹ˆæ˜¯å—è®¾å¤‡">ä»€ä¹ˆæ˜¯å—è®¾å¤‡ï¼Ÿ&lt;/h2>
&lt;p>å—è®¾å¤‡å…è®¸å¯¹å›ºå®šå¤§å°çš„å—ä¸­çš„æ•°æ®è¿›è¡Œéšæœºè®¿é—®ã€‚ç¡¬ç›˜é©±åŠ¨å™¨ã€SSD å’Œ CD-ROM é©±åŠ¨å™¨éƒ½æ˜¯å—è®¾å¤‡çš„ä¾‹å­ã€‚&lt;/p>
&lt;p>é€šå¸¸ï¼ŒæŒä¹…æ€§æ€§å­˜å‚¨æ˜¯åœ¨é€šè¿‡åœ¨å—è®¾å¤‡ï¼ˆä¾‹å¦‚ç£ç›˜æˆ– SSDï¼‰ä¹‹ä¸Šæ„é€ æ–‡ä»¶ç³»ç»Ÿï¼ˆä¾‹å¦‚ ext4ï¼‰çš„åˆ†å±‚æ–¹å¼å®ç°çš„ã€‚è¿™æ ·åº”ç”¨ç¨‹åºå°±å¯ä»¥è¯»å†™æ–‡ä»¶è€Œä¸æ˜¯æ“ä½œæ•°æ®å—è¿›ã€‚æ“ä½œç³»ç»Ÿè´Ÿè´£ä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶ç³»ç»Ÿå°†æ–‡ä»¶è¯»å†™è½¬æ¢ä¸ºå¯¹åº•å±‚è®¾å¤‡çš„æ•°æ®å—è¯»å†™ã€‚&lt;/p>
&lt;p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ•´ä¸ªç£ç›˜éƒ½æ˜¯å—è®¾å¤‡ï¼Œç£ç›˜åˆ†åŒºä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå­˜å‚¨åŒºåŸŸç½‘ç»œï¼ˆSANï¼‰è®¾å¤‡ä¸­çš„ LUN ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚&lt;/p>
&lt;!--
## Why add raw block volumes to kubernetes?
There are some specialized applications that require direct access to a block device because, for example, the file system layer introduces unneeded overhead. The most common case is databases, which prefer to organize their data directly on the underlying storage. Raw block devices are also commonly used by any software which itself implements some kind of storage service (software defined storage systems).
--->
&lt;h2 id="ä¸ºä»€ä¹ˆè¦å°†-raw-block-volume-æ·»åŠ åˆ°-kubernetes">ä¸ºä»€ä¹ˆè¦å°† raw block volume æ·»åŠ åˆ° kubernetesï¼Ÿ&lt;/h2>
&lt;p>æœ‰äº›ç‰¹æ®Šçš„åº”ç”¨ç¨‹åºéœ€è¦ç›´æ¥è®¿é—®å—è®¾å¤‡ï¼ŒåŸå› ä¾‹å¦‚ï¼Œæ–‡ä»¶ç³»ç»Ÿå±‚ä¼šå¼•å…¥ä¸å¿…è¦çš„å¼€é”€ã€‚æœ€å¸¸è§çš„æƒ…å†µæ˜¯æ•°æ®åº“ï¼Œé€šå¸¸ä¼šç›´æ¥åœ¨åº•å±‚å­˜å‚¨ä¸Šç»„ç»‡æ•°æ®ã€‚åŸç”Ÿçš„å—è®¾å¤‡ï¼ˆRaw Block Devicesï¼‰è¿˜é€šå¸¸ç”±èƒ½è‡ªå·±å®ç°æŸç§å­˜å‚¨æœåŠ¡çš„è½¯ä»¶ï¼ˆè½¯ä»¶å®šä¹‰çš„å­˜å‚¨ç³»ç»Ÿï¼‰ä½¿ç”¨ã€‚&lt;/p>
&lt;!--
From a programmer's perspective, a block device is a very large array of bytes, usually with some minimum granularity for reads and writes, often 512 bytes, but frequently 4K or larger.
As it becomes more common to run database software and storage infrastructure software inside of Kubernetes, the need for raw block device support in Kubernetes becomes more important.
--->
&lt;p>ä»ç¨‹åºå‘˜çš„è§’åº¦æ¥çœ‹ï¼Œå—è®¾å¤‡æ˜¯ä¸€ä¸ªéå¸¸å¤§çš„å­—èŠ‚æ•°ç»„ï¼Œå…·æœ‰æŸç§æœ€å°è¯»å†™ç²’åº¦ï¼Œé€šå¸¸ä¸º 512 ä¸ªå­—èŠ‚ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸º 4K æˆ–æ›´å¤§ã€‚&lt;/p>
&lt;p>éšç€åœ¨ Kubernetes ä¸­è¿è¡Œæ•°æ®åº“è½¯ä»¶å’Œå­˜å‚¨åŸºç¡€æ¶æ„è½¯ä»¶å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œåœ¨ Kubernetes ä¸­æ”¯æŒåŸç”Ÿå—è®¾å¤‡çš„éœ€æ±‚å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚&lt;/p>
&lt;!--
## Which volume plugins support raw blocks?
As of the publishing of this blog, the following in-tree volumes types support raw blocks:
--->
&lt;h2 id="å“ªäº›å·æ’ä»¶æ”¯æŒ-raw-block">å“ªäº›å·æ’ä»¶æ”¯æŒ raw blockï¼Ÿ&lt;/h2>
&lt;p>åœ¨å‘å¸ƒæ­¤åšå®¢æ—¶ï¼Œä»¥ä¸‹ in-tree å·ç±»å‹æ”¯æŒåŸç”Ÿå—è®¾å¤‡ï¼š&lt;/p>
&lt;ul>
&lt;li>AWS EBS&lt;/li>
&lt;li>Azure Disk&lt;/li>
&lt;li>Cinder&lt;/li>
&lt;li>Fibre Channel&lt;/li>
&lt;li>GCE PD&lt;/li>
&lt;li>iSCSI&lt;/li>
&lt;li>Local volumes&lt;/li>
&lt;li>RBD (Ceph)&lt;/li>
&lt;li>Vsphere&lt;/li>
&lt;/ul>
&lt;!--
Out-of-tree [CSI volume drivers](https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/) may also support raw block volumes. Kubernetes CSI support for raw block volumes is currently alpha. See documentation [here](https://kubernetes-csi.github.io/docs/raw-block.html).
--->
&lt;p>Out-of-tree &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">CSI å·é©±åŠ¨ç¨‹åº&lt;/a> å¯èƒ½ä¹Ÿæ”¯æŒåŸç”Ÿæ•°æ®å—å·ã€‚Kubernetes CSI å¯¹åŸç”Ÿæ•°æ®å—å·çš„æ”¯æŒç›®å‰ä¸º alpha é˜¶æ®µã€‚å‚è€ƒ &lt;a href="https://kubernetes-csi.github.io/docs/raw-block.html">è¿™ç¯‡&lt;/a> æ–‡æ¡£ã€‚&lt;/p>
&lt;!--
## Kubernetes raw block volume API
Raw block volumes share a lot in common with ordinary volumes. Both are requested by creating `PersistentVolumeClaim` objects which bind to `PersistentVolume` objects, and are attached to Pods in Kubernetes by including them in the volumes array of the `PodSpec`.
There are 2 important differences however. First, to request a raw block `PersistentVolumeClaim`, you must set `volumeMode = "Block"` in the `PersistentVolumeClaimSpec`. Leaving `volumeMode` blank is the same as specifying `volumeMode = "Filesystem"` which results in the traditional behavior. `PersistentVolumes` also have a `volumeMode` field in their `PersistentVolumeSpec`, and `"Block"` type PVCs can only bind to `"Block"` type PVs and `"Filesystem"` PVCs can only bind to `"Filesystem"` PVs.
--->
&lt;h2 id="kubernetes-raw-block-volume-çš„-api">Kubernetes raw block volume çš„ API&lt;/h2>
&lt;p>åŸç”Ÿæ•°æ®å—å·ä¸æ™®é€šå­˜å‚¨å·æœ‰å¾ˆå¤šå…±åŒç‚¹ã€‚ä¸¤è€…éƒ½é€šè¿‡åˆ›å»ºä¸ &lt;code>PersistentVolume&lt;/code> å¯¹è±¡ç»‘å®šçš„ &lt;code>PersistentVolumeClaim&lt;/code> å¯¹è±¡å‘èµ·è¯·æ±‚ï¼Œå¹¶é€šè¿‡å°†å®ƒä»¬åŠ å…¥åˆ° &lt;code>PodSpec&lt;/code> çš„ volumes æ•°ç»„ä¸­æ¥è¿æ¥åˆ° Kubernetes ä¸­çš„ Podã€‚&lt;/p>
&lt;p>ä½†æ˜¯æœ‰ä¸¤ä¸ªé‡è¦çš„åŒºåˆ«ã€‚é¦–å…ˆï¼Œè¦è¯·æ±‚åŸç”Ÿæ•°æ®å—è®¾å¤‡çš„ &lt;code>PersistentVolumeClaim&lt;/code> å¿…é¡»åœ¨ &lt;code>PersistentVolumeClaimSpec&lt;/code> ä¸­è®¾ç½® &lt;code>volumeMode = &amp;quot;Block&amp;quot;&lt;/code>ã€‚&lt;code>volumeMode&lt;/code> ä¸ºç©ºæ—¶ä¸ä¼ ç»Ÿè®¾ç½®æ–¹å¼ä¸­çš„æŒ‡å®š &lt;code>volumeMode = &amp;quot;Filesystem&amp;quot;&lt;/code> æ˜¯ä¸€æ ·çš„ã€‚&lt;code>PersistentVolumes&lt;/code> åœ¨å…¶ &lt;code>PersistentVolumeSpec&lt;/code> ä¸­ä¹Ÿæœ‰ä¸€ä¸ª &lt;code>volumeMode&lt;/code> å­—æ®µï¼Œ&lt;code>&amp;quot;Block&amp;quot;&lt;/code> ç±»å‹çš„ PVC åªèƒ½ç»‘å®šåˆ° &lt;code>&amp;quot;Block&amp;quot;&lt;/code> ç±»å‹çš„ PV ä¸Šï¼Œè€Œ&lt;code>&amp;quot;Filesystem&amp;quot;&lt;/code> ç±»å‹çš„ PVC åªèƒ½ç»‘å®šåˆ° &lt;code>&amp;quot;Filesystem&amp;quot;&lt;/code> PV ä¸Šã€‚&lt;/p>
&lt;!--
Secondly, when using a raw block volume in your Pods, you must specify a `VolumeDevice` in the Container portion of the `PodSpec` rather than a `VolumeMount`. `VolumeDevices` have `devicePaths` instead of `mountPaths`, and inside the container, applications will see a device at that path instead of a mounted file system.
Applications open, read, and write to the device node inside the container just like they would interact with any block device on a system in a non-containerized or virtualized context.
--->
&lt;p>å…¶æ¬¡ï¼Œåœ¨ Pod ä¸­ä½¿ç”¨åŸç”Ÿæ•°æ®å—å·æ—¶ï¼Œå¿…é¡»åœ¨ &lt;code>PodSpec&lt;/code> çš„ Container éƒ¨åˆ†æŒ‡å®šä¸€ä¸ª &lt;code>VolumeDevice&lt;/code>ï¼Œè€Œä¸æ˜¯ &lt;code>VolumeMount&lt;/code>ã€‚&lt;code>VolumeDevices&lt;/code> å…·å¤‡ &lt;code>devicePaths&lt;/code> è€Œä¸æ˜¯ &lt;code>mountPaths&lt;/code>ï¼Œåœ¨å®¹å™¨ä¸­ï¼Œåº”ç”¨ç¨‹åºå°†çœ‹åˆ°ä½äºè¯¥è·¯å¾„çš„è®¾å¤‡ï¼Œè€Œä¸æ˜¯æŒ‚è½½äº†çš„æ–‡ä»¶ç³»ç»Ÿã€‚&lt;/p>
&lt;p>åº”ç”¨ç¨‹åºæ‰“å¼€ã€è¯»å–å’Œå†™å…¥å®¹å™¨å†…çš„è®¾å¤‡èŠ‚ç‚¹ï¼Œå°±åƒå®ƒä»¬åœ¨éå®¹å™¨åŒ–æˆ–è™šæ‹Ÿç¯å¢ƒä¸­ä¸ç³»ç»Ÿä¸Šçš„ä»»ä½•å—è®¾å¤‡äº¤äº’ä¸€æ ·ã€‚&lt;/p>
&lt;!--
## Creating a new raw block PVC
First, ensure that the provisioner associated with the storage class you choose is one that support raw blocks. Then create the PVC.
--->
&lt;h2 id="åˆ›å»ºä¸€ä¸ªæ–°çš„åŸç”Ÿå—è®¾å¤‡-pvc">åˆ›å»ºä¸€ä¸ªæ–°çš„åŸç”Ÿå—è®¾å¤‡ PVC&lt;/h2>
&lt;p>é¦–å…ˆï¼Œè¯·ç¡®ä¿ä¸æ‚¨é€‰æ‹©çš„å­˜å‚¨ç±»å…³è”çš„é©±åŠ¨æ”¯æŒåŸç”Ÿå—è®¾å¤‡ã€‚ç„¶ååˆ›å»º PVCã€‚&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: my-pvc
spec:
accessModes:
- ReadWriteMany
volumeMode: Block
storageClassName: my-sc
resources:
requests:
storage: 1Gi
&lt;/code>&lt;/pre>&lt;!--
## Using a raw block PVC
When you use the PVC in a pod definition, you get to choose the device path for the block device rather than the mount path for the file system.
--->
&lt;h2 id="ä½¿ç”¨åŸç”Ÿå—-pvc">ä½¿ç”¨åŸç”Ÿå— PVC&lt;/h2>
&lt;p>åœ¨ Pod å®šä¹‰ä¸­ä½¿ç”¨ PVC æ—¶ï¼Œéœ€è¦é€‰æ‹©å—è®¾å¤‡çš„è®¾å¤‡è·¯å¾„ï¼Œè€Œä¸æ˜¯æ–‡ä»¶ç³»ç»Ÿçš„å®‰è£…è·¯å¾„ã€‚&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: my-pod
spec:
containers:
- name: my-container
image: busybox
command:
- sleep
- â€œ3600â€
volumeDevices:
- devicePath: /dev/block
name: my-volume
imagePullPolicy: IfNotPresent
volumes:
- name: my-volume
persistentVolumeClaim:
claimName: my-pvc
&lt;/code>&lt;/pre>&lt;!--
## As a storage vendor, how do I add support for raw block devices to my CSI plugin?
Raw block support for CSI plugins is still alpha, but support can be added today. The [CSI specification](https://github.com/container-storage-interface/spec/blob/master/spec.md) details how to handle requests for volume that have the `BlockVolume` capability instead of the `MountVolume` capability. CSI plugins can support both kinds of volumes, or one or the other. For more details see [documentation here](https://kubernetes-csi.github.io/docs/raw-block.html).
--->
&lt;h2 id="ä½œä¸ºå­˜å‚¨ä¾›åº”å•†-æˆ‘å¦‚ä½•åœ¨-csi-æ’ä»¶ä¸­æ·»åŠ å¯¹åŸç”Ÿå—è®¾å¤‡çš„æ”¯æŒ">ä½œä¸ºå­˜å‚¨ä¾›åº”å•†ï¼Œæˆ‘å¦‚ä½•åœ¨ CSI æ’ä»¶ä¸­æ·»åŠ å¯¹åŸç”Ÿå—è®¾å¤‡çš„æ”¯æŒï¼Ÿ&lt;/h2>
&lt;p>CSI æ’ä»¶çš„åŸç”Ÿå—æ”¯æŒä»ç„¶æ˜¯ alpha ç‰ˆæœ¬ï¼Œä½†æ˜¯ç°åœ¨å¯ä»¥æ”¹è¿›äº†ã€‚&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI è§„èŒƒ&lt;/a> è¯¦ç»†è¯´æ˜äº†å¦‚ä½•å¤„ç†å…·æœ‰ &lt;code>BlockVolume&lt;/code> èƒ½åŠ›è€Œä¸æ˜¯ &lt;code>MountVolume&lt;/code> èƒ½åŠ›çš„å·çš„è¯·æ±‚ã€‚CSI æ’ä»¶å¯ä»¥æ”¯æŒä¸¤ç§ç±»å‹çš„å·ï¼Œä¹Ÿå¯ä»¥æ”¯æŒå…¶ä¸­ä¸€ç§æˆ–å¦ä¸€ç§ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ &lt;a href="https://kubernetes-csi.github.io/docs/raw-block.html">è¿™ä¸ªæ–‡æ¡£&lt;/a>ã€‚&lt;/p>
&lt;!--
## Issues/gotchas
Because block devices are actually devices, itâ€™s possible to do low-level actions on them from inside containers that wouldnâ€™t be possible with file system volumes. For example, block devices that are actually SCSI disks support sending SCSI commands to the device using Linux ioctls.
--->
&lt;h2 id="é—®é¢˜-é™·é˜±">é—®é¢˜/é™·é˜±&lt;/h2>
&lt;p>ç”±äºå—è®¾å¤‡å®è´¨ä¸Šè¿˜æ˜¯è®¾å¤‡ï¼Œå› æ­¤å¯ä»¥ä»å®¹å™¨å†…éƒ¨å¯¹å…¶è¿›è¡Œåº•å±‚æ“ä½œï¼Œè€Œæ–‡ä»¶ç³»ç»Ÿçš„å·åˆ™æ— æ³•æ‰§è¡Œè¿™äº›æ“ä½œã€‚ä¾‹å¦‚ï¼Œå®é™…ä¸Šæ˜¯å—è®¾å¤‡çš„ SCSI ç£ç›˜æ”¯æŒä½¿ç”¨ Linux ioctl å‘è®¾å¤‡å‘é€ SCSI å‘½ä»¤ã€‚&lt;/p>
&lt;!--
By default, Linux wonâ€™t allow containers to send SCSI commands to disks from inside containers though. In order to do so, you must grant the `SYS_RAWIO` capability to the container security context to allow this. See documentation [here](/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container).
Also, while Kubernetes is guaranteed to deliver a block device to the container, thereâ€™s no guarantee that itâ€™s actually a SCSI disk or any other kind of disk for that matter. The user must either ensure that the desired disk type is used with his pods, or only deploy applications that can handle a variety of block device types.
--->
&lt;p>é»˜è®¤æƒ…å†µä¸‹ï¼ŒLinux ä¸å…è®¸å®¹å™¨å°† SCSI å‘½ä»¤ä»å®¹å™¨å†…éƒ¨å‘é€åˆ°ç£ç›˜ã€‚ä¸ºæ­¤ï¼Œå¿…é¡»å‘å®¹å™¨å®‰å…¨å±‚çº§è®¤è¯ &lt;code>SYS_RAWIO&lt;/code> åŠŸèƒ½å®ç°è¿™ç§è¡Œä¸ºã€‚è¯·å‚é˜… &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">è¿™ç¯‡&lt;/a> æ–‡æ¡£ã€‚&lt;/p>
&lt;p>å¦å¤–ï¼Œå°½ç®¡ Kubernetes ä¿è¯å¯ä»¥å°†å—è®¾å¤‡äº¤ä»˜åˆ°å®¹å™¨ä¸­ï¼Œä½†ä¸èƒ½ä¿è¯å®ƒå®é™…ä¸Šæ˜¯ SCSI ç£ç›˜æˆ–ä»»ä½•å…¶ä»–ç±»å‹çš„ç£ç›˜ã€‚ç”¨æˆ·å¿…é¡»ç¡®ä¿æ‰€éœ€çš„ç£ç›˜ç±»å‹ä¸ Pod ä¸€èµ·ä½¿ç”¨ï¼Œæˆ–åªéƒ¨ç½²å¯ä»¥å¤„ç†å„ç§å—è®¾å¤‡ç±»å‹çš„åº”ç”¨ç¨‹åºã€‚&lt;/p>
&lt;!--
## How can I learn more?
Check out additional documentation on the snapshot feature here: [Raw Block Volume Support](/docs/concepts/storage/persistent-volumes/#raw-block-volume-support)
How do I get involved?
Join the Kubernetes storage SIG and the CSI community and help us add more great features and improve existing ones like raw block storage!
--->
&lt;h2 id="å¦‚ä½•å­¦ä¹ æ›´å¤š">å¦‚ä½•å­¦ä¹ æ›´å¤šï¼Ÿ&lt;/h2>
&lt;p>åœ¨æ­¤å¤„æŸ¥çœ‹æœ‰å…³ snapshot åŠŸèƒ½çš„å…¶ä»–æ–‡æ¡£ï¼š&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support">Raw Block Volume æ”¯æŒ&lt;/a>&lt;/p>
&lt;p>å¦‚ä½•å‚ä¸è¿›æ¥ï¼Ÿ&lt;/p>
&lt;p>åŠ å…¥ Kubernetes å­˜å‚¨ SIG å’Œ CSI ç¤¾åŒºï¼Œå¸®åŠ©æˆ‘ä»¬æ·»åŠ æ›´å¤šå‡ºè‰²çš„åŠŸèƒ½å¹¶æ”¹è¿›ç°æœ‰åŠŸèƒ½ï¼Œå°±åƒ raw block å­˜å‚¨ä¸€æ ·ï¼&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">https://github.com/kubernetes/community/tree/master/sig-storage&lt;/a>
&lt;a href="https://github.com/container-storage-interface/community/blob/master/README.md">https://github.com/container-storage-interface/community/blob/master/README.md&lt;/a>&lt;/p>
&lt;!--
Special thanks to all the contributors who helped add block volume support to Kubernetes including:
--->
&lt;p>ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰ä¸º Kubernetes å¢åŠ  block volume æ”¯æŒçš„è´¡çŒ®è€…ï¼ŒåŒ…æ‹¬ï¼š&lt;/p>
&lt;ul>
&lt;li>Ben Swartzlander (&lt;a href="https://github.com/bswartz">https://github.com/bswartz&lt;/a>)&lt;/li>
&lt;li>Brad Childs (&lt;a href="https://github.com/childsb">https://github.com/childsb&lt;/a>)&lt;/li>
&lt;li>Erin Boyd (&lt;a href="https://github.com/erinboyd">https://github.com/erinboyd&lt;/a>)&lt;/li>
&lt;li>Masaki Kimura (&lt;a href="https://github.com/mkimuram">https://github.com/mkimuram&lt;/a>)&lt;/li>
&lt;li>Matthew Wong (&lt;a href="https://github.com/wongma7">https://github.com/wongma7&lt;/a>)&lt;/li>
&lt;li>Michelle Au (&lt;a href="https://github.com/msau42">https://github.com/msau42&lt;/a>)&lt;/li>
&lt;li>Mitsuhiro Tanino (&lt;a href="https://github.com/mtanino">https://github.com/mtanino&lt;/a>)&lt;/li>
&lt;li>Saad Ali (&lt;a href="https://github.com/saad-ali">https://github.com/saad-ali&lt;/a>)&lt;/li>
&lt;/ul></description></item><item><title>Blog: æ–°è´¡çŒ®è€…å·¥ä½œåŠä¸Šæµ·ç«™</title><link>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</guid><description>
&lt;!--
---
layout: blog
title: 'New Contributor Workshop Shanghai'
date: 2018-12-05
---
-->
&lt;!--
**Authors**: Josh Berkus (Red Hat), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ZTE)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Josh Berkus (çº¢å¸½), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ä¸­å…´é€šè®¯)&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png"
alt="KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png"
alt="KubeCon ä¸Šæµ·ç«™æ–°è´¡çŒ®è€…å³°ä¼šä¸ä¼šè€…ï¼Œæ‘„å½±ï¼šJerry Zhang"/> &lt;figcaption>
&lt;p>KubeCon ä¸Šæµ·ç«™æ–°è´¡çŒ®è€…å³°ä¼šä¸ä¼šè€…ï¼Œæ‘„å½±ï¼šJerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
We recently completed our first New Contributor Summit in China, at the first KubeCon in China. It was very exciting to see all of the Chinese and Asian developers (plus a few folks from around the world) interested in becoming contributors. Over the course of a long day, they learned how, why, and where to contribute to Kubernetes, created pull requests, attended a panel of current contributors, and got their CLAs signed.
-->
&lt;p>æœ€è¿‘ï¼Œåœ¨ä¸­å›½çš„é¦–æ¬¡ KubeCon ä¸Šï¼Œæˆ‘ä»¬å®Œæˆäº†åœ¨ä¸­å›½çš„é¦–æ¬¡æ–°è´¡çŒ®è€…å³°ä¼šã€‚çœ‹åˆ°æ‰€æœ‰ä¸­å›½å’Œäºšæ´²çš„å¼€å‘è€…ï¼ˆä»¥åŠæ¥è‡ªä¸–ç•Œå„åœ°çš„ä¸€äº›äººï¼‰æœ‰å…´è¶£æˆä¸ºè´¡çŒ®è€…ï¼Œè¿™ä»¤äººéå¸¸å…´å¥‹ã€‚åœ¨é•¿è¾¾ä¸€å¤©çš„è¯¾ç¨‹ä¸­ï¼Œä»–ä»¬äº†è§£äº†å¦‚ä½•ã€ä¸ºä»€ä¹ˆä»¥åŠåœ¨ä½•å¤„ä¸º Kubernetes ä½œå‡ºè´¡çŒ®ï¼Œåˆ›å»ºäº† PRï¼Œå‚åŠ äº†è´¡çŒ®è€…åœ†æ¡Œè®¨è®ºï¼Œå¹¶ç­¾ç½²äº†ä»–ä»¬çš„ CLAã€‚&lt;/p>
&lt;!--
This was our second New Contributor Workshop (NCW), building on the one created and led by SIG Contributor Experience members in Copenhagen. Because of the audience, it was held in both Chinese and English, taking advantage of the superb simultaneous interpretation services the CNCF sponsored. Likewise, the NCW team included both English and Chinese-speaking members of the community: Yang Li, XiangPeng Zhao, Puja Abbassi, Noah Abrahams, Tim Pepper, Zach Corleissen, Sen Lu, and Josh Berkus. In addition to presenting and helping students, the bilingual members of the team translated all of the slides into Chinese. Fifty-one students attended.
-->
&lt;p>è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬äºŒå±Šæ–°è´¡çŒ®è€…å·¥ä½œåŠï¼ˆNCWï¼‰ï¼Œå®ƒç”±å‰ä¸€æ¬¡è´¡çŒ®è€…ä½“éªŒ SIG æˆå‘˜åˆ›å»ºå’Œé¢†å¯¼çš„å“¥æœ¬å“ˆæ ¹ç ”è®¨ä¼šå»¶ä¼¸è€Œæ¥ã€‚æ ¹æ®å—ä¼—æƒ…å†µï¼Œæœ¬æ¬¡æ´»åŠ¨é‡‡ç”¨äº†ä¸­è‹±æ–‡ä¸¤ç§è¯­è¨€ï¼Œå……åˆ†åˆ©ç”¨äº† CNCF èµåŠ©çš„ä¸€æµçš„åŒå£°ä¼ è¯‘æœåŠ¡ã€‚åŒæ ·ï¼ŒNCW å›¢é˜Ÿç”±ç¤¾åŒºæˆå‘˜ç»„æˆï¼Œæ—¢æœ‰è¯´è‹±è¯­çš„ï¼Œä¹Ÿæœ‰è¯´æ±‰è¯­çš„ï¼šYang Liã€XiangPeng Zhaoã€Puja Abbassiã€Noah Abrahamsã€Tim Pepperã€Zach Corleissenã€Sen Lu å’Œ Josh Berkusã€‚é™¤äº†æ¼”è®²å’Œå¸®åŠ©å­¦å‘˜å¤–ï¼Œå›¢é˜Ÿçš„åŒè¯­æˆå‘˜è¿˜å°†æ‰€æœ‰å¹»ç¯ç‰‡ç¿»è¯‘æˆäº†ä¸­æ–‡ã€‚å…±æœ‰äº”åä¸€åå­¦å‘˜å‚åŠ ã€‚&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png"
alt="Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png"
alt="Noah Abrahams è®²è§£ Kubernetes æ²Ÿé€šæ¸ é“ã€‚æ‘„å½±ï¼šJerry Zhang"/> &lt;figcaption>
&lt;p>Noah Abrahams è®²è§£ Kubernetes æ²Ÿé€šæ¸ é“ã€‚æ‘„å½±ï¼šJerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
The NCW takes participants through the stages of contributing to Kubernetes, starting from deciding where to contribute, followed by an introduction to the SIG system and our repository structure. We also have "guest speakers" from Docs and Test Infrastructure who cover contributing in those areas. We finally wind up with some hands-on exercises in filing issues and creating and approving PRs.
-->
&lt;p>NCW è®©å‚ä¸è€…å®Œæˆäº†ä¸º Kubernetes ä½œå‡ºè´¡çŒ®çš„å„ä¸ªé˜¶æ®µï¼Œä»å†³å®šåœ¨å“ªé‡Œä½œå‡ºè´¡çŒ®å¼€å§‹ï¼Œæ¥ç€ä»‹ç»äº† SIG ç³»ç»Ÿå’Œæˆ‘ä»¬çš„ä»£ç ä»“åº“ç»“æ„ã€‚æˆ‘ä»¬è¿˜æœ‰æ¥è‡ªæ–‡æ¡£å’Œæµ‹è¯•åŸºç¡€è®¾æ–½é¢†åŸŸçš„ã€Œå®¢åº§è®²è€…ã€ï¼Œä»–ä»¬è´Ÿè´£è®²è§£æœ‰å…³çš„è´¡çŒ®ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨åˆ›å»º issueã€æäº¤å¹¶æ‰¹å‡† PR çš„å®è·µç»ƒä¹ åï¼Œç»“æŸäº†å·¥ä½œåŠã€‚&lt;/p>
&lt;!--
Those hands-on exercises use a repository known as [the contributor playground](https://github.com/kubernetes-sigs/contributor-playground), created by SIG Contributor Experience as a place for new contributors to try out performing various actions on a Kubernetes repo. It has modified Prow and Tide automation, uses Owners files like in the real repositories. This lets students learn how the mechanics of contributing to our repositories work without disrupting normal development.
-->
&lt;p>è¿™äº›å®è·µç»ƒä¹ ä½¿ç”¨ä¸€ä¸ªåä¸º&lt;a href="https://github.com/kubernetes-sigs/contributor-playground">è´¡çŒ®è€…æ¸¸ä¹åœº&lt;/a>çš„ä»£ç ä»“åº“ï¼Œç”±è´¡çŒ®è€…ä½“éªŒ SIG åˆ›å»ºï¼Œè®©æ–°è´¡çŒ®è€…å°è¯•åœ¨ä¸€ä¸ª Kubernetes ä»“åº“ä¸­æ‰§è¡Œå„ç§æ“ä½œã€‚å®ƒä¿®æ”¹äº† Prow å’Œ Tide è‡ªåŠ¨åŒ–ï¼Œä½¿ç”¨ä¸çœŸå®ä»£ç ä»“åº“ç±»ä¼¼çš„ Owners æ–‡ä»¶ã€‚è¿™å¯ä»¥è®©å­¦å‘˜äº†è§£ä¸ºæˆ‘ä»¬çš„ä»“åº“åšå‡ºè´¡çŒ®çš„æœ‰å…³æœºåˆ¶ï¼ŒåŒæ—¶åˆä¸å¦¨ç¢æ­£å¸¸çš„å¼€å‘æµç¨‹ã€‚&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png"
alt="Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus"/> &lt;figcaption>
&lt;p>Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png"
alt="Yang Li è®²åˆ°å¦‚ä½•è®©ä½ çš„ PR é€šè¿‡è¯„å®¡ã€‚æ‘„å½±ï¼šJosh Berkus"/> &lt;figcaption>
&lt;p>Yang Li è®²åˆ°å¦‚ä½•è®©ä½ çš„ PR é€šè¿‡è¯„å®¡ã€‚æ‘„å½±ï¼šJosh Berkus&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
Both the "Great Firewall" and the language barrier prevent contributing Kubernetes from China from being straightforward. What's more, because open source business models are not mature in China, the time for employees work on open source projects is limited.
-->
&lt;p>ã€Œé˜²ç«é•¿åŸã€å’Œè¯­è¨€éšœç¢éƒ½ä½¿å¾—åœ¨ä¸­å›½ä¸º Kubernetes ä½œå‡ºè´¡çŒ®å˜å¾—å›°éš¾ã€‚è€Œä¸”ï¼Œä¸­å›½çš„å¼€æºå•†ä¸šæ¨¡å¼å¹¶ä¸æˆç†Ÿï¼Œå‘˜å·¥åœ¨å¼€æºé¡¹ç›®ä¸Šå·¥ä½œçš„æ—¶é—´æœ‰é™ã€‚&lt;/p>
&lt;!--
Chinese engineers are eager to participate in the development of Kubernetes, but many of them don't know where to start since Kubernetes is such a large project. With this workshop, we hope to help those who want to contribute, whether they wish to fix some bugs they encountered, improve or localize documentation, or they need to work with Kubernetes at their work. We are glad to see more and more Chinese contributors joining the community in the past few years, and we hope to see more of them in the future.
-->
&lt;p>ä¸­å›½å·¥ç¨‹å¸ˆæ¸´æœ›å‚ä¸ Kubernetes çš„ç ”å‘ï¼Œä½†ä»–ä»¬ä¸­çš„è®¸å¤šäººä¸çŸ¥é“ä»ä½•å¤„å¼€å§‹ï¼Œå› ä¸º Kubernetes æ˜¯ä¸€ä¸ªå¦‚æ­¤åºå¤§çš„é¡¹ç›®ã€‚é€šè¿‡æœ¬æ¬¡å·¥ä½œåŠï¼Œæˆ‘ä»¬å¸Œæœ›å¸®åŠ©é‚£äº›æƒ³è¦å‚ä¸è´¡çŒ®çš„äººï¼Œä¸è®ºä»–ä»¬å¸Œæœ›ä¿®å¤ä»–ä»¬é‡åˆ°çš„ä¸€äº›é”™è¯¯ã€æ”¹è¿›æˆ–æœ¬åœ°åŒ–æ–‡æ¡£ï¼Œæˆ–è€…ä»–ä»¬éœ€è¦åœ¨å·¥ä½œä¸­ç”¨åˆ° Kubernetesã€‚æˆ‘ä»¬å¾ˆé«˜å…´çœ‹åˆ°è¶Šæ¥è¶Šå¤šçš„ä¸­å›½è´¡çŒ®è€…åœ¨è¿‡å»å‡ å¹´é‡ŒåŠ å…¥ç¤¾åŒºï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›å°†æ¥å¯ä»¥çœ‹åˆ°æ›´å¤šã€‚&lt;/p>
&lt;!--
"I have been participating in the Kubernetes community for about three years," said XiangPeng Zhao. "In the community, I notice that more and more Chinese developers are showing their interest in contributing to Kubernetes. However, it's not easy to start contributing to such a project. I tried my best to help those who I met in the community, but I think there might still be some new contributors leaving the community due to not knowing where to get help when in trouble. Fortunately, the community initiated NCW at KubeCon Copenhagen and held a second one at KubeCon Shanghai. I was so excited to be invited by Josh Berkus to help organize this workshop. During the workshop, I met community friends in person, mentored attendees in the exercises, and so on. All of this was a memorable experience for me. I also learned a lot as a contributor who already has years of contributing experience. I wish I had attended such a workshop when I started contributing to Kubernetes years ago."
-->
&lt;p>ã€Œæˆ‘å·²ç»å‚ä¸äº† Kubernetes ç¤¾åŒºå¤§çº¦ä¸‰å¹´ã€ï¼ŒXiangPeng Zhao è¯´ï¼Œã€Œåœ¨ç¤¾åŒºï¼Œæˆ‘æ³¨æ„åˆ°è¶Šæ¥è¶Šå¤šçš„ä¸­å›½å¼€å‘è€…è¡¨ç°å‡ºå¯¹ Kubernetes è´¡çŒ®çš„å…´è¶£ã€‚ä½†æ˜¯ï¼Œå¼€å§‹ä¸ºè¿™æ ·ä¸€ä¸ªé¡¹ç›®åšè´¡çŒ®å¹¶ä¸å®¹æ˜“ã€‚æˆ‘å°½åŠ›å¸®åŠ©é‚£äº›æˆ‘åœ¨ç¤¾åŒºé‡åˆ°çš„äººï¼Œä½†æ˜¯ï¼Œæˆ‘è®¤ä¸ºå¯èƒ½ä»æœ‰ä¸€äº›æ–°çš„è´¡çŒ®è€…ç¦»å¼€ç¤¾åŒºï¼Œå› ä¸ºä»–ä»¬åœ¨é‡åˆ°éº»çƒ¦æ—¶ä¸çŸ¥é“ä»å“ªé‡Œè·å¾—å¸®åŠ©ã€‚å¹¸è¿çš„æ˜¯ï¼Œç¤¾åŒºåœ¨ KubeCon å“¥æœ¬å“ˆæ ¹ç«™å‘èµ·äº† NCWï¼Œå¹¶åœ¨ KubeCon ä¸Šæµ·ç«™ä¸¾åŠäº†ç¬¬äºŒå±Šã€‚æˆ‘å¾ˆé«˜å…´å—åˆ° Josh Berkus çš„é‚€è¯·ï¼Œå¸®åŠ©ç»„ç»‡è¿™ä¸ªå·¥ä½œåŠã€‚åœ¨å·¥ä½œåŠæœŸé—´ï¼Œæˆ‘å½“é¢è§åˆ°äº†ç¤¾åŒºé‡Œçš„æœ‹å‹ï¼Œåœ¨ç»ƒä¹ ä¸­æŒ‡å¯¼äº†ä¸ä¼šè€…ï¼Œç­‰ç­‰ã€‚æ‰€æœ‰è¿™äº›å¯¹æˆ‘æ¥è¯´éƒ½æ˜¯éš¾å¿˜çš„ç»å†ã€‚ä½œä¸ºæœ‰ç€å¤šå¹´è´¡çŒ®è€…ç»éªŒçš„æˆ‘ï¼Œä¹Ÿå­¦ä¹ åˆ°äº†å¾ˆå¤šã€‚æˆ‘å¸Œæœ›å‡ å¹´å‰æˆ‘å¼€å§‹ä¸º Kubernetes åšè´¡çŒ®æ—¶å‚åŠ è¿‡è¿™æ ·çš„å·¥ä½œåŠã€ã€‚&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png"
alt="Panel of contributors. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>Panel of contributors. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png"
alt="è´¡çŒ®è€…åœ†æ¡Œè®¨è®ºã€‚æ‘„å½±ï¼šJerry Zhang"/> &lt;figcaption>
&lt;p>è´¡çŒ®è€…åœ†æ¡Œè®¨è®ºã€‚æ‘„å½±ï¼šJerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
The workshop ended with a panel of current contributors, featuring Lucas KÃ¤ldstrÃ¶m, Janet Kuo, Da Ma, Pengfei Ni, Zefeng Wang, and Chao Xu. The panel aimed to give both new and current contributors a look behind the scenes on the day-to-day of some of the most active contributors and maintainers, both from China and around the world. Panelists talked about where to begin your contributor's journey, but also how to interact with reviewers and maintainers. They further touched upon the main issues of contributing from China and gave attendees an outlook into exciting features they can look forward to in upcoming releases of Kubernetes.
-->
&lt;p>å·¥ä½œåŠä»¥ç°æœ‰è´¡çŒ®è€…åœ†æ¡Œè®¨è®ºç»“æŸï¼Œå˜‰å®¾åŒ…æ‹¬ Lucas KÃ¤ldstrÃ¶mã€Janet Kuoã€Da Maã€Pengfei Niã€Zefeng Wang å’Œ Chao Xuã€‚è¿™åœºåœ†æ¡Œè®¨è®ºæ—¨åœ¨è®©æ–°çš„å’Œç°æœ‰çš„è´¡çŒ®è€…äº†è§£ä¸€äº›æœ€æ´»è·ƒçš„è´¡çŒ®è€…å’Œç»´æŠ¤è€…çš„å¹•åæ—¥å¸¸å·¥ä½œï¼Œä¸è®ºä»–ä»¬æ¥è‡ªä¸­å›½è¿˜æ˜¯ä¸–ç•Œå„åœ°ã€‚å˜‰å®¾ä»¬è®¨è®ºäº†ä»å“ªé‡Œå¼€å§‹è´¡çŒ®è€…çš„æ—…ç¨‹ï¼Œä»¥åŠå¦‚ä½•ä¸è¯„å®¡è€…å’Œç»´æŠ¤è€…è¿›è¡Œäº’åŠ¨ã€‚ä»–ä»¬è¿›ä¸€æ­¥æ¢è®¨äº†åœ¨ä¸­å›½å‚ä¸è´¡çŒ®çš„ä¸»è¦é—®é¢˜ï¼Œå¹¶å‘ä¸ä¼šè€…é¢„å‘Šäº†åœ¨ Kubernetes çš„æœªæ¥ç‰ˆæœ¬ä¸­å¯ä»¥æœŸå¾…çš„ä»¤äººå…´å¥‹çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
After the workshop, XiangPeng Zhao chatted with some attendees on WeChat and Twitter about their experiences. They were very glad to have attended the NCW and had some suggestions on improving the workshop. One attendee, Mohammad, said, "I had a great time at the workshop and learned a lot about the entire process of k8s for a contributor." Another attendee, Jie Jia, said, "The workshop was wonderful. It systematically explained how to contribute to Kubernetes. The attendee could understand the process even if s/he knew nothing about that before. For those who were already contributors, they could also learn something new. Furthermore, I could make new friends from inside or outside of China in the workshop. It was awesome!"
-->
&lt;p>å·¥ä½œåŠç»“æŸåï¼ŒXiangPeng Zhao å’Œä¸€äº›ä¸ä¼šè€…å°±ä»–ä»¬çš„ç»å†åœ¨å¾®ä¿¡å’Œ Twitter ä¸Šè¿›è¡Œäº†äº¤è°ˆã€‚ä»–ä»¬å¾ˆé«˜å…´å‚åŠ äº† NCWï¼Œå¹¶å°±æ”¹è¿›å·¥ä½œåŠæå‡ºäº†ä¸€äº›å»ºè®®ã€‚ä¸€ä½åå« Mohammad çš„ä¸ä¼šè€…è¯´ï¼šã€Œæˆ‘åœ¨å·¥ä½œåŠä¸Šç©å¾—å¾ˆå¼€å¿ƒï¼Œå­¦ä¹ äº†å‚ä¸ k8s è´¡çŒ®çš„æ•´ä¸ªè¿‡ç¨‹ã€‚ã€å¦ä¸€ä½ä¸ä¼šè€… Jie Jia è¯´ï¼šã€Œå·¥ä½œåŠéå¸¸ç²¾å½©ã€‚å®ƒç³»ç»Ÿåœ°è§£é‡Šäº†å¦‚ä½•ä¸º Kubernetes åšå‡ºè´¡çŒ®ã€‚å³ä½¿å‚ä¸è€…ä¹‹å‰å¯¹æ­¤ä¸€æ— æ‰€çŸ¥ï¼Œä»–ï¼ˆå¥¹ï¼‰ä¹Ÿå¯ä»¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ã€‚å¯¹äºé‚£äº›å·²ç»æ˜¯è´¡çŒ®è€…çš„äººï¼Œä»–ä»¬ä¹Ÿå¯ä»¥å­¦ä¹ åˆ°æ–°ä¸œè¥¿ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜å¯ä»¥åœ¨å·¥ä½œåŠä¸Šç»“è¯†æ¥è‡ªå›½å†…å¤–çš„æ–°æœ‹å‹ã€‚çœŸæ˜¯æ£’æäº†ï¼ã€&lt;/p>
&lt;!--
SIG Contributor Experience will continue to run New Contributor Workshops at each upcoming KubeCon, including Seattle, Barcelona, and the return to Shanghai in June 2019. If you failed to get into one this year, register for one at a future KubeCon. And, when you meet an NCW attendee, make sure to welcome them to the community.
-->
&lt;p>è´¡çŒ®è€…ä½“éªŒ SIG å°†ç»§ç»­åœ¨æœªæ¥çš„ KubeCon ä¸Šä¸¾åŠæ–°è´¡çŒ®è€…å·¥ä½œåŠï¼ŒåŒ…æ‹¬è¥¿é›…å›¾ç«™ã€å·´å¡ç½—é‚£ç«™ï¼Œç„¶ååœ¨ 2019 å¹´å…­æœˆå›åˆ°ä¸Šæµ·ã€‚å¦‚æœä½ ä»Šå¹´æœªèƒ½å‚åŠ ï¼Œè¯·åœ¨æœªæ¥çš„ KubeCon ä¸Šæ³¨å†Œã€‚å¹¶ä¸”ï¼Œå¦‚æœä½ é‡åˆ°å·¥ä½œåŠçš„ä¸ä¼šè€…ï¼Œè¯·åŠ¡å¿…æ¬¢è¿ä»–ä»¬åŠ å…¥ç¤¾åŒºã€‚&lt;/p>
&lt;!--
Links:
-->
&lt;p>é“¾æ¥ï¼š&lt;/p>
&lt;!--
* English versions of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf) or [Google Docs with speaker notes](https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing)
* Chinese version of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf)
* [Contributor playground](https://github.com/kubernetes-sigs/contributor-playground)
-->
&lt;ul>
&lt;li>ä¸­æ–‡ç‰ˆå¹»ç¯ç‰‡ï¼š&lt;a href="https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf">PDF&lt;/a>&lt;/li>
&lt;li>è‹±æ–‡ç‰ˆå¹»ç¯ç‰‡ï¼š&lt;a href="https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf">PDF&lt;/a> æˆ– &lt;a href="https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing">å¸¦æœ‰æ¼”è®²è€…ç¬”è®°çš„ Google Docs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/contributor-playground">è´¡çŒ®è€…æ¸¸ä¹åœº&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes æ–‡æ¡£æ›´æ–°ï¼Œå›½é™…ç‰ˆ</title><link>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</link><pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</guid><description>
&lt;!--
---
layout: blog
title: 'Kubernetes Docs Updates, International Edition'
date: 2018-11-08
---
-->
&lt;!-- **Author**: Zach Corleissen (Linux Foundation) -->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šZach Corleissen ï¼ˆLinux åŸºé‡‘ä¼šï¼‰&lt;/p>
&lt;!-- As a co-chair of SIG Docs, I'm excited to share that Kubernetes docs have a fully mature workflow for localization (l10n). -->
&lt;p>ä½œä¸ºæ–‡æ¡£ç‰¹åˆ«å…´è¶£å°ç»„ï¼ˆSIG Docsï¼‰çš„è”åˆä¸»å¸­ï¼Œæˆ‘å¾ˆé«˜å…´èƒ½ä¸å¤§å®¶åˆ†äº« Kubernetes æ–‡æ¡£åœ¨æœ¬åœ°åŒ–ï¼ˆl10nï¼‰æ–¹é¢æ‰€æ‹¥æœ‰çš„ä¸€ä¸ªå®Œå…¨æˆç†Ÿçš„å·¥ä½œæµã€‚&lt;/p>
&lt;!-- ## Abbreviations galore -->
&lt;h2 id="ä¸°å¯Œçš„ç¼©å†™">ä¸°å¯Œçš„ç¼©å†™&lt;/h2>
&lt;!-- L10n is an abbreviation for _localization_. -->
&lt;p>L10n æ˜¯ &lt;em>localization&lt;/em> çš„ç¼©å†™ã€‚&lt;/p>
&lt;!-- I18n is an abbreviation for _internationalization_. -->
&lt;p>I18n æ˜¯ &lt;em>internationalization&lt;/em> çš„ç¼©å†™ã€‚&lt;/p>
&lt;!-- I18n is [what you do](https://www.w3.org/International/questions/qa-i18n) to make l10n easier. L10n is a fuller, more comprehensive process than translation (_t9n_). -->
&lt;p>I18n å®šä¹‰äº†&lt;a href="https://www.w3.org/International/questions/qa-i18n">åšä»€ä¹ˆ&lt;/a> èƒ½è®© l10n æ›´å®¹æ˜“ã€‚è€Œ L10n æ›´å…¨é¢ï¼Œç›¸æ¯”ç¿»è¯‘ï¼ˆ &lt;em>t9n&lt;/em> ï¼‰å…·å¤‡æ›´å®Œå–„çš„æµç¨‹ã€‚&lt;/p>
&lt;!-- ## Why localization matters -->
&lt;h2 id="ä¸ºä»€ä¹ˆæœ¬åœ°åŒ–å¾ˆé‡è¦">ä¸ºä»€ä¹ˆæœ¬åœ°åŒ–å¾ˆé‡è¦&lt;/h2>
&lt;!-- The goal of SIG Docs is to make Kubernetes easier to use for as many people as possible. -->
&lt;p>SIG Docs çš„ç›®æ ‡æ˜¯è®© Kubernetes æ›´å®¹æ˜“ä¸ºå°½å¯èƒ½å¤šçš„äººä½¿ç”¨ã€‚&lt;/p>
&lt;!-- One year ago, we looked at whether it was possible to host the output of a Chinese team working independently to translate the Kubernetes docs. After many conversations (including experts on OpenStack l10n), [much transformation](https://kubernetes.io/blog/2018/05/05/hugo-migration/), and [renewed commitment to easier localization](https://github.com/kubernetes/website/pull/10485), we realized that open source documentation is, like open source software, an ongoing exercise at the edges of what's possible. -->
&lt;p>ä¸€å¹´å‰ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ˜¯å¦æœ‰å¯èƒ½ç”±ä¸€ä¸ªç‹¬ç«‹ç¿»è¯‘ Kubernetes æ–‡æ¡£çš„ä¸­å›½å›¢é˜Ÿæ¥ä¸»æŒæ–‡æ¡£è¾“å‡ºã€‚ç»è¿‡å¤šæ¬¡äº¤è°ˆï¼ˆåŒ…æ‹¬ OpenStack l10n çš„ä¸“å®¶ï¼‰ï¼Œ&lt;a href="https://kubernetes.io/blog/2018/05/05/hugo-migration/">å¤šæ¬¡è½¬å˜&lt;/a>ï¼Œä»¥åŠ&lt;a href="https://github.com/kubernetes/website/pull/10485">é‡æ–°è‡´åŠ›äºæ›´è½»æ¾çš„æœ¬åœ°åŒ–&lt;/a>ï¼Œæˆ‘ä»¬æ„è¯†åˆ°ï¼Œå¼€æºæ–‡æ¡£å°±åƒå¼€æºè½¯ä»¶ä¸€æ ·ï¼Œæ˜¯åœ¨å¯èƒ½çš„è¾¹ç¼˜ä¸æ–­è¿›è¡Œå®è·µã€‚&lt;/p>
&lt;!-- Consolidating workflows, language labels, and team-level ownership may seem like simple improvements, but these features make l10n scalable for increasing numbers of l10n teams. While SIG Docs continues to iterate improvements, we've paid off a significant amount of technical debt and streamlined l10n in a single workflow. That's great for the future as well as the present. -->
&lt;p>æ•´åˆå·¥ä½œæµç¨‹ã€è¯­è¨€æ ‡ç­¾å’Œå›¢é˜Ÿçº§æ‰€æœ‰æƒå¯èƒ½çœ‹èµ·æ¥åƒæ˜¯ååˆ†ç®€å•çš„æ”¹è¿›ï¼Œä½†æ˜¯è¿™äº›åŠŸèƒ½ä½¿ l10n å¯ä»¥æ‰©å±•åˆ°è§„æ¨¡è¶Šæ¥è¶Šå¤§çš„ l10n å›¢é˜Ÿã€‚éšç€ SIG Docs ä¸æ–­æ”¹è¿›ï¼Œæˆ‘ä»¬å·²ç»åœ¨å•ä¸€å·¥ä½œæµç¨‹ä¸­å¿è¿˜äº†å¤§é‡æŠ€æœ¯å€ºåŠ¡å¹¶ç®€åŒ–äº† l10nã€‚è¿™å¯¹æœªæ¥å’Œç°åœ¨éƒ½å¾ˆæœ‰ç›Šã€‚&lt;/p>
&lt;!-- ## Consolidated workflow -->
&lt;h2 id="æ•´åˆçš„å·¥ä½œæµç¨‹">æ•´åˆçš„å·¥ä½œæµç¨‹&lt;/h2>
&lt;!-- Localization is now consolidated in the [kubernetes/website](https://github.com/kubernetes/website) repository. We've configured the Kubernetes CI/CD system, [Prow](https://github.com/kubernetes/test-infra/tree/master/prow), to handle automatic language label assignment as well as team-level PR review and approval. -->
&lt;p>ç°åœ¨ï¼Œæœ¬åœ°åŒ–å·²æ•´åˆåˆ° &lt;a href="https://github.com/kubernetes/website">kubernetes/website&lt;/a> å­˜å‚¨åº“ã€‚æˆ‘ä»¬å·²ç»é…ç½®äº† Kubernetes CI/CD ç³»ç»Ÿï¼Œ&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a> æ¥å¤„ç†è‡ªåŠ¨è¯­è¨€æ ‡ç­¾åˆ†é…ä»¥åŠå›¢é˜Ÿçº§ PR å®¡æŸ¥å’Œæ‰¹å‡†ã€‚&lt;/p>
&lt;!-- ### Language labels -->
&lt;h3 id="è¯­è¨€æ ‡ç­¾">è¯­è¨€æ ‡ç­¾&lt;/h3>
&lt;!-- Prow automatically applies language labels based on file path. Thanks to SIG Docs contributor [June Yi](https://github.com/kubernetes/test-infra/pull/9835), folks can also manually assign language labels in pull request (PR) comments. For example, when left as a comment on an issue or PR, this command assigns the label `language/ko` (Korean). -->
&lt;p>Prow æ ¹æ®æ–‡ä»¶è·¯å¾„è‡ªåŠ¨æ·»åŠ è¯­è¨€æ ‡ç­¾ã€‚æ„Ÿè°¢ SIG Docs è´¡çŒ®è€… &lt;a href="https://github.com/kubernetes/test-infra/pull/9835">June Yi&lt;/a>ï¼Œä»–è®©äººä»¬è¿˜å¯ä»¥åœ¨ pull requestï¼ˆPRï¼‰æ³¨é‡Šä¸­æ‰‹åŠ¨åˆ†é…è¯­è¨€æ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œå½“ä¸º issue æˆ– PR ç•™ä¸‹ä¸‹è¿°æ³¨é‡Šæ—¶ï¼Œå°†ä¸ºä¹‹åˆ†é…æ ‡ç­¾ &lt;code>language/ko&lt;/code>ï¼ˆKoreanï¼‰ã€‚&lt;/p>
&lt;pre>&lt;code>/language ko
&lt;/code>&lt;/pre>&lt;!-- These repo labels let reviewers filter for PRs and issues by language. For example, you can now filter the k/website dashboard for [PRs with Chinese content](https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh). -->
&lt;p>è¿™äº›å­˜å‚¨åº“æ ‡ç­¾å…è®¸å®¡é˜…è€…æŒ‰è¯­è¨€è¿‡æ»¤ PR å’Œ issueã€‚ä¾‹å¦‚ï¼Œæ‚¨ç°åœ¨å¯ä»¥è¿‡æ»¤ kubernetes/website é¢æ¿ä¸­&lt;a href="https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh">å…·æœ‰ä¸­æ–‡å†…å®¹çš„ PR&lt;/a>ã€‚&lt;/p>
&lt;!-- ### Team review -->
&lt;h3 id="å›¢é˜Ÿå®¡æ ¸">å›¢é˜Ÿå®¡æ ¸&lt;/h3>
&lt;!-- L10n teams can now review and approve their own PRs. For example, review and approval permissions for English are [assigned in an OWNERS file](https://github.com/kubernetes/website/blob/master/content/en/OWNERS) in the top subfolder for English content. -->
&lt;p>L10n å›¢é˜Ÿç°åœ¨å¯ä»¥å®¡æŸ¥å’Œæ‰¹å‡†ä»–ä»¬è‡ªå·±çš„ PRã€‚ä¾‹å¦‚ï¼Œè‹±è¯­çš„å®¡æ ¸å’Œæ‰¹å‡†æƒé™åœ¨ä½äºç”¨äºæ˜¾ç¤ºè‹±è¯­å†…å®¹çš„é¡¶çº§å­æ–‡ä»¶å¤¹ä¸­çš„ &lt;a href="https://github.com/kubernetes/website/blob/master/content/en/OWNERS">OWNERS æ–‡ä»¶ä¸­æŒ‡å®š&lt;/a>ã€‚&lt;/p>
&lt;!-- Adding `OWNERS` files to subdirectories lets localization teams review and approve changes without requiring a rubber stamp approval from reviewers who may lack fluency. -->
&lt;p>å°† &lt;code>OWNERS&lt;/code> æ–‡ä»¶æ·»åŠ åˆ°å­ç›®å½•å¯ä»¥è®©æœ¬åœ°åŒ–å›¢é˜Ÿå®¡æŸ¥å’Œæ‰¹å‡†æ›´æ”¹ï¼Œè€Œæ— éœ€ç”±å¯èƒ½å¹¶ä¸æ“…é•¿è¯¥é—¨è¯­è¨€çš„å®¡é˜…è€…è¿›è¡Œæ‰¹å‡†ã€‚&lt;/p>
&lt;!-- ## What's next -->
&lt;h2 id="ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆ">ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆ&lt;/h2>
&lt;!-- We're looking forward to the [doc sprint in Shanghai](https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required) to serve as a resource for the Chinese l10n team. -->
&lt;p>æˆ‘ä»¬æœŸå¾…ç€&lt;a href="https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required">ä¸Šæµ·çš„ doc sprint&lt;/a> èƒ½ä½œä¸ºä¸­å›½ l10n å›¢é˜Ÿçš„èµ„æºã€‚&lt;/p>
&lt;!-- We're excited to continue supporting the Japanese and Korean l10n teams, who are making excellent progress. -->
&lt;p>æˆ‘ä»¬å¾ˆé«˜å…´ç»§ç»­æ”¯æŒæ­£åœ¨å–å¾—è‰¯å¥½è¿›å±•çš„æ—¥æœ¬å’ŒéŸ©å›½ l10n é˜Ÿä¼ã€‚&lt;/p>
&lt;!-- If you're interested in localizing Kubernetes for your own language or region, check out our [guide to localizing Kubernetes docs](https://kubernetes.io/docs/contribute/localization/) and reach out to a [SIG Docs chair](https://github.com/kubernetes/community/tree/master/sig-docs#leadership) for support. -->
&lt;p>å¦‚æœæ‚¨æœ‰å…´è¶£å°† Kubernetes æœ¬åœ°åŒ–ä¸ºæ‚¨è‡ªå·±çš„è¯­è¨€æˆ–åœ°åŒºï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„&lt;a href="https://kubernetes.io/docs/contribute/localization/">æœ¬åœ°åŒ– Kubernetes æ–‡æ¡£æŒ‡å—&lt;/a>ï¼Œå¹¶è”ç³» &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs#leadership">SIG Docs ä¸»å¸­å›¢&lt;/a>è·å–æ”¯æŒã€‚&lt;/p>
&lt;!-- ### Get involved with SIG Docs -->
&lt;h3 id="åŠ å…¥sig-docs">åŠ å…¥SIG Docs&lt;/h3>
&lt;!-- If you're interested in Kubernetes documentation, come to a SIG Docs [weekly meeting](https://github.com/kubernetes/community/tree/master/sig-docs#meetings), or join [#sig-docs in Kubernetes Slack](https://kubernetes.slack.com/messages/C1J0BPD2M/details/). -->
&lt;p>å¦‚æœæ‚¨å¯¹ Kubernetes æ–‡æ¡£æ„Ÿå…´è¶£ï¼Œè¯·å‚åŠ  SIG Docs &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs#meetings">æ¯å‘¨ä¼šè®®&lt;/a>ï¼Œæˆ–åœ¨ &lt;a href="https://kubernetes.slack.com/messages/C1J0BPD2M/details/">Kubernetes Slack åŠ å…¥ #sig-docs&lt;/a>ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes 2018 å¹´åŒ—ç¾è´¡çŒ®è€…å³°ä¼š</title><link>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link><pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid><description>
&lt;!--
---
layout: "Blog"
title: "Kubernetes 2018 North American Contributor Summit"
date: 2018-10-16
---
-->
&lt;!--
**Authors:**
-->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong>&lt;/p>
&lt;!--
[Bob Killen][bob] (University of Michigan)
[Sahdev Zala][sahdev] (IBM),
[Ihor Dvoretskyi][ihor] (CNCF)
-->
&lt;p>&lt;a href="https://twitter.com/mrbobbytables">Bob Killen&lt;/a>ï¼ˆå¯†æ­‡æ ¹å¤§å­¦ï¼‰
&lt;a href="https://twitter.com/sp_zala">Sahdev Zala&lt;/a>ï¼ˆIBMï¼‰ï¼Œ
&lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>ï¼ˆCNCFï¼‰&lt;/p>
&lt;!--
The 2018 North American Kubernetes Contributor Summit to be hosted right before
[KubeCon + CloudNativeCon][kubecon] Seattle is shaping up to be the largest yet.
-->
&lt;p>2018 å¹´åŒ—ç¾ Kubernetes è´¡çŒ®è€…å³°ä¼šå°†åœ¨è¥¿é›…å›¾ &lt;a href="https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/">KubeCon + CloudNativeCon&lt;/a> ä¼šè®®ä¹‹å‰ä¸¾åŠï¼Œè¿™å°†æ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„ä¸€æ¬¡ç››ä¼šã€‚&lt;/p>
&lt;!--
It is an event that brings together new and current contributors alike to
connect and share face-to-face; and serves as an opportunity for existing
contributors to help shape the future of community development. For new
community members, it offers a welcoming space to learn, explore and put the
contributor workflow to practice.
-->
&lt;p>è¿™æ˜¯ä¸€ä¸ªå°†æ–°è€è´¡çŒ®è€…èšé›†åœ¨ä¸€èµ·ï¼Œé¢å¯¹é¢äº¤æµå’Œåˆ†äº«çš„æ´»åŠ¨ï¼›å¹¶ä¸ºç°æœ‰çš„è´¡çŒ®è€…æä¾›ä¸€ä¸ªæœºä¼šï¼Œå¸®åŠ©å¡‘é€ ç¤¾åŒºå‘å±•çš„æœªæ¥ã€‚å®ƒä¸ºæ–°çš„ç¤¾åŒºæˆå‘˜æä¾›äº†ä¸€ä¸ªå­¦ä¹ ã€æ¢ç´¢å’Œå®è·µè´¡çŒ®å·¥ä½œæµç¨‹çš„è‰¯å¥½ç©ºé—´ã€‚&lt;/p>
&lt;!--
Unlike previous Contributor Summits, the event now spans two-days with a more
relaxed â€˜hallwayâ€™ track and general Contributor get-together to be hosted from
5-8pm on Sunday December 9th at the [Garage Lounge and Gaming Hall][garage], just
a short walk away from the Convention Center. There, contributors can enjoy
billiards, bowling, trivia and more; accompanied by a variety of food and drink.
-->
&lt;p>ä¸ä¹‹å‰çš„è´¡çŒ®è€…å³°ä¼šä¸åŒï¼Œæœ¬æ¬¡æ´»åŠ¨ä¸ºæœŸä¸¤å¤©ï¼Œæœ‰ä¸€ä¸ªæ›´ä¸ºè½»æ¾çš„è¡Œç¨‹å®‰æ’ï¼Œä¸€èˆ¬è´¡çŒ®è€…å°†äº 12 æœˆ 9 æ—¥ï¼ˆå‘¨æ—¥ï¼‰ä¸‹åˆ 5 ç‚¹è‡³ 8 ç‚¹åœ¨è·ç¦»ä¼šè®®ä¸­å¿ƒä»…å‡ æ­¥è¿œçš„ &lt;a href="https://www.garagebilliards.com/">Garage Lounge and Gaming Hall&lt;/a> ä¸¾åŠå³°ä¼šã€‚åœ¨é‚£é‡Œï¼Œè´¡çŒ®è€…ä¹Ÿå¯ä»¥è¿›è¡Œå°çƒã€ä¿é¾„çƒç­‰å¨±ä¹æ´»åŠ¨ï¼Œè€Œä¸”è¿˜æœ‰å„ç§é£Ÿå“å’Œé¥®æ–™ã€‚&lt;/p>
&lt;!--
Things pick up the following day, Monday the 10th with three separate tracks:
-->
&lt;p>æ¥ä¸‹æ¥çš„ä¸€å¤©ï¼Œä¹Ÿå°±æ˜¯ 10 å·æ˜ŸæœŸä¸€ï¼Œæœ‰ä¸‰ä¸ªç‹¬ç«‹çš„ä¼šè®®ä½ å¯ä»¥é€‰æ‹©å‚ä¸ï¼š&lt;/p>
&lt;!--
### New Contributor Workshop:
A half day workshop aimed at getting new and first time contributors onboarded
and comfortable with working within the Kubernetes Community. Staying for the
duration is required; this is not a workshop you can drop into.
-->
&lt;h3 id="æ–°è´¡çŒ®è€…ç ”è®¨ä¼š">æ–°è´¡çŒ®è€…ç ”è®¨ä¼šï¼š&lt;/h3>
&lt;p>ä¸ºæœŸåŠå¤©çš„ç ”è®¨ä¼šæ—¨åœ¨è®©æ–°è´¡çŒ®è€…åŠ å…¥ç¤¾åŒºï¼Œå¹¶è¥é€ ä¸€ä¸ªè‰¯å¥½çš„ Kubernetes ç¤¾åŒºå·¥ä½œç¯å¢ƒã€‚
è¯·åœ¨å¼€ä¼šæœŸé—´ä¿æŒåœ¨åœºï¼Œè¯¥è®¨è®ºä¼šä¸å…è®¸éšæ„è¿›å‡ºã€‚&lt;/p>
&lt;!--
### Current Contributor Track:
Reserved for those that are actively engaged with the development of the
project; the Current Contributor Track includes Talks, Workshops, Birds of a
Feather, Unconferences, Steering Committee Sessions, and more! Keep an eye on
the [schedule in GitHub][schedule] as content is frequently being updated.
-->
&lt;h3 id="å½“å‰è´¡çŒ®è€…è¿½è¸ª">å½“å‰è´¡çŒ®è€…è¿½è¸ªï¼š&lt;/h3>
&lt;p>ä¿ç•™ç»™é‚£äº›ç§¯æå‚ä¸é¡¹ç›®å¼€å‘çš„è´¡çŒ®è€…ï¼›ç›®å‰çš„è´¡çŒ®è€…è¿½è¸ªåŒ…æ‹¬è®²åº§ã€ç ”è®¨ä¼šã€èšä¼šã€Unconferences ä¼šè®®ã€æŒ‡å¯¼å§”å‘˜ä¼šä¼šè®®ç­‰ç­‰!
è¯·ç•™æ„ &lt;a href="https://git.k8s.io/community/events/2018/12-contributor-summit#agenda">GitHub ä¸­çš„æ—¶é—´è¡¨&lt;/a>ï¼Œå› ä¸ºå†…å®¹ç»å¸¸æ›´æ–°ã€‚&lt;/p>
&lt;!--
### Docs Sprint:
SIG-Docs will have a curated list of issues and challenges to be tackled closer
to the event date.
-->
&lt;h3 id="docs-å†²åˆº">Docs å†²åˆºï¼š&lt;/h3>
&lt;p>SIG-Docs å°†åœ¨æ´»åŠ¨æ—¥æœŸä¸´è¿‘çš„æ—¶å€™åˆ—å‡ºä¸€ä¸ªéœ€è¦å¤„ç†çš„é—®é¢˜å’ŒæŒ‘æˆ˜åˆ—è¡¨ã€‚&lt;/p>
&lt;!--
## To Register:
To register for the Contributor Summit, see the [Registration section of the
Event Details in GitHub][register]. Please note that registrations are being
reviewed. If you select the â€œCurrent Contributor Trackâ€ and are not an active
contributor, you will be asked to attend the New Contributor Workshop, or asked
to be put on a waitlist. With thousands of contributors and only 300 spots, we
need to make sure the right folks are in the room.
-->
&lt;h2 id="æ³¨å†Œ">æ³¨å†Œï¼š&lt;/h2>
&lt;p>è¦æ³¨å†Œè´¡çŒ®è€…å³°ä¼šï¼Œè¯·å‚é˜… Git Hub ä¸Šçš„&lt;a href="https://git.k8s.io/community/events/2018/12-contributor-summit#registration">æ´»åŠ¨è¯¦æƒ…æ³¨å†Œéƒ¨åˆ†&lt;/a>ã€‚è¯·æ³¨æ„æŠ¥åæ­£åœ¨å®¡æ ¸ä¸­ã€‚
å¦‚æœæ‚¨é€‰æ‹©äº† â€œå½“å‰è´¡çŒ®è€…è¿½è¸ªâ€ï¼Œè€Œæ‚¨å´ä¸æ˜¯ä¸€ä¸ªæ´»è·ƒçš„è´¡çŒ®è€…ï¼Œæ‚¨å°†è¢«è¦æ±‚å‚åŠ æ–°è´¡çŒ®è€…ç ”è®¨ä¼šï¼Œæˆ–è€…è¢«è¦æ±‚è¿›å…¥å€™è¡¥åå•ã€‚
æˆåƒä¸Šä¸‡çš„è´¡çŒ®è€…åªæœ‰ 300 ä¸ªä½ç½®ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ­£ç¡®çš„äººè¢«å®‰æ’å¸­ä½ã€‚&lt;/p>
&lt;!--
If you have any questions or concerns, please donâ€™t hesitate to reach out to
the Contributor Summit Events Team at community@kubernetes.io.
-->
&lt;p>å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ï¼Œè¯·éšæ—¶é€šè¿‡ &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a> è”ç³»è´¡çŒ®è€…å³°ä¼šç»„ç»‡å›¢é˜Ÿã€‚&lt;/p>
&lt;!--
Look forward to seeing everyone there!
-->
&lt;p>æœŸå¾…åœ¨é‚£é‡Œçœ‹åˆ°æ¯ä¸ªäººï¼&lt;/p></description></item><item><title>Blog: 2018 å¹´ç£å¯¼å§”å‘˜ä¼šé€‰ä¸¾ç»“æœ</title><link>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</link><pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</guid><description>
&lt;!--
---
layout: blog
title: '2018 Steering Committee Election Results'
date: 2018-10-15
---
-->
&lt;!-- **Authors**: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google) -->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google)&lt;/p>
&lt;!--
## Results
-->
&lt;h2 id="ç»“æœ">ç»“æœ&lt;/h2>
&lt;!--
The [Kubernetes Steering Committee Election](https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/) is now complete and the following candidates came ahead to secure two year terms that start immediately:
-->
&lt;p>&lt;a href="https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/">Kubernetes ç£å¯¼å§”å‘˜ä¼šé€‰ä¸¾&lt;/a>ç°å·²å®Œæˆï¼Œä»¥ä¸‹å€™é€‰äººè·å¾—äº†ç«‹å³å¼€å§‹çš„ä¸¤å¹´ä»»æœŸï¼š&lt;/p>
&lt;ul>
&lt;li>Aaron Crickenberger, Google, &lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>&lt;/li>
&lt;li>Davanum Srinivas, Huawei, &lt;a href="https://github.com/dims">@dims&lt;/a>&lt;/li>
&lt;li>Tim St. Clair, Heptio, &lt;a href="https://github.com/timothysc">@timothysc&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Big Thanks!
-->
&lt;h2 id="ååˆ†æ„Ÿè°¢">ååˆ†æ„Ÿè°¢ï¼&lt;/h2>
&lt;!--
* Steering Committee Member Emeritus [Quinton Hoole](https://github.com/quinton-hoole) for his service to the community over the past year. We look forward to
* The candidates that came forward to run for election. May we always have a strong set of people who want to push community forward like yours in every election.
* All 307 voters who cast a ballot.
* And last but not least...Cornell University for hosting [CIVS](https://civs.cs.cornell.edu/)!
-->
&lt;ul>
&lt;li>ç£å¯¼å§”å‘˜ä¼šè£èª‰é€€ä¼‘æˆå‘˜ &lt;a href="https://github.com/quinton-hoole">Quinton Hoole&lt;/a>ï¼Œè¡¨æ‰¬ä»–åœ¨è¿‡å»ä¸€å¹´ä¸ºç¤¾åŒºæ‰€ä½œçš„è´¡çŒ®ã€‚æˆ‘ä»¬æœŸå¾…ç€&lt;/li>
&lt;li>å‚åŠ ç«é€‰çš„å€™é€‰äººã€‚æ„¿æˆ‘ä»¬æ°¸è¿œæ‹¥æœ‰ä¸€ç¾¤å¼ºå¤§çš„äººï¼Œä»–ä»¬å¸Œæœ›åœ¨æ¯ä¸€æ¬¡é€‰ä¸¾ä¸­éƒ½èƒ½åƒä½ ä»¬ä¸€æ ·æ¨åŠ¨ç¤¾åŒºå‘å‰å‘å±•ã€‚&lt;/li>
&lt;li>å…±è®¡ 307 åé€‰æ°‘å‚ä¸æŠ•ç¥¨ã€‚&lt;/li>
&lt;li>æœ¬æ¬¡é€‰ä¸¾ç”±åº·å¥ˆå°”å¤§å­¦ä¸»åŠ &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>ï¼&lt;/li>
&lt;/ul>
&lt;!--
## Get Involved with the Steering Committee
-->
&lt;h2 id="åŠ å…¥ç£å¯¼å§”å‘˜ä¼š">åŠ å…¥ç£å¯¼å§”å‘˜ä¼š&lt;/h2>
&lt;!--
You can follow along to Steering Committee [backlog items](https://git.k8s.io/steering/backlog.md) and weigh in by filing an issue or creating a PR against their [repo](https://github.com/kubernetes/steering). They meet bi-weekly on [Wednesdays at 8pm UTC](https://github.com/kubernetes/steering) and regularly attend Meet Our Contributors.
-->
&lt;p>ä½ å¯ä»¥å…³æ³¨ç£å¯¼å§”å‘˜ä¼šçš„&lt;a href="https://git.k8s.io/steering/backlog.md">ä»»åŠ¡æ¸…å•&lt;/a>ï¼Œå¹¶é€šè¿‡å‘ä»–ä»¬çš„&lt;a href="https://github.com/kubernetes/steering">ä»£ç ä»“åº“&lt;/a>æäº¤ issue æˆ– PR çš„æ–¹å¼æ¥å‚ä¸ã€‚ä»–ä»¬ä¹Ÿä¼šåœ¨&lt;a href="https://github.com/kubernetes/steering">UTC æ—¶é—´æ¯å‘¨ä¸‰æ™š 8 ç‚¹&lt;/a>ä¸¾è¡Œä¼šè®®ï¼Œå¹¶å®šæœŸä¸æˆ‘ä»¬çš„è´¡çŒ®è€…è§é¢ã€‚&lt;/p>
&lt;!--
Steering Committee Meetings:
-->
&lt;p>ç£å¯¼å§”å‘˜ä¼šä¼šè®®ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube æ’­æ”¾åˆ—è¡¨&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Meet Our Contributors Steering AMAâ€™s:
-->
&lt;p>ä¸æˆ‘ä»¬çš„è´¡çŒ®è€…ä¼šé¢ï¼š&lt;/p>
&lt;!--
* [Oct 3 2018](https://youtu.be/x6Jm8p0K-IQ)
* [Sept 5 2018](https://youtu.be/UbxWV12Or58)
-->
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/x6Jm8p0K-IQ">2018 å¹´ 10 æœˆ 3 æ—¥&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/UbxWV12Or58">2018 å¹´ 7 æœˆ 5 æ—¥&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes ä¸­çš„æ‹“æ‰‘æ„ŸçŸ¥æ•°æ®å·ä¾›åº”</title><link>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</link><pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</guid><description>
&lt;!--
---
layout: blog
title: 'Topology-Aware Volume Provisioning in Kubernetes'
date: 2018-10-11
---
-->
&lt;!--
**Author**: Michelle Au (Google)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Michelle Auï¼ˆè°·æ­Œï¼‰&lt;/p>
&lt;!--
The multi-zone cluster experience with persistent volumes is improving in Kubernetes 1.12 with the topology-aware dynamic provisioning beta feature. This feature allows Kubernetes to make intelligent decisions when dynamically provisioning volumes by getting scheduler input on the best place to provision a volume for a pod. In multi-zone clusters, this means that volumes will get provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance.
-->
&lt;p>é€šè¿‡æä¾›æ‹“æ‰‘æ„ŸçŸ¥åŠ¨æ€å·ä¾›åº”åŠŸèƒ½ï¼Œå…·æœ‰æŒä¹…å·çš„å¤šåŒºåŸŸé›†ç¾¤ä½“éªŒåœ¨ Kubernetes 1.12 ä¸­å¾—åˆ°äº†æ”¹è¿›ã€‚æ­¤åŠŸèƒ½ä½¿å¾— Kubernetes åœ¨åŠ¨æ€ä¾›åº”å·æ—¶èƒ½åšå‡ºæ˜æ™ºçš„å†³ç­–ï¼Œæ–¹æ³•æ˜¯ä»è°ƒåº¦å™¨è·å¾—ä¸º Pod æä¾›æ•°æ®å·çš„æœ€ä½³ä½ç½®ã€‚åœ¨å¤šåŒºåŸŸé›†ç¾¤ç¯å¢ƒï¼Œè¿™æ„å‘³ç€æ•°æ®å·èƒ½å¤Ÿåœ¨æ»¡è¶³ä½ çš„ Pod è¿è¡Œéœ€è¦çš„åˆé€‚çš„åŒºåŸŸè¢«ä¾›åº”ï¼Œä»è€Œå…è®¸æ‚¨è·¨æ•…éšœåŸŸè½»æ¾éƒ¨ç½²å’Œæ‰©å±•æœ‰çŠ¶æ€å·¥ä½œè´Ÿè½½ï¼Œä»è€Œæä¾›é«˜å¯ç”¨æ€§å’Œå®¹é”™èƒ½åŠ›ã€‚&lt;/p>
&lt;!--
## Previous challenges
-->
&lt;h2 id="ä»¥å‰çš„æŒ‘æˆ˜">ä»¥å‰çš„æŒ‘æˆ˜&lt;/h2>
&lt;!--
Before this feature, running stateful workloads with zonal persistent disks (such as AWS ElasticBlockStore, Azure Disk, GCE PersistentDisk) in multi-zone clusters had many challenges. Dynamic provisioning was handled independently from pod scheduling, which meant that as soon as you created a PersistentVolumeClaim (PVC), a volume would get provisioned. This meant that the provisioner had no knowledge of what pods were using the volume, and any pod constraints it had that could impact scheduling.
-->
&lt;p>åœ¨æ­¤åŠŸèƒ½è¢«æä¾›ä¹‹å‰ï¼Œåœ¨å¤šåŒºåŸŸé›†ç¾¤ä¸­ä½¿ç”¨åŒºåŸŸåŒ–çš„æŒä¹…ç£ç›˜ï¼ˆä¾‹å¦‚ AWS ElasticBlockStoreï¼ŒAzure Diskï¼ŒGCE PersistentDiskï¼‰è¿è¡Œæœ‰çŠ¶æ€å·¥ä½œè´Ÿè½½å­˜åœ¨è®¸å¤šæŒ‘æˆ˜ã€‚åŠ¨æ€ä¾›åº”ç‹¬ç«‹äº Pod è°ƒåº¦å¤„ç†ï¼Œè¿™æ„å‘³ç€åªè¦æ‚¨åˆ›å»ºäº†ä¸€ä¸ª PersistentVolumeClaimï¼ˆPVCï¼‰ï¼Œä¸€ä¸ªå·å°±ä¼šè¢«ä¾›åº”ã€‚è¿™ä¹Ÿæ„å‘³ç€ä¾›åº”è€…ä¸çŸ¥é“å“ªäº› Pod æ­£åœ¨ä½¿ç”¨è¯¥å·ï¼Œä¹Ÿä¸æ¸…æ¥šä»»ä½•å¯èƒ½å½±å“è°ƒåº¦çš„ Pod çº¦æŸã€‚&lt;/p>
&lt;!--
This resulted in unschedulable pods because volumes were provisioned in zones that:
-->
&lt;p>è¿™å¯¼è‡´äº†ä¸å¯è°ƒåº¦çš„ Podï¼Œå› ä¸ºåœ¨ä»¥ä¸‹åŒºåŸŸä¸­é…ç½®äº†å·ï¼š&lt;/p>
&lt;!--
* did not have enough CPU or memory resources to run the pod
* conflicted with node selectors, pod affinity or anti-affinity policies
* could not run the pod due to taints
-->
&lt;ul>
&lt;li>æ²¡æœ‰è¶³å¤Ÿçš„ CPU æˆ–å†…å­˜èµ„æºæ¥è¿è¡Œ Pod&lt;/li>
&lt;li>ä¸èŠ‚ç‚¹é€‰æ‹©å™¨ã€Pod äº²å’Œæˆ–åäº²å’Œç­–ç•¥å†²çª&lt;/li>
&lt;li>ç”±äºæ±¡ç‚¹ï¼ˆtaintï¼‰ä¸èƒ½è¿è¡Œ Pod&lt;/li>
&lt;/ul>
&lt;!--
Another common issue was that a non-StatefulSet pod using multiple persistent volumes could have each volume provisioned in a different zone, again resulting in an unschedulable pod.
-->
&lt;p>å¦ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯ï¼Œä½¿ç”¨å¤šä¸ªæŒä¹…å·çš„éæœ‰çŠ¶æ€ Pod å¯èƒ½ä¼šåœ¨ä¸åŒçš„åŒºåŸŸä¸­é…ç½®æ¯ä¸ªå·ï¼Œä»è€Œå¯¼è‡´ä¸€ä¸ªä¸å¯è°ƒåº¦çš„ Podã€‚&lt;/p>
&lt;!--
Suboptimal workarounds included overprovisioning of nodes, or manual creation of volumes in the correct zones, making it difficult to dynamically deploy and scale stateful workloads.
-->
&lt;p>æ¬¡ä¼˜çš„è§£å†³æ–¹æ³•åŒ…æ‹¬èŠ‚ç‚¹è¶…é…ï¼Œæˆ–åœ¨æ­£ç¡®çš„åŒºåŸŸä¸­æ‰‹åŠ¨åˆ›å»ºå·ï¼Œä½†è¿™ä¼šé€ æˆéš¾ä»¥åŠ¨æ€éƒ¨ç½²å’Œæ‰©å±•æœ‰çŠ¶æ€å·¥ä½œè´Ÿè½½çš„é—®é¢˜ã€‚&lt;/p>
&lt;!--
The topology-aware dynamic provisioning feature addresses all of the above issues.
-->
&lt;p>æ‹“æ‰‘æ„ŸçŸ¥åŠ¨æ€ä¾›åº”åŠŸèƒ½è§£å†³äº†ä¸Šè¿°æ‰€æœ‰é—®é¢˜ã€‚&lt;/p>
&lt;!--
## Supported Volume Types
-->
&lt;h2 id="æ”¯æŒçš„å·ç±»å‹">æ”¯æŒçš„å·ç±»å‹&lt;/h2>
&lt;!--
In 1.12, the following drivers support topology-aware dynamic provisioning:
-->
&lt;p>åœ¨ 1.12 ä¸­ï¼Œä»¥ä¸‹é©±åŠ¨ç¨‹åºæ”¯æŒæ‹“æ‰‘æ„ŸçŸ¥åŠ¨æ€ä¾›åº”ï¼š&lt;/p>
&lt;!--
* AWS EBS
* Azure Disk
* GCE PD (including Regional PD)
* CSI (alpha) - currently only the GCE PD CSI driver has implemented topology support
-->
&lt;ul>
&lt;li>AWS EBS&lt;/li>
&lt;li>Azure Disk&lt;/li>
&lt;li>GCE PD ï¼ˆåŒ…æ‹¬ Regional PDï¼‰&lt;/li>
&lt;li>CSIï¼ˆalphaï¼‰ - ç›®å‰åªæœ‰ GCE PD CSI é©±åŠ¨å®ç°äº†æ‹“æ‰‘æ”¯æŒ&lt;/li>
&lt;/ul>
&lt;!--
## Design Principles
-->
&lt;h2 id="è®¾è®¡åŸåˆ™">è®¾è®¡åŸåˆ™&lt;/h2>
&lt;!--
While the initial set of supported plugins are all zonal-based, we designed this feature to adhere to the Kubernetes principle of portability across environments. Topology specification is generalized and uses a similar label-based specification like in Pod nodeSelectors and nodeAffinity. This mechanism allows you to define your own topology boundaries, such as racks in on-premise clusters, without requiring modifications to the scheduler to understand these custom topologies.
-->
&lt;p>è™½ç„¶æœ€åˆæ”¯æŒçš„æ’ä»¶é›†éƒ½æ˜¯åŸºäºåŒºåŸŸçš„ï¼Œä½†æˆ‘ä»¬è®¾è®¡æ­¤åŠŸèƒ½æ—¶éµå¾ª Kubernetes è·¨ç¯å¢ƒå¯ç§»æ¤æ€§çš„åŸåˆ™ã€‚
æ‹“æ‰‘è§„èŒƒæ˜¯é€šç”¨çš„ï¼Œå¹¶ä½¿ç”¨ç±»ä¼¼äºåŸºäºæ ‡ç­¾çš„è§„èŒƒï¼Œå¦‚ Pod nodeSelectors å’Œ nodeAffinityã€‚
è¯¥æœºåˆ¶å…è®¸æ‚¨å®šä¹‰è‡ªå·±çš„æ‹“æ‰‘è¾¹ç•Œï¼Œä¾‹å¦‚å†…éƒ¨éƒ¨ç½²é›†ç¾¤ä¸­çš„æœºæ¶ï¼Œè€Œæ— éœ€ä¿®æ”¹è°ƒåº¦ç¨‹åºä»¥äº†è§£è¿™äº›è‡ªå®šä¹‰æ‹“æ‰‘ã€‚&lt;/p>
&lt;!--
In addition, the topology information is abstracted away from the pod specification, so a pod does not need knowledge of the underlying storage systemâ€™s topology characteristics. This means that you can use the same pod specification across multiple clusters, environments, and storage systems.
-->
&lt;p>æ­¤å¤–ï¼Œæ‹“æ‰‘ä¿¡æ¯æ˜¯ä» Pod è§„èŒƒä¸­æŠ½è±¡å‡ºæ¥çš„ï¼Œå› æ­¤ Pod ä¸éœ€è¦äº†è§£åº•å±‚å­˜å‚¨ç³»ç»Ÿçš„æ‹“æ‰‘ç‰¹å¾ã€‚
è¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨å¤šä¸ªé›†ç¾¤ã€ç¯å¢ƒå’Œå­˜å‚¨ç³»ç»Ÿä¸­ä½¿ç”¨ç›¸åŒçš„ Pod è§„èŒƒã€‚&lt;/p>
&lt;!--
## Getting Started
-->
&lt;h2 id="å…¥é—¨">å…¥é—¨&lt;/h2>
&lt;!--
To enable this feature, all you need to do is to create a StorageClass with `volumeBindingMode` set to `WaitForFirstConsumer`:
-->
&lt;p>è¦å¯ç”¨æ­¤åŠŸèƒ½ï¼Œæ‚¨éœ€è¦åšçš„å°±æ˜¯åˆ›å»ºä¸€ä¸ªå°† &lt;code>volumeBindingMode&lt;/code> è®¾ç½®ä¸º &lt;code>WaitForFirstConsumer&lt;/code> çš„ StorageClassï¼š&lt;/p>
&lt;pre>&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: topology-aware-standard
provisioner: kubernetes.io/gce-pd
volumeBindingMode: WaitForFirstConsumer
parameters:
type: pd-standard
&lt;/code>&lt;/pre>&lt;!--
This new setting instructs the volume provisioner to not create a volume immediately, and instead, wait for a pod using an associated PVC to run through scheduling. Note that previous StorageClass `zone` and `zones` parameters do not need to be specified anymore, as pod policies now drive the decision of which zone to provision a volume in.
-->
&lt;p>è¿™ä¸ªæ–°è®¾ç½®è¡¨æ˜å·é…ç½®å™¨ä¸ç«‹å³åˆ›å»ºå·ï¼Œè€Œæ˜¯ç­‰å¾…ä½¿ç”¨å…³è”çš„ PVC çš„ Pod é€šè¿‡è°ƒåº¦è¿è¡Œã€‚
è¯·æ³¨æ„ï¼Œä¸å†éœ€è¦æŒ‡å®šä»¥å‰çš„ StorageClass &lt;code>zone&lt;/code> å’Œ &lt;code>zones&lt;/code> å‚æ•°ï¼Œå› ä¸ºç°åœ¨åœ¨å“ªä¸ªåŒºåŸŸä¸­é…ç½®å·ç”± Pod ç­–ç•¥å†³å®šã€‚&lt;/p>
&lt;!--
Next, create a pod and PVC with this StorageClass. This sequence is the same as before, but with a different StorageClass specified in the PVC. The following is a hypothetical example, demonstrating the capabilities of the new feature by specifying many pod constraints and scheduling policies:
-->
&lt;p>æ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ­¤ StorageClass åˆ›å»ºä¸€ä¸ª Pod å’Œ PVCã€‚
æ­¤è¿‡ç¨‹ä¸ä¹‹å‰ç›¸åŒï¼Œä½†åœ¨ PVC ä¸­æŒ‡å®šäº†ä¸åŒçš„ StorageClassã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªå‡è®¾ç¤ºä¾‹ï¼Œé€šè¿‡æŒ‡å®šè®¸å¤š Pod çº¦æŸå’Œè°ƒåº¦ç­–ç•¥æ¥æ¼”ç¤ºæ–°åŠŸèƒ½ç‰¹æ€§ï¼š&lt;/p>
&lt;!--
* multiple PVCs in a pod
* nodeAffinity across a subset of zones
* pod anti-affinity on zones
-->
&lt;ul>
&lt;li>ä¸€ä¸ª Pod å¤šä¸ª PVC&lt;/li>
&lt;li>è·¨å­åŒºåŸŸçš„èŠ‚ç‚¹äº²å’Œ&lt;/li>
&lt;li>åŒä¸€åŒºåŸŸ Pod åäº²å’Œ&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: StatefulSet
metadata:
name: web
spec:
serviceName: &amp;quot;nginx&amp;quot;
replicas: 2
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
affinity:
nodeAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
nodeSelectorTerms:
- matchExpressions:
- key: failure-domain.beta.kubernetes.io/zone
operator: In
values:
- us-central1-a
- us-central1-f
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: app
operator: In
values:
- nginx
topologyKey: failure-domain.beta.kubernetes.io/zone
containers:
- name: nginx
image: gcr.io/google_containers/nginx-slim:0.8
ports:
- containerPort: 80
name: web
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
- name: logs
mountPath: /logs
volumeClaimTemplates:
- metadata:
name: www
spec:
accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
storageClassName: topology-aware-standard
resources:
requests:
storage: 10Gi
- metadata:
name: logs
spec:
accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
storageClassName: topology-aware-standard
resources:
requests:
storage: 1Gi
&lt;/code>&lt;/pre>&lt;!--
Afterwards, you can see that the volumes were provisioned in zones according to the policies set by the pod:
-->
&lt;p>ä¹‹åï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ ¹æ® Pod è®¾ç½®çš„ç­–ç•¥åœ¨åŒºåŸŸä¸­é…ç½®å·ï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl get pv -o=jsonpath='{range .items[*]}{.spec.claimRef.name}{&amp;quot;\t&amp;quot;}{.metadata.labels.failure\-domain\.beta\.kubernetes\.io/zone}{&amp;quot;\n&amp;quot;}{end}'
www-web-0 us-central1-f
logs-web-0 us-central1-f
www-web-1 us-central1-a
logs-web-1 us-central1-a
&lt;/code>&lt;/pre>&lt;!--
## How can I learn more?
-->
&lt;h2 id="æˆ‘æ€æ ·æ‰èƒ½äº†è§£æ›´å¤š">æˆ‘æ€æ ·æ‰èƒ½äº†è§£æ›´å¤šï¼Ÿ&lt;/h2>
&lt;!--
Official documentation on the topology-aware dynamic provisioning feature is available here:https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode
-->
&lt;p>æœ‰å…³æ‹“æ‰‘æ„ŸçŸ¥åŠ¨æ€ä¾›åº”åŠŸèƒ½çš„å®˜æ–¹æ–‡æ¡£å¯åœ¨æ­¤å¤„è·å–ï¼šhttps://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&lt;/p>
&lt;!--
Documentation for CSI drivers is available at https://kubernetes-csi.github.io/docs/
-->
&lt;p>æœ‰å…³ CSI é©±åŠ¨ç¨‹åºçš„æ–‡æ¡£ï¼Œè¯·è®¿é—®ï¼šhttps://kubernetes-csi.github.io/docs/&lt;/p>
&lt;!--
## Whatâ€™s next?
-->
&lt;h2 id="ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆ">ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h2>
&lt;!--
We are actively working on improving this feature to support:
-->
&lt;p>æˆ‘ä»¬æ­£ç§¯æè‡´åŠ›äºæ”¹è¿›æ­¤åŠŸèƒ½ä»¥æ”¯æŒï¼š&lt;/p>
&lt;!--
* more volume types, including dynamic provisioning for local volumes
* dynamic volume attachable count and capacity limits per node
-->
&lt;ul>
&lt;li>æ›´å¤šå·ç±»å‹ï¼ŒåŒ…æ‹¬æœ¬åœ°å·çš„åŠ¨æ€ä¾›åº”&lt;/li>
&lt;li>åŠ¨æ€å®¹é‡å¯é™„åŠ è®¡æ•°å’Œæ¯ä¸ªèŠ‚ç‚¹çš„å®¹é‡é™åˆ¶&lt;/li>
&lt;/ul>
&lt;!--
## How do I get involved?
-->
&lt;h2 id="æˆ‘å¦‚ä½•å‚ä¸">æˆ‘å¦‚ä½•å‚ä¸ï¼Ÿ&lt;/h2>
&lt;!--
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Kubernetes Storage Special-Interest-Group](https://github.com/kubernetes/community/tree/master/sig-storage) (SIG). Weâ€™re rapidly growing and always welcome new contributors.
-->
&lt;p>å¦‚æœæ‚¨å¯¹æ­¤åŠŸèƒ½æœ‰åé¦ˆæ„è§æˆ–æœ‰å…´è¶£å‚ä¸è®¾è®¡å’Œå¼€å‘ï¼Œè¯·åŠ å…¥ &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£å°ç»„&lt;/a>ï¼ˆSIGï¼‰ã€‚æˆ‘ä»¬æ­£åœ¨å¿«é€Ÿæˆé•¿ï¼Œå¹¶å§‹ç»ˆæ¬¢è¿æ–°çš„è´¡çŒ®è€…ã€‚&lt;/p>
&lt;!--
Special thanks to all the contributors that helped bring this feature to beta, including Cheng Xing ([verult](https://github.com/verult)), Chuqiang Li ([lichuqiang](https://github.com/lichuqiang)), David Zhu ([davidz627](https://github.com/davidz627)), Deep Debroy ([ddebroy](https://github.com/ddebroy)), Jan Å afrÃ¡nek ([jsafrane](https://github.com/jsafrane)), Jordan Liggitt ([liggitt](https://github.com/liggitt)), Michelle Au ([msau42](https://github.com/msau42)), Pengfei Ni ([feiskyer](https://github.com/feiskyer)), Saad Ali ([saad-ali](https://github.com/saad-ali)), Tim Hockin ([thockin](https://github.com/thockin)), and Yecheng Fu ([cofyc](https://github.com/cofyc)).
-->
&lt;p>ç‰¹åˆ«æ„Ÿè°¢å¸®åŠ©æ¨å‡ºæ­¤åŠŸèƒ½çš„æ‰€æœ‰è´¡çŒ®è€…ï¼ŒåŒ…æ‹¬ Cheng Xing (&lt;a href="https://github.com/verult">verult&lt;/a>)ã€Chuqiang Li (&lt;a href="https://github.com/lichuqiang">lichuqiang&lt;/a>)ã€David Zhu (&lt;a href="https://github.com/davidz627">davidz627&lt;/a>)ã€Deep Debroy (&lt;a href="https://github.com/ddebroy">ddebroy&lt;/a>)ã€Jan Å afrÃ¡nek (&lt;a href="https://github.com/jsafrane">jsafrane&lt;/a>)ã€Jordan Liggitt (&lt;a href="https://github.com/liggitt">liggitt&lt;/a>)ã€Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>)ã€Pengfei Ni (&lt;a href="https://github.com/feiskyer">feiskyer&lt;/a>)ã€Saad Ali (&lt;a href="https://github.com/saad-ali">saad-ali&lt;/a>)ã€Tim Hockin (&lt;a href="https://github.com/thockin">thockin&lt;/a>)ï¼Œä»¥åŠ Yecheng Fu (&lt;a href="https://github.com/cofyc">cofyc&lt;/a>)ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes v1.12: RuntimeClass ç®€ä»‹</title><link>https://kubernetes.io/zh/blog/2018/10/10/kubernetes-v1.12-runtimeclass-%E7%AE%80%E4%BB%8B/</link><pubDate>Wed, 10 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/10/kubernetes-v1.12-runtimeclass-%E7%AE%80%E4%BB%8B/</guid><description>
&lt;!--
---
layout: blog
title: 'Kubernetes v1.12: Introducing RuntimeClass'
date: 2018-10-10
---
-->
&lt;!--
**Author**: Tim Allclair (Google)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Tim Allclair (Google)&lt;/p>
&lt;!--
Kubernetes originally launched with support for Docker containers running native applications on a Linux host. Starting with [rkt](https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/) in Kubernetes 1.3 more runtimes were coming, which lead to the development of the [Container Runtime Interface](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) (CRI). Since then, the set of alternative runtimes has only expanded: projects like [Kata Containers](https://katacontainers.io/) and [gVisor](https://github.com/google/gvisor) were announced for stronger workload isolation, and Kubernetes' Windows support has been [steadily progressing](https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/).
-->
&lt;p>Kubernetes æœ€åˆæ˜¯ä¸ºäº†æ”¯æŒåœ¨ Linux ä¸»æœºä¸Šè¿è¡Œæœ¬æœºåº”ç”¨ç¨‹åºçš„ Docker å®¹å™¨è€Œåˆ›å»ºçš„ã€‚
ä» Kubernetes 1.3ä¸­çš„ &lt;a href="https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/">rkt&lt;/a> å¼€å§‹ï¼Œæ›´å¤šçš„è¿è¡Œæ—¶é—´å¼€å§‹æ¶Œç°ï¼Œ
è¿™å¯¼è‡´äº†&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">å®¹å™¨è¿è¡Œæ—¶æ¥å£ï¼ˆContainer Runtime Interfaceï¼‰&lt;/a>ï¼ˆCRIï¼‰çš„å¼€å‘ã€‚
ä»é‚£æ—¶èµ·ï¼Œå¤‡ç”¨è¿è¡Œæ—¶é›†åˆè¶Šæ¥è¶Šå¤§ï¼š
ä¸ºäº†åŠ å¼ºå·¥ä½œè´Ÿè½½éš”ç¦»ï¼Œ&lt;a href="https://katacontainers.io/">Kata Containers&lt;/a> å’Œ &lt;a href="https://github.com/google/gvisor">gVisor&lt;/a> ç­‰é¡¹ç›®è¢«å‘èµ·ï¼Œ
å¹¶ä¸” Kubernetes å¯¹ Windows çš„æ”¯æŒæ­£åœ¨ &lt;a href="https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/">ç¨³æ­¥å‘å±•&lt;/a> ã€‚&lt;/p>
&lt;!--
With runtimes targeting so many different use cases, a clear need for mixed runtimes in a cluster arose. But all these different ways of running containers have brought a new set of problems to deal with:
-->
&lt;p>ç”±äºå­˜åœ¨è¯¸å¤šé’ˆå¯¹ä¸åŒç”¨ä¾‹çš„è¿è¡Œæ—¶ï¼Œé›†ç¾¤å¯¹æ··åˆè¿è¡Œæ—¶çš„éœ€æ±‚å˜å¾—æ˜æ™°èµ·æ¥ã€‚
ä½†æ˜¯ï¼Œæ‰€æœ‰è¿™äº›ä¸åŒçš„å®¹å™¨è¿è¡Œæ–¹å¼éƒ½å¸¦æ¥äº†ä¸€ç³»åˆ—æ–°é—®é¢˜è¦å¤„ç†ï¼š&lt;/p>
&lt;!--
- How do users know which runtimes are available, and select the runtime for their workloads?
- How do we ensure pods are scheduled to the nodes that support the desired runtime?
- Which runtimes support which features, and how can we surface incompatibilities to the user?
- How do we account for the varying resource overheads of the runtimes?
-->
&lt;ul>
&lt;li>ç”¨æˆ·å¦‚ä½•çŸ¥é“å“ªäº›è¿è¡Œæ—¶å¯ç”¨ï¼Œå¹¶ä¸ºå…¶å·¥ä½œè´Ÿè·é€‰æ‹©è¿è¡Œæ—¶ï¼Ÿ&lt;/li>
&lt;li>æˆ‘ä»¬å¦‚ä½•ç¡®ä¿å°† Pod è¢«è°ƒåº¦åˆ°æ”¯æŒæ‰€éœ€è¿è¡Œæ—¶çš„èŠ‚ç‚¹ä¸Šï¼Ÿ&lt;/li>
&lt;li>å“ªäº›è¿è¡Œæ—¶æ”¯æŒå“ªäº›åŠŸèƒ½ï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•å‘ç”¨æˆ·æ˜¾ç¤ºä¸å…¼å®¹æ€§ï¼Ÿ&lt;/li>
&lt;li>æˆ‘ä»¬å¦‚ä½•è€ƒè™‘è¿è¡Œæ—¶çš„å„ç§èµ„æºå¼€é”€ï¼Ÿ&lt;/li>
&lt;/ul>
&lt;!--
**RuntimeClass** aims to solve these issues.
-->
&lt;p>&lt;strong>RuntimeClass&lt;/strong> æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚&lt;/p>
&lt;!--
## RuntimeClass in Kubernetes 1.12
-->
&lt;h2 id="kubernetes-1-12-ä¸­çš„-runtimeclass">Kubernetes 1.12 ä¸­çš„ RuntimeClass&lt;/h2>
&lt;!--
RuntimeClass was recently introduced as an alpha feature in Kubernetes 1.12. The initial implementation focuses on providing a runtime selection API, and paves the way to address the other open problems.
-->
&lt;p>æœ€è¿‘ï¼ŒRuntimeClass åœ¨ Kubernetes 1.12 ä¸­ä½œä¸º alpha åŠŸèƒ½å¼•å…¥ã€‚
æœ€åˆçš„å®ç°ä¾§é‡äºæä¾›è¿è¡Œæ—¶é€‰æ‹© API ï¼Œå¹¶ä¸ºè§£å†³å…¶ä»–æœªè§£å†³çš„é—®é¢˜é“ºå¹³é“è·¯ã€‚&lt;/p>
&lt;!--
The RuntimeClass resource represents a container runtime supported in a Kubernetes cluster. The cluster provisioner sets up, configures, and defines the concrete runtimes backing the RuntimeClass. In its current form, a RuntimeClassSpec holds a single field, the **RuntimeHandler**. The RuntimeHandler is interpreted by the CRI implementation running on a node, and mapped to the actual runtime configuration. Meanwhile the PodSpec has been expanded with a new field, **RuntimeClassName**, which names the RuntimeClass that should be used to run the pod.
-->
&lt;p>RuntimeClass èµ„æºä»£è¡¨ Kubernetes é›†ç¾¤ä¸­æ”¯æŒçš„å®¹å™¨è¿è¡Œæ—¶ã€‚
é›†ç¾¤åˆ¶å¤‡ç»„ä»¶å®‰è£…ã€é…ç½®å’Œå®šä¹‰æ”¯æŒ RuntimeClass çš„å…·ä½“è¿è¡Œæ—¶ã€‚
åœ¨ RuntimeClassSpec çš„å½“å‰å½¢å¼ä¸­ï¼Œåªæœ‰ä¸€ä¸ªå­—æ®µï¼Œå³ &lt;strong>RuntimeHandler&lt;/strong>ã€‚
RuntimeHandler ç”±åœ¨èŠ‚ç‚¹ä¸Šè¿è¡Œçš„ CRI å®ç°è§£é‡Šï¼Œå¹¶æ˜ å°„åˆ°å®é™…çš„è¿è¡Œæ—¶é…ç½®ã€‚
åŒæ—¶ï¼ŒPodSpec è¢«æ‰©å±•æ·»åŠ äº†ä¸€ä¸ªæ–°å­—æ®µ &lt;strong>RuntimeClassName&lt;/strong>ï¼Œå‘½ååº”è¯¥ç”¨äºè¿è¡Œ Pod çš„ RuntimeClassã€‚&lt;/p>
&lt;!--
Why is RuntimeClass a pod level concept? The Kubernetes resource model expects certain resources to be shareable between containers in the pod. If the pod is made up of different containers with potentially different resource models, supporting the necessary level of resource sharing becomes very challenging. For example, it is extremely difficult to support a loopback (localhost) interface across a VM boundary, but this is a common model for communication between two containers in a pod.
-->
&lt;p>ä¸ºä»€ä¹ˆ RuntimeClass æ˜¯ Pod çº§åˆ«çš„æ¦‚å¿µï¼Ÿ
Kubernetes èµ„æºæ¨¡å‹æœŸæœ› Pod ä¸­çš„å®¹å™¨ä¹‹é—´å¯ä»¥å…±äº«æŸäº›èµ„æºã€‚
å¦‚æœ Pod ç”±å…·æœ‰ä¸åŒèµ„æºæ¨¡å‹çš„ä¸åŒå®¹å™¨ç»„æˆï¼Œæ”¯æŒå¿…è¦æ°´å¹³çš„èµ„æºå…±äº«å˜å¾—éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
ä¾‹å¦‚ï¼Œè¦è·¨ VM è¾¹ç•Œæ”¯æŒæœ¬åœ°å›è·¯ï¼ˆlocalhostï¼‰æ¥å£éå¸¸å›°éš¾ï¼Œä½†è¿™æ˜¯ Pod ä¸­ä¸¤ä¸ªå®¹å™¨ä¹‹é—´é€šä¿¡çš„é€šç”¨æ¨¡å‹ã€‚&lt;/p>
&lt;!--
## What's next?
-->
&lt;h2 id="ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆ">ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h2>
&lt;!--
The RuntimeClass resource is an important foundation for surfacing runtime properties to the control plane. For example, to implement scheduler support for clusters with heterogeneous nodes supporting different runtimes, we might add [NodeAffinity](/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity) terms to the RuntimeClass definition. Another area to address is managing the variable resource requirements to run pods of different runtimes. The [Pod Overhead proposal](https://docs.google.com/document/d/1EJKT4gyl58-kzt2bnwkv08MIUZ6lkDpXcxkHqCvvAp4/preview) was an early take on this that aligns nicely with the RuntimeClass design, and may be pursued further.
-->
&lt;p>RuntimeClass èµ„æºæ˜¯å°†è¿è¡Œæ—¶å±æ€§æ˜¾ç¤ºåˆ°æ§åˆ¶å¹³é¢çš„é‡è¦åŸºç¡€ã€‚
ä¾‹å¦‚ï¼Œè¦å¯¹å…·æœ‰æ”¯æŒä¸åŒè¿è¡Œæ—¶é—´çš„å¼‚æ„èŠ‚ç‚¹çš„ç¾¤é›†å®æ–½è°ƒåº¦ç¨‹åºæ”¯æŒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ RuntimeClass å®šä¹‰ä¸­æ·»åŠ 
&lt;a href="https://kubernetes.io/zh/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">NodeAffinity&lt;/a>æ¡ä»¶ã€‚
å¦ä¸€ä¸ªéœ€è¦è§£å†³çš„é¢†åŸŸæ˜¯ç®¡ç†å¯å˜èµ„æºéœ€æ±‚ä»¥è¿è¡Œä¸åŒè¿è¡Œæ—¶çš„ Podã€‚
&lt;a href="https://docs.google.com/document/d/1EJKT4gyl58-kzt2bnwkv08MIUZ6lkDpXcxkHqCvvAp4/preview">Pod Overhead ææ¡ˆ&lt;/a>
æ˜¯ä¸€é¡¹è¾ƒæ—©çš„å°è¯•ï¼Œä¸ RuntimeClass è®¾è®¡éå¸¸å»åˆï¼Œå¹¶ä¸”å¯èƒ½ä¼šè¿›ä¸€æ­¥æ¨å¹¿ã€‚&lt;/p>
&lt;!--
Many other RuntimeClass extensions have also been proposed, and will be revisited as the feature continues to develop and mature. A few more extensions that are being considered include:
-->
&lt;p>äººä»¬è¿˜æå‡ºäº†è®¸å¤šå…¶ä»– RuntimeClass æ‰©å±•ï¼Œéšç€åŠŸèƒ½çš„ä¸æ–­å‘å±•å’Œæˆç†Ÿï¼Œæˆ‘ä»¬ä¼šé‡æ–°è®¨è®ºè¿™äº›æè®®ã€‚
æ­£åœ¨è€ƒè™‘çš„å…¶ä»–æ‰©å±•åŒ…æ‹¬ï¼š&lt;/p>
&lt;!--
- Surfacing optional features supported by runtimes, and better visibility into errors caused by incompatible features.
- Automatic runtime or feature discovery, to support scheduling decisions without manual configuration.
- Standardized or conformant RuntimeClass names that define a set of properties that should be supported across clusters with RuntimeClasses of the same name.
- Dynamic registration of additional runtimes, so users can install new runtimes on existing clusters with no downtime.
- "Fitting" a RuntimeClass to a pod's requirements. For instance, specifying runtime properties and letting the system match an appropriate RuntimeClass, rather than explicitly assigning a RuntimeClass by name.
-->
&lt;ul>
&lt;li>æä¾›è¿è¡Œæ—¶æ”¯æŒçš„å¯é€‰åŠŸèƒ½ï¼Œå¹¶æ›´å¥½åœ°æŸ¥çœ‹ç”±ä¸å…¼å®¹åŠŸèƒ½å¯¼è‡´çš„é”™è¯¯ã€‚&lt;/li>
&lt;li>è‡ªåŠ¨è¿è¡Œæ—¶æˆ–åŠŸèƒ½å‘ç°ï¼Œæ”¯æŒæ— éœ€æ‰‹åŠ¨é…ç½®çš„è°ƒåº¦å†³ç­–ã€‚&lt;/li>
&lt;li>æ ‡å‡†åŒ–æˆ–ä¸€è‡´çš„ RuntimeClass åç§°ï¼Œç”¨äºå®šä¹‰ä¸€ç»„å…·æœ‰ç›¸åŒåç§°çš„ RuntimeClass çš„é›†ç¾¤åº”æ”¯æŒçš„å±æ€§ã€‚&lt;/li>
&lt;li>åŠ¨æ€æ³¨å†Œé™„åŠ çš„è¿è¡Œæ—¶ï¼Œå› æ­¤ç”¨æˆ·å¯ä»¥åœ¨ä¸åœæœºçš„æƒ…å†µä¸‹åœ¨ç°æœ‰ç¾¤é›†ä¸Šå®‰è£…æ–°çš„è¿è¡Œæ—¶ã€‚&lt;/li>
&lt;li>æ ¹æ® Pod çš„è¦æ±‚â€œåŒ¹é…â€ RuntimeClassã€‚
ä¾‹å¦‚ï¼ŒæŒ‡å®šè¿è¡Œæ—¶å±æ€§å¹¶ä½¿ç³»ç»Ÿä¸é€‚å½“çš„ RuntimeClass åŒ¹é…ï¼Œè€Œä¸æ˜¯é€šè¿‡åç§°æ˜¾å¼åˆ†é… RuntimeClassã€‚&lt;/li>
&lt;/ul>
&lt;!--
RuntimeClass will be under active development at least through 2019, and weâ€™re excited to see the feature take shape, starting with the RuntimeClass alpha in Kubernetes 1.12.
-->
&lt;p>è‡³å°‘è¦åˆ°2019å¹´ï¼ŒRuntimeClass æ‰ä¼šå¾—åˆ°ç§¯æçš„å¼€å‘ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´çœ‹åˆ°ä» Kubernetes 1.12 ä¸­çš„ RuntimeClass alpha å¼€å§‹ï¼Œæ­¤åŠŸèƒ½å¾—ä»¥å½¢æˆã€‚&lt;/p>
&lt;!--
## Learn More
-->
&lt;h2 id="å­¦åˆ°æ›´å¤š">å­¦åˆ°æ›´å¤š&lt;/h2>
&lt;!--
- Take it for a spin! As an alpha feature, there are some additional setup steps to use RuntimeClass. Refer to the [RuntimeClass documentation](/docs/concepts/containers/runtime-class/#runtime-class) for how to get it running.
- Check out the [RuntimeClass Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md) for more nitty-gritty design details.
- The [Sandbox Isolation Level Decision](https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview) documents the thought process that initially went into making RuntimeClass a pod-level choice.
- Join the discussions and help shape the future of RuntimeClass with the [SIG-Node community](https://github.com/kubernetes/community/tree/master/sig-node)
-->
&lt;ul>
&lt;li>è¯•è¯•å§ï¼ ä½œä¸ºAlphaåŠŸèƒ½ï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–è®¾ç½®æ­¥éª¤å¯ä»¥ä½¿ç”¨RuntimeClassã€‚
æœ‰å…³å¦‚ä½•ä½¿å…¶è¿è¡Œï¼Œè¯·å‚è€ƒ &lt;a href="https://kubernetes.io/zh/docs/concepts/containers/runtime-class/#runtime-class">RuntimeClassæ–‡æ¡£&lt;/a> ã€‚&lt;/li>
&lt;li>æŸ¥çœ‹ &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md">RuntimeClass Kubernetes å¢å¼ºå»ºè®®&lt;/a> ä»¥è·å–æ›´å¤šç»†èŠ‚è®¾è®¡ç»†èŠ‚ã€‚&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview">æ²™ç›’éš”ç¦»çº§åˆ«å†³ç­–&lt;/a>
è®°å½•äº†æœ€åˆä½¿ RuntimeClass æˆä¸º Pod çº§åˆ«é€‰é¡¹çš„æ€è€ƒè¿‡ç¨‹ã€‚&lt;/li>
&lt;li>åŠ å…¥è®¨è®ºï¼Œå¹¶é€šè¿‡ &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG-Nodeç¤¾åŒº&lt;/a> å¸®åŠ©å¡‘é€  RuntimeClass çš„æœªæ¥ã€‚&lt;/li>
&lt;/ul></description></item><item><title>Blog: KubeDirectorï¼šåœ¨ Kubernetes ä¸Šè¿è¡Œå¤æ‚çŠ¶æ€åº”ç”¨ç¨‹åºçš„ç®€å•æ–¹æ³•</title><link>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</guid><description>
&lt;!--
layout: blog
title: 'KubeDirector: The easy way to run complex stateful applications on Kubernetes'
date: 2018-10-03
-->
&lt;!--
**Author**: Thomas Phelan (BlueData)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šThomas Phelanï¼ˆBlueDataï¼‰&lt;/p>
&lt;!--
KubeDirector is an open source project designed to make it easy to run complex stateful scale-out application clusters on Kubernetes. KubeDirector is built using the custom resource definition (CRD) framework and leverages the native Kubernetes API extensions and design philosophy. This enables transparent integration with Kubernetes user/resource management as well as existing clients and tools.
-->
&lt;p>KubeDirector æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨ç®€åŒ–åœ¨ Kubernetes ä¸Šè¿è¡Œå¤æ‚çš„æœ‰çŠ¶æ€æ‰©å±•åº”ç”¨ç¨‹åºé›†ç¾¤ã€‚KubeDirector ä½¿ç”¨è‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰
æ¡†æ¶æ„å»ºï¼Œå¹¶åˆ©ç”¨äº†æœ¬åœ° Kubernetes API æ‰©å±•å’Œè®¾è®¡å“²å­¦ã€‚è¿™æ”¯æŒä¸ Kubernetes ç”¨æˆ·/èµ„æº ç®¡ç†ä»¥åŠç°æœ‰å®¢æˆ·ç«¯å’Œå·¥å…·çš„é€æ˜é›†æˆã€‚&lt;/p>
&lt;!--
We recently [introduced the KubeDirector project](https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/), as part of a broader open source Kubernetes initiative we call BlueK8s. Iâ€™m happy to announce that the pre-alpha
code for [KubeDirector](https://github.com/bluek8s/kubedirector/) is now available. And in this blog post, Iâ€™ll show how it works.
-->
&lt;p>æˆ‘ä»¬æœ€è¿‘&lt;a href="https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/">ä»‹ç»äº† KubeDirector é¡¹ç›®&lt;/a>ï¼Œä½œä¸ºæˆ‘ä»¬ç§°ä¸º BlueK8s çš„æ›´å¹¿æ³›çš„ Kubernetes å¼€æºé¡¹ç›®çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘å¾ˆé«˜å…´åœ°å®£å¸ƒ &lt;a href="https://github.com/bluek8s/kubedirector/">KubeDirector&lt;/a> çš„
pre-alpha ä»£ç ç°åœ¨å·²ç»å¯ç”¨ã€‚åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚&lt;/p>
&lt;!--
KubeDirector provides the following capabilities:
-->
&lt;p>KubeDirector æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š&lt;/p>
&lt;!--
* The ability to run non-cloud native stateful applications on Kubernetes without modifying the code. In other words, itâ€™s not necessary to decompose these existing applications to fit a microservices design pattern.
* Native support for preserving application-specific configuration and state.
* An application-agnostic deployment pattern, minimizing the time to onboard new stateful applications to Kubernetes.
-->
&lt;ul>
&lt;li>æ— éœ€ä¿®æ”¹ä»£ç å³å¯åœ¨ Kubernetes ä¸Šè¿è¡Œéäº‘åŸç”Ÿæœ‰çŠ¶æ€åº”ç”¨ç¨‹åºã€‚æ¢å¥è¯è¯´ï¼Œä¸éœ€è¦åˆ†è§£è¿™äº›ç°æœ‰çš„åº”ç”¨ç¨‹åºæ¥é€‚åº”å¾®æœåŠ¡è®¾è®¡æ¨¡å¼ã€‚&lt;/li>
&lt;li>æœ¬æœºæ”¯æŒä¿å­˜ç‰¹å®šäºåº”ç”¨ç¨‹åºçš„é…ç½®å’ŒçŠ¶æ€ã€‚&lt;/li>
&lt;li>ä¸åº”ç”¨ç¨‹åºæ— å…³çš„éƒ¨ç½²æ¨¡å¼ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘å°†æ–°çš„æœ‰çŠ¶æ€åº”ç”¨ç¨‹åºè£…è½½åˆ° Kubernetes çš„æ—¶é—´ã€‚&lt;/li>
&lt;/ul>
&lt;!--
KubeDirector enables data scientists familiar with data-intensive distributed applications such as Hadoop, Spark, Cassandra, TensorFlow, Caffe2, etc. to run these applications on Kubernetes -- with a minimal learning curve and no need to write GO code. The applications controlled by KubeDirector are defined by some basic metadata and an associated package of configuration artifacts. The application metadata is referred to as a KubeDirectorApp resource.
-->
&lt;p>KubeDirector ä½¿ç†Ÿæ‚‰æ•°æ®å¯†é›†å‹åˆ†å¸ƒå¼åº”ç”¨ç¨‹åºï¼ˆå¦‚ Hadoopã€Sparkã€Cassandraã€TensorFlowã€Caffe2 ç­‰ï¼‰çš„æ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿåœ¨ Kubernetes ä¸Šè¿è¡Œè¿™äº›åº”ç”¨ç¨‹åº -- åªéœ€æå°‘çš„å­¦ä¹ æ›²çº¿ï¼Œæ— éœ€ç¼–å†™ GO ä»£ç ã€‚ç”± KubeDirector æ§åˆ¶çš„åº”ç”¨ç¨‹åºç”±ä¸€äº›åŸºæœ¬å…ƒæ•°æ®å’Œç›¸å…³çš„é…ç½®å·¥ä»¶åŒ…å®šä¹‰ã€‚åº”ç”¨ç¨‹åºå…ƒæ•°æ®ç§°ä¸º KubeDirectorApp èµ„æºã€‚&lt;/p>
&lt;!--
To understand the components of KubeDirector, clone the repository on [GitHub](https://github.com/bluek8s/kubedirector/) using a command similar to:
-->
&lt;p>è¦äº†è§£ KubeDirector çš„ç»„ä»¶ï¼Œè¯·ä½¿ç”¨ç±»ä¼¼äºä»¥ä¸‹çš„å‘½ä»¤åœ¨ &lt;a href="https://github.com/bluek8s/kubedirector/">GitHub&lt;/a> ä¸Šå…‹éš†å­˜å‚¨åº“ï¼š&lt;/p>
&lt;pre>&lt;code>git clone http://&amp;lt;userid&amp;gt;@github.com/bluek8s/kubedirector.
&lt;/code>&lt;/pre>&lt;!--
The KubeDirectorApp definition for the Spark 2.2.1 application is located
in the file `kubedirector/deploy/example_catalog/cr-app-spark221e2.json`.
-->
&lt;p>Spark 2.2.1 åº”ç”¨ç¨‹åºçš„ KubeDirectorApp å®šä¹‰ä½äºæ–‡ä»¶ &lt;code>kubedirector/deploy/example_catalog/cr-app-spark221e2.json&lt;/code> ä¸­ã€‚&lt;/p>
&lt;pre>&lt;code>~&amp;gt; cat kubedirector/deploy/example_catalog/cr-app-spark221e2.json
{
&amp;quot;apiVersion&amp;quot;: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;,
&amp;quot;kind&amp;quot;: &amp;quot;KubeDirectorApp&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;name&amp;quot; : &amp;quot;spark221e2&amp;quot;
},
&amp;quot;spec&amp;quot; : {
&amp;quot;systemctlMounts&amp;quot;: true,
&amp;quot;config&amp;quot;: {
&amp;quot;node_services&amp;quot;: [
{
&amp;quot;service_ids&amp;quot;: [
&amp;quot;ssh&amp;quot;,
&amp;quot;spark&amp;quot;,
&amp;quot;spark_master&amp;quot;,
&amp;quot;spark_worker&amp;quot;
],
â€¦
&lt;/code>&lt;/pre>&lt;!--
The configuration of an application cluster is referred to as a KubeDirectorCluster resource. The
KubeDirectorCluster definition for a sample Spark 2.2.1 cluster is located in the file
`kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml`.
-->
&lt;p>åº”ç”¨ç¨‹åºé›†ç¾¤çš„é…ç½®ç§°ä¸º KubeDirectorCluster èµ„æºã€‚ç¤ºä¾‹ Spark 2.2.1 é›†ç¾¤çš„ KubeDirectorCluster å®šä¹‰ä½äºæ–‡ä»¶
&lt;code>kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml&lt;/code> ä¸­ã€‚&lt;/p>
&lt;pre>&lt;code>~&amp;gt; cat kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml
apiVersion: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;
kind: &amp;quot;KubeDirectorCluster&amp;quot;
metadata:
name: &amp;quot;spark221e2&amp;quot;
spec:
app: spark221e2
roles:
- name: controller
replicas: 1
resources:
requests:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
limits:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
- name: worker
replicas: 2
resources:
requests:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
limits:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
- name: jupyter
â€¦
&lt;/code>&lt;/pre>&lt;!--
## Running Spark on Kubernetes with KubeDirector
-->
&lt;h2 id="ä½¿ç”¨-kubedirector-åœ¨-kubernetes-ä¸Šè¿è¡Œ-spark">ä½¿ç”¨ KubeDirector åœ¨ Kubernetes ä¸Šè¿è¡Œ Spark&lt;/h2>
&lt;!--
With KubeDirector, itâ€™s easy to run Spark clusters on Kubernetes.
-->
&lt;p>ä½¿ç”¨ KubeDirectorï¼Œå¯ä»¥è½»æ¾åœ¨ Kubernetes ä¸Šè¿è¡Œ Spark é›†ç¾¤ã€‚&lt;/p>
&lt;!--
First, verify that Kubernetes (version 1.9 or later) is running, using the command `kubectl version`
-->
&lt;p>é¦–å…ˆï¼Œä½¿ç”¨å‘½ä»¤ &lt;code>kubectl version&lt;/code> éªŒè¯ Kubernetesï¼ˆç‰ˆæœ¬ 1.9 æˆ–æ›´é«˜ï¼‰æ˜¯å¦æ­£åœ¨è¿è¡Œ&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T18:02:47Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T17:53:03Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code>&lt;/pre>&lt;!--
Deploy the KubeDirector service and the example KubeDirectorApp resource definitions with the commands:
-->
&lt;p>ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤éƒ¨ç½² KubeDirector æœåŠ¡å’Œç¤ºä¾‹ KubeDirectorApp èµ„æºå®šä¹‰ï¼š&lt;/p>
&lt;pre>&lt;code>cd kubedirector
make deploy
&lt;/code>&lt;/pre>&lt;!--
These will start the KubeDirector pod:
-->
&lt;p>è¿™äº›å°†å¯åŠ¨ KubeDirector podï¼š&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
kubedirector-58cf59869-qd9hb 1/1 Running 0 1m
&lt;/code>&lt;/pre>&lt;!--
List the installed KubeDirector applications with `kubectl get KubeDirectorApp`
-->
&lt;p>&lt;code>kubectl get KubeDirectorApp&lt;/code> åˆ—å‡ºä¸­å·²å®‰è£…çš„ KubeDirector åº”ç”¨ç¨‹åº&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get KubeDirectorApp
NAME AGE
cassandra311 30m
spark211up 30m
spark221e2 30m
&lt;/code>&lt;/pre>&lt;!--
Now you can launch a Spark 2.2.1 cluster using the example KubeDirectorCluster file and the
`kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml` command.
Verify that the Spark cluster has been started:
-->
&lt;p>ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ç¤ºä¾‹ KubeDirectorCluster æ–‡ä»¶å’Œ &lt;code>kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml&lt;/code> å‘½ä»¤
å¯åŠ¨ Spark 2.2.1 é›†ç¾¤ã€‚éªŒè¯ Spark é›†ç¾¤å·²ç»å¯åŠ¨:&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
kubedirector-58cf59869-djdwl 1/1 Running 0 19m
spark221e2-controller-zbg4d-0 1/1 Running 0 23m
spark221e2-jupyter-2km7q-0 1/1 Running 0 23m
spark221e2-worker-4gzbz-0 1/1 Running 0 23m
spark221e2-worker-4gzbz-1 1/1 Running 0 23m
&lt;/code>&lt;/pre>&lt;!--
The running services now include the Spark services:
-->
&lt;p>ç°åœ¨è¿è¡Œçš„æœåŠ¡åŒ…æ‹¬ Spark æœåŠ¡ï¼š&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubedirector ClusterIP 10.98.234.194 &amp;lt;none&amp;gt; 60000/TCP 1d
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 1d
svc-spark221e2-5tg48 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 21s
svc-spark221e2-controller-tq8d6-0 NodePort 10.104.181.123 &amp;lt;none&amp;gt; 22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 20s
svc-spark221e2-jupyter-6989v-0 NodePort 10.105.227.249 &amp;lt;none&amp;gt; 22:30632/TCP,8888:30355/TCP 20s
svc-spark221e2-worker-d9892-0 NodePort 10.107.131.165 &amp;lt;none&amp;gt; 22:30358/TCP,8081:32144/TCP 20s
svc-spark221e2-worker-d9892-1 NodePort 10.110.88.221 &amp;lt;none&amp;gt; 22:30294/TCP,8081:31436/TCP 20s
&lt;/code>&lt;/pre>&lt;!--
Pointing the browser at port 31533 connects to the Spark Master UI:
-->
&lt;p>å°†æµè§ˆå™¨æŒ‡å‘ç«¯å£ 31533 è¿æ¥åˆ° Spark ä¸»èŠ‚ç‚¹ UIï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-10-03-kubedirector/kubedirector.png" alt="kubedirector">&lt;/p>
&lt;!--
Thatâ€™s all there is to it!
In fact, in the example above we also deployed a Jupyter notebook along with the Spark cluster.
-->
&lt;p>å°±æ˜¯è¿™æ ·!
äº‹å®ä¸Šï¼Œåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¿˜éƒ¨ç½²äº†ä¸€ä¸ª Jupyter notebook å’Œ Spark é›†ç¾¤ã€‚&lt;/p>
&lt;!--
To start another application (e.g. Cassandra), just specify another KubeDirectorApp file:
-->
&lt;p>è¦å¯åŠ¨å¦ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ Cassandraï¼‰ï¼Œåªéœ€æŒ‡å®šå¦ä¸€ä¸ª KubeDirectorApp æ–‡ä»¶ï¼š&lt;/p>
&lt;pre>&lt;code>kubectl create -f deploy/example_clusters/cr-cluster-cassandra311.yaml
&lt;/code>&lt;/pre>&lt;!--
See the running Cassandra cluster:
-->
&lt;p>æŸ¥çœ‹æ­£åœ¨è¿è¡Œçš„ Cassandra é›†ç¾¤ï¼š&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
cassandra311-seed-v24r6-0 1/1 Running 0 1m
cassandra311-seed-v24r6-1 1/1 Running 0 1m
cassandra311-worker-rqrhl-0 1/1 Running 0 1m
cassandra311-worker-rqrhl-1 1/1 Running 0 1m
kubedirector-58cf59869-djdwl 1/1 Running 0 1d
spark221e2-controller-tq8d6-0 1/1 Running 0 22m
spark221e2-jupyter-6989v-0 1/1 Running 0 22m
spark221e2-worker-d9892-0 1/1 Running 0 22m
spark221e2-worker-d9892-1 1/1 Running 0 22m
&lt;/code>&lt;/pre>&lt;!--
Now you have a Spark cluster (with a Jupyter notebook) and a Cassandra cluster running on Kubernetes.
Use `kubectl get service` to see the set of services.
-->
&lt;p>ç°åœ¨ï¼Œæ‚¨æœ‰ä¸€ä¸ª Spark é›†ç¾¤ï¼ˆå¸¦æœ‰ Jupyter notebook ï¼‰å’Œä¸€ä¸ªè¿è¡Œåœ¨ Kubernetes ä¸Šçš„ Cassandra é›†ç¾¤ã€‚
ä½¿ç”¨ &lt;code>kubectl get service&lt;/code> æŸ¥çœ‹æœåŠ¡é›†ã€‚&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubedirector ClusterIP 10.98.234.194 &amp;lt;none&amp;gt; 60000/TCP 1d
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 1d
svc-cassandra311-seed-v24r6-0 NodePort 10.96.94.204 &amp;lt;none&amp;gt; 22:31131/TCP,9042:30739/TCP 3m
svc-cassandra311-seed-v24r6-1 NodePort 10.106.144.52 &amp;lt;none&amp;gt; 22:30373/TCP,9042:32662/TCP 3m
svc-cassandra311-vhh29 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 3m
svc-cassandra311-worker-rqrhl-0 NodePort 10.109.61.194 &amp;lt;none&amp;gt; 22:31832/TCP,9042:31962/TCP 3m
svc-cassandra311-worker-rqrhl-1 NodePort 10.97.147.131 &amp;lt;none&amp;gt; 22:31454/TCP,9042:31170/TCP 3m
svc-spark221e2-5tg48 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 24m
svc-spark221e2-controller-tq8d6-0 NodePort 10.104.181.123 &amp;lt;none&amp;gt; 22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 24m
svc-spark221e2-jupyter-6989v-0 NodePort 10.105.227.249 &amp;lt;none&amp;gt; 22:30632/TCP,8888:30355/TCP 24m
svc-spark221e2-worker-d9892-0 NodePort 10.107.131.165 &amp;lt;none&amp;gt; 22:30358/TCP,8081:32144/TCP 24m
svc-spark221e2-worker-d9892-1 NodePort 10.110.88.221 &amp;lt;none&amp;gt; 22:30294/TCP,8081:31436/TCP 24m
&lt;/code>&lt;/pre>&lt;!--
## Get Involved
-->
&lt;h2 id="å‚ä¸å…¶ä¸­">å‚ä¸å…¶ä¸­&lt;/h2>
&lt;!--
KubeDirector is a fully open source, Apache v2 licensed, project â€“ the first of multiple open source projects within a broader initiative we call BlueK8s.
The pre-alpha code for KubeDirector has just been released and we would love for you to join the growing community of developers, contributors, and adopters.
Follow [@BlueK8s](https://twitter.com/BlueK8s/) on Twitter and get involved through these channels:
-->
&lt;p>KubeDirector æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾æºç çš„ Apache v2 æˆæƒé¡¹ç›® â€“ åœ¨æˆ‘ä»¬ç§°ä¸º BlueK8s çš„æ›´å¹¿æ³›çš„è®¡åˆ’ä¸­ï¼Œå®ƒæ˜¯å¤šä¸ªå¼€æ”¾æºç é¡¹ç›®ä¸­çš„ç¬¬ä¸€ä¸ªã€‚
KubeDirector çš„ pre-alpha ä»£ç åˆšåˆšå‘å¸ƒï¼Œæˆ‘ä»¬å¸Œæœ›æ‚¨åŠ å…¥åˆ°ä¸æ–­å¢é•¿çš„å¼€å‘äººå‘˜ã€è´¡çŒ®è€…å’Œä½¿ç”¨è€…ç¤¾åŒºã€‚
åœ¨ Twitter ä¸Šå…³æ³¨ &lt;a href="https://twitter.com/BlueK8s/">@BlueK8s&lt;/a>ï¼Œå¹¶é€šè¿‡ä»¥ä¸‹æ¸ é“å‚ä¸:&lt;/p>
&lt;!--
* KubeDirector [chat room on Slack](https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/)
* KubeDirector [GitHub repo](https://github.com/bluek8s/kubedirector/)
-->
&lt;ul>
&lt;li>KubeDirector &lt;a href="https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/">Slack èŠå¤©å®¤&lt;/a>&lt;/li>
&lt;li>KubeDirector &lt;a href="https://github.com/bluek8s/kubedirector/">GitHub ä»“åº“&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: åœ¨ Kubernetes ä¸Šå¯¹ gRPC æœåŠ¡å™¨è¿›è¡Œå¥åº·æ£€æŸ¥</title><link>https://kubernetes.io/zh/blog/2018/10/01/%E5%9C%A8-kubernetes-%E4%B8%8A%E5%AF%B9-grpc-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/01/%E5%9C%A8-kubernetes-%E4%B8%8A%E5%AF%B9-grpc-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</guid><description>
&lt;!--
---
layout: blog
title: 'Health checking gRPC servers on Kubernetes'
date: 2018-10-01
---
--->
&lt;!--
**Author**: [Ahmet Alp Balkan](https://twitter.com/ahmetb) (Google)
--->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼š &lt;a href="https://twitter.com/ahmetb">Ahmet Alp Balkan&lt;/a> (Google)&lt;/p>
&lt;!--
**Update (December 2021):** _Kubernetes now has built-in gRPC health probes starting in v1.23.
To learn more, see [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe).
This article was originally written about an external tool to achieve the same task._
-->
&lt;p>&lt;strong>æ›´æ–°ï¼ˆ2021 å¹´ 12 æœˆï¼‰ï¼š&lt;/strong> â€œKubernetes ä» v1.23 å¼€å§‹å…·æœ‰å†…ç½® gRPC å¥åº·æ¢æµ‹ã€‚
äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…&lt;a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe">é…ç½®å­˜æ´»æ¢é’ˆã€å°±ç»ªæ¢é’ˆå’Œå¯åŠ¨æ¢é’ˆ&lt;/a>ã€‚
æœ¬æ–‡æœ€åˆæ˜¯ä¸ºæœ‰å…³å®ç°ç›¸åŒä»»åŠ¡çš„å¤–éƒ¨å·¥å…·æ‰€å†™ã€‚â€&lt;/p>
&lt;!--
[gRPC](https://grpc.io) is on its way to becoming the lingua franca for
communication between cloud-native microservices. If you are deploying gRPC
applications to Kubernetes today, you may be wondering about the best way to
configure health checks. In this article, we will talk about
[grpc-health-probe](https://github.com/grpc-ecosystem/grpc-health-probe/), a
Kubernetes-native way to health check gRPC apps.
--->
&lt;p>&lt;a href="https://grpc.io">gRPC&lt;/a> å°†æˆä¸ºæœ¬åœ°äº‘å¾®æœåŠ¡é—´è¿›è¡Œé€šä¿¡çš„é€šç”¨è¯­è¨€ã€‚å¦‚æœæ‚¨ç°åœ¨å°† gRPC åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ° Kubernetesï¼Œæ‚¨å¯èƒ½ä¼šæƒ³è¦äº†è§£é…ç½®å¥åº·æ£€æŸ¥çš„æœ€ä½³æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç» &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">grpc-health-probe&lt;/a>ï¼Œè¿™æ˜¯ Kubernetes åŸç”Ÿçš„å¥åº·æ£€æŸ¥ gRPC åº”ç”¨ç¨‹åºçš„æ–¹æ³•ã€‚&lt;/p>
&lt;!--
If you're unfamiliar, Kubernetes [health
checks](/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
(liveness and readiness probes) is what's keeping your applications available
while you're sleeping. They detect unresponsive pods, mark them unhealthy, and
cause these pods to be restarted or rescheduled.
--->
&lt;p>å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ï¼ŒKubernetesçš„ &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">å¥åº·æ£€æŸ¥&lt;/a>ï¼ˆå­˜æ´»æ¢é’ˆå’Œå°±ç»ªæ¢é’ˆï¼‰å¯ä»¥ä½¿æ‚¨çš„åº”ç”¨ç¨‹åºåœ¨ç¡çœ æ—¶ä¿æŒå¯ç”¨çŠ¶æ€ã€‚å½“æ£€æµ‹åˆ°æ²¡æœ‰å›åº”çš„ Pod æ—¶ï¼Œä¼šå°†å…¶æ ‡è®°ä¸ºä¸å¥åº·ï¼Œå¹¶ä½¿è¿™äº› Pod é‡æ–°å¯åŠ¨æˆ–é‡æ–°å®‰æ’ã€‚&lt;/p>
&lt;!--
Kubernetes [does not
support](https://github.com/kubernetes/kubernetes/issues/21493) gRPC health
checks natively. This leaves the gRPC developers with the following three
approaches when they deploy to Kubernetes:
[![options for health checking grpc on kubernetes today](/images/blog/2019-09-30-health-checking-grpc/options.png)](/images/blog/2019-09-30-health-checking-grpc/options.png)
--->
&lt;p>Kubernetes åŸæœ¬ &lt;a href="https://github.com/kubernetes/kubernetes/issues/21493">ä¸æ”¯æŒ&lt;/a> gRPC å¥åº·æ£€æŸ¥ã€‚gRPC çš„å¼€å‘äººå‘˜åœ¨ Kubernetes ä¸­éƒ¨ç½²æ—¶å¯ä»¥é‡‡ç”¨ä»¥ä¸‹ä¸‰ç§æ–¹æ³•ï¼š&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/options.png">&lt;img src="https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/options.png" alt="å½“å‰åœ¨ kubernetes ä¸Šè¿›è¡Œ gRPC å¥åº·æ£€æŸ¥çš„é€‰é¡¹">&lt;/a>&lt;/p>
&lt;!--
1. **httpGet probe:** Cannot be natively used with gRPC. You need to refactor
your app to serve both gRPC and HTTP/1.1 protocols (on different port
numbers).
2. **tcpSocket probe:** Opening a socket to gRPC server is not meaningful,
since it cannot read the response body.
3. **exec probe:** This invokes a program in a container's ecosystem
periodically. In the case of gRPC, this means you implement a health RPC
yourself, then write and ship a client tool with your container.
Can we do better? Absolutely.
--->
&lt;ol>
&lt;li>&lt;strong>httpGet probï¼š&lt;/strong> ä¸èƒ½ä¸ gRPC ä¸€èµ·ä½¿ç”¨ã€‚æ‚¨éœ€è¦é‡æ„æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œå¿…é¡»åŒæ—¶æ”¯æŒ gRPC å’Œ HTTP/1.1 åè®®ï¼ˆåœ¨ä¸åŒçš„ç«¯å£å·ä¸Šï¼‰ã€‚&lt;/li>
&lt;li>&lt;strong>tcpSocket probeï¼š&lt;/strong> æ‰“å¼€ gRPC æœåŠ¡å™¨çš„ Socket æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºå®ƒæ— æ³•è¯»å–å“åº”ä¸»ä½“ã€‚&lt;/li>
&lt;li>&lt;strong>exec probeï¼š&lt;/strong> å°†å®šæœŸè°ƒç”¨å®¹å™¨ç”Ÿæ€ç³»ç»Ÿä¸­çš„ç¨‹åºã€‚å¯¹äº gRPCï¼Œè¿™æ„å‘³ç€æ‚¨è¦è‡ªå·±å®ç°å¥åº· RPCï¼Œç„¶åä½¿ç”¨å®¹å™¨ç¼–å†™å¹¶äº¤ä»˜å®¢æˆ·ç«¯å·¥å…·ã€‚&lt;/li>
&lt;/ol>
&lt;p>æˆ‘ä»¬å¯ä»¥åšå¾—æ›´å¥½å—ï¼Ÿè¿™æ˜¯è‚¯å®šçš„ã€‚&lt;/p>
&lt;!--
## Introducing â€œgrpc-health-probeâ€
To standardize the "exec probe" approach mentioned above, we need:
- a **standard** health check "protocol" that can be implemented in any gRPC
server easily.
- a **standard** health check "tool" that can query the health protocol easily.
--->
&lt;h2 id="ä»‹ç»-grpc-health-probe">ä»‹ç» â€œgrpc-health-probeâ€&lt;/h2>
&lt;p>ä¸ºäº†ä½¿ä¸Šè¿° &amp;quot;exec probe&amp;quot; æ–¹æ³•æ ‡å‡†åŒ–ï¼Œæˆ‘ä»¬éœ€è¦ï¼š&lt;/p>
&lt;ul>
&lt;li>å¯ä»¥åœ¨ä»»ä½• gRPC æœåŠ¡å™¨ä¸­è½»æ¾å®ç°çš„ &lt;strong>æ ‡å‡†&lt;/strong> å¥åº·æ£€æŸ¥ &amp;quot;åè®®&amp;quot; ã€‚&lt;/li>
&lt;li>ä¸€ç§ &lt;strong>æ ‡å‡†&lt;/strong> å¥åº·æ£€æŸ¥ &amp;quot;å·¥å…·&amp;quot; ï¼Œå¯ä»¥è½»æ¾æŸ¥è¯¢å¥åº·åè®®ã€‚&lt;/li>
&lt;/ul>
&lt;!--
Thankfully, gRPC has a [standard health checking
protocol](https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md). It
can be used easily from any language. Generated code and the utilities for
setting the health status are shipped in nearly all language implementations of
gRPC.
--->
&lt;p>å¹¸è¿çš„æ˜¯ï¼ŒgRPC å…·æœ‰ &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md">æ ‡å‡†çš„å¥åº·æ£€æŸ¥åè®®&lt;/a>ã€‚å¯ä»¥ç”¨ä»»ä½•è¯­è¨€è½»æ¾è°ƒç”¨å®ƒã€‚å‡ ä¹æ‰€æœ‰å®ç° gRPC çš„è¯­è¨€éƒ½é™„å¸¦äº†ç”Ÿæˆçš„ä»£ç å’Œç”¨äºè®¾ç½®å¥åº·çŠ¶æ€çš„å®ç”¨ç¨‹åºã€‚&lt;/p>
&lt;!--
If you
[implement](https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto)
this health check protocol in your gRPC apps, you can then use a standard/common
tool to invoke this `Check()` method to determine server status.
--->
&lt;p>å¦‚æœæ‚¨åœ¨ gRPC åº”ç”¨ç¨‹åºä¸­ &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto">å®ç°&lt;/a> æ­¤å¥åº·æ£€æŸ¥åè®®ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨æ ‡å‡†æˆ–é€šç”¨å·¥å…·è°ƒç”¨ &lt;code>Check()&lt;/code> æ–¹æ³•æ¥ç¡®å®šæœåŠ¡å™¨çŠ¶æ€ã€‚&lt;/p>
&lt;!--
The next thing you need is the "standard tool", and it's the
[**grpc-health-probe**](https://github.com/grpc-ecosystem/grpc-health-probe/).
--->
&lt;p>æ¥ä¸‹æ¥æ‚¨éœ€è¦çš„æ˜¯ &amp;quot;æ ‡å‡†å·¥å…·&amp;quot; &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">&lt;strong>grpc-health-probe&lt;/strong>&lt;/a>ã€‚&lt;/p>
&lt;a href='https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png'>
&lt;img width="768" title='grpc-health-probe on kubernetes'
src='https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png'/>
&lt;/a>
&lt;!--
With this tool, you can use the same health check configuration in all your gRPC
applications. This approach requires you to:
--->
&lt;p>ä½¿ç”¨æ­¤å·¥å…·ï¼Œæ‚¨å¯ä»¥åœ¨æ‰€æœ‰ gRPC åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ç›¸åŒçš„å¥åº·æ£€æŸ¥é…ç½®ã€‚è¿™ç§æ–¹æ³•æœ‰ä»¥ä¸‹è¦æ±‚ï¼š&lt;/p>
&lt;!--
1. Find the gRPC "health" module in your favorite language and start using it
(example [Go library](https://godoc.org/github.com/grpc/grpc-go/health)).
2. Ship the
[grpc_health_probe](https://github.com/grpc-ecosystem/grpc-health-probe/)
binary in your container.
3. [Configure](https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes)
Kubernetes "exec" probe to invoke the "grpc_health_probe" tool in the
container.
--->
&lt;ol>
&lt;li>ç”¨æ‚¨å–œæ¬¢çš„è¯­è¨€æ‰¾åˆ° gRPC çš„ &amp;quot;å¥åº·&amp;quot; æ¨¡å—å¹¶å¼€å§‹ä½¿ç”¨å®ƒï¼ˆä¾‹å¦‚ &lt;a href="https://godoc.org/github.com/grpc/grpc-go/health">Go åº“&lt;/a>ï¼‰ã€‚&lt;/li>
&lt;li>å°†äºŒè¿›åˆ¶æ–‡ä»¶ &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">grpc_health_probe&lt;/a> é€åˆ°å®¹å™¨ä¸­ã€‚&lt;/li>
&lt;li>&lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes">é…ç½®&lt;/a> Kubernetes çš„ &amp;quot;exec&amp;quot; æ£€æŸ¥æ¨¡å—æ¥è°ƒç”¨å®¹å™¨ä¸­çš„ &amp;quot;grpc_health_probe&amp;quot; å·¥å…·ã€‚&lt;/li>
&lt;/ol>
&lt;!--
In this case, executing "grpc_health_probe" will call your gRPC server over
`localhost`, since they are in the same pod.
--->
&lt;p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰§è¡Œ &amp;quot;grpc_health_probe&amp;quot; å°†é€šè¿‡ &lt;code>localhost&lt;/code> è°ƒç”¨æ‚¨çš„ gRPC æœåŠ¡å™¨ï¼Œå› ä¸ºå®ƒä»¬ä½äºåŒä¸€ä¸ªå®¹å™¨ä¸­ã€‚&lt;/p>
&lt;!--
## What's next
**grpc-health-probe** project is still in its early days and it needs your
feedback. It supports a variety of features like communicating with TLS servers
and configurable connection/RPC timeouts.
--->
&lt;h2 id="ä¸‹ä¸€æ­¥å·¥ä½œ">ä¸‹ä¸€æ­¥å·¥ä½œ&lt;/h2>
&lt;p>&lt;strong>grpc-health-probe&lt;/strong> é¡¹ç›®ä»å¤„äºåˆæœŸé˜¶æ®µï¼Œéœ€è¦æ‚¨çš„åé¦ˆã€‚å®ƒæ”¯æŒå¤šç§åŠŸèƒ½ï¼Œä¾‹å¦‚ä¸ TLS æœåŠ¡å™¨é€šä¿¡å’Œé…ç½®å»¶æ—¶è¿æ¥/RPCã€‚&lt;/p>
&lt;!--
If you are running a gRPC server on Kubernetes today, try using the gRPC Health
Protocol and try the grpc-health-probe in your deployments, and [give
feedback](https://github.com/grpc-ecosystem/grpc-health-probe/).
--->
&lt;p>å¦‚æœæ‚¨æœ€è¿‘è¦åœ¨ Kubernetes ä¸Šè¿è¡Œ gRPC æœåŠ¡å™¨ï¼Œè¯·å°è¯•ä½¿ç”¨ gRPC Health Protocolï¼Œå¹¶åœ¨æ‚¨çš„ Deployment ä¸­å°è¯• grpc-health-probeï¼Œç„¶å &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">è¿›è¡Œåé¦ˆ&lt;/a>ã€‚&lt;/p>
&lt;!--
## Further reading
- Protocol: [GRPC Health Checking Protocol](https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md) ([health.proto](https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto))
- Documentation: [Kubernetes liveness and readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
- Article: [Advanced Kubernetes Health Check Patterns](https://ahmet.im/blog/advanced-kubernetes-health-checks/)
--->
&lt;h2 id="æ›´å¤šå†…å®¹">æ›´å¤šå†…å®¹&lt;/h2>
&lt;ul>
&lt;li>åè®®ï¼š &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md">GRPC Health Checking Protocol&lt;/a> (&lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto">health.proto&lt;/a>)&lt;/li>
&lt;li>æ–‡æ¡£ï¼š &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">Kubernetes å­˜æ´»å’Œå°±ç»ªæ¢é’ˆ&lt;/a>&lt;/li>
&lt;li>æ–‡ç« ï¼š &lt;a href="https://ahmet.im/blog/advanced-kubernetes-health-checks/">å‡çº§ç‰ˆ Kubernetes å¥åº·æ£€æŸ¥æ¨¡å¼&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: The Machines Can Do the Work, a Story of Kubernetes Testing, CI, and Automating the Contributor Experience</title><link>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</link><pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</guid><description>
&lt;hr>
&lt;h2 id="date-2019-08-29">layout: blog
title: 'æœºå™¨å¯ä»¥å®Œæˆè¿™é¡¹å·¥ä½œï¼Œä¸€ä¸ªå…³äº kubernetes æµ‹è¯•ã€CI å’Œè‡ªåŠ¨åŒ–è´¡çŒ®è€…ä½“éªŒçš„æ•…äº‹'
date: 2019-08-29&lt;/h2>
&lt;!--
**Author**: Aaron Crickenberger (Google) and Benjamin Elder (Google)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šAaron Crickenbergerï¼ˆè°·æ­Œï¼‰å’Œ Benjamin Elderï¼ˆè°·æ­Œï¼‰&lt;/p>
&lt;!--
_â€œLarge projects have a lot of less exciting, yet, hard work. We value time spent automating repetitive work more highly than toil. Where that work cannot be automated, it is our culture to recognize and reward all types of contributions. However, heroism is not sustainable.â€_ - [Kubernetes Community Values](https://git.k8s.io/community/values.md#automation-over-process)
-->
&lt;p>&lt;em>â€å¤§å‹é¡¹ç›®æœ‰å¾ˆå¤šä¸é‚£ä¹ˆä»¤äººå…´å¥‹ï¼Œä½†å´å¾ˆè¾›è‹¦çš„å·¥ä½œã€‚æ¯”èµ·è¾›è‹¦å·¥ä½œï¼Œæˆ‘ä»¬æ›´é‡è§†æŠŠæ—¶é—´èŠ±åœ¨è‡ªåŠ¨åŒ–é‡å¤æ€§å·¥ä½œä¸Šï¼Œå¦‚æœè¿™é¡¹å·¥ä½œæ— æ³•å®ç°è‡ªåŠ¨åŒ–ï¼Œæˆ‘ä»¬çš„æ–‡åŒ–å°±æ˜¯æ‰¿è®¤å¹¶å¥–åŠ±æ‰€æœ‰ç±»å‹çš„è´¡çŒ®ã€‚ç„¶è€Œï¼Œè‹±é›„ä¸»ä¹‰æ˜¯ä¸å¯æŒç»­çš„ã€‚â€œ&lt;/em> - &lt;a href="https://git.k8s.io/community/values.md#automation-over-process">Kubernetes Community Values&lt;/a>&lt;/p>
&lt;!--
Like many open source projects, Kubernetes is hosted on GitHub. We felt the barrier to participation would be lowest if the project lived where developers already worked, using tools and processes developers already knew. Thus the project embraced the service fully: it was the basis of our workflow, our issue tracker, our documentation, our blog platform, our team structure, and more.
-->
&lt;p>åƒè®¸å¤šå¼€æºé¡¹ç›®ä¸€æ ·ï¼ŒKubernetes æ‰˜ç®¡åœ¨ GitHub ä¸Šã€‚ å¦‚æœé¡¹ç›®ä½äºåœ¨å¼€å‘äººå‘˜å·²ç»å·¥ä½œçš„åœ°æ–¹ï¼Œä½¿ç”¨çš„å¼€å‘äººå‘˜å·²ç»çŸ¥é“çš„å·¥å…·å’Œæµç¨‹ï¼Œé‚£ä¹ˆå‚ä¸çš„éšœç¢å°†æ˜¯æœ€ä½çš„ã€‚ å› æ­¤ï¼Œè¯¥é¡¹ç›®å®Œå…¨æ¥å—äº†è¿™é¡¹æœåŠ¡ï¼šå®ƒæ˜¯æˆ‘ä»¬å·¥ä½œæµç¨‹ï¼Œé—®é¢˜è·Ÿè¸ªï¼Œæ–‡æ¡£ï¼Œåšå®¢å¹³å°ï¼Œå›¢é˜Ÿç»“æ„ç­‰çš„åŸºç¡€ã€‚&lt;/p>
&lt;!--
This strategy worked. It worked so well that the project quickly scaled past its contributorsâ€™ capacity as humans. What followed was an incredible journey of automation and innovation. We didnâ€™t just need to rebuild our airplane mid-flight without crashing, we needed to convert it into a rocketship and launch into orbit. We needed machines to do the work.
-->
&lt;p>è¿™ä¸ªç­–ç•¥å¥æ•ˆäº†ã€‚ å®ƒè¿ä½œè‰¯å¥½ï¼Œä»¥è‡³äºè¯¥é¡¹ç›®è¿…é€Ÿè¶…è¶Šäº†å…¶è´¡çŒ®è€…çš„äººç±»èƒ½åŠ›ã€‚ æ¥ä¸‹æ¥æ˜¯ä¸€æ¬¡ä»¤äººéš¾ä»¥ç½®ä¿¡çš„è‡ªåŠ¨åŒ–å’Œåˆ›æ–°ä¹‹æ—…ã€‚ æˆ‘ä»¬ä¸ä»…éœ€è¦åœ¨é£è¡Œé€”ä¸­é‡å»ºæˆ‘ä»¬çš„é£æœºè€Œä¸ä¼šå´©æºƒï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºç«ç®­é£èˆ¹å¹¶å‘å°„åˆ°è½¨é“ã€‚ æˆ‘ä»¬éœ€è¦æœºå™¨æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚&lt;/p>
&lt;!--
## The Work
-->
&lt;p>##ã€€å·¥ä½œ&lt;/p>
&lt;!--
Initially, we focused on the fact that we needed to support the sheer volume of tests mandated by a complex distributed system such as Kubernetes. Real world failure scenarios had to be exercised via end-to-end (e2e) tests to ensure proper functionality. Unfortunately, e2e tests were susceptible to flakes (random failures) and took anywhere from an hour to a day to complete.
-->
&lt;p>æœ€åˆï¼Œæˆ‘ä»¬å…³æ³¨çš„äº‹å®æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦æ”¯æŒå¤æ‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼ˆå¦‚ Kubernetesï¼‰æ‰€è¦æ±‚çš„å¤§é‡æµ‹è¯•ã€‚ çœŸå®ä¸–ç•Œä¸­çš„æ•…éšœåœºæ™¯å¿…é¡»é€šè¿‡ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰æµ‹è¯•æ¥æ‰§è¡Œï¼Œç¡®ä¿æ­£ç¡®çš„åŠŸèƒ½ã€‚ ä¸å¹¸çš„æ˜¯ï¼Œe2e æµ‹è¯•å®¹æ˜“å—åˆ°è–„ç‰‡ï¼ˆéšæœºæ•…éšœï¼‰çš„å½±å“ï¼Œå¹¶ä¸”éœ€è¦èŠ±è´¹ä¸€ä¸ªå°æ—¶åˆ°ä¸€å¤©æ‰èƒ½å®Œæˆã€‚&lt;/p>
&lt;!--
Further experience revealed other areas where machines could do the work for us:
-->
&lt;p>è¿›ä¸€æ­¥çš„ç»éªŒæ­ç¤ºäº†æœºå™¨å¯ä»¥ä¸ºæˆ‘ä»¬å·¥ä½œçš„å…¶ä»–é¢†åŸŸï¼š&lt;/p>
&lt;!--
* PR Workflow
* Did the contributor sign our CLA?
* Did the PR pass tests?
* Is the PR mergeable?
* Did the merge commit pass tests?
* Triage
* Who should be reviewing PRs?
* Is there enough information to route an issue to the right people?
* Is an issue still relevant?
* Project Health
* What is happening in the project?
* What should we be paying attention to?
-->
&lt;ul>
&lt;li>Pull Request å·¥ä½œæµç¨‹
&lt;ul>
&lt;li>è´¡çŒ®è€…æ˜¯å¦ç­¾ç½²äº†æˆ‘ä»¬çš„ CLAï¼Ÿ&lt;/li>
&lt;li>Pull Request é€šè¿‡æµ‹è¯•å—ï¼Ÿ&lt;/li>
&lt;li>Pull Request å¯ä»¥åˆå¹¶å—ï¼Ÿ&lt;/li>
&lt;li>åˆå¹¶æäº¤æ˜¯å¦é€šè¿‡äº†æµ‹è¯•ï¼Ÿ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>é‰´åˆ«åˆ†ç±»
&lt;ul>
&lt;li>è°åº”è¯¥å®¡æŸ¥ Pull Requestï¼Ÿ&lt;/li>
&lt;li>æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¿¡æ¯å°†é—®é¢˜å‘é€ç»™åˆé€‚çš„äººï¼Ÿ&lt;/li>
&lt;li>é—®é¢˜æ˜¯å¦ä¾æ—§å­˜åœ¨ï¼Ÿ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>é¡¹ç›®å¥åº·
&lt;ul>
&lt;li>é¡¹ç›®ä¸­å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ&lt;/li>
&lt;li>æˆ‘ä»¬åº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
As we developed automation to improve our situation, we followed a few guiding principles:
-->
&lt;p>å½“æˆ‘ä»¬å¼€å‘è‡ªåŠ¨åŒ–æ¥æ”¹å–„æˆ‘ä»¬çš„æƒ…å†µæ—¶ï¼Œæˆ‘ä»¬éµå¾ªäº†ä»¥ä¸‹å‡ ä¸ªæŒ‡å¯¼åŸåˆ™ï¼š&lt;/p>
&lt;!--
* Follow the push/pull control loop patterns that worked well for Kubernetes
* Prefer stateless loosely coupled services that do one thing well
* Prefer empowering the entire community over empowering a few core contributors
* Eat our own dogfood and avoid reinventing wheels
-->
&lt;ul>
&lt;li>éµå¾ªé€‚ç”¨äº Kubernetes çš„æ¨é€/æ‹‰å–æ§åˆ¶å¾ªç¯æ¨¡å¼&lt;/li>
&lt;li>é¦–é€‰æ— çŠ¶æ€æ¾æ•£è€¦åˆæœåŠ¡&lt;/li>
&lt;li>æ›´å€¾å‘äºæˆæƒæ•´ä¸ªç¤¾åŒºæƒåˆ©ï¼Œè€Œä¸æ˜¯èµ‹äºˆå°‘æ•°æ ¸å¿ƒè´¡çŒ®è€…æƒåŠ›&lt;/li>
&lt;li>åšå¥½è‡ªå·±çš„äº‹ï¼Œè€Œä¸è¦é‡æ–°é€ è½®å­&lt;/li>
&lt;/ul>
&lt;!--
## Enter Prow
-->
&lt;h2 id="äº†è§£-prow">äº†è§£ Prow&lt;/h2>
&lt;!--
This led us to create [Prow](https://git.k8s.io/test-infra/prow) as the central component for our automation. Prow is sort of like an [If This, Then That](https://ifttt.com/) for GitHub events, with a built-in library of [commands](https://prow.k8s.io/command-help), [plugins](https://prow.k8s.io/plugins), and utilities. We built Prow on top of Kubernetes to free ourselves from worrying about resource management and scheduling, and ensure a more pleasant operational experience.
-->
&lt;p>è¿™ä¿ƒä½¿æˆ‘ä»¬åˆ›å»º &lt;a href="https://git.k8s.io/test-infra/prow">Prow&lt;/a> ä½œä¸ºæˆ‘ä»¬è‡ªåŠ¨åŒ–çš„æ ¸å¿ƒç»„ä»¶ã€‚ Prowæœ‰ç‚¹åƒ &lt;a href="https://ifttt.com/">If This, Then That&lt;/a> ç”¨äº GitHub äº‹ä»¶ï¼Œ å†…ç½® &lt;a href="https://prow.k8s.io/command-help">commands&lt;/a>ï¼Œ &lt;a href="https://prow.k8s.io/plugins">plugins&lt;/a>ï¼Œ å’Œå®ç”¨ç¨‹åºã€‚ æˆ‘ä»¬åœ¨ Kubernetes ä¹‹ä¸Šå»ºç«‹äº† Prowï¼Œè®©æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒèµ„æºç®¡ç†å’Œæ—¥ç¨‹å®‰æ’ï¼Œå¹¶ç¡®ä¿æ›´æ„‰å¿«çš„è¿è¥ä½“éªŒã€‚&lt;/p>
&lt;!--
Prow lets us do things like:
-->
&lt;p>Prow è®©æˆ‘ä»¬åšä»¥ä¸‹äº‹æƒ…ï¼š&lt;/p>
&lt;!--
* Allow our community to triage issues/PRs by commenting commands such as â€œ/priority critical-urgentâ€, â€œ/assign maryâ€ or â€œ/closeâ€
* Auto-label PRs based on how much code they change, or which files they touch
* Age out issues/PRs that have remained inactive for too long
* Auto-merge PRs that meet our PR workflow requirements
* Run CI jobs defined as [Knative Builds](https://github.com/knative/build), Kubernetes Pods, or Jenkins jobs
* Enforce org-wide and per-repo GitHub policies like [branch protection](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector) and [GitHub labels](https://github.com/kubernetes/test-infra/tree/master/label_sync)
-->
&lt;ul>
&lt;li>å…è®¸æˆ‘ä»¬çš„ç¤¾åŒºé€šè¿‡è¯„è®ºè¯¸å¦‚â€œ/priority critical-urgentâ€ï¼Œâ€œ/assign maryâ€æˆ–â€œ/closeâ€ä¹‹ç±»çš„å‘½ä»¤å¯¹ issues/Pull Requests è¿›è¡Œåˆ†ç±»&lt;/li>
&lt;li>æ ¹æ®ç”¨æˆ·æ›´æ”¹çš„ä»£ç æ•°é‡æˆ–åˆ›å»ºçš„æ–‡ä»¶è‡ªåŠ¨æ ‡è®° Pull Requests&lt;/li>
&lt;li>æ ‡å‡ºé•¿æ—¶é—´ä¿æŒä¸æ´»åŠ¨çŠ¶æ€ issues/Pull Requests&lt;/li>
&lt;li>è‡ªåŠ¨åˆå¹¶ç¬¦åˆæˆ‘ä»¬PRå·¥ä½œæµç¨‹è¦æ±‚çš„ Pull Requests&lt;/li>
&lt;li>è¿è¡Œå®šä¹‰ä¸º&lt;a href="https://github.com/knative/build">Knative Builds&lt;/a>çš„ Kubernetes Podsæˆ– Jenkins jobsçš„ CI ä½œä¸š&lt;/li>
&lt;li>å®æ–½ç»„ç»‡èŒƒå›´å’Œé‡æ„ GitHub ä»“åº“ç­–ç•¥ï¼Œå¦‚&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector">Knative Builds&lt;/a>å’Œ&lt;a href="https://github.com/kubernetes/test-infra/tree/master/label_sync">GitHub labels&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Prow was initially developed by the engineering productivity team building Google Kubernetes Engine, and is actively contributed to by multiple members of Kubernetes SIG Testing. Prow has been adopted by several other open source projects, including Istio, JetStack, Knative and OpenShift. [Getting started with Prow](https://github.com/kubernetes/test-infra/tree/master/prow#getting-started) takes a Kubernetes cluster and `kubectl apply starter.yaml` (running pods on a Kubernetes cluster).
-->
&lt;p>Prowæœ€åˆç”±æ„å»º Google Kubernetes Engine çš„å·¥ç¨‹æ•ˆç‡å›¢é˜Ÿå¼€å‘ï¼Œå¹¶ç”± Kubernetes SIG Testing çš„å¤šä¸ªæˆå‘˜ç§¯æè´¡çŒ®ã€‚ Prow å·²è¢«å…¶ä»–å‡ ä¸ªå¼€æºé¡¹ç›®é‡‡ç”¨ï¼ŒåŒ…æ‹¬ Istioï¼ŒJetStackï¼ŒKnative å’Œ OpenShiftã€‚ &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow#getting-started">Getting started with Prow&lt;/a>éœ€è¦ä¸€ä¸ª Kubernetes é›†ç¾¤å’Œ &lt;code>kubectl apply starter.yaml&lt;/code>ï¼ˆåœ¨ Kubernetes é›†ç¾¤ä¸Šè¿è¡Œ podï¼‰ã€‚&lt;/p>
&lt;!--
Once we had Prow in place, we began to hit other scaling bottlenecks, and so produced additional tooling to support testing at the scale required by Kubernetes, including:
-->
&lt;p>ä¸€æ—¦æˆ‘ä»¬å®‰è£…äº† Prowï¼Œæˆ‘ä»¬å°±å¼€å§‹é‡åˆ°å…¶ä»–çš„é—®é¢˜ï¼Œå› æ­¤éœ€è¦é¢å¤–çš„å·¥å…·ä»¥æ”¯æŒ Kubernetes æ‰€éœ€çš„è§„æ¨¡æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š&lt;/p>
&lt;!--
- [Boskos](https://github.com/kubernetes/test-infra/tree/master/boskos): manages job resources (such as GCP projects) in pools, checking them out for jobs and cleaning them up automatically ([with monitoring](http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1))
- [ghProxy](https://github.com/kubernetes/test-infra/tree/master/ghproxy): a reverse proxy HTTP cache optimized for use with the GitHub API, to ensure our token usage doesnâ€™t hit API limits ([with monitoring](http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;orgId=1))
- [Greenhouse](https://github.com/kubernetes/test-infra/tree/master/greenhouse): allows us to use a remote bazel cache to provide faster build and test results for PRs ([with monitoring](http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1))
- [Splice](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice): allows us to test and merge PRs in a batch, ensuring our merge velocity is not limited to our test velocity
- [Tide](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide): allows us to merge PRs selected via GitHub queries rather than ordered in a queue, allowing for significantly higher merge velocity in tandem with splice
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/boskos">Boskos&lt;/a>: ç®¡ç†æ± ä¸­çš„ä½œä¸šèµ„æºï¼ˆä¾‹å¦‚ GCP é¡¹ç›®ï¼‰ï¼Œæ£€æŸ¥å®ƒä»¬æ˜¯å¦æœ‰å·¥ä½œå¹¶è‡ªåŠ¨æ¸…ç†å®ƒä»¬ (&lt;a href="http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/ghproxy">ghProxy&lt;/a>: ä¼˜åŒ–ç”¨äº GitHub API çš„åå‘ä»£ç† HTTP ç¼“å­˜ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„ä»¤ç‰Œä½¿ç”¨ä¸ä¼šè¾¾åˆ° API é™åˆ¶ (&lt;a href="http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;amp;orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/greenhouse">Greenhouse&lt;/a>: å…è®¸æˆ‘ä»¬ä½¿ç”¨è¿œç¨‹ bazel ç¼“å­˜ä¸º Pull requests æä¾›æ›´å¿«çš„æ„å»ºå’Œæµ‹è¯•ç»“æœ (&lt;a href="http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice">Splice&lt;/a>: å…è®¸æˆ‘ä»¬æ‰¹é‡æµ‹è¯•å’Œåˆå¹¶ Pull requestsï¼Œç¡®ä¿æˆ‘ä»¬çš„åˆå¹¶é€Ÿåº¦ä¸ä»…é™äºæˆ‘ä»¬çš„æµ‹è¯•é€Ÿåº¦&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide">Tide&lt;/a>: å…è®¸æˆ‘ä»¬åˆå¹¶é€šè¿‡ GitHub æŸ¥è¯¢é€‰æ‹©çš„ Pull requestsï¼Œè€Œä¸æ˜¯åœ¨é˜Ÿåˆ—ä¸­æ’åºï¼Œå…è®¸æ˜¾ç€æ›´é«˜åˆå¹¶é€Ÿåº¦ä¸æ‹¼æ¥ä¸€èµ·&lt;/li>
&lt;/ul>
&lt;!--
## Scaling Project Health
-->
&lt;p>##ã€€å…³æ³¨é¡¹ç›®å¥åº·çŠ¶å†µ&lt;/p>
&lt;!--
With workflow automation addressed, we turned our attention to project health. We chose to use Google Cloud Storage (GCS) as our source of truth for all test data, allowing us to lean on established infrastructure, and allowed the community to contribute results. We then built a variety of tools to help individuals and the project as a whole make sense of this data, including:
-->
&lt;p>éšç€å·¥ä½œæµè‡ªåŠ¨åŒ–çš„å®æ–½ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘äº†é¡¹ç›®å¥åº·ã€‚æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ Google Cloud Storage (GCS)ä½œä¸ºæ‰€æœ‰æµ‹è¯•æ•°æ®çš„çœŸå®æ¥æºï¼Œå…è®¸æˆ‘ä»¬ä¾èµ–å·²å»ºç«‹çš„åŸºç¡€è®¾æ–½ï¼Œå¹¶å…è®¸ç¤¾åŒºè´¡çŒ®ç»“æœã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºäº†å„ç§å·¥å…·æ¥å¸®åŠ©ä¸ªäººå’Œæ•´ä¸ªé¡¹ç›®ç†è§£è¿™äº›æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š&lt;/p>
&lt;!--
* [Gubernator](https://github.com/kubernetes/test-infra/tree/master/gubernator): display the results and test history for a given PR
* [Kettle](https://github.com/kubernetes/test-infra/tree/master/kettle): transfer data from GCS to a publicly accessible bigquery dataset
* [PR dashboard](https://k8s-gubernator.appspot.com/pr): a workflow-aware dashboard that allows contributors to understand which PRs require attention and why
* [Triage](https://storage.googleapis.com/k8s-gubernator/triage/index.html): identify common failures that happen across all jobs and tests
* [Testgrid](https://k8s-testgrid.appspot.com/): display test results for a given job across all runs, summarize test results across groups of jobs
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/gubernator">Gubernator&lt;/a>: æ˜¾ç¤ºç»™å®š Pull Request çš„ç»“æœå’Œæµ‹è¯•å†å²&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/kettle">Kettle&lt;/a>: å°†æ•°æ®ä» GCS ä¼ è¾“åˆ°å¯å…¬å¼€è®¿é—®çš„ bigquery æ•°æ®é›†&lt;/li>
&lt;li>&lt;a href="https://k8s-gubernator.appspot.com/pr">PR dashboard&lt;/a>: ä¸€ä¸ªå·¥ä½œæµç¨‹è¯†åˆ«ä»ªè¡¨æ¿ï¼Œå…è®¸å‚ä¸è€…äº†è§£å“ªäº› Pull Request éœ€è¦æ³¨æ„ä»¥åŠä¸ºä»€ä¹ˆ&lt;/li>
&lt;li>&lt;a href="https://storage.googleapis.com/k8s-gubernator/triage/index.html">Triage&lt;/a>: è¯†åˆ«æ‰€æœ‰ä½œä¸šå’Œæµ‹è¯•ä¸­å‘ç”Ÿçš„å¸¸è§æ•…éšœ&lt;/li>
&lt;li>&lt;a href="https://k8s-testgrid.appspot.com/">Testgrid&lt;/a>: æ˜¾ç¤ºæ‰€æœ‰è¿è¡Œä¸­ç»™å®šä½œä¸šçš„æµ‹è¯•ç»“æœï¼Œæ±‡æ€»å„ç»„ä½œä¸šçš„æµ‹è¯•ç»“æœ&lt;/li>
&lt;/ul>
&lt;!--
We approached the Cloud Native Computing Foundation (CNCF) to develop DevStats to glean insights from our GitHub events such as:
-->
&lt;p>æˆ‘ä»¬ä¸äº‘è®¡ç®—æœ¬åœ°è®¡ç®—åŸºé‡‘ä¼šï¼ˆCNCFï¼‰è”ç³»ï¼Œå¼€å‘ DevStatsï¼Œä»¥ä¾¿ä»æˆ‘ä»¬çš„ GitHub æ´»åŠ¨ä¸­æ”¶é›†è§è§£ï¼Œä¾‹å¦‚ï¼š&lt;/p>
&lt;!--
* [Which prow commands are people most actively using](https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1)
* [PR reviews by contributor over time](https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;var-period=d7&amp;var-repo_name=All&amp;var-reviewers=All)
* [Time spent in each phase of our PR workflow](https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1)
-->
&lt;ul>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1">Which prow commands are people most actively using&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;amp;var-period=d7&amp;amp;var-repo_name=All&amp;amp;var-reviewers=All">PR reviews by contributor over time&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1">Time spent in each phase of our PR workflow&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Into the Beyond
-->
&lt;h2 id="into-the-beyond">Into the Beyond&lt;/h2>
&lt;!--
Today, the Kubernetes project spans over 125 repos across five orgs. There are 31 Special Interests Groups and 10 Working Groups coordinating development within the project. In the last year the project has had [participation from over 13,800 unique developers](https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;var-period_name=Last%20year&amp;var-metric=contributions&amp;var-repogroup_name=All) on GitHub.
-->
&lt;p>ä»Šå¤©ï¼ŒKubernetes é¡¹ç›®è·¨è¶Šäº†5ä¸ªç»„ç»‡125ä¸ªä»“åº“ã€‚æœ‰31ä¸ªç‰¹æ®Šåˆ©ç›Šé›†å›¢å’Œ10ä¸ªå·¥ä½œç»„åœ¨é¡¹ç›®å†…åè°ƒå‘å±•ã€‚åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œè¯¥é¡¹ç›®æœ‰ &lt;a href="https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All">æ¥è‡ª13800å¤šåç‹¬ç«‹å¼€å‘äººå‘˜çš„å‚ä¸&lt;/a>ã€‚&lt;/p>
&lt;!--
On any given weekday our Prow instance [runs over 10,000 CI jobs](http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;fullscreen&amp;orgId=1&amp;from=now-6M&amp;to=now); from March 2017 to March 2018 it ran 4.3 million jobs. Most of these jobs involve standing up an entire Kubernetes cluster, and exercising it using real world scenarios. They allow us to ensure all supported releases of Kubernetes work across cloud providers, container engines, and networking plugins. They make sure the latest releases of Kubernetes work with various optional features enabled, upgrade safely, meet performance requirements, and work across architectures.
-->
&lt;p>åœ¨ä»»ä½•ç»™å®šçš„å·¥ä½œæ—¥ï¼Œæˆ‘ä»¬çš„ Prow å®ä¾‹&lt;a href="http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;amp;fullscreen&amp;amp;orgId=1&amp;amp;from=now-6M&amp;amp;to=now">è¿è¡Œè¶…è¿‡10,000ä¸ª CI å·¥ä½œ&lt;/a>; ä»2017å¹´3æœˆåˆ°2018å¹´3æœˆï¼Œå®ƒæœ‰430ä¸‡ä¸ªå·¥ä½œå²—ä½ã€‚ è¿™äº›å·¥ä½œä¸­çš„å¤§å¤šæ•°æ¶‰åŠå»ºç«‹æ•´ä¸ª Kubernetes é›†ç¾¤ï¼Œå¹¶ä½¿ç”¨çœŸå®åœºæ™¯æ¥å®æ–½å®ƒã€‚ å®ƒä»¬ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¡®ä¿æ‰€æœ‰å—æ”¯æŒçš„ Kubernetes ç‰ˆæœ¬è·¨äº‘æä¾›å•†ï¼Œå®¹å™¨å¼•æ“å’Œç½‘ç»œæ’ä»¶å·¥ä½œã€‚ ä»–ä»¬ç¡®ä¿æœ€æ–°ç‰ˆæœ¬çš„ Kubernetes èƒ½å¤Ÿå¯ç”¨å„ç§å¯é€‰åŠŸèƒ½ï¼Œå®‰å…¨å‡çº§ï¼Œæ»¡è¶³æ€§èƒ½è¦æ±‚ï¼Œå¹¶è·¨æ¶æ„å·¥ä½œã€‚&lt;/p>
&lt;!--
With todayâ€™s [announcement from CNCF](https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google) â€“ noting that Google Cloud has begun transferring ownership and management of the Kubernetes projectâ€™s cloud resources to CNCF community contributors, we are excited to embark on another journey. One that allows the project infrastructure to be owned and operated by the community of contributors, following the same open governance model that has worked for the rest of the project. Sound exciting to you? Come talk to us at #sig-testing on kubernetes.slack.com.
-->
&lt;p>ä»Šå¤©&lt;a href="https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google">æ¥è‡ªCNCFçš„å…¬å‘Š&lt;/a> - æ³¨æ„åˆ° Google Cloud æœ‰å¼€å§‹å°† Kubernetes é¡¹ç›®çš„äº‘èµ„æºçš„æ‰€æœ‰æƒå’Œç®¡ç†æƒè½¬è®©ç»™ CNCF ç¤¾åŒºè´¡çŒ®è€…ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´èƒ½å¤Ÿå¼€å§‹å¦ä¸€ä¸ªæ—…ç¨‹ã€‚ å…è®¸é¡¹ç›®åŸºç¡€è®¾æ–½ç”±è´¡çŒ®è€…ç¤¾åŒºæ‹¥æœ‰å’Œè¿è¥ï¼Œéµå¾ªå¯¹é¡¹ç›®å…¶ä½™éƒ¨åˆ†æœ‰æ•ˆçš„ç›¸åŒå¼€æ”¾æ²»ç†æ¨¡å‹ã€‚ å¬èµ·æ¥ä»¤äººå…´å¥‹ã€‚ è¯·æ¥ kubernetes.slack.com ä¸Šçš„ #sig-testing on kubernetes.slack.com ä¸æˆ‘ä»¬è”ç³»ã€‚&lt;/p>
&lt;!--
Want to find out more? Come check out these resources:
-->
&lt;p>æƒ³äº†è§£æ›´å¤šï¼Ÿ å¿«æ¥çœ‹çœ‹è¿™äº›èµ„æºï¼š&lt;/p>
&lt;!--
* [Prow: Testing the way to Kubernetes Next](https://elder.dev/posts/prow)
* [Automation and the Kubernetes Contributor Experience](https://www.youtube.com/watch?v=BsIC7gPkH5M)
-->
&lt;ul>
&lt;li>&lt;a href="https://elder.dev/posts/prow">Prow: Testing the way to Kubernetes Next&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=BsIC7gPkH5M">Automation and the Kubernetes Contributor Experience&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: ä½¿ç”¨ CSI å’Œ Kubernetes å®ç°å·çš„åŠ¨æ€æ‰©å®¹</title><link>https://kubernetes.io/zh/blog/2018/08/02/%E4%BD%BF%E7%94%A8-csi-%E5%92%8C-kubernetes-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%9A%84%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9/</link><pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/08/02/%E4%BD%BF%E7%94%A8-csi-%E5%92%8C-kubernetes-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%9A%84%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9/</guid><description>
&lt;!--
---
layout: blog
title: 'Dynamically Expand Volume with CSI and Kubernetes'
date: 2018-08-02
---
-->
&lt;!--
**Author**: Orain Xiong (Co-Founder, WoquTech)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šOrain Xiongï¼ˆè”åˆåˆ›å§‹äºº, WoquTechï¼‰&lt;/p>
&lt;!--
_There is a very powerful storage subsystem within Kubernetes itself, covering a fairly broad spectrum of use cases. Whereas, when planning to build a product-grade relational database platform with Kubernetes, we face a big challenge: coming up with storage. This article describes how to extend latest Container Storage Interface 0.2.0 and integrate with Kubernetes, and demonstrates the essential facet of dynamically expanding volume capacity._
-->
&lt;p>&lt;em>Kubernetes æœ¬èº«æœ‰ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å­˜å‚¨å­ç³»ç»Ÿï¼Œæ¶µç›–äº†ç›¸å½“å¹¿æ³›çš„ç”¨ä¾‹ã€‚è€Œå½“æˆ‘ä»¬è®¡åˆ’ä½¿ç”¨ Kubernetes æ„å»ºäº§å“çº§å…³ç³»å‹æ•°æ®åº“å¹³å°æ—¶ï¼Œæˆ‘ä»¬é¢ä¸´ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼šæä¾›å­˜å‚¨ã€‚æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•æ‰©å±•æœ€æ–°çš„ Container Storage Interface 0.2.0 å’Œä¸ Kubernetes é›†æˆï¼Œå¹¶æ¼”ç¤ºäº†å·åŠ¨æ€æ‰©å®¹çš„åŸºæœ¬æ–¹é¢ã€‚&lt;/em>&lt;/p>
&lt;!--
## Introduction
-->
&lt;h2 id="ä»‹ç»">ä»‹ç»&lt;/h2>
&lt;!--
As we focalize our customers, especially in financial space, there is a huge upswell in the adoption of container orchestration technology.
-->
&lt;p>å½“æˆ‘ä»¬ä¸“æ³¨äºå®¢æˆ·æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨é‡‘èé¢†åŸŸï¼Œé‡‡ç”¨å®¹å™¨ç¼–æ’æŠ€æœ¯çš„æƒ…å†µå¤§å¤§å¢åŠ ã€‚&lt;/p>
&lt;!--
They are looking forward to open source solutions to redesign already existing monolithic applications, which have been running for several years on virtualization infrastructure or bare metal.
-->
&lt;p>ä»–ä»¬æœŸå¾…ç€èƒ½ç”¨å¼€æºè§£å†³æ–¹æ¡ˆé‡æ–°è®¾è®¡å·²ç»å­˜åœ¨çš„æ•´ä½“åº”ç”¨ç¨‹åºï¼Œè¿™äº›åº”ç”¨ç¨‹åºå·²ç»åœ¨è™šæ‹ŸåŒ–åŸºç¡€æ¶æ„æˆ–è£¸æœºä¸Šè¿è¡Œäº†å‡ å¹´ã€‚&lt;/p>
&lt;!--
Considering extensibility and the extent of technical maturity, Kubernetes and Docker are at the very top of the list. But migrating monolithic applications to a distributed orchestration like Kubernetes is challenging, the relational database is critical for the migration.
-->
&lt;p>è€ƒè™‘åˆ°å¯æ‰©å±•æ€§å’ŒæŠ€æœ¯æˆç†Ÿç¨‹åº¦ï¼ŒKubernetes å’Œ Docker æ’åœ¨æˆ‘ä»¬é€‰æ‹©åˆ—è¡¨çš„é¦–ä½ã€‚ä½†æ˜¯å°†æ•´ä½“åº”ç”¨ç¨‹åºè¿ç§»åˆ°ç±»ä¼¼äº Kubernetes ä¹‹ç±»çš„åˆ†å¸ƒå¼å®¹å™¨ç¼–æ’å¹³å°ä¸Šå¾ˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå…¶ä¸­å…³ç³»æ•°æ®åº“å¯¹äºè¿ç§»æ¥è¯´è‡³å…³é‡è¦ã€‚&lt;/p>
&lt;!--
With respect to the relational database, we should pay attention to storage. There is a very powerful storage subsystem within Kubernetes itself. It is very useful and covers a fairly broad spectrum of use cases. When planning to run a relational database with Kubernetes in production, we face a big challenge: coming up with storage. There are still some fundamental functionalities which are left unimplemented. Specifically, dynamically expanding volume. It sounds boring but is highly required, except for actions like create and delete and mount and unmount.
-->
&lt;p>å…³äºå…³ç³»æ•°æ®åº“ï¼Œæˆ‘ä»¬åº”è¯¥æ³¨æ„å­˜å‚¨ã€‚Kubernetes æœ¬èº«å†…éƒ¨æœ‰ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å­˜å‚¨å­ç³»ç»Ÿã€‚å®ƒéå¸¸æœ‰ç”¨ï¼Œæ¶µç›–äº†ç›¸å½“å¹¿æ³›çš„ç”¨ä¾‹ã€‚å½“æˆ‘ä»¬è®¡åˆ’åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ Kubernetes è¿è¡Œå…³ç³»å‹æ•°æ®åº“æ—¶ï¼Œæˆ‘ä»¬é¢ä¸´ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ï¼šæä¾›å­˜å‚¨ã€‚ç›®å‰ï¼Œä»æœ‰ä¸€äº›åŸºæœ¬åŠŸèƒ½å°šæœªå®ç°ã€‚ç‰¹åˆ«æ˜¯ï¼Œå·çš„åŠ¨æ€æ‰©å®¹ã€‚è¿™å¬èµ·æ¥å¾ˆæ— èŠï¼Œä½†åœ¨é™¤åˆ›å»ºï¼Œåˆ é™¤ï¼Œå®‰è£…å’Œå¸è½½ä¹‹ç±»çš„æ“ä½œå¤–ï¼Œå®ƒæ˜¯éå¸¸å¿…è¦çš„ã€‚&lt;/p>
&lt;!--
Currently, expanding volume is only available with those storage provisioners:
-->
&lt;p>ç›®å‰ï¼Œæ‰©å±•å·ä»…é€‚ç”¨äºè¿™äº›å­˜å‚¨ä¾›åº”å•†ï¼š&lt;/p>
&lt;ul>
&lt;li>gcePersistentDisk&lt;/li>
&lt;li>awsElasticBlockStore&lt;/li>
&lt;li>OpenStack Cinder&lt;/li>
&lt;li>glusterfs&lt;/li>
&lt;li>rbd&lt;/li>
&lt;/ul>
&lt;!--
In order to enable this feature, we should set feature gate `ExpandPersistentVolumes` true and turn on the `PersistentVolumeClaimResize` admission plugin. Once `PersistentVolumeClaimResize` has been enabled, resizing will be allowed by a Storage Class whose `allowVolumeExpansion` field is set to true.
-->
&lt;p>ä¸ºäº†å¯ç”¨æ­¤åŠŸèƒ½ï¼Œæˆ‘ä»¬åº”è¯¥å°†ç‰¹æ€§å¼€å…³ &lt;code>ExpandPersistentVolumes&lt;/code> è®¾ç½®ä¸º true å¹¶æ‰“å¼€ &lt;code>PersistentVolumeClaimResize&lt;/code> å‡†å…¥æ’ä»¶ã€‚ ä¸€æ—¦å¯ç”¨äº† &lt;code>PersistentVolumeClaimResize&lt;/code>ï¼Œåˆ™å…¶å¯¹åº”çš„ &lt;code>allowVolumeExpansion&lt;/code> å­—æ®µè®¾ç½®ä¸º true çš„å­˜å‚¨ç±»å°†å…è®¸è°ƒæ•´å¤§å°ã€‚&lt;/p>
&lt;!--
Unfortunately, dynamically expanding volume through the Container Storage Interface (CSI) and Kubernetes is unavailable, even though the underlying storage providers have this feature.
-->
&lt;p>ä¸å¹¸çš„æ˜¯ï¼Œå³ä½¿åŸºç¡€å­˜å‚¨æä¾›è€…å…·æœ‰æ­¤åŠŸèƒ½ï¼Œä¹Ÿæ— æ³•é€šè¿‡å®¹å™¨å­˜å‚¨æ¥å£ï¼ˆCSIï¼‰å’Œ Kubernetes åŠ¨æ€æ‰©å±•å·ã€‚&lt;/p>
&lt;!--
This article will give a simplified view of CSI, followed by a walkthrough of how to introduce a new expanding volume feature on the existing CSI and Kubernetes. Finally, the article will demonstrate how to dynamically expand volume capacity.
-->
&lt;p>æœ¬æ–‡å°†ç»™å‡º CSI çš„ç®€åŒ–è§†å›¾ï¼Œç„¶åé€æ­¥ä»‹ç»å¦‚ä½•åœ¨ç°æœ‰ CSI å’Œ Kubernetes ä¸Šå¼•å…¥æ–°çš„æ‰©å±•å·åŠŸèƒ½ã€‚æœ€åï¼Œæœ¬æ–‡å°†æ¼”ç¤ºå¦‚ä½•åŠ¨æ€æ‰©å±•å·å®¹é‡ã€‚&lt;/p>
&lt;!--
## Container Storage Interface (CSI)
-->
&lt;h2 id="å®¹å™¨å­˜å‚¨æ¥å£-csi">å®¹å™¨å­˜å‚¨æ¥å£ï¼ˆCSIï¼‰&lt;/h2>
&lt;!--
To have a better understanding of what we're going to do, the first thing we need to know is what the Container Storage Interface is. Currently, there are still some problems for already existing storage subsystem within Kubernetes. Storage driver code is maintained in the Kubernetes core repository which is difficult to test. But beyond that, Kubernetes needs to give permissions to storage vendors to check code into the Kubernetes core repository. Ideally, that should be implemented externally.
-->
&lt;p>ä¸ºäº†æ›´å¥½åœ°äº†è§£æˆ‘ä»¬å°†è¦åšä»€ä¹ˆï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦çŸ¥é“ä»€ä¹ˆæ˜¯å®¹å™¨å­˜å‚¨æ¥å£ã€‚å½“å‰ï¼ŒKubernetes ä¸­å·²ç»å­˜åœ¨çš„å­˜å‚¨å­ç³»ç»Ÿä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚ å­˜å‚¨é©±åŠ¨ç¨‹åºä»£ç åœ¨ Kubernetes æ ¸å¿ƒå­˜å‚¨åº“ä¸­ç»´æŠ¤ï¼Œè¿™å¾ˆéš¾æµ‹è¯•ã€‚ ä½†æ˜¯é™¤æ­¤ä¹‹å¤–ï¼ŒKubernetes è¿˜éœ€è¦æˆäºˆå­˜å‚¨ä¾›åº”å•†è®¸å¯ï¼Œä»¥å°†ä»£ç ç­¾å…¥ Kubernetes æ ¸å¿ƒå­˜å‚¨åº“ã€‚ ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™äº›åº”åœ¨å¤–éƒ¨å®æ–½ã€‚&lt;/p>
&lt;!--
CSI is designed to define an industry standard that will enable storage providers who enable CSI to be available across container orchestration systems that support CSI.
-->
&lt;p>CSI æ—¨åœ¨å®šä¹‰è¡Œä¸šæ ‡å‡†ï¼Œè¯¥æ ‡å‡†å°†ä½¿æ”¯æŒ CSI çš„å­˜å‚¨æä¾›å•†èƒ½å¤Ÿåœ¨æ”¯æŒ CSI çš„å®¹å™¨ç¼–æ’ç³»ç»Ÿä¸­ä½¿ç”¨ã€‚&lt;/p>
&lt;!--
This diagram depicts a kind of high-level Kubernetes archetypes integrated with CSI:
![csi diagram](/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png)
-->
&lt;p>è¯¥å›¾æè¿°äº†ä¸€ç§ä¸ CSI é›†æˆçš„é«˜çº§ Kubernetes åŸå‹ï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png" alt="csi diagram">&lt;/p>
&lt;!--
* Three new external components are introduced to decouple Kubernetes and Storage Provider logic
* Blue arrows present the conventional way to call against API Server
* Red arrows present gRPC to call against Volume Driver
-->
&lt;ul>
&lt;li>å¼•å…¥äº†ä¸‰ä¸ªæ–°çš„å¤–éƒ¨ç»„ä»¶ä»¥è§£è€¦ Kubernetes å’Œå­˜å‚¨æä¾›ç¨‹åºé€»è¾‘&lt;/li>
&lt;li>è“è‰²ç®­å¤´è¡¨ç¤ºé’ˆå¯¹ API æœåŠ¡å™¨è¿›è¡Œè°ƒç”¨çš„å¸¸è§„æ–¹æ³•&lt;/li>
&lt;li>çº¢è‰²ç®­å¤´æ˜¾ç¤º gRPC ä»¥é’ˆå¯¹ Volume Driver è¿›è¡Œè°ƒç”¨&lt;/li>
&lt;/ul>
&lt;!--
For more details, please visit: https://github.com/container-storage-interface/spec/blob/master/spec.md
-->
&lt;p>æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®ï¼šhttps://github.com/container-storage-interface/spec/blob/master/spec.md&lt;/p>
&lt;!--
## Extend CSI and Kubernetes
-->
&lt;h2 id="æ‰©å±•-csi-å’Œ-kubernetes">æ‰©å±• CSI å’Œ Kubernetes&lt;/h2>
&lt;!--
In order to enable the feature of expanding volume atop Kubernetes, we should extend several components including CSI specification, â€œin-treeâ€ volume plugin, external-provisioner and external-attacher.
-->
&lt;p>ä¸ºäº†å®ç°åœ¨ Kubernetes ä¸Šæ‰©å±•å·çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬åº”è¯¥æ‰©å±•å‡ ä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬ CSI è§„èŒƒï¼Œâ€œin-treeâ€ å·æ’ä»¶ï¼Œexternal-provisioner å’Œ external-attacherã€‚&lt;/p>
&lt;!--
## Extend CSI spec
-->
&lt;h2 id="æ‰©å±•csiè§„èŒƒ">æ‰©å±•CSIè§„èŒƒ&lt;/h2>
&lt;!--
The feature of expanding volume is still undefined in latest CSI 0.2.0. The new 3 RPCs, including `RequiresFSResize` and `ControllerResizeVolume` and `NodeResizeVolume`, should be introduced.
-->
&lt;p>æœ€æ–°çš„ CSI 0.2.0 ä»æœªå®šä¹‰æ‰©å±•å·çš„åŠŸèƒ½ã€‚åº”è¯¥å¼•å…¥æ–°çš„3ä¸ª RPCï¼ŒåŒ…æ‹¬ &lt;code>RequiresFSResize&lt;/code>ï¼Œ &lt;code>ControllerResizeVolume&lt;/code> å’Œ &lt;code>NodeResizeVolume&lt;/code>ã€‚&lt;/p>
&lt;pre>&lt;code>service Controller {
rpc CreateVolume (CreateVolumeRequest)
returns (CreateVolumeResponse) {}
â€¦â€¦
rpc RequiresFSResize (RequiresFSResizeRequest)
returns (RequiresFSResizeResponse) {}
rpc ControllerResizeVolume (ControllerResizeVolumeRequest)
returns (ControllerResizeVolumeResponse) {}
}
service Node {
rpc NodeStageVolume (NodeStageVolumeRequest)
returns (NodeStageVolumeResponse) {}
â€¦â€¦
rpc NodeResizeVolume (NodeResizeVolumeRequest)
returns (NodeResizeVolumeResponse) {}
}
&lt;/code>&lt;/pre>&lt;!--
## Extend â€œIn-Treeâ€ Volume Plugin
-->
&lt;h2 id="æ‰©å±•-in-tree-å·æ’ä»¶">æ‰©å±• â€œIn-Treeâ€ å·æ’ä»¶&lt;/h2>
&lt;!--
In addition to the extend CSI specification, the `csiPluginï»¿` interface within Kubernetes should also implement `expandablePlugin`. The `csiPlugin` interface will expand `PersistentVolumeClaim` representing for `ExpanderController`.
-->
&lt;p>é™¤äº†æ‰©å±•çš„ CSI è§„èŒƒä¹‹å¤–ï¼ŒKubernetes ä¸­çš„ &lt;code>csiPlugin&lt;/code> æ¥å£è¿˜åº”è¯¥å®ç° &lt;code>expandablePlugin&lt;/code>ã€‚&lt;code>csiPlugin&lt;/code> æ¥å£å°†æ‰©å±•ä»£è¡¨ &lt;code>ExpanderController&lt;/code> çš„ &lt;code>PersistentVolumeClaim&lt;/code>ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ExpandableVolumePlugin &lt;span style="color:#a2f;font-weight:bold">interface&lt;/span> {
VolumePlugin
&lt;span style="color:#00a000">ExpandVolumeDevice&lt;/span>(spec Spec, newSize resource.Quantity, oldSize resource.Quantity) (resource.Quantity, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>)
&lt;span style="color:#00a000">RequiresFSResize&lt;/span>() &lt;span style="color:#0b0;font-weight:bold">bool&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Implement Volume Driver
-->
&lt;h3 id="å®ç°å·é©±åŠ¨ç¨‹åº">å®ç°å·é©±åŠ¨ç¨‹åº&lt;/h3>
&lt;!--
Finally, to abstract complexity of the implementation, we should hard code the separate storage provider management logic into the following functions which is well-defined in the CSI specification:
-->
&lt;p>æœ€åï¼Œä¸ºäº†æŠ½è±¡åŒ–å®ç°çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬åº”è¯¥å°†å•ç‹¬çš„å­˜å‚¨æä¾›ç¨‹åºç®¡ç†é€»è¾‘ç¡¬ç¼–ç ä¸ºä»¥ä¸‹åŠŸèƒ½ï¼Œè¿™äº›åŠŸèƒ½åœ¨ CSI è§„èŒƒä¸­å·²æ˜ç¡®å®šä¹‰ï¼š&lt;/p>
&lt;ul>
&lt;li>CreateVolume&lt;/li>
&lt;li>DeleteVolume&lt;/li>
&lt;li>ControllerPublishVolume&lt;/li>
&lt;li>ControllerUnpublishVolume&lt;/li>
&lt;li>ValidateVolumeCapabilities&lt;/li>
&lt;li>ListVolumes&lt;/li>
&lt;li>GetCapacity&lt;/li>
&lt;li>ControllerGetCapabilities&lt;/li>
&lt;li>RequiresFSResize&lt;/li>
&lt;li>ControllerResizeVolume&lt;/li>
&lt;/ul>
&lt;!--
## Demonstration
Letâ€™s demonstrate this feature with a concrete user case.
* Create storage class for CSI storage provisioner
-->
&lt;h2 id="å±•ç¤º">å±•ç¤º&lt;/h2>
&lt;p>è®©æˆ‘ä»¬ä»¥å…·ä½“çš„ç”¨æˆ·æ¡ˆä¾‹æ¥æ¼”ç¤ºæ­¤åŠŸèƒ½ã€‚&lt;/p>
&lt;ul>
&lt;li>ä¸º CSI å­˜å‚¨ä¾›åº”å•†åˆ›å»ºå­˜å‚¨ç±»&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">allowVolumeExpansion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csiProvisionerSecretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>orain-test&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csiProvisionerSecretNamespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfsplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">reclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeBindingMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Immediate&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
* Deploy CSI Volume Driver including storage provisioner `csi-qcfsplugin` across Kubernetes cluster
* Create PVC `qcfs-pvc` which will be dynamically provisioned by storage class `csi-qcfs`
-->
&lt;ul>
&lt;li>
&lt;p>åœ¨ Kubernetes é›†ç¾¤ä¸Šéƒ¨ç½²åŒ…æ‹¬å­˜å‚¨ä¾›åº”å•† &lt;code>csi-qcfsplugin&lt;/code> åœ¨å†…çš„ CSI å·é©±åŠ¨&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åˆ›å»º PVC &lt;code>qcfs-pvc&lt;/code>ï¼Œå®ƒå°†ç”±å­˜å‚¨ç±» &lt;code>csi-qcfs&lt;/code> åŠ¨æ€é…ç½®&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>qcfs-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">...&lt;/span>.&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfs&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
* Create MySQL 5.7 instance to use PVC `qcfs-pvc`
* In order to mirror the exact same production-level scenario, there are actually two different types of workloads including:
* Batch insert to make MySQL consuming more file system capacity
* Surge query request
* Dynamically expand volume capacity through edit pvc `qcfs-pvc` configuration
-->
&lt;ul>
&lt;li>åˆ›å»º MySQL 5.7 å®ä¾‹ä»¥ä½¿ç”¨ PVC &lt;code>qcfs-pvc&lt;/code>&lt;/li>
&lt;li>ä¸ºäº†åæ˜ å®Œå…¨ç›¸åŒçš„ç”Ÿäº§çº§åˆ«æ–¹æ¡ˆï¼Œå®é™…ä¸Šæœ‰ä¸¤ç§ä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½ï¼ŒåŒ…æ‹¬ï¼š
Â Â Â Â  * æ‰¹é‡æ’å…¥ä½¿ MySQL æ¶ˆè€—æ›´å¤šçš„æ–‡ä»¶ç³»ç»Ÿå®¹é‡
Â Â Â Â  * æµªæ¶ŒæŸ¥è¯¢è¯·æ±‚&lt;/li>
&lt;li>é€šè¿‡ç¼–è¾‘ pvc &lt;code>qcfs-pvc&lt;/code> é…ç½®åŠ¨æ€æ‰©å±•å·å®¹é‡&lt;/li>
&lt;/ul>
&lt;!--
The Prometheus and Grafana integration allows us to visualize corresponding critical metrics.
![prometheus grafana](/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png)
We notice that the middle reading shows MySQL datafile size increasing slowly during bulk inserting. At the same time, the bottom reading shows file system expanding twice in about 20 minutes, from 300 GiB to 400 GiB and then 500 GiB. Meanwhile, the upper reading shows the whole process of expanding volume immediately completes and hardly impacts MySQL QPS.
-->
&lt;p>Prometheus å’Œ Grafana çš„é›†æˆä½¿æˆ‘ä»¬å¯ä»¥å¯è§†åŒ–ç›¸åº”çš„å…³é”®æŒ‡æ ‡ã€‚&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png" alt="prometheus grafana">&lt;/p>
&lt;p>æˆ‘ä»¬æ³¨æ„åˆ°ä¸­é—´çš„è¯»æ•°æ˜¾ç¤ºåœ¨æ‰¹é‡æ’å…¥æœŸé—´ MySQL æ•°æ®æ–‡ä»¶çš„å¤§å°ç¼“æ…¢å¢åŠ ã€‚ åŒæ—¶ï¼Œåº•éƒ¨è¯»æ•°æ˜¾ç¤ºæ–‡ä»¶ç³»ç»Ÿåœ¨å¤§çº¦20åˆ†é’Ÿå†…æ‰©å±•äº†ä¸¤æ¬¡ï¼Œä» 300 GiB æ‰©å±•åˆ° 400 GiBï¼Œç„¶åæ‰©å±•åˆ° 500 GiBã€‚ åŒæ—¶ï¼Œä¸ŠåŠéƒ¨åˆ†æ˜¾ç¤ºï¼Œæ‰©å±•å·çš„æ•´ä¸ªè¿‡ç¨‹ç«‹å³å®Œæˆï¼Œå‡ ä¹ä¸ä¼šå½±å“ MySQL QPSã€‚&lt;/p>
&lt;!--
## Conclusion
Regardless of whatever infrastructure applications have been running on, the database is always a critical resource. It is essential to have a more advanced storage subsystem out there to fully support database requirements. This will help drive the more broad adoption of cloud native technology.
-->
&lt;h2 id="ç»“è®º">ç»“è®º&lt;/h2>
&lt;p>ä¸ç®¡è¿è¡Œä»€ä¹ˆåŸºç¡€ç»“æ„åº”ç”¨ç¨‹åºï¼Œæ•°æ®åº“å§‹ç»ˆæ˜¯å…³é”®èµ„æºã€‚æ‹¥æœ‰æ›´é«˜çº§çš„å­˜å‚¨å­ç³»ç»Ÿä»¥å®Œå…¨æ”¯æŒæ•°æ®åº“éœ€æ±‚è‡³å…³é‡è¦ã€‚è¿™å°†æœ‰åŠ©äºæ¨åŠ¨äº‘åŸç”ŸæŠ€æœ¯çš„æ›´å¹¿æ³›é‡‡ç”¨ã€‚&lt;/p></description></item><item><title>Blog: åŠ¨æ€ Kubelet é…ç½®</title><link>https://kubernetes.io/zh/blog/2018/07/11/%E5%8A%A8%E6%80%81-kubelet-%E9%85%8D%E7%BD%AE/</link><pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/11/%E5%8A%A8%E6%80%81-kubelet-%E9%85%8D%E7%BD%AE/</guid><description>
&lt;!--
---
layout: blog
title: 'Dynamic Kubelet Configuration'
date: 2018-07-11
---
-->
&lt;!--
**Author**: Michael Taufen (Google)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Michael Taufen (Google)&lt;/p>
&lt;!--
**Editorâ€™s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/) on whatâ€™s new in Kubernetes 1.11**
-->
&lt;p>&lt;strong>ç¼–è€…æ³¨ï¼šè¿™ç¯‡æ–‡ç« æ˜¯&lt;a href="https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/">ä¸€ç³»åˆ—æ·±åº¦æ–‡ç« &lt;/a> çš„ä¸€éƒ¨åˆ†ï¼Œè¿™ä¸ªç³»åˆ—ä»‹ç»äº† Kubernetes 1.11 ä¸­çš„æ–°å¢åŠŸèƒ½&lt;/strong>&lt;/p>
&lt;!--
## Why Dynamic Kubelet Configuration?
-->
&lt;h2 id="ä¸ºä»€ä¹ˆè¦è¿›è¡ŒåŠ¨æ€-kubelet-é…ç½®">ä¸ºä»€ä¹ˆè¦è¿›è¡ŒåŠ¨æ€ Kubelet é…ç½®ï¼Ÿ&lt;/h2>
&lt;!--
Kubernetes provides API-centric tooling that significantly improves workflows for managing applications and infrastructure. Most Kubernetes installations, however, run the Kubelet as a native process on each host, outside the scope of standard Kubernetes APIs.
-->
&lt;p>Kubernetes æä¾›äº†ä»¥ API ä¸ºä¸­å¿ƒçš„å·¥å…·ï¼Œå¯æ˜¾ç€æ”¹å–„ç”¨äºç®¡ç†åº”ç”¨ç¨‹åºå’ŒåŸºç¡€æ¶æ„çš„å·¥ä½œæµç¨‹ã€‚
ä½†æ˜¯ï¼Œåœ¨å¤§å¤šæ•°çš„ Kubernetes å®‰è£…ä¸­ï¼Œkubelet åœ¨æ¯ä¸ªä¸»æœºä¸Šä½œä¸ºæœ¬æœºè¿›ç¨‹è¿è¡Œï¼Œå› æ­¤
æœªè¢«æ ‡å‡† Kubernetes API è¦†ç›–ã€‚&lt;/p>
&lt;!--
In the past, this meant that cluster administrators and service providers could not rely on Kubernetes APIs to reconfigure Kubelets in a live cluster. In practice, this required operators to either ssh into machines to perform manual reconfigurations, use third-party configuration management automation tools, or create new VMs with the desired configuration already installed, then migrate work to the new machines. These approaches are environment-specific and can be expensive.
-->
&lt;p>è¿‡å»ï¼Œè¿™æ„å‘³ç€é›†ç¾¤ç®¡ç†å‘˜å’ŒæœåŠ¡æä¾›å•†æ— æ³•ä¾é  Kubernetes API åœ¨æ´»åŠ¨é›†ç¾¤ä¸­é‡æ–°é…ç½® Kubeletsã€‚
å®é™…ä¸Šï¼Œè¿™è¦æ±‚æ“ä½œå‘˜è¦ SSH ç™»å½•åˆ°è®¡ç®—æœºä»¥æ‰§è¡Œæ‰‹åŠ¨é‡æ–°é…ç½®ï¼Œè¦ä¹ˆä½¿ç”¨ç¬¬ä¸‰æ–¹é…ç½®ç®¡ç†è‡ªåŠ¨åŒ–å·¥å…·ï¼Œ
æˆ–åˆ›å»ºå·²ç»å®‰è£…äº†æ‰€éœ€é…ç½®çš„æ–° VMï¼Œç„¶åå°†å·¥ä½œè¿ç§»åˆ°æ–°è®¡ç®—æœºä¸Šã€‚
è¿™äº›æ–¹æ³•æ˜¯ç‰¹å®šäºç¯å¢ƒçš„ï¼Œå¹¶ä¸”å¯èƒ½å¾ˆè€—æ—¶è´¹åŠ›ã€‚&lt;/p>
&lt;!--
Dynamic Kubelet configuration gives cluster administrators and service providers the ability to reconfigure Kubelets in a live cluster via Kubernetes APIs.
-->
&lt;p>åŠ¨æ€ Kubelet é…ç½®ä½¿é›†ç¾¤ç®¡ç†å‘˜å’ŒæœåŠ¡æä¾›å•†èƒ½å¤Ÿé€šè¿‡ Kubernetes API åœ¨æ´»åŠ¨é›†ç¾¤ä¸­é‡æ–°é…ç½® Kubeletã€‚&lt;/p>
&lt;!--
## What is Dynamic Kubelet Configuration?
-->
&lt;h2 id="ä»€ä¹ˆæ˜¯åŠ¨æ€-kubelet-é…ç½®">ä»€ä¹ˆæ˜¯åŠ¨æ€ Kubelet é…ç½®ï¼Ÿ&lt;/h2>
&lt;!--
Kubernetes v1.10 made it possible to configure the Kubelet via a beta [config file](/docs/tasks/administer-cluster/kubelet-config-file/) API. Kubernetes already provides the ConfigMap abstraction for storing arbitrary file data in the API server.
-->
&lt;p>Kubernetes v1.10 ä½¿å¾—å¯ä»¥é€šè¿‡ Beta ç‰ˆæœ¬çš„&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/kubelet-config-file/">é…ç½®æ–‡ä»¶&lt;/a>
API é…ç½® kubeletã€‚
Kubernetes å·²ç»æä¾›äº†ç”¨äºåœ¨ API æœåŠ¡å™¨ä¸­å­˜å‚¨ä»»æ„æ–‡ä»¶æ•°æ®çš„ ConfigMap æŠ½è±¡ã€‚&lt;/p>
&lt;!--
Dynamic Kubelet configuration extends the Node object so that a Node can refer to a ConfigMap that contains the same type of config file. When a Node is updated to refer to a new ConfigMap, the associated Kubelet will attempt to use the new configuration.
-->
&lt;p>åŠ¨æ€ Kubelet é…ç½®æ‰©å±•äº† Node å¯¹è±¡ï¼Œä»¥ä¾¿ Node å¯ä»¥å¼•ç”¨åŒ…å«ç›¸åŒç±»å‹é…ç½®æ–‡ä»¶çš„ ConfigMapã€‚
å½“èŠ‚ç‚¹æ›´æ–°ä¸ºå¼•ç”¨æ–°çš„ ConfigMap æ—¶ï¼Œå…³è”çš„ Kubelet å°†å°è¯•ä½¿ç”¨æ–°çš„é…ç½®ã€‚&lt;/p>
&lt;!--
## How does it work?
-->
&lt;h2 id="å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„">å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ&lt;/h2>
&lt;!--
Dynamic Kubelet configuration provides the following core features:
-->
&lt;p>åŠ¨æ€ Kubelet é…ç½®æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š&lt;/p>
&lt;!--
* Kubelet attempts to use the dynamically assigned configuration.
* Kubelet "checkpoints" configuration to local disk, enabling restarts without API server access.
* Kubelet reports assigned, active, and last-known-good configuration sources in the Node status.
* When invalid configuration is dynamically assigned, Kubelet automatically falls back to a last-known-good configuration and reports errors in the Node status.
-->
&lt;ul>
&lt;li>Kubelet å°è¯•ä½¿ç”¨åŠ¨æ€åˆ†é…çš„é…ç½®ã€‚&lt;/li>
&lt;li>Kubelet å°†å…¶é…ç½®å·²æ£€æŸ¥ç‚¹çš„å½¢å¼ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜ï¼Œæ— éœ€ API æœåŠ¡å™¨è®¿é—®å³å¯é‡æ–°å¯åŠ¨ã€‚&lt;/li>
&lt;li>Kubelet åœ¨ Node çŠ¶æ€ä¸­æŠ¥å‘Šå·²æŒ‡å®šçš„ã€æ´»è·ƒçš„å’Œæœ€è¿‘å·²çŸ¥è‰¯å¥½çš„é…ç½®æºã€‚&lt;/li>
&lt;li>å½“åŠ¨æ€åˆ†é…äº†æ— æ•ˆçš„é…ç½®æ—¶ï¼ŒKubelet ä¼šè‡ªåŠ¨é€€å›åˆ°æœ€åä¸€æ¬¡æ­£ç¡®çš„é…ç½®ï¼Œå¹¶åœ¨ Node çŠ¶æ€ä¸­æŠ¥å‘Šé”™è¯¯ã€‚&lt;/li>
&lt;/ul>
&lt;!--
To use the dynamic Kubelet configuration feature, a cluster administrator or service provider will first post a ConfigMap containing the desired configuration, then set each Node.Spec.ConfigSource.ConfigMap reference to refer to the new ConfigMap. Operators can update these references at their preferred rate, giving them the ability to perform controlled rollouts of new configurations.
-->
&lt;p>è¦ä½¿ç”¨åŠ¨æ€ Kubelet é…ç½®åŠŸèƒ½ï¼Œç¾¤é›†ç®¡ç†å‘˜æˆ–æœåŠ¡æä¾›å•†å°†é¦–å…ˆå‘å¸ƒåŒ…å«æ‰€éœ€é…ç½®çš„ ConfigMapï¼Œ
ç„¶åè®¾ç½®æ¯ä¸ª Node.Spec.ConfigSource.ConfigMap å¼•ç”¨ä»¥æŒ‡å‘æ–°çš„ ConfigMapã€‚
è¿è¥å•†å¯ä»¥ä»¥ä»–ä»¬å–œæ¬¢çš„é€Ÿç‡æ›´æ–°è¿™äº›å‚è€ƒï¼Œä»è€Œä½¿ä»–ä»¬èƒ½å¤Ÿæ‰§è¡Œæ–°é…ç½®çš„å—æ§éƒ¨ç½²ã€‚&lt;/p>
&lt;!--
Each Kubelet watches its associated Node object for changes. When the Node.Spec.ConfigSource.ConfigMap reference is updated, the Kubelet will "checkpoint" the new ConfigMap by writing the files it contains to local disk. The Kubelet will then exit, and the OS-level process manager will restart it. Note that if the Node.Spec.ConfigSource.ConfigMap reference is not set, the Kubelet uses the set of flags and config files local to the machine it is running on.
-->
&lt;p>æ¯ä¸ª Kubelet éƒ½ä¼šç›‘è§†å…¶å…³è”çš„ Node å¯¹è±¡çš„æ›´æ”¹ã€‚
æ›´æ–° Node.Spec.ConfigSource.ConfigMap å¼•ç”¨åï¼Œ
Kubelet å°†é€šè¿‡å°†å…¶åŒ…å«çš„æ–‡ä»¶é€šè¿‡æ£€æŸ¥ç‚¹æœºåˆ¶å†™å…¥æœ¬åœ°ç£ç›˜ä¿å­˜æ–°çš„ ConfigMapã€‚
ç„¶åï¼ŒKubelet å°†é€€å‡ºï¼Œè€Œæ“ä½œç³»ç»Ÿçº§è¿›ç¨‹ç®¡ç†å™¨å°†é‡æ–°å¯åŠ¨å®ƒã€‚
è¯·æ³¨æ„ï¼Œå¦‚æœæœªè®¾ç½® Node.Spec.ConfigSource.ConfigMap å¼•ç”¨ï¼Œ
åˆ™ Kubelet å°†ä½¿ç”¨å…¶æ­£åœ¨è¿è¡Œçš„è®¡ç®—æœºæœ¬åœ°çš„ä¸€ç»„æ ‡å¿—å’Œé…ç½®æ–‡ä»¶ã€‚&lt;/p>
&lt;!--
Once restarted, the Kubelet will attempt to use the configuration from the new checkpoint. If the new configuration passes the Kubelet's internal validation, the Kubelet will update Node.Status.Config to reflect that it is using the new configuration. If the new configuration is invalid, the Kubelet will fall back to its last-known-good configuration and report an error in Node.Status.Config.
-->
&lt;p>é‡æ–°å¯åŠ¨åï¼ŒKubelet å°†å°è¯•ä½¿ç”¨æ¥è‡ªæ–°æ£€æŸ¥ç‚¹çš„é…ç½®ã€‚
å¦‚æœæ–°é…ç½®é€šè¿‡äº† Kubelet çš„å†…éƒ¨éªŒè¯ï¼Œåˆ™ Kubelet å°†æ›´æ–°
Node.Status.Config ç”¨ä»¥åæ˜ å®ƒæ­£åœ¨ä½¿ç”¨æ–°é…ç½®ã€‚
å¦‚æœæ–°é…ç½®æ— æ•ˆï¼Œåˆ™ Kubelet å°†é€€å›åˆ°å…¶æœ€åä¸€ä¸ªæ­£ç¡®çš„é…ç½®ï¼Œå¹¶åœ¨ Node.Status.Config ä¸­æŠ¥å‘Šé”™è¯¯ã€‚&lt;/p>
&lt;!--
Note that the default last-known-good configuration is the combination of Kubelet command-line flags with the Kubelet's local configuration file. Command-line flags that overlap with the config file always take precedence over both the local configuration file and dynamic configurations, for backwards-compatibility.
-->
&lt;p>è¯·æ³¨æ„ï¼Œé»˜è®¤çš„æœ€åä¸€æ¬¡æ­£ç¡®é…ç½®æ˜¯ Kubelet å‘½ä»¤è¡Œæ ‡å¿—ä¸ Kubelet çš„æœ¬åœ°é…ç½®æ–‡ä»¶çš„ç»„åˆã€‚
ä¸é…ç½®æ–‡ä»¶é‡å çš„å‘½ä»¤è¡Œæ ‡å¿—å§‹ç»ˆä¼˜å…ˆäºæœ¬åœ°é…ç½®æ–‡ä»¶å’ŒåŠ¨æ€é…ç½®ï¼Œä»¥å®ç°å‘åå…¼å®¹ã€‚&lt;/p>
&lt;!--
See the following diagram for a high-level overview of a configuration update for a single Node:
-->
&lt;p>æœ‰å…³å•ä¸ªèŠ‚ç‚¹çš„é…ç½®æ›´æ–°çš„é«˜çº§æ¦‚è¿°ï¼Œè¯·å‚è§ä¸‹å›¾ï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-07-11-dynamic-kubelet-configuration/kubelet-diagram.png" alt="kubelet-diagram">&lt;/p>
&lt;!--
## How can I learn more?
-->
&lt;h2 id="æˆ‘å¦‚ä½•äº†è§£æ›´å¤š">æˆ‘å¦‚ä½•äº†è§£æ›´å¤šï¼Ÿ&lt;/h2>
&lt;!--
Please see the official tutorial at /docs/tasks/administer-cluster/reconfigure-kubelet/, which contains more in-depth details on user workflow, how a configuration becomes "last-known-good," how the Kubelet "checkpoints" config, and possible failure modes.
-->
&lt;p>è¯·å‚é˜…/docs/tasks/administer-cluster/reconfigure-kubelet/ä¸Šçš„å®˜æ–¹æ•™ç¨‹ï¼Œ
å…¶ä¸­åŒ…å«æœ‰å…³ç”¨æˆ·å·¥ä½œæµï¼ŒæŸé…ç½®å¦‚ä½•æˆä¸ºâ€œæœ€æ–°çš„æ­£ç¡®çš„â€é…ç½®ï¼ŒKubelet å¦‚ä½•å¯¹é…ç½®æ‰§è¡Œâ€œæ£€æŸ¥ç‚¹â€æ“ä½œç­‰ï¼Œ
æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œä»¥åŠå¯èƒ½çš„æ•…éšœæ¨¡å¼ã€‚&lt;/p></description></item><item><title>Blog: ç”¨äº Kubernetes é›†ç¾¤ DNS çš„ CoreDNS GA æ­£å¼å‘å¸ƒ</title><link>https://kubernetes.io/zh/blog/2018/07/10/%E7%94%A8%E4%BA%8E-kubernetes-%E9%9B%86%E7%BE%A4-dns-%E7%9A%84-coredns-ga-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/10/%E7%94%A8%E4%BA%8E-kubernetes-%E9%9B%86%E7%BE%A4-dns-%E7%9A%84-coredns-ga-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</guid><description>
&lt;!--
---
layout: blog
title: "CoreDNS GA for Kubernetes Cluster DNS"
date: 2018-07-10
---
--->
&lt;!--
**Author**: John Belamaric (Infoblox)
**Editorâ€™s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/) on whatâ€™s new in Kubernetes 1.11**
--->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šJohn Belamaric (Infoblox)&lt;/p>
&lt;p>**ç¼–è€…æ³¨ï¼šè¿™ç¯‡æ–‡ç« æ˜¯ &lt;a href="https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/">ç³»åˆ—æ·±åº¦æ–‡ç« &lt;/a> ä¸­çš„ä¸€ç¯‡ï¼Œä»‹ç»äº† Kubernetes 1.11 æ–°å¢çš„åŠŸèƒ½&lt;/p>
&lt;!--
## Introduction
In Kubernetes 1.11, [CoreDNS](https://coredns.io) has reached General Availability (GA) for DNS-based service discovery, as an alternative to the kube-dns addon. This means that CoreDNS will be offered as an option in upcoming versions of the various installation tools. In fact, the kubeadm team chose to make it the default option starting with Kubernetes 1.11.
--->
&lt;h2 id="ä»‹ç»">ä»‹ç»&lt;/h2>
&lt;p>åœ¨ Kubernetes 1.11 ä¸­ï¼Œ&lt;a href="https://coredns.io">CoreDNS&lt;/a> å·²ç»è¾¾åˆ°åŸºäº DNS æœåŠ¡å‘ç°çš„ General Availability (GA)ï¼Œå¯ä»¥æ›¿ä»£ kube-dns æ’ä»¶ã€‚è¿™æ„å‘³ç€ CoreDNS ä¼šä½œä¸ºå³å°†å‘å¸ƒçš„å®‰è£…å·¥å…·çš„é€‰é¡¹ä¹‹ä¸€ä¸Šçº¿ã€‚å®é™…ä¸Šï¼Œä» Kubernetes 1.11 å¼€å§‹ï¼Œkubeadm å›¢é˜Ÿé€‰æ‹©å°†å®ƒè®¾ä¸ºé»˜è®¤é€‰é¡¹ã€‚&lt;/p>
&lt;!--
DNS-based service discovery has been part of Kubernetes for a long time with the kube-dns cluster addon. This has generally worked pretty well, but there have been some concerns around the reliability, flexibility and security of the implementation.
CoreDNS is a general-purpose, authoritative DNS server that provides a backwards-compatible, but extensible, integration with Kubernetes. It resolves the issues seen with kube-dns, and offers a number of unique features that solve a wider variety of use cases.
In this article, you will learn about the differences in the implementations of kube-dns and CoreDNS, and some of the helpful extensions offered by CoreDNS.
--->
&lt;p>å¾ˆä¹…ä»¥æ¥ï¼Œ kube-dns é›†ç¾¤æ’ä»¶ä¸€ç›´æ˜¯ Kubernetes çš„ä¸€éƒ¨åˆ†ï¼Œç”¨æ¥å®ç°åŸºäº DNS çš„æœåŠ¡å‘ç°ã€‚
é€šå¸¸ï¼Œæ­¤æ’ä»¶è¿è¡Œå¹³ç¨³ï¼Œä½†å¯¹äºå®ç°çš„å¯é æ€§ã€çµæ´»æ€§å’Œå®‰å…¨æ€§ä»å­˜åœ¨ä¸€äº›ç–‘è™‘ã€‚&lt;/p>
&lt;p>CoreDNS æ˜¯é€šç”¨çš„ã€æƒå¨çš„ DNS æœåŠ¡å™¨ï¼Œæä¾›ä¸ Kubernetes å‘åå…¼å®¹ä½†å¯æ‰©å±•çš„é›†æˆã€‚å®ƒè§£å†³äº† kube-dns é‡åˆ°çš„é—®é¢˜ï¼Œå¹¶æä¾›äº†è®¸å¤šç‹¬ç‰¹çš„åŠŸèƒ½ï¼Œå¯ä»¥è§£å†³å„ç§ç”¨ä¾‹ã€‚&lt;/p>
&lt;p>åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†äº†è§£ kube-dns å’Œ CoreDNS çš„å®ç°æœ‰ä½•å·®å¼‚ï¼Œä»¥åŠ CoreDNS æä¾›çš„ä¸€äº›éå¸¸æœ‰ç”¨çš„æ‰©å±•ã€‚&lt;/p>
&lt;!--
## Implementation differences
In kube-dns, several containers are used within a single pod: `kubedns`, `dnsmasq`, and `sidecar`. The `kubedns`
container watches the Kubernetes API and serves DNS records based on the [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md), `dnsmasq` provides caching and stub domain support, and `sidecar` provides metrics and health checks.
--->
&lt;h2 id="å®ç°å·®å¼‚">å®ç°å·®å¼‚&lt;/h2>
&lt;p>åœ¨ kube-dns ä¸­ï¼Œä¸€ä¸ª Pod ä¸­ä½¿ç”¨å¤šä¸ª å®¹å™¨ï¼š&lt;code>kubedns&lt;/code>ã€&lt;code>dnsmasq&lt;/code>ã€å’Œ &lt;code>sidecar&lt;/code>ã€‚&lt;code>kubedns&lt;/code> å®¹å™¨ç›‘è§† Kubernetes API å¹¶æ ¹æ® &lt;a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS è§„èŒƒ&lt;/a> æä¾› DNS è®°å½•ï¼Œ&lt;code>dnsmasq&lt;/code> æä¾›ç¼“å­˜å’Œå­˜æ ¹åŸŸæ”¯æŒï¼Œ&lt;code>sidecar&lt;/code> æä¾›æŒ‡æ ‡å’Œå¥åº·æ£€æŸ¥ã€‚&lt;/p>
&lt;!--
This setup leads to a few issues that have been seen over time. For one, security vulnerabilities in `dnsmasq` have led to the need
for a security-patch release of Kubernetes in the past. Additionally, because `dnsmasq` handles the stub domains,
but `kubedns` handles the External Services, you cannot use a stub domain in an external service, which is very
limiting to that functionality (see [dns#131](https://github.com/kubernetes/dns/issues/131)).
All of these functions are done in a single container in CoreDNS, which is running a process written in Go. The
different plugins that are enabled replicate (and enhance) the functionality found in kube-dns.
--->
&lt;p>éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæ­¤è®¾ç½®ä¼šå¯¼è‡´ä¸€äº›é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼Œä»¥å¾€ &lt;code>dnsmasq&lt;/code> ä¸­çš„å®‰å…¨æ¼æ´éœ€è¦é€šè¿‡å‘å¸ƒ Kubernetes çš„å®‰å…¨è¡¥ä¸æ¥è§£å†³ã€‚ä½†æ˜¯ï¼Œç”±äº &lt;code>dnsmasq&lt;/code> å¤„ç†å­˜æ ¹åŸŸï¼Œè€Œ &lt;code>kubedns&lt;/code> å¤„ç†å¤–éƒ¨æœåŠ¡ï¼Œå› æ­¤æ‚¨ä¸èƒ½åœ¨å¤–éƒ¨æœåŠ¡ä¸­ä½¿ç”¨å­˜æ ¹åŸŸï¼Œå¯¼è‡´è¿™ä¸ªåŠŸèƒ½å…·æœ‰å±€é™æ€§ï¼ˆè¯·å‚é˜… &lt;a href="https://github.com/kubernetes/dns/issues/131">dns#131&lt;/a>ï¼‰ã€‚&lt;/p>
&lt;p>åœ¨ CoreDNS ä¸­ï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½æ˜¯åœ¨ä¸€ä¸ªå®¹å™¨ä¸­å®Œæˆçš„ï¼Œè¯¥å®¹å™¨è¿è¡Œç”¨ Go ç¼–å†™çš„è¿›ç¨‹ã€‚æ‰€å¯ç”¨çš„ä¸åŒæ’ä»¶å¯å¤åˆ¶ï¼ˆå¹¶å¢å¼ºï¼‰åœ¨ kube-dns ä¸­å­˜åœ¨çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
## Configuring CoreDNS
In kube-dns, you can [modify a ConfigMap](https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/) to change the behavior of your service discovery. This allows the addition of
features such as serving stub domains, modifying upstream nameservers, and enabling federation.
--->
&lt;h2 id="é…ç½®-coredns">é…ç½® CoreDNS&lt;/h2>
&lt;p>åœ¨ kube-dns ä¸­ï¼Œæ‚¨å¯ä»¥ &lt;a href="https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/">ä¿®æ”¹ ConfigMap&lt;/a> æ¥æ›´æ”¹æœåŠ¡å‘ç°çš„è¡Œä¸ºã€‚ç”¨æˆ·å¯ä»¥æ·»åŠ è¯¸å¦‚ä¸ºå­˜æ ¹åŸŸæä¾›æœåŠ¡ã€ä¿®æ”¹ä¸Šæ¸¸åç§°æœåŠ¡å™¨ä»¥åŠå¯ç”¨è”ç›Ÿä¹‹ç±»çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
In CoreDNS, you similarly can modify the ConfigMap for the CoreDNS [Corefile](https://coredns.io/2017/07/23/corefile-explained/) to change how service discovery
works. This Corefile configuration offers many more options than you will find in kube-dns, since it is the
primary configuration file that CoreDNS uses for configuration of all of its features, even those that are not
Kubernetes related.
When upgrading from kube-dns to CoreDNS using `kubeadm`, your existing ConfigMap will be used to generate the
customized Corefile for you, including all of the configuration for stub domains, federation, and upstream nameservers. See [Using CoreDNS for Service Discovery](/docs/tasks/administer-cluster/coredns/) for more details.
--->
&lt;p>åœ¨ CoreDNS ä¸­ï¼Œæ‚¨å¯ä»¥ç±»ä¼¼åœ°ä¿®æ”¹ CoreDNS &lt;a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile&lt;/a> çš„ ConfigMapï¼Œä»¥æ›´æ”¹æœåŠ¡å‘ç°çš„å·¥ä½œæ–¹å¼ã€‚è¿™ç§ Corefile é…ç½®æä¾›äº†æ¯” kube-dns ä¸­æ›´å¤šçš„é€‰é¡¹ï¼Œå› ä¸ºå®ƒæ˜¯ CoreDNS ç”¨äºé…ç½®æ‰€æœ‰åŠŸèƒ½çš„ä¸»è¦é…ç½®æ–‡ä»¶ï¼Œå³ä½¿ä¸ Kubernetes ä¸ç›¸å…³çš„åŠŸèƒ½ä¹Ÿå¯ä»¥æ“ä½œã€‚&lt;/p>
&lt;p>ä½¿ç”¨ &lt;code>kubeadm&lt;/code> å°† kube-dns å‡çº§åˆ° CoreDNS æ—¶ï¼Œç°æœ‰çš„ ConfigMap å°†è¢«ç”¨æ¥ä¸ºæ‚¨ç”Ÿæˆè‡ªå®šä¹‰çš„ Corefileï¼ŒåŒ…æ‹¬å­˜æ ¹åŸŸã€è”ç›Ÿå’Œä¸Šæ¸¸åç§°æœåŠ¡å™¨çš„æ‰€æœ‰é…ç½®ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§
&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/coredns/">ä½¿ç”¨ CoreDNS è¿›è¡ŒæœåŠ¡å‘ç°&lt;/a>ã€‚&lt;/p>
&lt;!--
## Bug fixes and enhancements
There are several open issues with kube-dns that are resolved in CoreDNS, either in default configuration or with some customized configurations.
--->
&lt;h2 id="é”™è¯¯ä¿®å¤å’Œå¢å¼º">é”™è¯¯ä¿®å¤å’Œå¢å¼º&lt;/h2>
&lt;p>åœ¨ CoreDNS ä¸­è§£å†³äº† kube-dn çš„å¤šä¸ªæœªè§£å†³é—®é¢˜ï¼Œæ— è®ºæ˜¯é»˜è®¤é…ç½®è¿˜æ˜¯æŸäº›è‡ªå®šä¹‰é…ç½®ã€‚&lt;/p>
&lt;!--
* [dns#55 - Custom DNS entries for kube-dns](https://github.com/kubernetes/dns/issues/55) may be handled using the "fallthrough" mechanism in the [kubernetes plugin](https://coredns.io/plugins/kubernetes), using the [rewrite plugin](https://coredns.io/plugins/rewrite), or simply serving a subzone with a different plugin such as the [file plugin](https://coredns.io/plugins/file).
* [dns#116 - Only one A record set for headless service with pods having single hostname](https://github.com/kubernetes/dns/issues/116). This issue is fixed without any additional configuration.
* [dns#131 - externalName not using stubDomains settings](https://github.com/kubernetes/dns/issues/131). This issue is fixed without any additional configuration.
* [dns#167 - enable skyDNS round robin A/AAAA records](https://github.com/kubernetes/dns/issues/167). The equivalent functionality can be configured using the [load balance plugin](https://coredns.io/plugins/loadbalance).
* [dns#190 - kube-dns cannot run as non-root user](https://github.com/kubernetes/dns/issues/190). This issue is solved today by using a non-default image, but it will be made the default CoreDNS behavior in a future release.
* [dns#232 - fix pod hostname to be podname for dns srv records](https://github.com/kubernetes/dns/issues/232) is an enhancement that is supported through the "endpoint_pod_names" feature described below.
--->
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/55">dns#55 - kube-dns çš„è‡ªå®šä¹‰ DNS æ¡ç›®&lt;/a> å¯ä»¥ä½¿ç”¨ &lt;a href="https://coredns.io/plugins/kubernetes">kubernetes æ’ä»¶&lt;/a> ä¸­çš„ &amp;quot;fallthrough&amp;quot; æœºåˆ¶ï¼Œä½¿ç”¨ &lt;a href="https://coredns.io/plugins/rewrite">rewrite æ’ä»¶&lt;/a>ï¼Œæˆ–è€…åˆ†åŒºä½¿ç”¨ä¸åŒçš„æ’ä»¶ï¼Œä¾‹å¦‚ &lt;a href="https://coredns.io/plugins/file">file æ’ä»¶&lt;/a>ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/116">dns#116 - å¯¹å…·æœ‰ç›¸åŒä¸»æœºåçš„ã€æä¾›æ— å¤´æœåŠ¡æœåŠ¡çš„ Pod ä»…è®¾ç½®äº†ä¸€ä¸ª A è®°å½•&lt;/a>ã€‚æ— éœ€ä»»ä½•å…¶ä»–é…ç½®å³å¯è§£å†³æ­¤é—®é¢˜ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/131">dns#131 - externalName æœªä½¿ç”¨ stubDomains è®¾ç½®&lt;/a>ã€‚æ— éœ€ä»»ä½•å…¶ä»–é…ç½®å³å¯è§£å†³æ­¤é—®é¢˜ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/167">dns#167 - å…è®¸ skyDNS ä¸º A/AAAA è®°å½•æä¾›è½®æ¢&lt;/a>ã€‚å¯ä»¥ä½¿ç”¨ &lt;a href="https://coredns.io/plugins/loadbalance">è´Ÿè½½å‡è¡¡æ’ä»¶&lt;/a> é…ç½®ç­‰æ•ˆåŠŸèƒ½ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/190">dns#190 - kube-dns æ— æ³•ä»¥é root ç”¨æˆ·èº«ä»½è¿è¡Œ&lt;/a>ã€‚ä»Šå¤©ï¼Œé€šè¿‡ä½¿ç”¨ non-default é•œåƒè§£å†³äº†æ­¤é—®é¢˜ï¼Œä½†æ˜¯åœ¨å°†æ¥çš„ç‰ˆæœ¬ä¸­ï¼Œå®ƒå°†æˆä¸ºé»˜è®¤çš„ CoreDNS è¡Œä¸ºã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/232">dns#232 - åœ¨ dns srv è®°å½•ä¸­ä¿®å¤ pod hostname ä¸º podname&lt;/a> æ˜¯é€šè¿‡ä¸‹é¢æåˆ°çš„ &amp;quot;endpoint_pod_names&amp;quot; åŠŸèƒ½è¿›è¡Œæ”¯æŒçš„å¢å¼ºåŠŸèƒ½ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
## Metrics
The functional behavior of the default CoreDNS configuration is the same as kube-dns. However,
one difference you need to be aware of is that the published metrics are not the same. In kube-dns,
you get separate metrics for `dnsmasq` and `kubedns` (skydns). In CoreDNS there is a completely
different set of metrics, since it is all a single process. You can find more details on these
metrics on the CoreDNS [Prometheus plugin](https://coredns.io/plugins/metrics/) page.
--->
&lt;h2 id="æŒ‡æ ‡">æŒ‡æ ‡&lt;/h2>
&lt;p>CoreDNS é»˜è®¤é…ç½®çš„åŠŸèƒ½æ€§è¡Œä¸ºä¸ kube-dns ç›¸åŒã€‚ä½†æ˜¯ï¼Œä½ éœ€è¦äº†è§£çš„å·®åˆ«ä¹‹ä¸€æ˜¯äºŒè€…å‘å¸ƒçš„æŒ‡æ ‡æ˜¯ä¸åŒçš„ã€‚åœ¨ kube-dns ä¸­ï¼Œæ‚¨å°†åˆ†åˆ«è·å¾— &lt;code>dnsmasq&lt;/code> å’Œ &lt;code>kubedns&lt;/code>ï¼ˆskydnsï¼‰çš„åº¦é‡å€¼ã€‚åœ¨ CoreDNS ä¸­ï¼Œå­˜åœ¨ä¸€ç»„å®Œå…¨ä¸åŒçš„æŒ‡æ ‡ï¼Œå› ä¸ºå®ƒä»¬åœ¨åŒä¸€ä¸ªè¿›ç¨‹ä¸­ã€‚æ‚¨å¯ä»¥åœ¨ CoreDNS &lt;a href="https://coredns.io/plugins/metrics/">Prometheus æ’ä»¶&lt;/a> é¡µé¢ä¸Šæ‰¾åˆ°æœ‰å…³è¿™äº›æŒ‡æ ‡çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
## Some special features
The standard CoreDNS Kubernetes configuration is designed to be backwards compatible with the prior
kube-dns behavior. But with some configuration changes, CoreDNS can allow you to modify how the
DNS service discovery works in your cluster. A number of these features are intended to still be
compliant with the [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md);
they enhance functionality but remain backward compatible. Since CoreDNS is not
*only* made for Kubernetes, but is instead a general-purpose DNS server, there are many things you
can do beyond that specification.
--->
&lt;h2 id="ä¸€äº›ç‰¹æ®ŠåŠŸèƒ½">ä¸€äº›ç‰¹æ®ŠåŠŸèƒ½&lt;/h2>
&lt;p>æ ‡å‡†çš„ CoreDNS Kubernetes é…ç½®æ—¨åœ¨ä¸ä»¥å‰çš„ kube-dns åœ¨è¡Œä¸ºä¸Šå‘åå…¼å®¹ã€‚ä½†æ˜¯ï¼Œé€šè¿‡è¿›è¡Œä¸€äº›é…ç½®æ›´æ”¹ï¼ŒCoreDNS å…è®¸æ‚¨ä¿®æ”¹ DNS æœåŠ¡å‘ç°åœ¨ç¾¤é›†ä¸­çš„å·¥ä½œæ–¹å¼ã€‚è¿™äº›åŠŸèƒ½ä¸­çš„è®¸å¤šåŠŸèƒ½ä»è¦ç¬¦åˆ &lt;a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNSè§„èŒƒ&lt;/a>ï¼›å®ƒä»¬åœ¨å¢å¼ºäº†åŠŸèƒ½çš„åŒæ—¶ä¿æŒå‘åå…¼å®¹ã€‚ç”±äº CoreDNS å¹¶é &lt;em>ä»…&lt;/em> ç”¨äº Kubernetesï¼Œè€Œæ˜¯é€šç”¨çš„ DNS æœåŠ¡å™¨ï¼Œå› æ­¤æ‚¨å¯ä»¥åšå¾ˆå¤šè¶…å‡ºè¯¥è§„èŒƒçš„äº‹æƒ…ã€‚&lt;/p>
&lt;!--
### Pods verified mode
In kube-dns, pod name records are "fake". That is, any "a-b-c-d.namespace.pod.cluster.local" query will
return the IP address "a.b.c.d". In some cases, this can weaken the identity guarantees offered by TLS. So,
CoreDNS offers a "pods verified" mode, which will only return the IP address if there is a pod in the
specified namespace with that IP address.
--->
&lt;h3 id="pod-éªŒè¯æ¨¡å¼">Pod éªŒè¯æ¨¡å¼&lt;/h3>
&lt;p>åœ¨ kube-dns ä¸­ï¼ŒPod åç§°è®°å½•æ˜¯ &amp;quot;ä¼ªé€ çš„&amp;quot;ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä»»ä½• &amp;quot;a-b-c-d.namespace.pod.cluster.local&amp;quot; æŸ¥è¯¢éƒ½å°†è¿”å› IP åœ°å€ &amp;quot;a.b.c.d&amp;quot;ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼± TLS æä¾›çš„èº«ä»½ç¡®è®¤ã€‚å› æ­¤ï¼ŒCoreDNS æä¾›äº†ä¸€ç§ &amp;quot;Pod éªŒè¯&amp;quot; çš„æ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä»…åœ¨æŒ‡å®šåç§°ç©ºé—´ä¸­å­˜åœ¨å…·æœ‰è¯¥ IP åœ°å€çš„ Pod æ—¶æ‰è¿”å› IP åœ°å€ã€‚&lt;/p>
&lt;!--
### Endpoint names based on pod names
In kube-dns, when using a headless service, you can use an SRV request to get a list of
all endpoints for the service:
--->
&lt;h3 id="åŸºäº-pod-åç§°çš„ç«¯ç‚¹åç§°">åŸºäº Pod åç§°çš„ç«¯ç‚¹åç§°&lt;/h3>
&lt;p>åœ¨ kube-dns ä¸­ï¼Œä½¿ç”¨æ— å¤´æœåŠ¡æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ SRV è¯·æ±‚è·å–è¯¥æœåŠ¡çš„æ‰€æœ‰ç«¯ç‚¹çš„åˆ—è¡¨ï¼š&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 10 33 0 6234396237313665.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6662363165353239.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6338633437303230.headless.default.svc.cluster.local.
dnstools#
&lt;/code>&lt;/pre>&lt;!--
However, the endpoint DNS names are (for practical purposes) random. In CoreDNS, by default, you get endpoint
DNS names based upon the endpoint IP address:
--->
&lt;p>ä½†æ˜¯ï¼Œç«¯ç‚¹ DNS åç§°ï¼ˆå‡ºäºå®é™…ç›®çš„ï¼‰æ˜¯éšæœºçš„ã€‚åœ¨ CoreDNS ä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ‚¨æ‰€è·å¾—çš„ç«¯ç‚¹ DNS åç§°æ˜¯åŸºäºç«¯ç‚¹ IP åœ°å€ç”Ÿæˆçš„ï¼š&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-14.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-18.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-4.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-9.headless.default.svc.cluster.local.
&lt;/code>&lt;/pre>&lt;!--
For some applications, it is desirable to have the pod name for this, rather than the pod IP
address (see for example [kubernetes#47992](https://github.com/kubernetes/kubernetes/issues/47992) and [coredns#1190](https://github.com/coredns/coredns/pull/1190)). To enable this in CoreDNS, you specify the "endpoint_pod_names" option in your Corefile, which results in this:
--->
&lt;p>å¯¹äºæŸäº›åº”ç”¨ç¨‹åºï¼Œä½ ä¼šå¸Œæœ›åœ¨è¿™é‡Œä½¿ç”¨ Pod åç§°ï¼Œè€Œä¸æ˜¯ Pod IP åœ°å€ï¼ˆä¾‹å¦‚ï¼Œå‚è§ &lt;a href="https://github.com/kubernetes/kubernetes/issues/47992">kubernetes#47992&lt;/a> å’Œ &lt;a href="https://github.com/coredns/coredns/pull/1190">coredns#1190&lt;/a>ï¼‰ã€‚è¦åœ¨ CoreDNS ä¸­å¯ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨ Corefile ä¸­æŒ‡å®š &amp;quot;endpoint_pod_names&amp;quot; é€‰é¡¹ï¼Œç»“æœå¦‚ä¸‹ï¼š&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-qv84p.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-zc8lx.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-q7lf2.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-566rt.headless.default.svc.cluster.local.
&lt;/code>&lt;/pre>&lt;!--
### Autopath
CoreDNS also has a special feature to improve latency in DNS requests for external names. In Kubernetes, the
DNS search path for pods specifies a long list of suffixes. This enables the use of short names when requesting
services in the cluster - for example, "headless" above, rather than "headless.default.svc.cluster.local". However,
when requesting an external name - "infoblox.com", for example - several invalid DNS queries are made by the client,
requiring a roundtrip from the client to kube-dns each time (actually to `dnsmasq` and then to `kubedns`, since [negative caching is disabled](https://github.com/kubernetes/dns/issues/121)):
--->
&lt;h3 id="è‡ªåŠ¨è·¯å¾„">è‡ªåŠ¨è·¯å¾„&lt;/h3>
&lt;p>CoreDNS è¿˜å…·æœ‰ä¸€é¡¹ç‰¹æ®ŠåŠŸèƒ½ï¼Œå¯ä»¥æ”¹å–„ DNS ä¸­å¤–éƒ¨åç§°è¯·æ±‚çš„å»¶è¿Ÿã€‚åœ¨ Kubernetes ä¸­ï¼ŒPod çš„ DNS æœç´¢è·¯å¾„æŒ‡å®šäº†ä¸€é•¿ä¸²åç¼€ã€‚è¿™ä¸€ç‰¹ç‚¹ä½¿å¾—ä½ å¯ä»¥é’ˆå¯¹é›†ç¾¤ä¸­æœåŠ¡ä½¿ç”¨çŸ­åç§° - ä¾‹å¦‚ï¼Œä¸Šé¢çš„ &amp;quot;headless&amp;quot;ï¼Œè€Œä¸æ˜¯ &amp;quot;headless.default.svc.cluster.local&amp;quot;ã€‚ä½†æ˜¯ï¼Œå½“è¯·æ±‚ä¸€ä¸ªå¤–éƒ¨åç§°ï¼ˆä¾‹å¦‚ &amp;quot;infoblox.com&amp;quot;ï¼‰æ—¶ï¼Œå®¢æˆ·ç«¯ä¼šè¿›è¡Œå‡ ä¸ªæ— æ•ˆçš„ DNS æŸ¥è¯¢ï¼Œæ¯æ¬¡éƒ½éœ€è¦ä»å®¢æˆ·ç«¯åˆ° kube-dns å¾€è¿”ï¼ˆå®é™…ä¸Šæ˜¯åˆ° &lt;code>dnsmasq&lt;/code>ï¼Œç„¶ååˆ° &lt;code>kubedns&lt;/code>ï¼‰ï¼Œå› ä¸º &lt;a href="https://github.com/kubernetes/dns/issues/121">ç¦ç”¨äº†è´Ÿç¼“å­˜&lt;/a>ï¼‰&lt;/p>
&lt;ul>
&lt;li>infoblox.com.default.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.your-internal-domain.com -&amp;gt; NXDOMAIN&lt;/li>
&lt;/ul>
&lt;!--
* infoblox.com -> returns a valid record
--->
&lt;ul>
&lt;li>infoblox.com -&amp;gt; è¿”å›æœ‰æ•ˆè®°å½•&lt;/li>
&lt;/ul>
&lt;!--
In CoreDNS, an optional feature called [autopath](https://coredns.io/plugins/autopath) can be enabled that will cause this search path to be followed
*in the server*. That is, CoreDNS will figure out from the source IP address which namespace the client pod is in,
and it will walk this search list until it gets a valid answer. Since the first 3 of these are resolved internally
within CoreDNS itself, it cuts out all of the back and forth between the client and server, reducing latency.
--->
&lt;p>åœ¨ CoreDNS ä¸­ï¼Œå¯ä»¥å¯ç”¨ &lt;a href="https://coredns.io/plugins/autopath">autopath&lt;/a> çš„å¯é€‰åŠŸèƒ½ï¼Œè¯¥åŠŸèƒ½ä½¿æœç´¢è·¯å¾„åœ¨ &lt;em>æœåŠ¡å™¨ç«¯&lt;/em> éå†ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒCoreDNS å°†åŸºäºæº IP åœ°å€åˆ¤æ–­å®¢æˆ·ç«¯ Pod æ‰€åœ¨çš„å‘½åç©ºé—´ï¼Œå¹¶ä¸”éå†æ­¤æœç´¢åˆ—è¡¨ï¼Œç›´åˆ°è·å¾—æœ‰æ•ˆç­”æ¡ˆä¸ºæ­¢ã€‚ç”±äºå…¶ä¸­çš„å‰ä¸‰ä¸ªæ˜¯åœ¨ CoreDNS æœ¬èº«å†…éƒ¨è§£å†³çš„ï¼Œå› æ­¤å®ƒæ¶ˆé™¤äº†å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´æ‰€æœ‰çš„æ¥å›é€šä¿¡ï¼Œä»è€Œå‡å°‘äº†å»¶è¿Ÿã€‚&lt;/p>
&lt;!--
### A few other Kubernetes specific features
In CoreDNS, you can use standard DNS zone transfer to export the entire DNS record set. This is useful for
debugging your services as well as importing the cluster zone into other DNS servers.
You can also filter by namespaces or a label selector. This can allow you to run specific CoreDNS instances that will only server records that match the filters, exposing only a limited set of your services via DNS.
--->
&lt;h3 id="å…¶ä»–ä¸€äº›ç‰¹å®šäº-kubernetes-çš„åŠŸèƒ½">å…¶ä»–ä¸€äº›ç‰¹å®šäº Kubernetes çš„åŠŸèƒ½&lt;/h3>
&lt;p>åœ¨ CoreDNS ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ ‡å‡† DNS åŒºåŸŸä¼ è¾“æ¥å¯¼å‡ºæ•´ä¸ª DNS è®°å½•é›†ã€‚è¿™å¯¹äºè°ƒè¯•æœåŠ¡ä»¥åŠå°†é›†ç¾¤åŒºå¯¼å…¥å…¶ä»– DNS æœåŠ¡å™¨å¾ˆæœ‰ç”¨ã€‚&lt;/p>
&lt;p>æ‚¨è¿˜å¯ä»¥æŒ‰åç§°ç©ºé—´æˆ–æ ‡ç­¾é€‰æ‹©å™¨è¿›è¡Œè¿‡æ»¤ã€‚è¿™æ ·ï¼Œæ‚¨å¯ä»¥è¿è¡Œç‰¹å®šçš„ CoreDNS å®ä¾‹ï¼Œè¯¥å®ä¾‹ä»…æœåŠ¡ä¸è¿‡æ»¤å™¨åŒ¹é…çš„è®°å½•ï¼Œä»è€Œé€šè¿‡ DNS å…¬å¼€å—é™çš„æœåŠ¡é›†ã€‚&lt;/p>
&lt;!--
## Extensibility
In addition to the features described above, CoreDNS is easily extended. It is possible to build custom versions
of CoreDNS that include your own features. For example, this ability has been used to extend CoreDNS to do recursive resolution
with the [unbound plugin](https://coredns.io/explugins/unbound), to server records directly from a database with the [pdsql plugin](https://coredns.io/explugins/pdsql), and to allow multiple CoreDNS instances to share a common level 2 cache with the [redisc plugin](https://coredns.io/explugins/redisc).
Many other interesting extensions have been added, which you will find on the [External Plugins](https://coredns.io/explugins/) page of the CoreDNS site. One that is really interesting for Kubernetes and Istio users is the [kubernetai plugin](https://coredns.io/explugins/kubernetai), which allows a single CoreDNS instance to connect to multiple Kubernetes clusters and provide service discovery across all of them.
--->
&lt;h2 id="å¯æ‰©å±•æ€§">å¯æ‰©å±•æ€§&lt;/h2>
&lt;p>é™¤äº†ä¸Šè¿°åŠŸèƒ½ä¹‹å¤–ï¼ŒCoreDNS è¿˜å¯è½»æ¾æ‰©å±•ï¼Œæ„å»ºåŒ…å«æ‚¨ç‹¬æœ‰çš„åŠŸèƒ½çš„è‡ªå®šä¹‰ç‰ˆæœ¬çš„ CoreDNSã€‚ä¾‹å¦‚ï¼Œè¿™ä¸€èƒ½åŠ›å·²è¢«ç”¨äºæ‰©å±• CoreDNS æ¥ä½¿ç”¨ &lt;a href="https://coredns.io/explugins/unbound">unbound æ’ä»¶&lt;/a> è¿›è¡Œé€’å½’è§£æã€ä½¿ç”¨ &lt;a href="https://coredns.io/explugins/pdsql">pdsql æ’ä»¶&lt;/a> ç›´æ¥ä»æ•°æ®åº“æä¾›è®°å½•ï¼Œä»¥åŠä½¿ç”¨ &lt;a href="https://coredns.io/explugins/redisc">redisc æ’ä»¶&lt;/a> ä¸å¤šä¸ª CoreDNS å®ä¾‹å…±äº«ä¸€ä¸ªå…¬å…±çš„ 2 çº§ç¼“å­˜ã€‚&lt;/p>
&lt;p>å·²æ·»åŠ çš„è¿˜æœ‰è®¸å¤šå…¶ä»–æœ‰è¶£çš„æ‰©å±•ï¼Œæ‚¨å¯ä»¥åœ¨ CoreDNS ç«™ç‚¹çš„ &lt;a href="https://coredns.io/explugins/">å¤–éƒ¨æ’ä»¶&lt;/a> é¡µé¢ä¸Šæ‰¾åˆ°è¿™äº›æ‰©å±•ã€‚Kubernetes å’Œ Istio ç”¨æˆ·çœŸæ­£æ„Ÿå…´è¶£çš„æ˜¯ &lt;a href="https://coredns.io/explugins/kubernetai">kubernetai æ’ä»¶&lt;/a>ï¼Œå®ƒå…è®¸å•ä¸ª CoreDNS å®ä¾‹è¿æ¥åˆ°å¤šä¸ª Kubernetes é›†ç¾¤å¹¶åœ¨æ‰€æœ‰é›†ç¾¤ä¸­æä¾›æœåŠ¡å‘ç° ã€‚&lt;/p>
&lt;!--
## What's Next?
CoreDNS is an independent project, and as such is developing many features that are not directly
related to Kubernetes. However, a number of these will have applications within Kubernetes. For example,
the upcoming integration with policy engines will allow CoreDNS to make intelligent choices about which endpoint
to return when a headless service is requested. This could be used to route traffic to a local pod, or
to a more responsive pod. Many other features are in development, and of course as an open source project, we welcome you to suggest and contribute your own features!
The features and differences described above are a few examples. There is much more you can do with CoreDNS.
You can find out more on the [CoreDNS Blog](https://coredns.io/blog).
--->
&lt;h2 id="ä¸‹ä¸€æ­¥å·¥ä½œ">ä¸‹ä¸€æ­¥å·¥ä½œ&lt;/h2>
&lt;p>CoreDNS æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é¡¹ç›®ï¼Œè®¸å¤šä¸ Kubernetes ä¸ç›´æ¥ç›¸å…³çš„åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ã€‚ä½†æ˜¯ï¼Œå…¶ä¸­è®¸å¤šåŠŸèƒ½å°†åœ¨ Kubernetes ä¸­å…·æœ‰å¯¹åº”çš„åº”ç”¨ã€‚ä¾‹å¦‚ï¼Œä¸ç­–ç•¥å¼•æ“å®Œæˆé›†æˆåï¼Œå½“è¯·æ±‚æ— å¤´æœåŠ¡æ—¶ï¼ŒCoreDNS èƒ½å¤Ÿæ™ºèƒ½åœ°é€‰æ‹©è¿”å›å“ªä¸ªç«¯ç‚¹ã€‚è¿™å¯ç”¨äºå°†æµé‡åˆ†æµåˆ°åˆ°æœ¬åœ° Pod æˆ–å“åº”æ›´å¿«çš„ Podã€‚æ›´å¤šçš„å…¶ä»–åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ï¼Œå½“ç„¶ä½œä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬æ¬¢è¿æ‚¨æå‡ºå»ºè®®å¹¶è´¡çŒ®è‡ªå·±çš„åŠŸèƒ½ç‰¹æ€§ï¼&lt;/p>
&lt;p>ä¸Šè¿°ç‰¹å¾å’Œå·®å¼‚æ˜¯å‡ ä¸ªç¤ºä¾‹ã€‚CoreDNS è¿˜å¯ä»¥åšæ›´å¤šçš„äº‹æƒ…ã€‚æ‚¨å¯ä»¥åœ¨ &lt;a href="https://coredns.io/blog">CoreDNS åšå®¢&lt;/a> ä¸Šæ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚&lt;/p>
&lt;!--
### Get involved with CoreDNS
CoreDNS is an incubated [CNCF](https:://cncf.io) project.
We're most active on Slack (and GitHub):
--->
&lt;h3 id="å‚ä¸-coredns">å‚ä¸ CoreDNS&lt;/h3>
&lt;p>CoreDNS æ˜¯ä¸€ä¸ª &lt;a href="https:://cncf.io">CNCF&lt;/a> å­µåŒ–é¡¹ç›®ã€‚&lt;/p>
&lt;p>æˆ‘ä»¬åœ¨ Slackï¼ˆå’Œ GitHubï¼‰ä¸Šæœ€æ´»è·ƒï¼š&lt;/p>
&lt;ul>
&lt;li>Slack: #coredns on &lt;a href="https://slack.cncf.io">https://slack.cncf.io&lt;/a>&lt;/li>
&lt;li>GitHub: &lt;a href="https://github.com/coredns/coredns">https://github.com/coredns/coredns&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
More resources can be found:
--->
&lt;p>æ›´å¤šèµ„æºè¯·æµè§ˆï¼š&lt;/p>
&lt;ul>
&lt;li>Website: &lt;a href="https://coredns.io">https://coredns.io&lt;/a>&lt;/li>
&lt;li>Blog: &lt;a href="https://blog.coredns.io">https://blog.coredns.io&lt;/a>&lt;/li>
&lt;li>Twitter: &lt;a href="https://twitter.com/corednsio">@corednsio&lt;/a>&lt;/li>
&lt;li>Mailing list/group: &lt;a href="mailto:coredns-discuss@googlegroups.com">coredns-discuss@googlegroups.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: IPVS-Based In-Cluster Load Balancing Deep Dive</title><link>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</link><pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</guid><description>
&lt;!--
Author: Jun Du(Huawei), Haibin Xie(Huawei), Wei Liang(Huawei)
Editorâ€™s note: this post is part of a series of in-depth articles on whatâ€™s new in Kubernetes 1.11
-->
&lt;p>ä½œè€…: Jun Du(åä¸º), Haibin Xie(åä¸º), Wei Liang(åä¸º)&lt;/p>
&lt;p>æ³¨æ„: è¿™ç¯‡æ–‡ç« å‡ºè‡ª ç³»åˆ—æ·±åº¦æ–‡ç«  ä»‹ç» Kubernetes 1.11 çš„æ–°ç‰¹æ€§&lt;/p>
&lt;!--
Introduction
Per the Kubernetes 1.11 release blog post , we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.
-->
&lt;p>ä»‹ç»&lt;/p>
&lt;p>æ ¹æ® Kubernetes 1.11 å‘å¸ƒçš„åšå®¢æ–‡ç« , æˆ‘ä»¬å®£å¸ƒåŸºäº IPVS çš„é›†ç¾¤å†…éƒ¨æœåŠ¡è´Ÿè½½å‡è¡¡å·²è¾¾åˆ°ä¸€èˆ¬å¯ç”¨æ€§ã€‚ åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†å¸¦æ‚¨æ·±å…¥äº†è§£è¯¥åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
What Is IPVS?
IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.
IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.
-->
&lt;p>ä»€ä¹ˆæ˜¯ IPVS ?&lt;/p>
&lt;p>IPVS (IP Virtual Server)æ˜¯åœ¨ Netfilter ä¸Šå±‚æ„å»ºçš„ï¼Œå¹¶ä½œä¸º Linux å†…æ ¸çš„ä¸€éƒ¨åˆ†ï¼Œå®ç°ä¼ è¾“å±‚è´Ÿè½½å‡è¡¡ã€‚&lt;/p>
&lt;p>IPVS é›†æˆåœ¨ LVSï¼ˆLinux Virtual Serverï¼ŒLinux è™šæ‹ŸæœåŠ¡å™¨ï¼‰ä¸­ï¼Œå®ƒåœ¨ä¸»æœºä¸Šè¿è¡Œï¼Œå¹¶åœ¨ç‰©ç†æœåŠ¡å™¨é›†ç¾¤å‰ä½œä¸ºè´Ÿè½½å‡è¡¡å™¨ã€‚IPVS å¯ä»¥å°†åŸºäº TCP å’Œ UDP æœåŠ¡çš„è¯·æ±‚å®šå‘åˆ°çœŸå®æœåŠ¡å™¨ï¼Œå¹¶ä½¿çœŸå®æœåŠ¡å™¨çš„æœåŠ¡åœ¨å•ä¸ªIPåœ°å€ä¸Šæ˜¾ç¤ºä¸ºè™šæ‹ŸæœåŠ¡ã€‚ å› æ­¤ï¼ŒIPVS è‡ªç„¶æ”¯æŒ Kubernetes æœåŠ¡ã€‚&lt;/p>
&lt;!--
Why IPVS for Kubernetes?
As Kubernetes grows in usage, the scalability of its resources becomes more and more important. In particular, the scalability of services is paramount to the adoption of Kubernetes by developers/companies running large workloads.
Kube-proxy, the building block of service routing has relied on the battle-hardened iptables to implement the core supported Service types such as ClusterIP and NodePort. However, iptables struggles to scale to tens of thousands of Services because it is designed purely for firewalling purposes and is based on in-kernel rule lists.
Even though Kubernetes already support 5000 nodes in release v1.6, the kube-proxy with iptables is actually a bottleneck to scale the cluster to 5000 nodes. One example is that with NodePort Service in a 5000-node cluster, if we have 2000 services and each services have 10 pods, this will cause at least 20000 iptable records on each worker node, and this can make the kernel pretty busy.
On the other hand, using IPVS-based in-cluster service load balancing can help a lot for such cases. IPVS is specifically designed for load balancing and uses more efficient data structures (hash tables) allowing for almost unlimited scale under the hood.
-->
&lt;p>ä¸ºä»€ä¹ˆä¸º Kubernetes é€‰æ‹© IPVS ?&lt;/p>
&lt;p>éšç€ Kubernetes çš„ä½¿ç”¨å¢é•¿ï¼Œå…¶èµ„æºçš„å¯æ‰©å±•æ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç‰¹åˆ«æ˜¯ï¼ŒæœåŠ¡çš„å¯æ‰©å±•æ€§å¯¹äºè¿è¡Œå¤§å‹å·¥ä½œè´Ÿè½½çš„å¼€å‘äººå‘˜/å…¬å¸é‡‡ç”¨ Kubernetes è‡³å…³é‡è¦ã€‚&lt;/p>
&lt;p>Kube-proxy æ˜¯æœåŠ¡è·¯ç”±çš„æ„å»ºå—ï¼Œå®ƒä¾èµ–äºç»è¿‡å¼ºåŒ–æ”»å‡»çš„ iptables æ¥å®ç°æ”¯æŒæ ¸å¿ƒçš„æœåŠ¡ç±»å‹ï¼Œå¦‚ ClusterIP å’Œ NodePortã€‚ ä½†æ˜¯ï¼Œiptables éš¾ä»¥æ‰©å±•åˆ°æˆåƒä¸Šä¸‡çš„æœåŠ¡ï¼Œå› ä¸ºå®ƒçº¯ç²¹æ˜¯ä¸ºé˜²ç«å¢™è€Œè®¾è®¡çš„ï¼Œå¹¶ä¸”åŸºäºå†…æ ¸è§„åˆ™åˆ—è¡¨ã€‚&lt;/p>
&lt;p>å°½ç®¡ Kubernetes åœ¨ç‰ˆæœ¬v1.6ä¸­å·²ç»æ”¯æŒ5000ä¸ªèŠ‚ç‚¹ï¼Œä½†ä½¿ç”¨ iptables çš„ kube-proxy å®é™…ä¸Šæ˜¯å°†é›†ç¾¤æ‰©å±•åˆ°5000ä¸ªèŠ‚ç‚¹çš„ç“¶é¢ˆã€‚ ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œåœ¨5000èŠ‚ç‚¹é›†ç¾¤ä¸­ä½¿ç”¨ NodePort æœåŠ¡ï¼Œå¦‚æœæˆ‘ä»¬æœ‰2000ä¸ªæœåŠ¡å¹¶ä¸”æ¯ä¸ªæœåŠ¡æœ‰10ä¸ª podï¼Œè¿™å°†åœ¨æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šè‡³å°‘äº§ç”Ÿ20000ä¸ª iptable è®°å½•ï¼Œè¿™å¯èƒ½ä½¿å†…æ ¸éå¸¸ç¹å¿™ã€‚&lt;/p>
&lt;p>å¦ä¸€æ–¹é¢ï¼Œä½¿ç”¨åŸºäº IPVS çš„é›†ç¾¤å†…æœåŠ¡è´Ÿè½½å‡è¡¡å¯ä»¥ä¸ºè¿™ç§æƒ…å†µæä¾›å¾ˆå¤šå¸®åŠ©ã€‚ IPVS ä¸“é—¨ç”¨äºè´Ÿè½½å‡è¡¡ï¼Œå¹¶ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„ï¼ˆå“ˆå¸Œè¡¨ï¼‰ï¼Œå…è®¸å‡ ä¹æ— é™çš„è§„æ¨¡æ‰©å¼ ã€‚&lt;/p>
&lt;!--
IPVS-based Kube-proxy
Parameter Changes
Parameter: --proxy-mode In addition to existing userspace and iptables modes, IPVS mode is configured via --proxy-mode=ipvs. It implicitly uses IPVS NAT mode for service port mapping.
-->
&lt;p>åŸºäº IPVS çš„ Kube-proxy&lt;/p>
&lt;p>å‚æ•°æ›´æ”¹&lt;/p>
&lt;p>å‚æ•°: --proxy-mode é™¤äº†ç°æœ‰çš„ç”¨æˆ·ç©ºé—´å’Œ iptables æ¨¡å¼ï¼ŒIPVS æ¨¡å¼é€šè¿‡--proxy-mode = ipvs è¿›è¡Œé…ç½®ã€‚ å®ƒéšå¼ä½¿ç”¨ IPVS NAT æ¨¡å¼è¿›è¡ŒæœåŠ¡ç«¯å£æ˜ å°„ã€‚&lt;/p>
&lt;!--
Parameter: --ipvs-scheduler
A new kube-proxy parameter has been added to specify the IPVS load balancing algorithm, with the parameter being --ipvs-scheduler. If itâ€™s not configured, then round-robin (rr) is the default value.
- rr: round-robin
- lc: least connection
- dh: destination hashing
- sh: source hashing
- sed: shortest expected delay
- nq: never queue
In the future, we can implement Service specific scheduler (potentially via annotation), which has higher priority and overwrites the value.
-->
&lt;p>å‚æ•°: --ipvs-scheduler&lt;/p>
&lt;p>æ·»åŠ äº†ä¸€ä¸ªæ–°çš„ kube-proxy å‚æ•°æ¥æŒ‡å®š IPVS è´Ÿè½½å‡è¡¡ç®—æ³•ï¼Œå‚æ•°ä¸º --ipvs-schedulerã€‚ å¦‚æœæœªé…ç½®ï¼Œåˆ™é»˜è®¤ä¸º round-robin ç®—æ³•ï¼ˆrrï¼‰ã€‚&lt;/p>
&lt;ul>
&lt;li>rr: round-robin&lt;/li>
&lt;li>lc: least connection&lt;/li>
&lt;li>dh: destination hashing&lt;/li>
&lt;li>sh: source hashing&lt;/li>
&lt;li>sed: shortest expected delay&lt;/li>
&lt;li>nq: never queue&lt;/li>
&lt;/ul>
&lt;p>å°†æ¥ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°ç‰¹å®šäºæœåŠ¡çš„è°ƒåº¦ç¨‹åºï¼ˆå¯èƒ½é€šè¿‡æ³¨é‡Šï¼‰ï¼Œè¯¥è°ƒåº¦ç¨‹åºå…·æœ‰æ›´é«˜çš„ä¼˜å…ˆçº§å¹¶è¦†ç›–è¯¥å€¼ã€‚&lt;/p>
&lt;!--
Parameter: --cleanup-ipvs Similar to the --cleanup-iptables parameter, if true, cleanup IPVS configuration and IPTables rules that are created in IPVS mode.
Parameter: --ipvs-sync-period Maximum interval of how often IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.
Parameter: --ipvs-min-sync-period Minimum interval of how often the IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.
-->
&lt;p>å‚æ•°: --cleanup-ipvs ç±»ä¼¼äº --cleanup-iptables å‚æ•°ï¼Œå¦‚æœä¸º trueï¼Œåˆ™æ¸…é™¤åœ¨ IPVS æ¨¡å¼ä¸‹åˆ›å»ºçš„ IPVS é…ç½®å’Œ IPTables è§„åˆ™ã€‚&lt;/p>
&lt;p>å‚æ•°: --ipvs-sync-period åˆ·æ–° IPVS è§„åˆ™çš„æœ€å¤§é—´éš”æ—¶é—´ï¼ˆä¾‹å¦‚'5s'ï¼Œ'1m'ï¼‰ã€‚ å¿…é¡»å¤§äº0ã€‚&lt;/p>
&lt;p>å‚æ•°: --ipvs-min-sync-period åˆ·æ–° IPVS è§„åˆ™çš„æœ€å°é—´éš”æ—¶é—´é—´éš”ï¼ˆä¾‹å¦‚'5s'ï¼Œ'1m'ï¼‰ã€‚ å¿…é¡»å¤§äº0ã€‚&lt;/p>
&lt;!--
Parameter: --ipvs-exclude-cidrs A comma-separated list of CIDR's which the IPVS proxier should not touch when cleaning up IPVS rules because IPVS proxier can't distinguish kube-proxy created IPVS rules from user original IPVS rules. If you are using IPVS proxier with your own IPVS rules in the environment, this parameter should be specified, otherwise your original rule will be cleaned.
-->
&lt;p>å‚æ•°: --ipvs-exclude-cidrs æ¸…é™¤ IPVS è§„åˆ™æ—¶ IPVS ä»£ç†ä¸åº”è§¦åŠçš„ CIDR çš„é€—å·åˆ†éš”åˆ—è¡¨ï¼Œå› ä¸º IPVS ä»£ç†æ— æ³•åŒºåˆ† kube-proxy åˆ›å»ºçš„ IPVS è§„åˆ™å’Œç”¨æˆ·åŸå§‹è§„åˆ™ IPVS è§„åˆ™ã€‚ å¦‚æœæ‚¨åœ¨ç¯å¢ƒä¸­ä½¿ç”¨ IPVS proxier å’Œæ‚¨è‡ªå·±çš„ IPVS è§„åˆ™ï¼Œåˆ™åº”æŒ‡å®šæ­¤å‚æ•°ï¼Œå¦åˆ™å°†æ¸…é™¤åŸå§‹è§„åˆ™ã€‚&lt;/p>
&lt;!--
Design Considerations
IPVS Service Network Topology
When creating a ClusterIP type Service, IPVS proxier will do the following three things:
- Make sure a dummy interface exists in the node, defaults to kube-ipvs0
- Bind Service IP addresses to the dummy interface
- Create IPVS virtual servers for each Service IP address respectively
-->
&lt;p>è®¾è®¡æ³¨æ„äº‹é¡¹&lt;/p>
&lt;p>IPVS æœåŠ¡ç½‘ç»œæ‹“æ‰‘&lt;/p>
&lt;p>åˆ›å»º ClusterIP ç±»å‹æœåŠ¡æ—¶ï¼ŒIPVS proxier å°†æ‰§è¡Œä»¥ä¸‹ä¸‰é¡¹æ“ä½œï¼š&lt;/p>
&lt;ul>
&lt;li>ç¡®ä¿èŠ‚ç‚¹ä¸­å­˜åœ¨è™šæ‹Ÿæ¥å£ï¼Œé»˜è®¤ä¸º kube-ipvs0&lt;/li>
&lt;li>å°†æœåŠ¡ IP åœ°å€ç»‘å®šåˆ°è™šæ‹Ÿæ¥å£&lt;/li>
&lt;li>åˆ†åˆ«ä¸ºæ¯ä¸ªæœåŠ¡ IP åœ°å€åˆ›å»º IPVS è™šæ‹ŸæœåŠ¡å™¨&lt;/li>
&lt;/ul>
&lt;!--
Here comes an example:
# kubectl describe svc nginx-service
Name: nginx-service
...
Type: ClusterIP
IP: 10.102.128.4
Port: http 3080/TCP
Endpoints: 10.244.0.235:8080,10.244.1.237:8080
Session Affinity: None
# ip addr
...
73: kube-ipvs0: &lt;BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN qlen 1000
link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
inet 10.102.128.4/32 scope global kube-ipvs0
valid_lft forever preferred_lft forever
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-> RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr
-> 10.244.0.235:8080 Masq 1 0 0
-> 10.244.1.237:8080 Masq 1 0 0
-->
&lt;p>è¿™æ˜¯ä¸€ä¸ªä¾‹å­:&lt;/p>
&lt;pre>&lt;code># kubectl describe svc nginx-service
Name: nginx-service
...
Type: ClusterIP
IP: 10.102.128.4
Port: http 3080/TCP
Endpoints: 10.244.0.235:8080,10.244.1.237:8080
Session Affinity: None
# ip addr
...
73: kube-ipvs0: &amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state DOWN qlen 1000
link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
inet 10.102.128.4/32 scope global kube-ipvs0
valid_lft forever preferred_lft forever
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-&amp;gt; RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr
-&amp;gt; 10.244.0.235:8080 Masq 1 0 0
-&amp;gt; 10.244.1.237:8080 Masq 1 0 0
&lt;/code>&lt;/pre>
&lt;!--
Please note that the relationship between a Kubernetes Service and IPVS virtual servers is 1:N. For example, consider a Kubernetes Service that has more than one IP address. An External IP type Service has two IP addresses - ClusterIP and External IP. Then the IPVS proxier will create 2 IPVS virtual servers - one for Cluster IP and another one for External IP. The relationship between a Kubernetes Endpoint (each IP+Port pair) and an IPVS virtual server is 1:1.
Deleting of a Kubernetes service will trigger deletion of the corresponding IPVS virtual server, IPVS real servers and its IP addresses bound to the dummy interface.
Port Mapping
There are three proxy modes in IPVS: NAT (masq), IPIP and DR. Only NAT mode supports port mapping. Kube-proxy leverages NAT mode for port mapping. The following example shows IPVS mapping Service port 3080 to Pod port 8080.
-->
&lt;p>è¯·æ³¨æ„ï¼ŒKubernetes æœåŠ¡å’Œ IPVS è™šæ‹ŸæœåŠ¡å™¨ä¹‹é—´çš„å…³ç³»æ˜¯â€œ1ï¼šNâ€ã€‚ ä¾‹å¦‚ï¼Œè€ƒè™‘å…·æœ‰å¤šä¸ª IP åœ°å€çš„ Kubernetes æœåŠ¡ã€‚ å¤–éƒ¨ IP ç±»å‹æœåŠ¡æœ‰ä¸¤ä¸ª IP åœ°å€ - é›†ç¾¤IPå’Œå¤–éƒ¨ IPã€‚ ç„¶åï¼ŒIPVS ä»£ç†å°†åˆ›å»º2ä¸ª IPVS è™šæ‹ŸæœåŠ¡å™¨ - ä¸€ä¸ªç”¨äºé›†ç¾¤ IPï¼Œå¦ä¸€ä¸ªç”¨äºå¤–éƒ¨ IPã€‚ Kubernetes çš„ endpointï¼ˆæ¯ä¸ªIP +ç«¯å£å¯¹ï¼‰ä¸ IPVS è™šæ‹ŸæœåŠ¡å™¨ä¹‹é—´çš„å…³ç³»æ˜¯â€œ1ï¼š1â€ã€‚&lt;/p>
&lt;p>åˆ é™¤ Kubernetes æœåŠ¡å°†è§¦å‘åˆ é™¤ç›¸åº”çš„ IPVS è™šæ‹ŸæœåŠ¡å™¨ï¼ŒIPVS ç‰©ç†æœåŠ¡å™¨åŠå…¶ç»‘å®šåˆ°è™šæ‹Ÿæ¥å£çš„ IP åœ°å€ã€‚&lt;/p>
&lt;p>ç«¯å£æ˜ å°„&lt;/p>
&lt;p>IPVS ä¸­æœ‰ä¸‰ç§ä»£ç†æ¨¡å¼ï¼šNATï¼ˆmasqï¼‰ï¼ŒIPIP å’Œ DRã€‚ åªæœ‰ NAT æ¨¡å¼æ”¯æŒç«¯å£æ˜ å°„ã€‚ Kube-proxy åˆ©ç”¨ NAT æ¨¡å¼è¿›è¡Œç«¯å£æ˜ å°„ã€‚ ä»¥ä¸‹ç¤ºä¾‹æ˜¾ç¤º IPVS æœåŠ¡ç«¯å£3080åˆ°Podç«¯å£8080çš„æ˜ å°„ã€‚&lt;/p>
&lt;pre>&lt;code>TCP 10.102.128.4:3080 rr
-&amp;gt; 10.244.0.235:8080 Masq 1 0 0
-&amp;gt; 10.244.1.237:8080 Masq 1 0
&lt;/code>&lt;/pre>
&lt;!--
Session Affinity
IPVS supports client IP session affinity (persistent connection). When a Service specifies session affinity, the IPVS proxier will set a timeout value (180min=10800s by default) in the IPVS virtual server. For example:
-->
&lt;p>ä¼šè¯å…³ç³»&lt;/p>
&lt;p>IPVS æ”¯æŒå®¢æˆ·ç«¯ IP ä¼šè¯å…³è”ï¼ˆæŒä¹…è¿æ¥ï¼‰ã€‚ å½“æœåŠ¡æŒ‡å®šä¼šè¯å…³ç³»æ—¶ï¼ŒIPVS ä»£ç†å°†åœ¨ IPVS è™šæ‹ŸæœåŠ¡å™¨ä¸­è®¾ç½®è¶…æ—¶å€¼ï¼ˆé»˜è®¤ä¸º180åˆ†é’Ÿ= 10800ç§’ï¼‰ã€‚ ä¾‹å¦‚ï¼š&lt;/p>
&lt;pre>&lt;code># kubectl describe svc nginx-service
Name: nginx-service
...
IP: 10.102.128.4
Port: http 3080/TCP
Session Affinity: ClientIP
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-&amp;gt; RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr persistent 10800
&lt;/code>&lt;/pre>
&lt;!--
Iptables &amp; Ipset in IPVS Proxier
IPVS is for load balancing and it can't handle other workarounds in kube-proxy, e.g. packet filtering, hairpin-masquerade tricks, SNAT, etc.
IPVS proxier leverages iptables in the above scenarios. Specifically, ipvs proxier will fall back on iptables in the following 4 scenarios:
- kube-proxy start with --masquerade-all=true
- Specify cluster CIDR in kube-proxy startup
- Support Loadbalancer type service
- Support NodePort type service
However, we don't want to create too many iptables rules. So we adopt ipset for the sake of decreasing iptables rules. The following is the table of ipset sets that IPVS proxier maintains:
-->
&lt;p>IPVS ä»£ç†ä¸­çš„ Iptables å’Œ Ipset&lt;/p>
&lt;p>IPVS ç”¨äºè´Ÿè½½å‡è¡¡ï¼Œå®ƒæ— æ³•å¤„ç† kube-proxy ä¸­çš„å…¶ä»–é—®é¢˜ï¼Œä¾‹å¦‚ åŒ…è¿‡æ»¤ï¼Œæ•°æ®åŒ…æ¬ºéª—ï¼ŒSNAT ç­‰&lt;/p>
&lt;p>IPVS proxier åœ¨ä¸Šè¿°åœºæ™¯ä¸­åˆ©ç”¨ iptablesã€‚ å…·ä½“æ¥è¯´ï¼Œipvs proxier å°†åœ¨ä»¥ä¸‹4ç§æƒ…å†µä¸‹ä¾èµ–äº iptablesï¼š&lt;/p>
&lt;ul>
&lt;li>kube-proxy ä»¥ --masquerade-all = true å¼€å¤´&lt;/li>
&lt;li>åœ¨ kube-proxy å¯åŠ¨ä¸­æŒ‡å®šé›†ç¾¤ CIDR&lt;/li>
&lt;li>æ”¯æŒ Loadbalancer ç±»å‹æœåŠ¡&lt;/li>
&lt;li>æ”¯æŒ NodePort ç±»å‹çš„æœåŠ¡&lt;/li>
&lt;/ul>
&lt;p>ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸æƒ³åˆ›å»ºå¤ªå¤šçš„ iptables è§„åˆ™ã€‚ æ‰€ä»¥æˆ‘ä»¬é‡‡ç”¨ ipset æ¥å‡å°‘ iptables è§„åˆ™ã€‚ ä»¥ä¸‹æ˜¯ IPVS proxier ç»´æŠ¤çš„ ipset é›†è¡¨ï¼š&lt;/p>
&lt;!--
set name members usage
KUBE-CLUSTER-IP All Service IP + port masquerade for cases that masquerade-all=true or clusterCIDR specified
KUBE-LOOP-BACK All Service IP + port + IP masquerade for resolving hairpin issue
KUBE-EXTERNAL-IP Service External IP + port masquerade for packets to external IPs
KUBE-LOAD-BALANCER Load Balancer ingress IP + port masquerade for packets to Load Balancer type service
KUBE-LOAD-BALANCER-LOCAL Load Balancer ingress IP + port with externalTrafficPolicy=local accept packets to Load Balancer with externalTrafficPolicy=local
KUBE-LOAD-BALANCER-FW Load Balancer ingress IP + port with loadBalancerSourceRanges Drop packets for Load Balancer type Service with loadBalancerSourceRanges specified
KUBE-LOAD-BALANCER-SOURCE-CIDR Load Balancer ingress IP + port + source CIDR accept packets for Load Balancer type Service with loadBalancerSourceRanges specified
KUBE-NODE-PORT-TCP NodePort type Service TCP port masquerade for packets to NodePort(TCP)
KUBE-NODE-PORT-LOCAL-TCP NodePort type Service TCP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
KUBE-NODE-PORT-UDP NodePort type Service UDP port masquerade for packets to NodePort(UDP)
KUBE-NODE-PORT-LOCAL-UDP NodePort type service UDP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
-->
&lt;p>è®¾ç½®åç§° æˆå‘˜ ç”¨æ³•
KUBE-CLUSTER-IP æ‰€æœ‰æœåŠ¡ IP + ç«¯å£ masquerade-all=true æˆ– clusterCIDR æŒ‡å®šçš„æƒ…å†µä¸‹è¿›è¡Œä¼ªè£…
KUBE-LOOP-BACK æ‰€æœ‰æœåŠ¡ IP +ç«¯å£+ IP è§£å†³æ•°æ®åŒ…æ¬ºéª—é—®é¢˜
KUBE-EXTERNAL-IP æœåŠ¡å¤–éƒ¨ IP +ç«¯å£ å°†æ•°æ®åŒ…ä¼ªè£…æˆå¤–éƒ¨ IP
KUBE-LOAD-BALANCER è´Ÿè½½å‡è¡¡å™¨å…¥å£ IP +ç«¯å£ å°†æ•°æ®åŒ…ä¼ªè£…æˆ Load Balancer ç±»å‹çš„æœåŠ¡
KUBE-LOAD-BALANCER-LOCAL è´Ÿè½½å‡è¡¡å™¨å…¥å£ IP +ç«¯å£ ä»¥åŠ externalTrafficPolicy=local æ¥å—æ•°æ®åŒ…åˆ° Load Balancer externalTrafficPolicy=local
KUBE-LOAD-BALANCER-FW è´Ÿè½½å‡è¡¡å™¨å…¥å£ IP +ç«¯å£ ä»¥åŠ loadBalancerSourceRanges ä½¿ç”¨æŒ‡å®šçš„ loadBalancerSourceRanges ä¸¢å¼ƒ Load Balancerç±»å‹Serviceçš„æ•°æ®åŒ…
KUBE-LOAD-BALANCER-SOURCE-CIDR è´Ÿè½½å‡è¡¡å™¨å…¥å£ IP +ç«¯å£ + æº CIDR æ¥å— Load Balancer ç±»å‹ Service çš„æ•°æ®åŒ…ï¼Œå¹¶æŒ‡å®šloadBalancerSourceRanges
KUBE-NODE-PORT-TCP NodePort ç±»å‹æœåŠ¡ TCP å°†æ•°æ®åŒ…ä¼ªè£…æˆ NodePortï¼ˆTCPï¼‰
KUBE-NODE-PORT-LOCAL-TCP NodePort ç±»å‹æœåŠ¡ TCP ç«¯å£ï¼Œå¸¦æœ‰ externalTrafficPolicy=local æ¥å—æ•°æ®åŒ…åˆ° NodePort æœåŠ¡ ä½¿ç”¨ externalTrafficPolicy=local
KUBE-NODE-PORT-UDP NodePort ç±»å‹æœåŠ¡ UDP ç«¯å£ å°†æ•°æ®åŒ…ä¼ªè£…æˆ NodePort(UDP)
KUBE-NODE-PORT-LOCAL-UDP NodePort ç±»å‹æœåŠ¡ UDP ç«¯å£ ä½¿ç”¨ externalTrafficPolicy=local æ¥å—æ•°æ®åŒ…åˆ°NodePortæœåŠ¡ ä½¿ç”¨ externalTrafficPolicy=local&lt;/p>
&lt;!--
In general, for IPVS proxier, the number of iptables rules is static, no matter how many Services/Pods we have.
-->
&lt;p>é€šå¸¸ï¼Œå¯¹äº IPVS proxierï¼Œæ— è®ºæˆ‘ä»¬æœ‰å¤šå°‘ Service/ Podï¼Œiptables è§„åˆ™çš„æ•°é‡éƒ½æ˜¯é™æ€çš„ã€‚&lt;/p>
&lt;!--
Run kube-proxy in IPVS Mode
Currently, local-up scripts, GCE scripts, and kubeadm support switching IPVS proxy mode via exporting environment variables (KUBE_PROXY_MODE=ipvs) or specifying flag (--proxy-mode=ipvs). Before running IPVS proxier, please ensure IPVS required kernel modules are already installed.
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
Finally, for Kubernetes v1.10, feature gate SupportIPVSProxyMode is set to true by default. For Kubernetes v1.11, the feature gate is entirely removed. However, you need to enable --feature-gates=SupportIPVSProxyMode=true explicitly for Kubernetes before v1.10.
-->
&lt;p>åœ¨ IPVS æ¨¡å¼ä¸‹è¿è¡Œ kube-proxy&lt;/p>
&lt;p>ç›®å‰ï¼Œæœ¬åœ°è„šæœ¬ï¼ŒGCE è„šæœ¬å’Œ kubeadm æ”¯æŒé€šè¿‡å¯¼å‡ºç¯å¢ƒå˜é‡ï¼ˆKUBE_PROXY_MODE=ipvsï¼‰æˆ–æŒ‡å®šæ ‡å¿—ï¼ˆ--proxy-mode=ipvsï¼‰æ¥åˆ‡æ¢ IPVS ä»£ç†æ¨¡å¼ã€‚ åœ¨è¿è¡ŒIPVS ä»£ç†ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… IPVS æ‰€éœ€çš„å†…æ ¸æ¨¡å—ã€‚&lt;/p>
&lt;pre>&lt;code>ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
&lt;/code>&lt;/pre>
&lt;p>æœ€åï¼Œå¯¹äº Kubernetes v1.10ï¼Œâ€œSupportIPVSProxyModeâ€ é»˜è®¤è®¾ç½®ä¸º â€œtrueâ€ã€‚ å¯¹äº Kubernetes v1.11 ï¼Œè¯¥é€‰é¡¹å·²å®Œå…¨åˆ é™¤ã€‚ ä½†æ˜¯ï¼Œæ‚¨éœ€è¦åœ¨v1.10ä¹‹å‰ä¸ºKubernetes æ˜ç¡®å¯ç”¨ --feature-gates = SupportIPVSProxyMode = trueã€‚&lt;/p>
&lt;!--
Get Involved
The simplest way to get involved with Kubernetes is by joining one of the many Special Interest Groups (SIGs) that align with your interests. Have something youâ€™d like to broadcast to the Kubernetes community? Share your voice at our weekly community meeting, and through the channels below.
Thank you for your continued feedback and support.
Post questions (or answer questions) on Stack Overflow
Join the community portal for advocates on K8sPort
Follow us on Twitter @Kubernetesio for latest updates
Chat with the community on Slack
Share your Kubernetes story
-->
&lt;p>å‚ä¸å…¶ä¸­&lt;/p>
&lt;p>å‚ä¸ Kubernetes çš„æœ€ç®€å•æ–¹æ³•æ˜¯åŠ å…¥ä¼—å¤š&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">ç‰¹åˆ«å…´è¶£å°ç»„&lt;/a> (SIGï¼‰ä¸­ä¸æ‚¨çš„å…´è¶£ä¸€è‡´çš„å°ç»„ã€‚ ä½ æœ‰ä»€ä¹ˆæƒ³è¦å‘ Kubernetes ç¤¾åŒºå¹¿æ’­çš„å—ï¼Ÿ åœ¨æˆ‘ä»¬çš„æ¯å‘¨&lt;a href="https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting">ç¤¾åŒºä¼šè®®&lt;/a>æˆ–é€šè¿‡ä»¥ä¸‹æ¸ é“åˆ†äº«æ‚¨çš„å£°éŸ³ã€‚&lt;/p>
&lt;p>æ„Ÿè°¢æ‚¨çš„æŒç»­åé¦ˆå’Œæ”¯æŒã€‚
åœ¨&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>ä¸Šå‘å¸ƒé—®é¢˜ï¼ˆæˆ–å›ç­”é—®é¢˜ï¼‰&lt;/p>
&lt;p>åŠ å…¥&lt;a href="http://k8sport.org/">K8sPort&lt;/a>çš„å€¡å¯¼è€…ç¤¾åŒºé—¨æˆ·ç½‘ç«™&lt;/p>
&lt;p>åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ä»¬ &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>è·å–æœ€æ–°æ›´æ–°&lt;/p>
&lt;p>åœ¨&lt;a href="http://slack.k8s.io/">Slack&lt;/a>ä¸Šä¸ç¤¾åŒºèŠå¤©&lt;/p>
&lt;p>åˆ†äº«æ‚¨çš„ Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">æ•…äº‹&lt;/a>&lt;/p></description></item><item><title>Blog: Airflowåœ¨Kubernetesä¸­çš„ä½¿ç”¨ï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰ï¼šä¸€ç§ä¸åŒçš„æ“ä½œå™¨</title><link>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</guid><description>
&lt;!--
Author: Daniel Imberman (Bloomberg LP)
-->
&lt;p>ä½œè€…: Daniel Imberman (Bloomberg LP)&lt;/p>
&lt;!--
## Introduction
As part of Bloomberg's continued commitment to developing the Kubernetes ecosystem, we are excited to announce the Kubernetes Airflow Operator; a mechanism for Apache Airflow, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.
-->
&lt;h2 id="ä»‹ç»">ä»‹ç»&lt;/h2>
&lt;p>ä½œä¸ºBloomberg [ç»§ç»­è‡´åŠ›äºå¼€å‘Kubernetesç”Ÿæ€ç³»ç»Ÿ]çš„ä¸€éƒ¨åˆ†ï¼ˆhttps://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/ï¼‰ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´èƒ½å¤Ÿå®£å¸ƒKubernetes Airflow Operatorçš„å‘å¸ƒ; &lt;a href="https://airflow.apache.org/">Apache Airflow&lt;/a>çš„æœºåˆ¶ï¼Œä¸€ç§æµè¡Œçš„å·¥ä½œæµç¨‹ç¼–æ’æ¡†æ¶ï¼Œä½¿ç”¨Kubernetes APIå¯ä»¥åœ¨æœ¬æœºå¯åŠ¨ä»»æ„çš„Kubernetes Podã€‚&lt;/p>
&lt;!--
## What Is Airflow?
Apache Airflow is one realization of the DevOps philosophy of "Configuration As Code." Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.
-->
&lt;h2 id="ä»€ä¹ˆæ˜¯airflow">ä»€ä¹ˆæ˜¯Airflow?&lt;/h2>
&lt;p>Apache Airflowæ˜¯DevOpsâ€œConfiguration As Codeâ€ç†å¿µçš„ä¸€ç§å®ç°ã€‚ Airflowå…è®¸ç”¨æˆ·ä½¿ç”¨ç®€å•çš„Pythonå¯¹è±¡DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰å¯åŠ¨å¤šæ­¥éª¤æµæ°´çº¿ã€‚ æ‚¨å¯ä»¥åœ¨æ˜“äºé˜…è¯»çš„UIä¸­å®šä¹‰ä¾èµ–å…³ç³»ï¼Œä»¥ç¼–ç¨‹æ–¹å¼æ„å»ºå¤æ‚çš„å·¥ä½œæµï¼Œå¹¶ç›‘è§†è°ƒåº¦çš„ä½œä¸šã€‚&lt;/p>
&lt;p>&amp;lt;img src =â€œ/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow_dags.pngâ€width =â€œ85ï¼…â€alt =â€œAirflow DAGsâ€/&amp;gt;&lt;/p>
&lt;p>&amp;lt;img src =â€œ/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow.pngâ€width =â€œ85ï¼…â€alt =â€œAirflow UIâ€/&amp;gt;&lt;/p>
&lt;!--
## Why Airflow on Kubernetes?
Since its inception, Airflow's greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.
To address this issue, we've utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an "any job you want" workflow orchestrator.
-->
&lt;h2 id="ä¸ºä»€ä¹ˆåœ¨kubernetesä¸Šä½¿ç”¨airflow">ä¸ºä»€ä¹ˆåœ¨Kubernetesä¸Šä½¿ç”¨Airflow?&lt;/h2>
&lt;p>è‡ªæˆç«‹ä»¥æ¥ï¼ŒAirflowçš„æœ€å¤§ä¼˜åŠ¿åœ¨äºå…¶çµæ´»æ€§ã€‚ Airflowæä¾›å¹¿æ³›çš„æœåŠ¡é›†æˆï¼ŒåŒ…æ‹¬Sparkå’ŒHBaseï¼Œä»¥åŠå„ç§äº‘æä¾›å•†çš„æœåŠ¡ã€‚ Airflowè¿˜é€šè¿‡å…¶æ’ä»¶æ¡†æ¶æä¾›è½»æ¾çš„å¯æ‰©å±•æ€§ã€‚ä½†æ˜¯ï¼Œè¯¥é¡¹ç›®çš„ä¸€ä¸ªé™åˆ¶æ˜¯Airflowç”¨æˆ·ä»…é™äºæ‰§è¡Œæ—¶Airflowç«™ç‚¹ä¸Šå­˜åœ¨çš„æ¡†æ¶å’Œå®¢æˆ·ç«¯ã€‚å•ä¸ªç»„ç»‡å¯ä»¥æ‹¥æœ‰å„ç§Airflowå·¥ä½œæµç¨‹ï¼ŒèŒƒå›´ä»æ•°æ®ç§‘å­¦æµåˆ°åº”ç”¨ç¨‹åºéƒ¨ç½²ã€‚ç”¨ä¾‹ä¸­çš„è¿™ç§å·®å¼‚ä¼šåœ¨ä¾èµ–å…³ç³»ç®¡ç†ä¸­äº§ç”Ÿé—®é¢˜ï¼Œå› ä¸ºä¸¤ä¸ªå›¢é˜Ÿå¯èƒ½ä¼šåœ¨å…¶å·¥ä½œæµç¨‹ä½¿ç”¨æˆªç„¶ä¸åŒçš„åº“ã€‚&lt;/p>
&lt;p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿Kuberneteså…è®¸ç”¨æˆ·å¯åŠ¨ä»»æ„Kubernetes podå’Œé…ç½®ã€‚ Airflowç”¨æˆ·ç°åœ¨å¯ä»¥åœ¨å…¶è¿è¡Œæ—¶ç¯å¢ƒï¼Œèµ„æºå’Œæœºå¯†ä¸Šæ‹¥æœ‰å…¨éƒ¨æƒé™ï¼ŒåŸºæœ¬ä¸Šå°†Airflowè½¬å˜ä¸ºâ€œæ‚¨æƒ³è¦çš„ä»»ä½•å·¥ä½œâ€å·¥ä½œæµç¨‹åè°ƒå™¨ã€‚&lt;/p>
&lt;!--
## The Kubernetes Operator
Before we move any further, we should clarify that an Operator in Airflow is a task definition. When a user creates a DAG, they would use an operator like the "SparkSubmitOperator" or the "PythonOperator" to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.
Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:
-->
&lt;h2 id="kubernetesè¿è¥å•†">Kubernetesè¿è¥å•†&lt;/h2>
&lt;p>åœ¨è¿›ä¸€æ­¥è®¨è®ºä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥æ¾„æ¸…Airflowä¸­çš„&lt;a href="https://airflow.apache.org/concepts.html#operators">Operator&lt;/a>æ˜¯ä¸€ä¸ªä»»åŠ¡å®šä¹‰ã€‚ å½“ç”¨æˆ·åˆ›å»ºDAGæ—¶ï¼Œä»–ä»¬å°†ä½¿ç”¨åƒâ€œSparkSubmitOperatorâ€æˆ–â€œPythonOperatorâ€è¿™æ ·çš„operatoråˆ†åˆ«æäº¤/ç›‘è§†Sparkä½œä¸šæˆ–Pythonå‡½æ•°ã€‚ Airflowé™„å¸¦äº†Apache Sparkï¼ŒBigQueryï¼ŒHiveå’ŒEMRç­‰æ¡†æ¶çš„å†…ç½®è¿ç®—ç¬¦ã€‚ å®ƒè¿˜æä¾›äº†ä¸€ä¸ªæ’ä»¶å…¥å£ç‚¹ï¼Œå…è®¸DevOpså·¥ç¨‹å¸ˆå¼€å‘è‡ªå·±çš„è¿æ¥å™¨ã€‚&lt;/p>
&lt;p>Airflowç”¨æˆ·ä¸€ç›´åœ¨å¯»æ‰¾æ›´æ˜“äºç®¡ç†éƒ¨ç½²å’ŒETLæµçš„æ–¹æ³•ã€‚ åœ¨å¢åŠ ç›‘æ§çš„åŒæ—¶ï¼Œä»»ä½•è§£è€¦æµç¨‹çš„æœºä¼šéƒ½å¯ä»¥å‡å°‘æœªæ¥çš„åœæœºç­‰é—®é¢˜ã€‚ ä»¥ä¸‹æ˜¯Airflow Kubernetes Operatoræä¾›çš„å¥½å¤„ï¼š&lt;/p>
&lt;!--
* Increased flexibility for deployments:
Airflow's plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.
-->
&lt;ul>
&lt;li>æé«˜éƒ¨ç½²çµæ´»æ€§ï¼š&lt;/li>
&lt;/ul>
&lt;p>Airflowçš„æ’ä»¶APIä¸€ç›´ä¸ºå¸Œæœ›åœ¨å…¶DAGä¸­æµ‹è¯•æ–°åŠŸèƒ½çš„å·¥ç¨‹å¸ˆæä¾›äº†é‡è¦çš„ç¦åˆ©ã€‚ ä¸åˆ©çš„ä¸€é¢æ˜¯ï¼Œæ¯å½“å¼€å‘äººå‘˜æƒ³è¦åˆ›å»ºä¸€ä¸ªæ–°çš„operatoræ—¶ï¼Œä»–ä»¬å°±å¿…é¡»å¼€å‘ä¸€ä¸ªå…¨æ–°çš„æ’ä»¶ã€‚ ç°åœ¨ï¼Œä»»ä½•å¯ä»¥åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œçš„ä»»åŠ¡éƒ½å¯ä»¥é€šè¿‡å®Œå…¨ç›¸åŒçš„è¿ç®—ç¬¦è®¿é—®ï¼Œè€Œæ— éœ€ç»´æŠ¤é¢å¤–çš„Airflowä»£ç ã€‚&lt;/p>
&lt;!--
* Flexibility of configurations and dependencies:
For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires SciPy and another that requires NumPy, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.
-->
&lt;ul>
&lt;li>é…ç½®å’Œä¾èµ–çš„çµæ´»æ€§ï¼š&lt;/li>
&lt;/ul>
&lt;p>å¯¹äºåœ¨é™æ€Airflowå·¥ä½œç¨‹åºä¸­è¿è¡Œçš„operatorï¼Œä¾èµ–å…³ç³»ç®¡ç†å¯èƒ½å˜å¾—éå¸¸å›°éš¾ã€‚ å¦‚æœå¼€å‘äººå‘˜æƒ³è¦è¿è¡Œä¸€ä¸ªéœ€è¦&lt;a href="https://www.scipy.org">SciPy&lt;/a>çš„ä»»åŠ¡å’Œå¦ä¸€ä¸ªéœ€è¦&lt;a href="http://www.numpy.org">NumPy&lt;/a>çš„ä»»åŠ¡ï¼Œå¼€å‘äººå‘˜å¿…é¡»ç»´æŠ¤æ‰€æœ‰AirflowèŠ‚ç‚¹ä¸­çš„ä¾èµ–å…³ç³»æˆ–å°†ä»»åŠ¡å¸è½½åˆ°å…¶ä»–è®¡ç®—æœºï¼ˆå¦‚æœå¤–éƒ¨è®¡ç®—æœºä»¥æœªè·Ÿè¸ªçš„æ–¹å¼æ›´æ”¹ï¼Œåˆ™å¯èƒ½å¯¼è‡´é”™è¯¯ï¼‰ã€‚ è‡ªå®šä¹‰Dockeré•œåƒå…è®¸ç”¨æˆ·ç¡®ä¿ä»»åŠ¡ç¯å¢ƒï¼Œé…ç½®å’Œä¾èµ–å…³ç³»å®Œå…¨æ˜¯å¹‚ç­‰çš„ã€‚&lt;/p>
&lt;!--
* Usage of kubernetes secrets for added security:
Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.
-->
&lt;ul>
&lt;li>ä½¿ç”¨kubernetes Secretä»¥å¢åŠ å®‰å…¨æ€§ï¼š&lt;/li>
&lt;/ul>
&lt;p>å¤„ç†æ•æ„Ÿæ•°æ®æ˜¯ä»»ä½•å¼€å‘å·¥ç¨‹å¸ˆçš„æ ¸å¿ƒèŒè´£ã€‚ Airflowç”¨æˆ·æ€»æœ‰æœºä¼šåœ¨ä¸¥æ ¼æ¡æ¬¾çš„åŸºç¡€ä¸Šéš”ç¦»ä»»ä½•APIå¯†é’¥ï¼Œæ•°æ®åº“å¯†ç å’Œç™»å½•å‡­æ®ã€‚ ä½¿ç”¨Kubernetesè¿ç®—ç¬¦ï¼Œç”¨æˆ·å¯ä»¥åˆ©ç”¨Kubernetes VaultæŠ€æœ¯å­˜å‚¨æ‰€æœ‰æ•æ„Ÿæ•°æ®ã€‚ è¿™æ„å‘³ç€Airflowå·¥ä½œäººå‘˜å°†æ°¸è¿œæ— æ³•è®¿é—®æ­¤ä¿¡æ¯ï¼Œå¹¶ä¸”å¯ä»¥å®¹æ˜“åœ°è¯·æ±‚ä»…ä½¿ç”¨ä»–ä»¬éœ€è¦çš„å¯†ç ä¿¡æ¯æ„å»ºpodã€‚&lt;/p>
&lt;!--
# Architecture
The Kubernetes Operator uses the Kubernetes Python Client to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you've defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.
-->
&lt;p>ï¼ƒæ¶æ„&lt;/p>
&lt;p>&amp;lt;img src =â€œ/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow-architecture.pngâ€width =â€œ85ï¼…â€alt =â€œAirflow Architectureâ€/&amp;gt;&lt;/p>
&lt;p>Kubernetes Operatorä½¿ç”¨&lt;a href="https://github.com/kubernetes-client/Python">Kubernetes Pythonå®¢æˆ·ç«¯&lt;/a>ç”Ÿæˆç”±APIServerå¤„ç†çš„è¯·æ±‚ï¼ˆ1ï¼‰ã€‚ ç„¶åï¼ŒKuberneteså°†ä½¿ç”¨æ‚¨å®šä¹‰çš„éœ€æ±‚å¯åŠ¨æ‚¨çš„podï¼ˆ2ï¼‰ã€‚æ˜ åƒæ–‡ä»¶ä¸­å°†åŠ è½½ç¯å¢ƒå˜é‡ï¼ŒSecretå’Œä¾èµ–é¡¹ï¼Œæ‰§è¡Œå•ä¸ªå‘½ä»¤ã€‚ ä¸€æ—¦å¯åŠ¨ä½œä¸šï¼Œoperatoråªéœ€è¦ç›‘è§†è·Ÿè¸ªæ—¥å¿—çš„çŠ¶å†µï¼ˆ3ï¼‰ã€‚ ç”¨æˆ·å¯ä»¥é€‰æ‹©å°†æ—¥å¿—æœ¬åœ°æ”¶é›†åˆ°è°ƒåº¦ç¨‹åºæˆ–å½“å‰ä½äºå…¶Kubernetesé›†ç¾¤ä¸­çš„ä»»ä½•åˆ†å¸ƒå¼æ—¥å¿—è®°å½•æœåŠ¡ã€‚&lt;/p>
&lt;!--
# Using the Kubernetes Operator
## A Basic Example
The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the passing-task pod should complete, while the failing-task pod returns a failure to the Airflow webserver.
-->
&lt;p>ï¼ƒä½¿ç”¨Kubernetes Operator&lt;/p>
&lt;p>##ä¸€ä¸ªåŸºæœ¬çš„ä¾‹å­&lt;/p>
&lt;p>ä»¥ä¸‹DAGå¯èƒ½æ˜¯æˆ‘ä»¬å¯ä»¥ç¼–å†™çš„æœ€ç®€å•çš„ç¤ºä¾‹ï¼Œä»¥æ˜¾ç¤ºKubernetes Operatorçš„å·¥ä½œåŸç†ã€‚ è¿™ä¸ªDAGåœ¨Kubernetesä¸Šåˆ›å»ºäº†ä¸¤ä¸ªpodï¼šä¸€ä¸ªå¸¦æœ‰Pythonçš„Linuxå‘è¡Œç‰ˆå’Œä¸€ä¸ªæ²¡æœ‰å®ƒçš„åŸºæœ¬Ubuntuå‘è¡Œç‰ˆã€‚ Python podå°†æ­£ç¡®è¿è¡ŒPythonè¯·æ±‚ï¼Œè€Œæ²¡æœ‰Pythonçš„é‚£ä¸ªå°†å‘ç”¨æˆ·æŠ¥å‘Šå¤±è´¥ã€‚ å¦‚æœOperatoræ­£å¸¸å·¥ä½œï¼Œåˆ™åº”è¯¥å®Œæˆâ€œpassing-taskâ€podï¼Œè€Œâ€œfalling-taskâ€podåˆ™å‘Airflowç½‘ç»œæœåŠ¡å™¨è¿”å›å¤±è´¥ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> DAG
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">datetime&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> datetime, timedelta
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow.contrib.operators.kubernetes_pod_operator&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> KubernetesPodOperator
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow.operators.dummy_operator&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> DummyOperator
default_args &lt;span style="color:#666">=&lt;/span> {
&lt;span style="color:#b44">&amp;#39;owner&amp;#39;&lt;/span>: &lt;span style="color:#b44">&amp;#39;airflow&amp;#39;&lt;/span>,
&lt;span style="color:#b44">&amp;#39;depends_on_past&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;start_date&amp;#39;&lt;/span>: datetime&lt;span style="color:#666">.&lt;/span>utcnow(),
&lt;span style="color:#b44">&amp;#39;email&amp;#39;&lt;/span>: [&lt;span style="color:#b44">&amp;#39;airflow@example.com&amp;#39;&lt;/span>],
&lt;span style="color:#b44">&amp;#39;email_on_failure&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;email_on_retry&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;retries&amp;#39;&lt;/span>: &lt;span style="color:#666">1&lt;/span>,
&lt;span style="color:#b44">&amp;#39;retry_delay&amp;#39;&lt;/span>: timedelta(minutes&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">5&lt;/span>)
}
dag &lt;span style="color:#666">=&lt;/span> DAG(
&lt;span style="color:#b44">&amp;#39;kubernetes_sample&amp;#39;&lt;/span>, default_args&lt;span style="color:#666">=&lt;/span>default_args, schedule_interval&lt;span style="color:#666">=&lt;/span>timedelta(minutes&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">10&lt;/span>))
start &lt;span style="color:#666">=&lt;/span> DummyOperator(task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;run_this_first&amp;#39;&lt;/span>, dag&lt;span style="color:#666">=&lt;/span>dag)
passing &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;Python:3.6&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
labels&lt;span style="color:#666">=&lt;/span>{&lt;span style="color:#b44">&amp;#34;foo&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;bar&amp;#34;&lt;/span>},
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;passing-test&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;passing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
failing &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;ubuntu:1604&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
labels&lt;span style="color:#666">=&lt;/span>{&lt;span style="color:#b44">&amp;#34;foo&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;bar&amp;#34;&lt;/span>},
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;fail&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;failing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
passing&lt;span style="color:#666">.&lt;/span>set_upstream(start)
failing&lt;span style="color:#666">.&lt;/span>set_upstream(start)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## But how does this relate to my workflow?
While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.
### 1: PR in github
Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR'ing your code, and merge to the master branch to trigger an automated CI build.
### 2: CI/CD via Jenkins -> Docker Image
Generate your Docker images and bump release version within your Jenkins build.
### 3: Airflow launches task
Finally, update your DAGs to reflect the new release version and you should be ready to go!
-->
&lt;p>##ä½†è¿™ä¸æˆ‘çš„å·¥ä½œæµç¨‹æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ&lt;/p>
&lt;p>è™½ç„¶è¿™ä¸ªä¾‹å­åªä½¿ç”¨åŸºæœ¬æ˜ åƒï¼Œä½†Dockerçš„ç¥å¥‡ä¹‹å¤„åœ¨äºï¼Œè¿™ä¸ªç›¸åŒçš„DAGå¯ä»¥ç”¨äºæ‚¨æƒ³è¦çš„ä»»ä½•å›¾åƒ/å‘½ä»¤é…å¯¹ã€‚ ä»¥ä¸‹æ˜¯æ¨èçš„CI / CDç®¡é“ï¼Œç”¨äºåœ¨Airflow DAGä¸Šè¿è¡Œç”Ÿäº§å°±ç»ªä»£ç ã€‚&lt;/p>
&lt;h3 id="1-githubä¸­çš„pr">1ï¼šgithubä¸­çš„PR&lt;/h3>
&lt;p>ä½¿ç”¨Travisæˆ–Jenkinsè¿è¡Œå•å…ƒå’Œé›†æˆæµ‹è¯•ï¼Œè¯·æ‚¨çš„æœ‹å‹PRæ‚¨çš„ä»£ç ï¼Œå¹¶åˆå¹¶åˆ°ä¸»åˆ†æ”¯ä»¥è§¦å‘è‡ªåŠ¨CIæ„å»ºã€‚&lt;/p>
&lt;h3 id="2-ci-cdæ„å»ºjenkins-docker-image">2ï¼šCI / CDæ„å»ºJenkins - &amp;gt; Docker Image&lt;/h3>
&lt;p>&lt;a href="https://getintodevops.com/blog/building-your-first-Docker-image-with-jenkins-2-guide-for-developers">åœ¨Jenkinsæ„å»ºä¸­ç”ŸæˆDockeré•œåƒå’Œç¼“å†²ç‰ˆæœ¬&lt;/a>ã€‚&lt;/p>
&lt;h3 id="3-airflowå¯åŠ¨ä»»åŠ¡">3ï¼šAirflowå¯åŠ¨ä»»åŠ¡&lt;/h3>
&lt;p>æœ€åï¼Œæ›´æ–°æ‚¨çš„DAGä»¥åæ˜ æ–°ç‰ˆæœ¬ï¼Œæ‚¨åº”è¯¥å‡†å¤‡å¥½äº†ï¼&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">
production_task &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
&lt;span style="color:#080;font-style:italic"># image=&amp;#34;my-production-job:release-1.0.1&amp;#34;, &amp;lt;-- old release&lt;/span>
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;my-production-job:release-1.0.2&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;fail&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;failing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
# Launching a test deployment
Since the Kubernetes Operator is not yet released, we haven't released an official helm chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:
## Step 1: Set your kubeconfig to point to a kubernetes cluster
## Step 2: Clone the Airflow Repo:
Run git clone https://github.com/apache/incubator-airflow.git to clone the official Airflow repo.
## Step 3: Run
To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:
-->
&lt;p>ï¼ƒå¯åŠ¨æµ‹è¯•éƒ¨ç½²&lt;/p>
&lt;p>ç”±äºKubernetesè¿è¥å•†å°šæœªå‘å¸ƒï¼Œæˆ‘ä»¬å°šæœªå‘å¸ƒå®˜æ–¹&lt;a href="https://helm.sh/">helm&lt;/a> å›¾è¡¨æˆ–operatorï¼ˆä½†ä¸¤è€…ç›®å‰éƒ½åœ¨è¿›è¡Œä¸­ï¼‰ã€‚ ä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢åˆ—å‡ºäº†åŸºæœ¬éƒ¨ç½²çš„è¯´æ˜ï¼Œå¹¶ä¸”æ­£åœ¨ç§¯æå¯»æ‰¾æµ‹è¯•äººå‘˜æ¥å°è¯•è¿™ä¸€æ–°åŠŸèƒ½ã€‚ è¦è¯•ç”¨æ­¤ç³»ç»Ÿï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š&lt;/p>
&lt;p>##æ­¥éª¤1ï¼šå°†kubeconfigè®¾ç½®ä¸ºæŒ‡å‘kubernetesé›†ç¾¤&lt;/p>
&lt;p>##æ­¥éª¤2ï¼šclone Airflow ä»“åº“ï¼š&lt;/p>
&lt;p>è¿è¡Œgit clone httpsï¼š// github.com / apache / incubator-airflow.gitæ¥cloneå®˜æ–¹Airflowä»“åº“ã€‚&lt;/p>
&lt;p>##æ­¥éª¤3ï¼šè¿è¡Œ&lt;/p>
&lt;p>ä¸ºäº†è¿è¡Œè¿™ä¸ªåŸºæœ¬Deploymentï¼Œæˆ‘ä»¬æ­£åœ¨é€‰æ‹©æˆ‘ä»¬ç›®å‰ç”¨äºKubernetes Executorçš„é›†æˆæµ‹è¯•è„šæœ¬ï¼ˆå°†åœ¨æœ¬ç³»åˆ—çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­å¯¹æ­¤è¿›è¡Œè§£é‡Šï¼‰ã€‚ è¦å¯åŠ¨æ­¤éƒ¨ç½²ï¼Œè¯·è¿è¡Œä»¥ä¸‹ä¸‰ä¸ªå‘½ä»¤ï¼š&lt;/p>
&lt;pre>&lt;code>
sed -ie &amp;quot;s/KubernetesExecutor/LocalExecutor/g&amp;quot; scripts/ci/kubernetes/kube/configmaps.yaml
./scripts/ci/kubernetes/Docker/build.sh
./scripts/ci/kubernetes/kube/deploy.sh
&lt;/code>&lt;/pre>&lt;!--
Before we move on, let's discuss what these commands are doing:
### sed -ie "s/KubernetesExecutor/LocalExecutor/g" scripts/ci/kubernetes/kube/configmaps.yaml
The Kubernetes Executor is another Airflow feature that allows for dynamic allocation of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.
### ./scripts/ci/kubernetes/Docker/build.sh
This script will tar the Airflow master source code build a Docker container based on the Airflow distribution
### ./scripts/ci/kubernetes/kube/deploy.sh
Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml
## Step 4: Log into your webserver
Now that your Airflow instance is running let's take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run
-->
&lt;p>åœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬è®¨è®ºè¿™äº›å‘½ä»¤æ­£åœ¨åšä»€ä¹ˆï¼š&lt;/p>
&lt;h3 id="sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml">sed -ieâ€œs / KubernetesExecutor / LocalExecutor / gâ€scripts / ci / kubernetes / kube / configmaps.yaml&lt;/h3>
&lt;p>Kubernetes Executoræ˜¯å¦ä¸€ç§AirflowåŠŸèƒ½ï¼Œå…è®¸åŠ¨æ€åˆ†é…ä»»åŠ¡å·²è§£å†³å¹‚ç­‰podçš„é—®é¢˜ã€‚æˆ‘ä»¬å°†å…¶åˆ‡æ¢åˆ°LocalExecutorçš„åŸå› åªæ˜¯ä¸€æ¬¡å¼•å…¥ä¸€ä¸ªåŠŸèƒ½ã€‚å¦‚æœæ‚¨æƒ³å°è¯•Kubernetes Executorï¼Œæ¬¢è¿æ‚¨è·³è¿‡æ­¤æ­¥éª¤ï¼Œä½†æˆ‘ä»¬å°†åœ¨ä»¥åçš„æ–‡ç« ä¸­è¯¦ç»†ä»‹ç»ã€‚&lt;/p>
&lt;h3 id="scripts-ci-kubernetes-docker-build-sh">./scripts/ci/kubernetes/Docker/build.sh&lt;/h3>
&lt;p>æ­¤è„šæœ¬å°†å¯¹Airflowä¸»åˆ†æ”¯ä»£ç è¿›è¡Œæ‰“åŒ…ï¼Œä»¥æ ¹æ®Airflowçš„å‘è¡Œæ–‡ä»¶æ„å»ºDockerå®¹å™¨&lt;/p>
&lt;h3 id="scripts-ci-kubernetes-kube-deploy-sh">./scripts/ci/kubernetes/kube/deploy.sh&lt;/h3>
&lt;p>æœ€åï¼Œæˆ‘ä»¬åœ¨æ‚¨çš„ç¾¤é›†ä¸Šåˆ›å»ºå®Œæ•´çš„Airflowéƒ¨ç½²ã€‚è¿™åŒ…æ‹¬Airflowé…ç½®ï¼Œpostgresåç«¯ï¼Œwebserver +è°ƒåº¦ç¨‹åºä»¥åŠä¹‹é—´çš„æ‰€æœ‰å¿…è¦æœåŠ¡ã€‚éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæä¾›çš„è§’è‰²ç»‘å®šæ˜¯é›†ç¾¤ç®¡ç†å‘˜ï¼Œå› æ­¤å¦‚æœæ‚¨æ²¡æœ‰è¯¥é›†ç¾¤çš„æƒé™çº§åˆ«ï¼Œå¯ä»¥åœ¨scripts / ci / kubernetes / kube / airflow.yamlä¸­è¿›è¡Œä¿®æ”¹ã€‚&lt;/p>
&lt;p>##æ­¥éª¤4ï¼šç™»å½•æ‚¨çš„ç½‘ç»œæœåŠ¡å™¨&lt;/p>
&lt;p>ç°åœ¨æ‚¨çš„Airflowå®ä¾‹æ­£åœ¨è¿è¡Œï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹UIï¼ç”¨æˆ·ç•Œé¢ä½äºAirflow podçš„8080ç«¯å£ï¼Œå› æ­¤åªéœ€è¿è¡Œå³å¯&lt;/p>
&lt;pre>&lt;code>
WEB=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}' | grep &amp;quot;airflow&amp;quot; | head -1)
kubectl port-forward $WEB 8080:8080
&lt;/code>&lt;/pre>&lt;!--
Now the Airflow UI will exist on http://localhost:8080. To log in simply enter airflow/airflow and you should have full access to the Airflow web UI.
## Step 5: Upload a test document
To modify/add your own DAGs, you can use kubectl cp to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:
-->
&lt;p>ç°åœ¨ï¼ŒAirflow UIå°†å­˜åœ¨äºhttp://localhost:8080ä¸Šã€‚ è¦ç™»å½•ï¼Œåªéœ€è¾“å…¥airflow /airflowï¼Œæ‚¨å°±å¯ä»¥å®Œå…¨è®¿é—®Airflow Web UIã€‚&lt;/p>
&lt;p>##æ­¥éª¤5ï¼šä¸Šä¼ æµ‹è¯•æ–‡æ¡£&lt;/p>
&lt;p>è¦ä¿®æ”¹/æ·»åŠ è‡ªå·±çš„DAGï¼Œå¯ä»¥ä½¿ç”¨kubectl cpå°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ°Airflowè°ƒåº¦ç¨‹åºçš„DAGæ–‡ä»¶å¤¹ä¸­ã€‚ ç„¶åï¼ŒAirflowå°†è¯»å–æ–°çš„DAGå¹¶è‡ªåŠ¨å°†å…¶ä¸Šä¼ åˆ°å…¶ç³»ç»Ÿã€‚ ä»¥ä¸‹å‘½ä»¤å°†ä»»ä½•æœ¬åœ°æ–‡ä»¶ä¸Šè½½åˆ°æ­£ç¡®çš„ç›®å½•ä¸­ï¼š&lt;/p>
&lt;p>kubectl cp &lt;local file> &lt;namespace>/&lt;pod>:/root/airflow/dags -c scheduler&lt;/p>
&lt;!--
## Step 6: Enjoy!
# So when will I be able to use this?
While this feature is still in the early stages, we hope to see it released for wide release in the next few months.
# Get Involved
This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the 1.10 release branch of Airflow (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributors can have a huge influence on the future of these features.
For those interested in joining these efforts, I'd recommend checkint out these steps:
* Join the airflow-dev mailing list at dev@airflow.apache.org.
* File an issue in Apache Airflow JIRA
* Join our SIG-BigData meetings on Wednesdays at 10am PST.
* Reach us on slack at #sig-big-data on kubernetes.slack.com
Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.
-->
&lt;p>##æ­¥éª¤6ï¼šä½¿ç”¨å®ƒï¼&lt;/p>
&lt;p>#é‚£ä¹ˆæˆ‘ä»€ä¹ˆæ—¶å€™å¯ä»¥ä½¿ç”¨å®ƒï¼Ÿ&lt;/p>
&lt;p>è™½ç„¶æ­¤åŠŸèƒ½ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œä½†æˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥å‡ ä¸ªæœˆå†…å‘å¸ƒè¯¥åŠŸèƒ½ä»¥è¿›è¡Œå¹¿æ³›å‘å¸ƒã€‚&lt;/p>
&lt;p>#å‚ä¸å…¶ä¸­&lt;/p>
&lt;p>æ­¤åŠŸèƒ½åªæ˜¯å°†Apache Airflowé›†æˆåˆ°Kubernetesä¸­çš„å¤šé¡¹ä¸»è¦å·¥ä½œçš„å¼€å§‹ã€‚ Kubernetes Operatorå·²åˆå¹¶åˆ°&lt;a href="https://github.com/apache/incubator-airflow/tree/v1-10-test">Airflowçš„1.10å‘å¸ƒåˆ†æ”¯&lt;/a>ï¼ˆå®éªŒæ¨¡å¼ä¸­çš„æ‰§è¡Œæ¨¡å—ï¼‰ï¼Œä»¥åŠå®Œæ•´çš„k8sæœ¬åœ°è°ƒåº¦ç¨‹åºç§°ä¸ºKubernetes Executorï¼ˆå³å°†å‘å¸ƒæ–‡ç« ï¼‰ã€‚è¿™äº›åŠŸèƒ½ä»å¤„äºæ—©æœŸé‡‡ç”¨è€…/è´¡çŒ®è€…å¯èƒ½å¯¹è¿™äº›åŠŸèƒ½çš„æœªæ¥äº§ç”Ÿå·¨å¤§å½±å“çš„é˜¶æ®µã€‚&lt;/p>
&lt;p>å¯¹äºæœ‰å…´è¶£åŠ å…¥è¿™äº›å·¥ä½œçš„äººï¼Œæˆ‘å»ºè®®æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼š&lt;/p>
&lt;p>*åŠ å…¥airflow-devé‚®ä»¶åˆ—è¡¨dev@airflow.apache.orgã€‚&lt;/p>
&lt;p>*åœ¨[Apache Airflow JIRA]ä¸­æå‡ºé—®é¢˜ï¼ˆhttps://issues.apache.org/jira/projects/AIRFLOW/issues/ï¼‰&lt;/p>
&lt;p>*å‘¨ä¸‰ä¸Šåˆ10ç‚¹å¤ªå¹³æ´‹æ ‡å‡†æ—¶é—´åŠ å…¥æˆ‘ä»¬çš„SIG-BigDataä¼šè®®ã€‚&lt;/p>
&lt;p>*åœ¨kubernetes.slack.comä¸Šçš„ï¼ƒsig-big-dataæ‰¾åˆ°æˆ‘ä»¬ã€‚&lt;/p>
&lt;p>ç‰¹åˆ«æ„Ÿè°¢Apache Airflowå’ŒKubernetesç¤¾åŒºï¼Œç‰¹åˆ«æ˜¯Grant Nicholasï¼ŒBen Goldbergï¼ŒAnirudh Ramanathanï¼ŒFokko Dreisprongå’ŒBolke de Bruinï¼Œæ„Ÿè°¢æ‚¨å¯¹è¿™äº›åŠŸèƒ½çš„å·¨å¤§å¸®åŠ©ä»¥åŠæˆ‘ä»¬æœªæ¥çš„åŠªåŠ›ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes å†…çš„åŠ¨æ€ Ingress</title><link>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</link><pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</guid><description>
&lt;!--
title: Dynamic Ingress in Kubernetes
date: 2018-06-07
Author: Richard Li (Datawire)
-->
&lt;p>ä½œè€…: Richard Li (Datawire)&lt;/p>
&lt;!--
Kubernetes makes it easy to deploy applications that consist of many microservices, but one of the key challenges with this type of architecture is dynamically routing ingress traffic to each of these services. One approach is Ambassador, a Kubernetes-native open source API Gateway built on the Envoy Proxy. Ambassador is designed for dynamic environment where services may come and go frequently.
Ambassador is configured using Kubernetes annotations. Annotations are used to configure specific mappings from a given Kubernetes service to a particular URL. A mapping can include a number of annotations for configuring a route. Examples include rate limiting, protocol, cross-origin request sharing, traffic shadowing, and routing rules.
-->
&lt;p>Kubernetes å¯ä»¥è½»æ¾éƒ¨ç½²ç”±è®¸å¤šå¾®æœåŠ¡ç»„æˆçš„åº”ç”¨ç¨‹åºï¼Œä½†è¿™ç§æ¶æ„çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€æ˜¯åŠ¨æ€åœ°å°†æµé‡è·¯ç”±åˆ°è¿™äº›æœåŠ¡ä¸­çš„æ¯ä¸€ä¸ªã€‚
ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ &lt;a href="https://www.getambassador.io">Ambassador&lt;/a>ï¼Œ
ä¸€ä¸ªåŸºäº &lt;a href="https://www.envoyproxy.io">Envoy Proxy&lt;/a> æ„å»ºçš„ Kubernetes åŸç”Ÿå¼€æº API ç½‘å…³ã€‚
Ambassador ä¸“ä¸ºåŠ¨æ€ç¯å¢ƒè€Œè®¾è®¡ï¼Œè¿™ç±»ç¯å¢ƒä¸­çš„æœåŠ¡å¯èƒ½è¢«é¢‘ç¹æ·»åŠ æˆ–åˆ é™¤ã€‚&lt;/p>
&lt;p>Ambassador ä½¿ç”¨ Kubernetes æ³¨è§£è¿›è¡Œé…ç½®ã€‚
æ³¨è§£ç”¨äºé…ç½®ä»ç»™å®š Kubernetes æœåŠ¡åˆ°ç‰¹å®š URL çš„å…·ä½“æ˜ å°„å…³ç³»ã€‚
æ¯ä¸ªæ˜ å°„ä¸­å¯ä»¥åŒ…æ‹¬å¤šä¸ªæ³¨è§£ï¼Œç”¨äºé…ç½®è·¯ç”±ã€‚
æ³¨è§£çš„ä¾‹å­æœ‰é€Ÿç‡é™åˆ¶ã€åè®®ã€è·¨æºè¯·æ±‚å…±äº«ï¼ˆCORSï¼‰ã€æµé‡å½±å°„å’Œè·¯ç”±è§„åˆ™ç­‰ã€‚&lt;/p>
&lt;!--
## A Basic Ambassador Example
Ambassador is typically installed as a Kubernetes deployment, and is also available as a Helm chart. To configure Ambassador, create a Kubernetes service with the Ambassador annotations. Here is an example that configures Ambassador to route requests to /httpbin/ to the public httpbin.org service:
-->
&lt;h2 id="ä¸€ä¸ªç®€å•çš„-ambassador-ç¤ºä¾‹">ä¸€ä¸ªç®€å•çš„ Ambassador ç¤ºä¾‹&lt;/h2>
&lt;p>Ambassador é€šå¸¸ä½œä¸º Kubernetes Deployment æ¥å®‰è£…ï¼Œä¹Ÿå¯ä»¥ä½œä¸º Helm Chart ä½¿ç”¨ã€‚
é…ç½® Ambassador æ—¶ï¼Œè¯·ä½¿ç”¨ Ambassador æ³¨è§£åˆ›å»º Kubernetes æœåŠ¡ã€‚
ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œç”¨æ¥é…ç½® Ambassadorï¼Œå°†é’ˆå¯¹ /httpbin/ çš„è¯·æ±‚è·¯ç”±åˆ°å…¬å…±çš„ httpbin.org æœåŠ¡ï¼š&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Service
metadata:
name: httpbin
annotations:
getambassador.io/config: |
---
apiVersion: ambassador/v0
kind: Mapping
name: httpbin_mapping
prefix: /httpbin/
service: httpbin.org:80
host_rewrite: httpbin.org
spec:
type: ClusterIP
ports:
- port: 80
&lt;/code>&lt;/pre>&lt;!--
A mapping object is created with a prefix of /httpbin/ and a service name of httpbin.org. The host_rewrite annotation specifies that the HTTP host header should be set to httpbin.org.
-->
&lt;p>ä¾‹å­ä¸­åˆ›å»ºäº†ä¸€ä¸ª Mapping å¯¹è±¡ï¼Œå…¶ prefix è®¾ç½®ä¸º /httpbin/ï¼Œservice åç§°ä¸º httpbin.orgã€‚
å…¶ä¸­çš„ host_rewrite æ³¨è§£æŒ‡å®š HTTP çš„ host å¤´éƒ¨å­—æ®µåº”è®¾ç½®ä¸º httpbin.orgã€‚&lt;/p>
&lt;!--
## Kubeflow
Kubeflow provides a simple way to easily deploy machine learning infrastructure on Kubernetes. The Kubeflow team needed a proxy that provided a central point of authentication and routing to the wide range of services used in Kubeflow, many of which are ephemeral in nature.
&lt;center>&lt;i>Kubeflow architecture, pre-Ambassador&lt;/center>&lt;/i>
-->
&lt;h2 id="kubeflow">Kubeflow&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubeflow/kubeflow">Kubeflow&lt;/a> æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œç”¨äºåœ¨ Kubernetes ä¸Šè½»æ¾éƒ¨ç½²æœºå™¨å­¦ä¹ åŸºç¡€è®¾æ–½ã€‚
Kubeflow å›¢é˜Ÿéœ€è¦ä¸€ä¸ªä»£ç†ï¼Œä¸º Kubeflow ä¸­æ‰€ä½¿ç”¨çš„å„ç§æœåŠ¡æä¾›é›†ä¸­åŒ–çš„è®¤è¯å’Œè·¯ç”±èƒ½åŠ›ï¼›Kubeflow ä¸­è®¸å¤šæœåŠ¡æœ¬è´¨ä¸Šéƒ½æ˜¯ç”Ÿå‘½æœŸå¾ˆçŸ­çš„ã€‚&lt;/p>
&lt;center>&lt;i>Kubeflow architecture, pre-Ambassador&lt;/center>&lt;/i>
&lt;!--
## Service configuration
With Ambassador, Kubeflow can use a distributed model for configuration. Instead of a central configuration file, Ambassador allows each service to configure its route in Ambassador via Kubernetes annotations. Here is a simplified example configuration:
-->
&lt;h2 id="æœåŠ¡é…ç½®">æœåŠ¡é…ç½®&lt;/h2>
&lt;p>æœ‰äº† Ambassadorï¼ŒKubeflow å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼æ¨¡å‹è¿›è¡Œé…ç½®ã€‚
Ambassador ä¸ä½¿ç”¨é›†ä¸­çš„é…ç½®æ–‡ä»¶ï¼Œè€Œæ˜¯å…è®¸æ¯ä¸ªæœåŠ¡é€šè¿‡ Kubernetes æ³¨è§£åœ¨ Ambassador ä¸­é…ç½®å…¶è·¯ç”±ã€‚
ä¸‹é¢æ˜¯ä¸€ä¸ªç®€åŒ–çš„é…ç½®ç¤ºä¾‹ï¼š&lt;/p>
&lt;pre>&lt;code>---
apiVersion: ambassador/v0
kind: Mapping
name: tfserving-mapping-test-post
prefix: /models/test/
rewrite: /model/test/:predict
method: POST
service: test.kubeflow:8000
&lt;/code>&lt;/pre>&lt;!--
In this example, the â€œtestâ€ service uses Ambassador annotations to dynamically configure a route to the service, triggered only when the HTTP method is a POST, and the annotation also specifies a rewrite rule.
-->
&lt;p>ç¤ºä¾‹ä¸­ï¼Œâ€œtestâ€ æœåŠ¡ä½¿ç”¨ Ambassador æ³¨è§£æ¥ä¸ºæœåŠ¡åŠ¨æ€é…ç½®è·¯ç”±ã€‚
æ‰€é…ç½®çš„è·¯ç”±ä»…åœ¨ HTTP æ–¹æ³•æ˜¯ POST æ—¶è§¦å‘ï¼›æ³¨è§£ä¸­åŒæ—¶è¿˜ç»™å‡ºäº†ä¸€æ¡é‡å†™è§„åˆ™ã€‚&lt;/p>
&lt;!--
With Ambassador, Kubeflow manages routing easily with Kubernetes annotations. Kubeflow configures a single ingress object that directs traffic to Ambassador, then creates services with Ambassador annotations as needed to direct traffic to specific backends. For example, when deploying TensorFlow services, Kubeflow creates and and annotates a K8s service so that the model will be served at https://&lt;ingress host>/models/&lt;model name>/. Kubeflow can also use the Envoy Proxy to do the actual L7 routing. Using Ambassador, Kubeflow takes advantage of additional routing configuration like URL rewriting and method-based routing.
If youâ€™re interested in using Ambassador with Kubeflow, the standard Kubeflow install automatically installs and configures Ambassador.
If youâ€™re interested in using Ambassador as an API Gateway or Kubernetes ingress solution for your non-Kubeflow services, check out the Getting Started with Ambassador guide.
## Kubeflow and Ambassador
-->
&lt;h2 id="kubeflow-å’Œ-ambassador">Kubeflow å’Œ Ambassador&lt;/h2>
&lt;p>é€šè¿‡ Ambassadorï¼ŒKubeflow å¯ä»¥ä½¿ç”¨ Kubernetes æ³¨è§£è½»æ¾ç®¡ç†è·¯ç”±ã€‚
Kubeflow é…ç½®åŒä¸€ä¸ª Ingress å¯¹è±¡ï¼Œå°†æµé‡å®šå‘åˆ° Ambassadorï¼Œç„¶åæ ¹æ®éœ€è¦åˆ›å»ºå…·æœ‰ Ambassador æ³¨è§£çš„æœåŠ¡ï¼Œä»¥å°†æµé‡å®šå‘åˆ°ç‰¹å®šåç«¯ã€‚
ä¾‹å¦‚ï¼Œåœ¨éƒ¨ç½² TensorFlow æœåŠ¡æ—¶ï¼ŒKubeflow ä¼šåˆ›å»º Kubernetes æœåŠ¡å¹¶ä¸ºå…¶æ·»åŠ æ³¨è§£ï¼Œ
ä»¥ä¾¿ç”¨æˆ·èƒ½å¤Ÿåœ¨ &lt;code>https://&amp;lt;ingressä¸»æœº&amp;gt;/models/&amp;lt;æ¨¡å‹åç§°&amp;gt;/&lt;/code> å¤„è®¿é—®åˆ°æ¨¡å‹æœ¬èº«ã€‚
Kubeflow è¿˜å¯ä»¥ä½¿ç”¨ Envoy Proxy æ¥è¿›è¡Œå®é™…çš„ L7 è·¯ç”±ã€‚
é€šè¿‡ Ambassadorï¼ŒKubeflow èƒ½å¤Ÿæ›´å……åˆ†åœ°åˆ©ç”¨ URL é‡å†™å’ŒåŸºäºæ–¹æ³•çš„è·¯ç”±ç­‰é¢å¤–çš„è·¯ç”±é…ç½®èƒ½åŠ›ã€‚&lt;/p>
&lt;p>å¦‚æœæ‚¨å¯¹åœ¨ Kubeflow ä¸­ä½¿ç”¨ Ambassador æ„Ÿå…´è¶£ï¼Œæ ‡å‡†çš„ Kubeflow å®‰è£…ä¼šè‡ªåŠ¨å®‰è£…å’Œé…ç½® Ambassadorã€‚&lt;/p>
&lt;p>å¦‚æœæ‚¨æœ‰å…´è¶£å°† Ambassador ç”¨ä½œ API ç½‘å…³æˆ– Kubernetes çš„ Ingress è§£å†³æ–¹æ¡ˆï¼Œ
è¯·å‚é˜… &lt;a href="https://www.getambassador.io/user-guide/getting-started">Ambassador å…¥é—¨æŒ‡å—&lt;/a>ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes è¿™å››å¹´</title><link>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</link><pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</guid><description>
&lt;!--
**Author**: Joe Beda (CTO and Founder, Heptio)
On June 6, 2014 I checked in the [first commit](https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56) of what would become the public repository for Kubernetes. Many would assume that is where the story starts. It is the beginning of history, right? But that really doesnâ€™t tell the whole story.
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šJoe Beda( Heptio é¦–å¸­æŠ€æœ¯å®˜å…¼åˆ›å§‹äºº)
2014 å¹´ 6 æœˆ 6 æ—¥ï¼Œæˆ‘æ£€æŸ¥äº† Kubernetes å…¬å…±ä»£ç åº“çš„&lt;a href="https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56">ç¬¬ä¸€æ¬¡ commit&lt;/a> ã€‚è®¸å¤šäººä¼šè®¤ä¸ºè¿™æ˜¯æ•…äº‹å¼€å§‹çš„åœ°æ–¹ã€‚è¿™éš¾é“ä¸æ˜¯ä¸€åˆ‡å¼€å§‹çš„åœ°æ–¹å—ï¼Ÿä½†è¿™çš„ç¡®ä¸èƒ½æŠŠæ•´ä¸ªè¿‡ç¨‹è¯´æ¸…æ¥šã€‚&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-06-06-4-years-of-k8s/k8s-first-commit.png" alt="k8s_first_commit">&lt;/p>
&lt;!--
The cast leading up to that commit was large and the success for Kubernetes since then is owed to an ever larger cast.
Kubernetes was built on ideas that had been proven out at Google over the previous ten years with Borg. And Borg, itself, owed its existence to even earlier efforts at Google and beyond.
Concretely, Kubernetes started as some prototypes from Brendan Burns combined with ongoing work from me and Craig McLuckie to better align the internal Google experience with the Google Cloud experience. Brendan, Craig, and I really wanted people to use this, so we made the case to build out this prototype as an open source project that would bring the best ideas from Borg out into the open.
After we got the nod, it was time to actually build the system. We took Brendanâ€™s prototype (in Java), rewrote it in Go, and built just enough to get the core ideas across. By this time the team had grown to include Ville Aikas, Tim Hockin, Brian Grant, Dawn Chen and Daniel Smith. Once we had something working, someone had to sign up to clean things up to get it ready for public launch. That ended up being me. Not knowing the significance at the time, I created a new repo, moved things over, and checked it in. So while I have the first public commit to the repo, there was work underway well before that.
The version of Kubernetes at that point was really just a shadow of what it was to become. The core concepts were there but it was very raw. For example, Pods were called Tasks. That was changed a day before we went public. All of this led up to the public announcement of Kubernetes on June 10th, 2014 in a keynote from Eric Brewer at the first DockerCon. You can watch that video here:
-->
&lt;p>ç¬¬ä¸€æ¬¡ commit æ¶‰åŠçš„äººå‘˜ä¼—å¤šï¼Œè‡ªé‚£ä»¥å Kubernetes çš„æˆåŠŸå½’åŠŸäºæ›´å¤§çš„å¼€å‘è€…é˜µå®¹ã€‚
Kubernetes å»ºç«‹åœ¨è¿‡å»åå¹´æ›¾ç»åœ¨ Google çš„ Borg é›†ç¾¤ç®¡ç†ç³»ç»Ÿä¸­éªŒè¯è¿‡çš„æ€è·¯ä¹‹ä¸Šã€‚è€Œ Borg æœ¬èº«ä¹Ÿæ˜¯ Google å’Œå…¶ä»–å…¬å¸æ—©æœŸåŠªåŠ›çš„ç»“æœã€‚
å…·ä½“è€Œè¨€ï¼ŒKubernetes æœ€åˆæ˜¯ä» Brendan Burns çš„ä¸€äº›åŸå‹å¼€å§‹ï¼Œç»“åˆæˆ‘å’Œ Craig McLuckie æ­£åœ¨è¿›è¡Œçš„å·¥ä½œï¼Œä»¥æ›´å¥½åœ°å°† Google å†…éƒ¨å®è·µä¸ Google Cloud çš„ç»éªŒç›¸ç»“åˆã€‚ Brendanï¼ŒCraig å’Œæˆ‘çœŸçš„å¸Œæœ›äººä»¬ä½¿ç”¨å®ƒï¼Œæ‰€ä»¥æˆ‘ä»¬å»ºè®®å°†è¿™ä¸ªåŸå‹æ„å»ºä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå°† Borg çš„æœ€ä½³åˆ›æ„å¸¦ç»™å¤§å®¶ã€‚
åœ¨æˆ‘ä»¬æ‰€æœ‰äººåŒæ„åï¼Œå°±å¼€å§‹ç€æ‰‹æ„å»ºè¿™ä¸ªç³»ç»Ÿäº†ã€‚æˆ‘ä»¬é‡‡ç”¨äº† Brendan çš„åŸå‹ï¼ˆJava è¯­è¨€ï¼‰ï¼Œç”¨ Go è¯­è¨€é‡å†™äº†å®ƒï¼Œå¹¶ä¸”ä»¥ä¸Šè¿°æ ¸å¿ƒæ€æƒ³å»æ„å»ºè¯¥ç³»ç»Ÿã€‚åˆ°è¿™ä¸ªæ—¶å€™ï¼Œå›¢é˜Ÿå·²ç»æˆé•¿ä¸ºåŒ…æ‹¬ Ville Aikasï¼ŒTim Hockinï¼ŒBrian Grantï¼ŒDawn Chen å’Œ Daniel Smithã€‚ä¸€æ—¦æˆ‘ä»¬æœ‰äº†ä¸€äº›å·¥ä½œéœ€æ±‚ï¼Œæœ‰äººå¿…é¡»æ‰¿æ‹…ä¸€äº›è„±æ•çš„å·¥ä½œï¼Œä»¥ä¾¿ä¸ºå…¬å¼€å‘å¸ƒåšå¥½å‡†å¤‡ã€‚è¿™ä¸ªè§’è‰²æœ€ç»ˆç”±æˆ‘æ‰¿æ‹…ã€‚å½“æ—¶ï¼Œæˆ‘ä¸çŸ¥é“è¿™ä»¶äº‹æƒ…çš„é‡è¦æ€§ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ä»“åº“ï¼ŒæŠŠä»£ç æ¬è¿‡æ¥ï¼Œç„¶åè¿›è¡Œäº†æ£€æŸ¥ã€‚æ‰€ä»¥åœ¨æˆ‘ç¬¬ä¸€æ¬¡æäº¤ public commit ä¹‹å‰ï¼Œå°±æœ‰å·¥ä½œå·²ç»å¯åŠ¨äº†ã€‚
é‚£æ—¶ Kubernetes çš„ç‰ˆæœ¬åªæ˜¯ç°åœ¨ç‰ˆæœ¬çš„ç®€å•é›å½¢ã€‚æ ¸å¿ƒæ¦‚å¿µå·²ç»æœ‰äº†ï¼Œä½†éå¸¸åŸå§‹ã€‚ä¾‹å¦‚ï¼ŒPods è¢«ç§°ä¸º Tasksï¼Œè¿™åœ¨æˆ‘ä»¬æ¨å¹¿å‰ä¸€å¤©å°±è¢«æ›¿æ¢ã€‚2014å¹´6æœˆ10æ—¥ Eric Brewe åœ¨ç¬¬ä¸€å±Š DockerCon ä¸Šçš„æ¼”è®²ä¸­æ­£å¼å‘å¸ƒäº† Kubernetes ã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„è§‚çœ‹è¯¥è§†é¢‘ï¼š&lt;/p>
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/YrxnVKZeqK8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
&lt;!--
But, however raw, that modest start was enough to pique the interest of a community that started strong and has only gotten stronger. Over the past four years Kubernetes has exceeded the expectations of all of us that were there early on. We owe the Kubernetes community a huge debt. The success the project has seen is based not just on code and technology but also the way that an amazing group of people have come together to create something special. The best expression of this is the [set of Kubernetes values](https://github.com/kubernetes/steering/blob/master/values.md) that Sarah Novotny helped curate.
Here is to another 4 years and beyond! ğŸ‰ğŸ‰ğŸ‰
-->
&lt;p>ä½†æ˜¯ï¼Œæ— è®ºå¤šä¹ˆåŸå§‹ï¼Œè¿™å°å°çš„ä¸€æ­¥è¶³ä»¥æ¿€èµ·ä¸€ä¸ªå¼€å§‹å¼ºå¤§è€Œä¸”å˜å¾—æ›´å¼ºå¤§çš„ç¤¾åŒºçš„å…´è¶£ã€‚åœ¨è¿‡å»çš„å››å¹´é‡Œï¼ŒKubernetes å·²ç»è¶…å‡ºäº†æˆ‘ä»¬æ‰€æœ‰äººçš„æœŸæœ›ã€‚æˆ‘ä»¬å¯¹ Kubernetes ç¤¾åŒºçš„æ‰€æœ‰äººå‘˜è¡¨ç¤ºæ„Ÿè°¢ã€‚è¯¥é¡¹ç›®æ‰€å–å¾—çš„æˆåŠŸä¸ä»…åŸºäºä»£ç å’ŒæŠ€æœ¯ï¼Œè¿˜åŸºäºä¸€ç¾¤å‡ºè‰²çš„äººèšé›†åœ¨ä¸€èµ·æ‰€åšçš„æœ‰æ„ä¹‰çš„äº‹æƒ…ã€‚Sarah Novotny ç­–åˆ’çš„ä¸€å¥— &lt;a href="https://github.com/kubernetes/steering/blob/master/values.md">Kubernetes ä»·å€¼è§‚&lt;/a>æ˜¯ä»¥ä¸Šæœ€å¥½çš„è¡¨ç°å½¢å¼ã€‚
è®©æˆ‘ä»¬ä¸€èµ·æœŸå¾…ä¸‹ä¸€ä¸ª4å¹´ï¼ğŸ‰ğŸ‰ğŸ‰&lt;/p></description></item></channel></rss>