<!doctype html><html lang=zh class=no-js>
<head>
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JPP6RFM2BP"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JPP6RFM2BP')</script>
<link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/production-environment/tools/>
<link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/production-environment/tools/>
<link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/production-environment/tools/>
<link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/setup/production-environment/tools/>
<link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/production-environment/tools/>
<link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/production-environment/tools/>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.87.0">
<link rel=canonical type=text/html href=https://kubernetes.io/zh/docs/setup/production-environment/tools/>
<link rel="shortcut icon" type=image/png href=/images/favicon.png>
<link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=manifest href=/manifest.webmanifest>
<link rel=apple-touch-icon href=/images/kubernetes-192x192.png>
<title>使用部署工具安装 Kubernetes | Kubernetes</title><meta property="og:title" content="使用部署工具安装 Kubernetes">
<meta property="og:description" content="生产级别的容器编排系统">
<meta property="og:type" content="website">
<meta property="og:url" content="https://kubernetes.io/zh/docs/setup/production-environment/tools/"><meta property="og:site_name" content="Kubernetes">
<meta itemprop=name content="使用部署工具安装 Kubernetes">
<meta itemprop=description content="生产级别的容器编排系统"><meta name=twitter:card content="summary">
<meta name=twitter:title content="使用部署工具安装 Kubernetes">
<meta name=twitter:description content="生产级别的容器编排系统">
<link href=/scss/main.css rel=stylesheet>
<script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script>
<meta name=theme-color content="#326ce5">
<link rel=stylesheet href=/css/feature-states.css>
<meta name=description content>
<meta property="og:description" content>
<meta name=twitter:description content>
<meta property="og:url" content="https://kubernetes.io/zh/docs/setup/production-environment/tools/">
<meta property="og:title" content="使用部署工具安装 Kubernetes">
<meta name=twitter:title content="使用部署工具安装 Kubernetes">
<meta name=twitter:image content="https://kubernetes.io/images/favicon.png">
<meta name=twitter:image:alt content="Kubernetes">
<meta property="og:image" content="/images/kubernetes-horizontal-color.png">
<meta property="og:type" content="article">
<script src=/js/script.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary>
<a class=navbar-brand href=/zh/></a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-2 mb-lg-0">
<a class="nav-link active" href=/zh/docs/>文档</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/blog/>Kubernetes 博客</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/training/>培训</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/partners/>合作伙伴</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/community/>社区</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/case-studies/>案例分析</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/zh/docs/setup/production-environment/tools/>v1.27</a>
<a class=dropdown-item href=https://v1-26.docs.kubernetes.io/zh/docs/setup/production-environment/tools/>v1.26</a>
<a class=dropdown-item href=https://v1-25.docs.kubernetes.io/zh/docs/setup/production-environment/tools/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/zh/docs/setup/production-environment/tools/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh/docs/setup/production-environment/tools/>v1.23</a>
</div>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/docs/setup/production-environment/tools/>English</a>
<a class=dropdown-item href=/ko/docs/setup/production-environment/tools/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/setup/production-environment/tools/>日本語 Japanese</a>
<a class=dropdown-item href=/fr/docs/setup/production-environment/tools/>Français</a>
<a class=dropdown-item href=/id/docs/setup/production-environment/tools/>Bahasa Indonesia</a>
<a class=dropdown-item href=/uk/docs/setup/production-environment/tools/>Українська</a>
</div>
</li>
</ul>
</div>
<button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.
</p><p>
<a href=/zh/docs/setup/production-environment/tools/>返回本页常规视图</a>.
</p>
</div>
<h1 class=title>使用部署工具安装 Kubernetes</h1>
<ul>
<li>1: <a href=#pg-a16f59f325a17cdeed324d5c889f7f73>使用 kubeadm 引导集群</a></li>
<ul>
<li>1.1: <a href=#pg-29e59491dd6118b23072dfe9ebb93323>安装 kubeadm</a></li>
<li>1.2: <a href=#pg-c3689df4b0c61a998e79d91a865aa244>对 kubeadm 进行故障排查</a></li>
<li>1.3: <a href=#pg-134ed1f6142a98e6ac681a1ba4920e53>使用 kubeadm 创建集群</a></li>
<li>1.4: <a href=#pg-4c656c5eda3e1c06ad1aedebdc04a211>使用 kubeadm API 定制组件</a></li>
<li>1.5: <a href=#pg-015edbc7cc688d31b1d1edce7c186135>高可用拓扑选项</a></li>
<li>1.6: <a href=#pg-3941d5c3409342219bf7e03128b8ecb6>利用 kubeadm 创建高可用集群</a></li>
<li>1.7: <a href=#pg-8160424c22d24f7d2d63c521e107dbf8>使用 kubeadm 创建一个高可用 etcd 集群</a></li>
<li>1.8: <a href=#pg-07709e71de6b4ac2573041c31213dbeb>使用 kubeadm 配置集群中的每个 kubelet</a></li>
<li>1.9: <a href=#pg-df2f3f20d404ebe2b03fcda1fcee50e7>使用 kubeadm 支持双协议栈</a></li>
</ul>
<li>2: <a href=#pg-478acca1934b6d89a0bc00fb25bfe5b6>使用 Kops 安装 Kubernetes</a></li>
<li>3: <a href=#pg-f8b4964187fe973644e06ee629eff1de>使用 Kubespray 安装 Kubernetes</a></li>
</ul>
<div class=content>
</div>
</div>
<div class=td-content>
<h1 id=pg-a16f59f325a17cdeed324d5c889f7f73>1 - 使用 kubeadm 引导集群</h1>
</div>
<div class=td-content>
<h1 id=pg-29e59491dd6118b23072dfe9ebb93323>1.1 - 安装 kubeadm</h1>
<p><img src=/images/kubeadm-stacked-color.png align=right width=150px>本页面显示如何安装 <code>kubeadm</code> 工具箱。
有关在执行此安装过程后如何使用 kubeadm 创建集群的信息，请参见
<a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>使用 kubeadm 创建集群</a> 页面。</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux
发行版以及一些不提供包管理器的发行版提供通用的指令</li>
<li>每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)</li>
<li>2 CPU 核或更多</li>
<li>集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)</li>
<li>节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见<a href=#verify-mac-address>这里</a>了解更多详细信息。</li>
<li>开启机器上的某些端口。请参见<a href=#check-required-ports>这里</a> 了解更多详细信息。</li>
<li>禁用交换分区。为了保证 kubelet 正常工作，你 <strong>必须</strong> 禁用交换分区。</li>
</ul>
<h2 id=verify-mac-address>确保每个节点上 MAC 地址和 product_uuid 的唯一性 </h2>
<ul>
<li>你可以使用命令 <code>ip link</code> 或 <code>ifconfig -a</code> 来获取网络接口的 MAC 地址</li>
<li>可以使用 <code>sudo cat /sys/class/dmi/id/product_uuid</code> 命令对 product_uuid 校验</li>
</ul>
<p>一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。
Kubernetes 使用这些值来唯一确定集群中的节点。
如果这些值在每个节点上不唯一，可能会导致安装
<a href=https://github.com/kubernetes/kubeadm/issues/31>失败</a>。</p>
<h2 id=检查网络适配器>检查网络适配器</h2>
<p>如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。</p>
<h2 id=允许-iptables-检查桥接流量>允许 iptables 检查桥接流量</h2>
<p>确保 <code>br_netfilter</code> 模块被加载。这一操作可以通过运行 <code>lsmod | grep br_netfilter</code>
来完成。若要显式加载该模块，可执行 <code>sudo modprobe br_netfilter</code>。</p>
<p>为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的
<code>sysctl</code> 配置中将 <code>net.bridge.bridge-nf-call-iptables</code> 设置为 1。例如：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span><span style=color:#b44>br_netfilter
</span><span style=color:#b44>EOF</span>

cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#b44>net.bridge.bridge-nf-call-iptables = 1
</span><span style=color:#b44>EOF</span>
sudo sysctl --system
</code></pre></div>
<p>更多的相关细节可查看<a href=/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>网络插件需求</a>页面。</p>
<h2 id=check-required-ports>检查所需端口</h2>
<p>启用这些<a href=/zh/docs/reference/ports-and-protocols/>必要的端口</a>后才能使 Kubernetes 的各组件相互通信。可以使用 netcat 之类的工具来检查端口是否启用，例如：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>nc 127.0.0.1 <span style=color:#666>6443</span>
</code></pre></div>
<p>你使用的 Pod 网络插件 (详见后续章节) 也可能需要开启某些特定端口。由于各个 Pod 网络插件的功能都有所不同，
请参阅他们各自文档中对端口的要求。</p>
<h2 id=installing-runtime>安装 runtime</h2>
<p>为了在 Pod 中运行容器，Kubernetes 使用
<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh/docs/setup/production-environment/container-runtimes target=_blank aria-label="容器运行时（Container Runtime）">容器运行时（Container Runtime）</a>。</p>
<ul class="nav nav-tabs" id=container-runtimes role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#container-runtimes-0 role=tab aria-controls=container-runtimes-0 aria-selected=true>Linux 节点</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#container-runtimes-1 role=tab aria-controls=container-runtimes-1>其它操作系统</a></li></ul>
<div class=tab-content id=container-runtimes><div id=container-runtimes-0 class="tab-pane show active" role=tabpanel aria-labelledby=container-runtimes-0>
<p>
<p>默认情况下，Kubernetes 使用
<a class=glossary-tooltip title="一组与 kubelet 集成的容器运行时 API" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/overview/components/#container-runtime target=_blank aria-label="容器运行时接口（Container Runtime Interface，CRI）">容器运行时接口（Container Runtime Interface，CRI）</a>
来与你所选择的容器运行时交互。</p>
<p>如果你不指定运行时，则 kubeadm 会自动尝试检测到系统上已经安装的运行时，
方法是扫描一组众所周知的 Unix 域套接字。
下面的表格列举了一些 kubeadm 查找的容器运行时及其对应的套接字路径：</p>
<table>
<thead>
<tr>
<th>运行时</th>
<th>域套接字</th>
</tr>
</thead>
<tbody>
<tr>
<td>Docker Engine</td>
<td><code>/var/run/dockershim.sock</code></td>
</tr>
<tr>
<td>containerd</td>
<td><code>/run/containerd/containerd.sock</code></td>
</tr>
<tr>
<td>CRI-O</td>
<td><code>/var/run/crio/crio.sock</code></td>
</tr>
</tbody>
</table>
<br>
如果同时检测到 Docker Engine 和 containerd，kubeadm 将优先考虑 Docker Engine。
这是必然的，因为 Docker 18.09 附带了 containerd 并且两者都是可以检测到的，
即使你仅安装了 Docker。
**如果检测到其他两个或多个运行时，kubeadm 输出错误信息并退出。**
<p>kubelet 可以使用已弃用的 dockershim 适配器与 Docker Engine 集成（dockershim 是 kubelet 本身的一部分）。</p>
<p>参阅<a href=/zh/docs/setup/production-environment/container-runtimes/>容器运行时</a>
以了解更多信息。</p>
</div>
<div id=container-runtimes-1 class=tab-pane role=tabpanel aria-labelledby=container-runtimes-1>
<p>
<p>默认情况下， kubeadm 使用 <a class=glossary-tooltip title="Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。" data-toggle=tooltip data-placement=top href=/zh/docs/reference/kubectl/docker-cli-to-kubectl/ target=_blank aria-label=Docker>Docker</a> 作为容器运行时。
kubelet 可以使用已弃用的 dockershim 适配器与 Docker Engine 集成（dockershim 是 kubelet 本身的一部分）。
参阅<a href=/zh/docs/setup/production-environment/container-runtimes/>容器运行时</a>
以了解更多信息。</p>
</div></div>
<h2 id=安装-kubeadm-kubelet-和-kubectl>安装 kubeadm、kubelet 和 kubectl</h2>
<p>你需要在每台机器上安装以下的软件包：</p>
<ul>
<li>
<p><code>kubeadm</code>：用来初始化集群的指令。</p>
</li>
<li>
<p><code>kubelet</code>：在集群中的每个节点上用来启动 Pod 和容器等。</p>
</li>
<li>
<p><code>kubectl</code>：用来与集群通信的命令行工具。</p>
</li>
</ul>
<p>kubeadm <strong>不能</strong> 帮你安装或者管理 <code>kubelet</code> 或 <code>kubectl</code>，所以你需要
确保它们与通过 kubeadm 安装的控制平面的版本相匹配。
如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。
然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet
的版本不可以超过 API 服务器的版本。
例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。</p>
<p>有关安装 <code>kubectl</code> 的信息，请参阅<a href=/zh/docs/tasks/tools/>安装和设置 kubectl</a>文档。</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong>
<p>这些指南不包括系统升级时使用的所有 Kubernetes 程序包。这是因为 kubeadm 和 Kubernetes
有<a href=/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>特殊的升级注意事项</a>。
</div>
<p>关于版本偏差的更多信息，请参阅以下文档：</p>
<ul>
<li>Kubernetes <a href=/zh/docs/setup/release/version-skew-policy/>版本与版本间的偏差策略</a></li>
<li>Kubeadm 特定的<a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy>版本偏差策略</a></li>
</ul>
<ul class="nav nav-tabs" id=k8s-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-0 role=tab aria-controls=k8s-install-0 aria-selected=true>基于 Debian 的发行版</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-1 role=tab aria-controls=k8s-install-1>基于 Red Hat 的发行版</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-2 role=tab aria-controls=k8s-install-2>无包管理器的情况</a></li></ul>
<div class=tab-content id=k8s-install><div id=k8s-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-0>
<p>
<ol>
<li>
<p>更新 <code>apt</code> 包索引并安装使用 Kubernetes <code>apt</code> 仓库所需要的包：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
</code></pre></div></li>
</ol>
<ol start=2>
<li>
<p>下载 Google Cloud 公开签名秘钥：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</code></pre></div></li>
</ol>
<ol start=3>
<li>
<p>添加 Kubernetes <code>apt</code> 仓库：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre></div></li>
</ol>
<ol start=4>
<li>
<p>更新 <code>apt</code> 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre></div></li>
</ol>
</div>
<div id=k8s-install-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-1>
<p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span><span style=color:#b44>[kubernetes]
</span><span style=color:#b44>name=Kubernetes
</span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
</span><span style=color:#b44>enabled=1
</span><span style=color:#b44>gpgcheck=1
</span><span style=color:#b44>repo_gpgcheck=1
</span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style=color:#b44>exclude=kubelet kubeadm kubectl
</span><span style=color:#b44>EOF</span>

<span style=color:#080;font-style:italic># 将 SELinux 设置为 permissive 模式（相当于将其禁用）</span>
sudo setenforce <span style=color:#666>0</span>
sudo sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config

sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes

sudo systemctl <span style=color:#a2f>enable</span> --now kubelet
</code></pre></div>
<p><strong>请注意：</strong></p>
<ul>
<li>
<p>通过运行命令 <code>setenforce 0</code> 和 <code>sed ...</code> 将 SELinux 设置为 permissive 模式
可以有效地将其禁用。
这是允许容器访问主机文件系统所必需的，而这些操作时为了例如 Pod 网络工作正常。</p>
<p>你必须这么做，直到 kubelet 做出对 SELinux 的支持进行升级为止。</p>
</li>
<li>
<p>如果你知道如何配置 SELinux 则可以将其保持启用状态，但可能需要设定 kubeadm 不支持的部分配置</p>
</li>
<li>
<p>如果由于该 Red Hat 的发行版无法解析 <code>basearch</code> 导致获取 <code>baseurl</code> 失败，请将 <code>\$basearch</code> 替换为你计算机的架构。
输入 <code>uname -m</code> 以查看该值。
例如，<code>x86_64</code> 的 <code>baseurl</code> URL 可以是：<code>https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</code>。</p>
</li>
</ul>
</div>
<div id=k8s-install-2 class=tab-pane role=tabpanel aria-labelledby=k8s-install-2>
<p>
<p>安装 CNI 插件（大多数 Pod 网络都需要）：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>CNI_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.8.2&#34;</span>
<span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
sudo mkdir -p /opt/cni/bin
curl -L <span style=color:#b44>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cni-plugins-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tgz&#34;</span> | sudo tar -C /opt/cni/bin -xz
</code></pre></div>
<p>定义要下载命令文件的目录。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p><code>DOWNLOAD_DIR</code> 变量必须被设置为一个可写入的目录。
如果你在运行 Flatcar Container Linux，可将 <code>DOWNLOAD_DIR</code> 设置为 <code>/opt/bin</code>。
</div>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#666>=</span>/usr/local/bin
sudo mkdir -p <span style=color:#b8860b>$DOWNLOAD_DIR</span>
</code></pre></div>
<p>安装 crictl（kubeadm/kubelet 容器运行时接口（CRI）所需）</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.22.0&#34;</span>
<span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
curl -L <span style=color:#b44>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/crictl-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tar.gz&#34;</span> | sudo tar -C <span style=color:#b8860b>$DOWNLOAD_DIR</span> -xz
</code></pre></div>
<p>安装 <code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code> 并添加 <code>kubelet</code> 系统服务：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>RELEASE</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
<span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
<span style=color:#a2f>cd</span> <span style=color:#b8860b>$DOWNLOAD_DIR</span>
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span>/bin/linux/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span>/<span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
sudo chmod +x <span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>

<span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.4.0&#34;</span>
curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre></div>
<p>激活并启动 <code>kubelet</code>：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl <span style=color:#a2f>enable</span> --now kubelet
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>Flatcar Container Linux 发行版会将 <code>/usr/</code> 目录挂载为一个只读文件系统。
在启动引导你的集群之前，你需要执行一些额外的操作来配置一个可写入的目录。
参见 <a href=/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#usr-mounted-read-only/>kubeadm 故障排查指南</a>
以了解如何配置一个可写入的目录。
</div>
</div></div>
<p>kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。</p>
<h2 id=configure-cgroup-driver>配置 cgroup 驱动程序 </h2>
<p>容器运行时和 kubelet 都具有名字为
<a href=/zh/docs/setup/production-environment/container-runtimes/>"cgroup driver"</a>
的属性，该属性对于在 Linux 机器上管理 CGroups 而言非常重要。</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong>
<p>你需要确保容器运行时和 kubelet 所使用的是相同的 cgroup 驱动，否则 kubelet
进程会失败。</p>
<p>相关细节可参见<a href=/zh/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/>配置 cgroup 驱动</a>。</p>
</div>
<h2 id=troubleshooting>故障排查 </h2>
<p>如果你在使用 kubeadm 时遇到困难，请参阅我们的
<a href=/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排查文档</a>。</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>使用 kubeadm 创建集群</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c3689df4b0c61a998e79d91a865aa244>1.2 - 对 kubeadm 进行故障排查</h1>
<p>与任何程序一样，你可能会在安装或者运行 kubeadm 时遇到错误。
本文列举了一些常见的故障场景，并提供可帮助你理解和解决这些问题的步骤。</p>
<p>如果你的问题未在下面列出，请执行以下步骤：</p>
<ul>
<li>
<p>如果你认为问题是 kubeadm 的错误：</p>
<ul>
<li>转到 <a href=https://github.com/kubernetes/kubeadm/issues>github.com/kubernetes/kubeadm</a> 并搜索存在的问题。</li>
<li>如果没有问题，请 <a href=https://github.com/kubernetes/kubeadm/issues/new>打开</a> 并遵循问题模板。</li>
</ul>
</li>
<li>
<p>如果你对 kubeadm 的工作方式有疑问，可以在 <a href=https://slack.k8s.io/>Slack</a> 上的 #kubeadm 频道提问，
或者在 <a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a> 上提问。
请加入相关标签，例如 <code>#kubernetes</code> 和 <code>#kubeadm</code>，这样其他人可以帮助你。</p>
</li>
</ul>
<h2 id=在安装过程中没有找到-ebtables-或者其他类似的可执行文件>在安装过程中没有找到 <code>ebtables</code> 或者其他类似的可执行文件</h2>
<p>如果在运行 <code>kubeadm init</code> 命令时，遇到以下的警告</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ebtables not found in system path
<span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ethtool not found in system path
</code></pre></div><p>那么或许在你的节点上缺失 <code>ebtables</code>、<code>ethtool</code> 或者类似的可执行文件。
你可以使用以下命令安装它们：</p>
<ul>
<li>对于 Ubuntu/Debian 用户，运行 <code>apt install ebtables ethtool</code> 命令。</li>
<li>对于 CentOS/Fedora 用户，运行 <code>yum install ebtables ethtool</code> 命令。</li>
</ul>
<h2 id=在安装过程中-kubeadm-一直等待控制平面就绪>在安装过程中，kubeadm 一直等待控制平面就绪</h2>
<p>如果你注意到 <code>kubeadm init</code> 在打印以下行后挂起：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#666>[</span>apiclient<span style=color:#666>]</span> Created API client, waiting <span style=color:#a2f;font-weight:700>for</span> the control plane to become ready
</code></pre></div>
<p>这可能是由许多问题引起的。最常见的是：</p>
<ul>
<li>网络连接问题。在继续之前，请检查你的计算机是否具有全部联通的网络连接。</li>
<li>容器运行时的 cgroup 驱动不同于 kubelet 使用的 cgroup 驱动。要了解如何正确配置 cgroup 驱动，
请参阅<a href=/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/>配置 cgroup 驱动</a>。</li>
<li>控制平面上的 Docker 容器持续进入崩溃状态或（因其他原因）挂起。你可以运行 <code>docker ps</code> 命令来检查以及 <code>docker logs</code> 命令来检视每个容器的运行日志。
对于其他容器运行时，请参阅<a href=/zh/docs/tasks/debug-application-cluster/crictl/>使用 crictl 对 Kubernetes 节点进行调试</a>。</li>
</ul>
<h2 id=当删除托管容器时-kubeadm-阻塞>当删除托管容器时 kubeadm 阻塞</h2>
<p>如果 Docker 停止并且不删除 Kubernetes 所管理的所有容器，可能发生以下情况：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo kubeadm reset
</code></pre></div><pre><code class=language-none data-lang=none>[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in &quot;/var/lib/kubelet&quot;
[reset] Removing kubernetes-managed containers
(block)
</code></pre><p>一个可行的解决方案是重新启动 Docker 服务，然后重新运行 <code>kubeadm reset</code>：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl restart docker.service
sudo kubeadm reset
</code></pre></div><p>检查 docker 的日志也可能有用：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>journalctl -ul docker
</code></pre></div>
<h2 id=pods-处于-runcontainererror-crashloopbackoff-或者-error-状态>Pods 处于 <code>RunContainerError</code>、<code>CrashLoopBackOff</code> 或者 <code>Error</code> 状态</h2>
<p>在 <code>kubeadm init</code> 命令运行后，系统中不应该有 pods 处于这类状态。</p>
<ul>
<li>
<p>在 <code>kubeadm init</code> 命令执行完后，如果有 pods 处于这些状态之一，请在 kubeadm
仓库提起一个 issue。<code>coredns</code> (或者 <code>kube-dns</code>) 应该处于 <code>Pending</code> 状态，
直到你部署了网络插件为止。</p>
</li>
<li>
<p>如果在部署完网络插件之后，有 Pods 处于 <code>RunContainerError</code>、<code>CrashLoopBackOff</code>
或 <code>Error</code> 状态之一，并且<code>coredns</code> （或者 <code>kube-dns</code>）仍处于 <code>Pending</code> 状态，
那很可能是你安装的网络插件由于某种原因无法工作。你或许需要授予它更多的
RBAC 特权或使用较新的版本。请在 Pod Network 提供商的问题跟踪器中提交问题，
然后在此处分类问题。</p>
</li>
<li>
<p>如果你安装的 Docker 版本早于 1.12.1，请在使用 <code>systemd</code> 来启动 <code>dockerd</code> 和重启 <code>docker</code> 时，
删除 <code>MountFlags=slave</code> 选项。
你可以在 <code>/usr/lib/systemd/system/docker.service</code> 中看到 MountFlags。
MountFlags 可能会干扰 Kubernetes 挂载的卷， 并使 Pods 处于 <code>CrashLoopBackOff</code> 状态。
当 Kubernetes 不能找到 <code>var/run/secrets/kubernetes.io/serviceaccount</code> 文件时会发生错误。</p>
</li>
</ul>
<h2 id=coredns-停滞在-pending-状态><code>coredns</code> 停滞在 <code>Pending</code> 状态</h2>
<p>这一行为是 <strong>预期之中</strong> 的，因为系统就是这么设计的。
kubeadm 的网络供应商是中立的，因此管理员应该选择
<a href=/zh/docs/concepts/cluster-administration/addons/>安装 pod 的网络插件</a>。
你必须完成 Pod 的网络配置，然后才能完全部署 CoreDNS。
在网络被配置好之前，DNS 组件会一直处于 <code>Pending</code> 状态。</p>
<h2 id=hostport-服务无法工作><code>HostPort</code> 服务无法工作</h2>
<p>此 <code>HostPort</code> 和 <code>HostIP</code> 功能是否可用取决于你的 Pod 网络配置。请联系 Pod 网络插件的作者，
以确认 <code>HostPort</code> 和 <code>HostIP</code> 功能是否可用。</p>
<p>已验证 Calico、Canal 和 Flannel CNI 驱动程序支持 HostPort。</p>
<p>有关更多信息，请参考 <a href=https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md>CNI portmap 文档</a>.</p>
<p>如果你的网络提供商不支持 portmap CNI 插件，你或许需要使用
<a href=/zh/docs/concepts/services-networking/service/#type-nodeport>NodePort 服务的功能</a>
或者使用 <code>HostNetwork=true</code>。</p>
<h2 id=无法通过其服务-ip-访问-pod>无法通过其服务 IP 访问 Pod</h2>
<ul>
<li>
<p>许多网络附加组件尚未启用 <a href=/zh/docs/tasks/debug-application-cluster/debug-service/#a-pod-fails-to-reach-itself-via-the-service-ip>hairpin 模式</a>
该模式允许 Pod 通过其服务 IP 进行访问。这是与 <a href=https://github.com/containernetworking/cni/issues/476>CNI</a> 有关的问题。
请与网络附加组件提供商联系，以获取他们所提供的 hairpin 模式的最新状态。</p>
</li>
<li>
<p>如果你正在使用 VirtualBox (直接使用或者通过 Vagrant 使用)，你需要
确保 <code>hostname -i</code> 返回一个可路由的 IP 地址。默认情况下，第一个接口连接不能路由的仅主机网络。
解决方法是修改 <code>/etc/hosts</code>，请参考示例 <a href=https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11>Vagrantfile</a>。</p>
</li>
</ul>
<h2 id=tls-证书错误>TLS 证书错误</h2>
<p>以下错误指出证书可能不匹配。</p>
<pre><code class=language-none data-lang=none># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;)
</code></pre><ul>
<li>
<p>验证 <code>$HOME/.kube/config</code> 文件是否包含有效证书，并
在必要时重新生成证书。在 kubeconfig 文件中的证书是 base64 编码的。
该 <code>base64 -d</code> 命令可以用来解码证书，<code>openssl x509 -text -noout</code> 命令
可以用于查看证书信息。</p>
</li>
<li>
<p>使用如下方法取消设置 <code>KUBECONFIG</code> 环境变量的值：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>unset</span> KUBECONFIG
</code></pre></div><p>或者将其设置为默认的 <code>KUBECONFIG</code> 位置：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</code></pre></div></li>
<li>
<p>另一个方法是覆盖 <code>kubeconfig</code> 的现有用户 "管理员" ：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>mv  <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.kube.bak
mkdir <span style=color:#b8860b>$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</code></pre></div></li>
</ul>
<h2 id=kubelet-client-cert>Kubelet 客户端证书轮换失败 </h2>
<p>默认情况下，kubeadm 使用 <code>/etc/kubernetes/kubelet.conf</code> 中指定的 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> 符号链接
来配置 kubelet 自动轮换客户端证书。如果此轮换过程失败，你可能会在 kube-apiserver 日志中看到
诸如 <code>x509: certificate has expired or is not yet valid</code> 之类的错误。要解决此问题，你必须执行以下步骤：</p>
<ol>
<li>从故障节点备份和删除 <code>/etc/kubernetes/kubelet.conf</code> 和 <code>/var/lib/kubelet/pki/kubelet-client*</code>。</li>
<li>在集群中具有 <code>/etc/kubernetes/pki/ca.key</code> 的、正常工作的控制平面节点上
执行 <code>kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf</code>。
<code>$NODE</code> 必须设置为集群中现有故障节点的名称。
手动修改生成的 <code>kubelet.conf</code> 以调整集群名称和服务器端点，
或传递 <code>kubeconfig user --config</code>（此命令接受 <code>InitConfiguration</code>）。
如果你的集群没有 <code>ca.key</code>，你必须在外部对 <code>kubelet.conf</code> 中的嵌入式证书进行签名。</li>
</ol>
<ol start=3>
<li>将得到的 <code>kubelet.conf</code> 文件复制到故障节点上，作为 <code>/etc/kubernetes/kubelet.conf</code>。</li>
<li>在故障节点上重启 kubelet（<code>systemctl restart kubelet</code>），等待 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> 重新创建。</li>
</ol>
<ol start=5>
<li>
<p>手动编辑 <code>kubelet.conf</code> 指向轮换的 kubelet 客户端证书，方法是将 <code>client-certificate-data</code> 和 <code>client-key-data</code> 替换为：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>client-certificate</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>client-key</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style=color:#bbb>
</span></code></pre></div></li>
</ol>
<ol start=6>
<li>重新启动 kubelet。</li>
<li>确保节点状况变为 <code>Ready</code>。</li>
</ol>
<h2 id=在-vagrant-中使用-flannel-作为-pod-网络时的默认-nic>在 Vagrant 中使用 flannel 作为 pod 网络时的默认 NIC</h2>
<p>以下错误可能表明 Pod 网络中出现问题：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>Error from server <span style=color:#666>(</span>NotFound<span style=color:#666>)</span>: the server could not find the requested resource
</code></pre></div><ul>
<li>
<p>如果你正在 Vagrant 中使用 flannel 作为 pod 网络，则必须指定 flannel 的默认接口名称。</p>
<p>Vagrant 通常为所有 VM 分配两个接口。第一个为所有主机分配了 IP 地址 <code>10.0.2.15</code>，用于获得 NATed 的外部流量。</p>
<p>这可能会导致 flannel 出现问题，它默认为主机上的第一个接口。这导致所有主机认为它们具有
相同的公共 IP 地址。为防止这种情况，传递 <code>--iface eth1</code> 标志给 flannel 以便选择第二个接口。</p>
</li>
</ul>
<h2 id=容器使用的非公共-ip>容器使用的非公共 IP</h2>
<p>在某些情况下 <code>kubectl logs</code> 和 <code>kubectl run</code> 命令或许会返回以下错误，即便除此之外集群一切功能正常：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
</code></pre></div><ul>
<li>
<p>这或许是由于 Kubernetes 使用的 IP 无法与看似相同的子网上的其他 IP 进行通信的缘故，
可能是由机器提供商的政策所导致的。</p>
</li>
<li>
<p>Digital Ocean 既分配一个共有 IP 给 <code>eth0</code>，也分配一个私有 IP 在内部用作其浮动 IP 功能的锚点，
然而 <code>kubelet</code> 将选择后者作为节点的 <code>InternalIP</code> 而不是公共 IP</p>
<p>使用 <code>ip addr show</code> 命令代替 <code>ifconfig</code> 命令去检查这种情况，因为 <code>ifconfig</code> 命令
不会显示有问题的别名 IP 地址。或者指定的 Digital Ocean 的 API 端口允许从 droplet 中
查询 anchor IP：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</code></pre></div><p>解决方法是通知 <code>kubelet</code> 使用哪个 <code>--node-ip</code>。当使用 Digital Ocean 时，可以是公网IP（分配给 <code>eth0</code>的），
或者是私网IP（分配给 <code>eth1</code> 的）。私网 IP 是可选的。
<a href=/zh/docs/reference/config-api/kubeadm-config.v1beta3/#kubeadm-k8s-io-v1beta3-NodeRegistrationOptions>kubadm <code>NodeRegistrationOptions</code> 结构</a>
的 <code>KubeletExtraArgs</code> 部分被用来处理这种情况。</p>
<p>然后重启 <code>kubelet</code>：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>systemctl daemon-reload
systemctl restart kubelet
</code></pre></div></li>
</ul>
<h2 id=coredns-pods-有-crashloopbackoff-或者-error-状态><code>coredns</code> pods 有 <code>CrashLoopBackOff</code> 或者 <code>Error</code> 状态</h2>
<p>如果有些节点运行的是旧版本的 Docker，同时启用了 SELinux，你或许会遇到 <code>coredns</code> pods 无法启动的情况。
要解决此问题，你可以尝试以下选项之一：</p>
<ul>
<li>
<p>升级到 <a href=/zh/docs/setup/production-environment/container-runtimes/#docker>Docker 的较新版本</a>。</p>
</li>
<li>
<p><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux>禁用 SELinux</a>.</p>
</li>
<li>
<p>修改 <code>coredns</code> 部署以设置 <code>allowPrivilegeEscalation</code> 为 <code>true</code>：</p>
</li>
</ul>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl -n kube-system get deployment coredns -o yaml | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  sed <span style=color:#b44>&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  kubectl apply -f -
</code></pre></div><p>CoreDNS 处于 <code>CrashLoopBackOff</code> 时的另一个原因是当 Kubernetes 中部署的 CoreDNS Pod 检测
到环路时。<a href=https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters>有许多解决方法</a>
可以避免在每次 CoreDNS 监测到循环并退出时，Kubernetes 尝试重启 CoreDNS Pod 的情况。</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> 禁用 SELinux 或设置 <code>allowPrivilegeEscalation</code> 为 <code>true</code> 可能会损害集群的安全性。
</div>
<h2 id=etcd-pods-持续重启>etcd pods 持续重启</h2>
<p>如果你遇到以下错误：</p>
<pre><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &quot;process_linux.go:110: decoding init error from pipe caused \&quot;read parent: connection reset by peer\&quot;&quot;
</code></pre><p>如果你使用 Docker 1.13.1.84 运行 CentOS 7 就会出现这种问题。
此版本的 Docker 会阻止 kubelet 在 etcd 容器中执行。</p>
<p>为解决此问题，请选择以下选项之一：</p>
<ul>
<li>
<p>回滚到早期版本的 Docker，例如 1.13.1-75</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</code></pre></div></li>
<li>
<p>安装较新的推荐版本之一，例如 18.06:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce-18.06.1.ce-3.el7.x86_64
</code></pre></div></li>
</ul>
<h2 id=无法将以逗号分隔的值列表传递给-component-extra-args-标志内的参数>无法将以逗号分隔的值列表传递给 <code>--component-extra-args</code> 标志内的参数</h2>
<p><code>kubeadm init</code> 标志例如 <code>--component-extra-args</code> 允许你将自定义参数传递给像
kube-apiserver 这样的控制平面组件。然而，由于解析 (<code>mapStringString</code>) 的基础类型值，此机制将受到限制。</p>
<p>如果你决定传递一个支持多个逗号分隔值（例如
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"</code>）参数，
将出现 <code>flag: malformed pair, expect string=string</code> 错误。
发生这种问题是因为参数列表 <code>--apiserver-extra-args</code> 预期的是 <code>key=value</code> 形式，
而这里的 <code>NamespacesExists</code> 被误认为是缺少取值的键名。</p>
<p>一种解决方法是尝试分离 <code>key=value</code> 对，像这样：
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"</code>
但这将导致键 <code>enable-admission-plugins</code> 仅有值 <code>NamespaceExists</code>。</p>
<p>已知的解决方法是使用 kubeadm
<a href=/zh/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>。</p>
<h2 id=在节点被云控制管理器初始化之前-kube-proxy-就被调度了>在节点被云控制管理器初始化之前，kube-proxy 就被调度了</h2>
<p>在云环境场景中，可能出现在云控制管理器完成节点地址初始化之前，kube-proxy 就被调度到新节点了。
这会导致 kube-proxy 无法正确获取节点的 IP 地址，并对管理负载平衡器的代理功能产生连锁反应。</p>
<p>在 kube-proxy Pod 中可以看到以下错误：</p>
<pre><code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
</code></pre><p>一种已知的解决方案是修补 kube-proxy DaemonSet，以允许在控制平面节点上调度它，
而不管它们的条件如何，将其与其他节点保持隔离，直到它们的初始保护条件消除：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl -n kube-system patch ds kube-proxy -p<span style=color:#666>=</span><span style=color:#b44>&#39;{ &#34;spec&#34;: { &#34;template&#34;: { &#34;spec&#34;: { &#34;tolerations&#34;: [ { &#34;key&#34;: &#34;CriticalAddonsOnly&#34;, &#34;operator&#34;: &#34;Exists&#34; }, { &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-role.kubernetes.io/master&#34; } ] } } } }&#39;</span>
</code></pre></div><p>此问题的跟踪<a href=https://github.com/kubernetes/kubeadm/issues/1027>在这里</a>。</p>
<h2 id=usr-mounted-read-only>节点上的 <code>/usr</code> 被以只读方式挂载</h2>
<p>在类似 Fedora CoreOS 或者 Flatcar Container Linux 这类 Linux 发行版本中，
目录 <code>/usr</code> 是以只读文件系统的形式挂载的。
在支持 <a href=https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md>FlexVolume</a>时，
类似 kubelet 和 kube-controller-manager 这类 Kubernetes 组件使用默认路径
<code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>，
而 FlexVolume 的目录 <em>必须是可写入的</em>，该功能特性才能正常工作。
（<strong>注意</strong>：FlexVolume 在 Kubernetes v1.23 版本中已被弃用）</p>
<p>为了解决这个问题，你可以使用 kubeadm 的<a href=/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a> 来配置 FlexVolume 的目录。</p>
<p>在（使用 <code>kubeadm init</code> 创建的）主控制节点上，使用 <code>-config</code>
参数传入如下文件：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>flex-volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<p>在加入到集群中的节点上，使用下面的文件：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<p>或者，你要可以更改 <code>/etc/fstab</code> 使得 <code>/usr</code> 目录能够以可写入的方式挂载，不过
请注意这样做本质上是在更改 Linux 发行版的某种设计原则。</p>
<h2 id=kubeadm-upgrade-plan-输出错误信息-context-deadline-exceeded><code>kubeadm upgrade plan</code> 输出错误信息 <code>context deadline exceeded</code></h2>
<p>在使用 <code>kubeadm</code> 来升级某运行外部 etcd 的 Kubernetes 集群时可能显示这一错误信息。
这并不是一个非常严重的一个缺陷，之所以出现此错误信息，原因是老的 kubeadm
版本会对外部 etcd 集群执行版本检查。你可以继续执行 <code>kubeadm upgrade apply ...</code>。</p>
<p>这一问题已经在 1.19 版本中得到修复。</p>
<h2 id=kubeadm-reset-会卸载-var-lib-kubelet><code>kubeadm reset</code> 会卸载 <code>/var/lib/kubelet</code></h2>
<p>如果已经挂载了 <code>/var/lib/kubelet</code> 目录，执行 <code>kubeadm reset</code> 操作的时候
会将其卸载。</p>
<p>要解决这一问题，可以在执行了 <code>kubeadm reset</code> 操作之后重新挂载
<code>/var/lib/kubelet</code> 目录。</p>
<p>这是一个在 1.15 中引入的故障，已经在 1.20 版本中修复。</p>
<h2 id=无法在-kubeadm-集群中安全地使用-metrics-server>无法在 kubeadm 集群中安全地使用 metrics-server</h2>
<p>在 kubeadm 集群中可以通过为 <a href=https://github.com/kubernetes-sigs/metrics-server>metrics-server</a>
设置 <code>--kubelet-insecure-tls</code> 来以不安全的形式使用该服务。
建议不要在生产环境集群中这样使用。</p>
<p>如果你需要在 metrics-server 和 kubelet 之间使用 TLS，会有一个问题，
kubeadm 为 kubelet 部署的是自签名的服务证书。这可能会导致 metrics-server
端报告下面的错误信息：</p>
<pre><code>x509: certificate signed by unknown authority
x509: certificate is valid for IP-foo not IP-bar
</code></pre>
<p>参见<a href=/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs>为 kubelet 启用签名的服务证书</a>
以进一步了解如何在 kubeadm 集群中配置 kubelet 使用正确签名了的服务证书。</p>
<p>另请参阅<a href=https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-to-run-metrics-server-securely>How to run the metrics-server securely</a>。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-134ed1f6142a98e6ac681a1ba4920e53>1.3 - 使用 kubeadm 创建集群</h1>
<p><img src=/images/kubeadm-stacked-color.png align=right width=150px>使用 <code>kubeadm</code>，你能创建一个符合最佳实践的最小化 Kubernetes 集群。事实上，你可以使用 <code>kubeadm</code> 配置一个通过 <a href=https://kubernetes.io/blog/2017/10/software-conformance-certification>Kubernetes 一致性测试</a> 的集群。
<code>kubeadm</code> 还支持其他集群生命周期功能，
例如 <a href=/zh/docs/reference/access-authn-authz/bootstrap-tokens/>启动引导令牌</a> 和集群升级。</p>
<p>kubeadm 工具很棒，如果你需要：</p>
<ul>
<li>一个尝试 Kubernetes 的简单方法。</li>
<li>一个现有用户可以自动设置集群并测试其应用程序的途径。</li>
<li>其他具有更大范围的生态系统和/或安装工具中的构建模块。</li>
</ul>
<p>你可以在各种机器上安装和使用 <code>kubeadm</code>：笔记本电脑，
一组云服务器，Raspberry Pi 等。无论是部署到云还是本地，
你都可以将 <code>kubeadm</code> 集成到预配置系统中，例如 Ansible 或 Terraform。</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>要遵循本指南，你需要：</p>
<ul>
<li>一台或多台运行兼容 deb/rpm 的 Linux 操作系统的计算机；例如：Ubuntu 或 CentOS。</li>
<li>每台机器 2 GB 以上的内存，内存不足时应用会受限制。</li>
<li>用作控制平面节点的计算机上至少有2个 CPU。</li>
<li>集群中所有计算机之间具有完全的网络连接。你可以使用公共网络或专用网络。</li>
</ul>
<p>你还需要使用可以在新集群中部署特定 Kubernetes 版本对应的 <code>kubeadm</code>。</p>
<p><a href=/zh/docs/setup/release/version-skew-policy/#supported-versions>Kubernetes 版本及版本倾斜支持策略</a> 适用于 <code>kubeadm</code> 以及整个 Kubernetes。
查阅该策略以了解支持哪些版本的 Kubernetes 和 <code>kubeadm</code>。
该页面是为 Kubernetes v1.23 编写的。</p>
<p><code>kubeadm</code> 工具的整体功能状态为一般可用性（GA）。一些子功能仍在积极开发中。
随着工具的发展，创建集群的实现可能会略有变化，但总体实现应相当稳定。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 根据定义，在 <code>kubeadm alpha</code> 下的所有命令均在 alpha 级别上受支持。
</div>
<h2 id=目标>目标</h2>
<ul>
<li>安装单个控制平面的 Kubernetes 集群</li>
<li>在集群上安装 Pod 网络，以便你的 Pod 可以相互连通</li>
</ul>
<h2 id=操作指南>操作指南</h2>
<h3 id=在你的主机上安装-kubeadm>在你的主机上安装 kubeadm</h3>
<p>查看 <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>"安装 kubeadm"</a>。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>如果你已经安装了kubeadm，执行 <code>apt-get update && apt-get upgrade</code> 或 <code>yum update</code> 以获取 kubeadm 的最新版本。</p>
<p>升级时，kubelet 每隔几秒钟重新启动一次，
在 crashloop 状态中等待 kubeadm 发布指令。crashloop 状态是正常现象。
初始化控制平面后，kubelet 将正常运行。</p>
</div>
<h3 id=准备所需的容器镜像>准备所需的容器镜像</h3>
<p>这个步骤是可选的，只适用于你希望 <code>kubeadm init</code> 和 <code>kubeadm join</code> 不去下载存放在 <code>k8s.gcr.io</code> 上的默认的容器镜像的情况。</p>
<p>当你在离线的节点上创建一个集群的时候，Kubeadm 有一些命令可以帮助你预拉取所需的镜像。
阅读<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images>离线运行 kubeadm</a>
获取更多的详情。</p>
<p>Kubeadm 允许你给所需要的镜像指定一个自定义的镜像仓库。
阅读<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images>使用自定义镜像</a>
获取更多的详情。</p>
<h3 id=初始化控制平面节点>初始化控制平面节点</h3>
<p>控制平面节点是运行控制平面组件的机器，
包括 <a class=glossary-tooltip title="etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。" data-toggle=tooltip data-placement=top href=/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a> （集群数据库）
和 <a class=glossary-tooltip title="提供 Kubernetes API 服务的控制面组件。" data-toggle=tooltip data-placement=top href=/zh/docs/reference/command-line-tools-reference/kube-apiserver/ target=_blank aria-label="API Server">API Server</a>
（命令行工具 <a class=glossary-tooltip title="kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。" data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a> 与之通信）。</p>
<ol>
<li>（推荐）如果计划将单个控制平面 kubeadm 集群升级成高可用，
你应该指定 <code>--control-plane-endpoint</code> 为所有控制平面节点设置共享端点。
端点可以是负载均衡器的 DNS 名称或 IP 地址。</li>
<li>选择一个 Pod 网络插件，并验证是否需要为 <code>kubeadm init</code> 传递参数。
根据你选择的第三方网络插件，你可能需要设置 <code>--pod-network-cidr</code> 的值。
请参阅 <a href=#pod-network>安装Pod网络附加组件</a>。</li>
<li>（可选）从版本1.14开始，<code>kubeadm</code> 尝试使用一系列众所周知的域套接字路径来检测 Linux 上的容器运行时。
要使用不同的容器运行时，
或者如果在预配置的节点上安装了多个容器，请为 <code>kubeadm init</code> 指定 <code>--cri-socket</code> 参数。
请参阅<a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>安装运行时</a>。</li>
<li>（可选）除非另有说明，否则 <code>kubeadm</code> 使用与默认网关关联的网络接口来设置此控制平面节点 API server 的广播地址。
要使用其他网络接口，请为 <code>kubeadm init</code> 设置 <code>--apiserver-advertise-address=&lt;ip-address></code> 参数。
要部署使用 IPv6 地址的 Kubernetes 集群，
必须指定一个 IPv6 地址，例如 <code>--apiserver-advertise-address=fd00::101</code></li>
</ol>
<p>要初始化控制平面节点，请运行：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm init &lt;args&gt;
</code></pre></div>
<h3 id=关于-apiserver-advertise-address-和-controlplaneendpoint-的注意事项>关于 apiserver-advertise-address 和 ControlPlaneEndpoint 的注意事项</h3>
<p><code>--apiserver-advertise-address</code> 可用于为控制平面节点的 API server 设置广播地址，
<code>--control-plane-endpoint</code> 可用于为所有控制平面节点设置共享端点。</p>
<p><code>--control-plane-endpoint</code> 允许 IP 地址和可以映射到 IP 地址的 DNS 名称。
请与你的网络管理员联系，以评估有关此类映射的可能解决方案。</p>
<p>这是一个示例映射：</p>
<pre><code>192.168.0.102 cluster-endpoint
</code></pre>
<p>其中 <code>192.168.0.102</code> 是此节点的 IP 地址，<code>cluster-endpoint</code> 是映射到该 IP 的自定义 DNS 名称。
这将允许你将 <code>--control-plane-endpoint=cluster-endpoint</code> 传递给 <code>kubeadm init</code>，并将相同的 DNS 名称传递给 <code>kubeadm join</code>。
稍后你可以修改 <code>cluster-endpoint</code> 以指向高可用性方案中的负载均衡器的地址。</p>
<p>kubeadm 不支持将没有 <code>--control-plane-endpoint</code> 参数的单个控制平面集群转换为高可用性集群。</p>
<h3 id=更多信息>更多信息</h3>
<p>有关 <code>kubeadm init</code> 参数的更多信息，请参见 <a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm 参考指南</a>。</p>
<p>要使用配置文件配置 <code>kubeadm init</code> 命令，请参见<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>带配置文件使用 kubeadm init</a>。</p>
<p>要自定义控制平面组件，包括可选的对控制平面组件和 etcd 服务器的活动探针提供 IPv6 支持，请参阅<a href=/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags/>自定义参数</a>。</p>
<p>要再次运行 <code>kubeadm init</code>，你必须首先<a href=#tear-down>卸载集群</a>。</p>
<p>如果将具有不同架构的节点加入集群，
请确保已部署的 DaemonSet 对这种体系结构具有容器镜像支持。</p>
<p><code>kubeadm init</code> 首先运行一系列预检查以确保机器
准备运行 Kubernetes。这些预检查会显示警告并在错误时退出。然后 <code>kubeadm init</code>
下载并安装集群控制平面组件。这可能会需要几分钟。
完成之后你应该看到：</p>
<pre><code class=language-none data-lang=none>Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre>
<p>要使非 root 用户可以运行 kubectl，请运行以下命令，
它们也是 <code>kubeadm init</code> 输出的一部分：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</code></pre></div>
<p>或者，如果你是 <code>root</code> 用户，则可以运行：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</code></pre></div><div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong>
<p>kubeadm 对 <code>admin.conf</code> 中的证书进行签名时，将其配置为
<code>Subject: O = system:masters, CN = kubernetes-admin</code>。
<code>system:masters</code> 是一个例外的、超级用户组，可以绕过鉴权层（例如 RBAC）。
不要将 <code>admin.conf</code> 文件与任何人共享，应该使用 <code>kubeadm kubeconfig user</code>
命令为其他用户生成 kubeconfig 文件，完成对他们的定制授权。
</div>
<p>记录 <code>kubeadm init</code> 输出的 <code>kubeadm join</code> 命令。
你需要此命令<a href=#join-nodes>将节点加入集群</a>。</p>
<p>令牌用于控制平面节点和加入节点之间的相互身份验证。
这里包含的令牌是密钥。确保它的安全，
因为拥有此令牌的任何人都可以将经过身份验证的节点添加到你的集群中。
可以使用 <code>kubeadm token</code> 命令列出，创建和删除这些令牌。
请参阅 <a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-token/>kubeadm 参考指南</a>。</p>
<h3 id=pod-network>安装 Pod 网络附加组件</h3>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong>
<p>本节包含有关网络设置和部署顺序的重要信息。
在继续之前，请仔细阅读所有建议。</p>
<p><strong>你必须部署一个基于 Pod 网络插件的
<a class=glossary-tooltip title="容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni target=_blank aria-label=容器网络接口>容器网络接口</a>
(CNI)，以便你的 Pod 可以相互通信。
在安装网络之前，集群 DNS (CoreDNS) 将不会启动。</strong></p>
<ul>
<li>注意你的 Pod 网络不得与任何主机网络重叠：
如果有重叠，你很可能会遇到问题。
（如果你发现网络插件的首选 Pod 网络与某些主机网络之间存在冲突，
则应考虑使用一个合适的 CIDR 块来代替，
然后在执行 <code>kubeadm init</code> 时使用 <code>--pod-network-cidr</code> 参数并在你的网络插件的 YAML 中替换它）。</li>
</ul>
<ul>
<li>默认情况下，<code>kubeadm</code> 将集群设置为使用和强制使用 <a href=/zh/docs/reference/access-authn-authz/rbac/>RBAC</a>（基于角色的访问控制）。
确保你的 Pod 网络插件支持 RBAC，以及用于部署它的 manifests 也是如此。</li>
</ul>
<ul>
<li>如果要为集群使用 IPv6（双协议栈或仅单协议栈 IPv6 网络），
请确保你的 Pod 网络插件支持 IPv6。
IPv6 支持已在 CNI <a href=https://github.com/containernetworking/cni/releases/tag/v0.6.0>v0.6.0</a> 版本中添加。</li>
</ul>
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> kubeadm 应该是与 CNI 无关的，对 CNI 驱动进行验证目前不在我们的端到端测试范畴之内。
如果你发现与 CNI 插件相关的问题，应在其各自的问题跟踪器中记录而不是在 kubeadm
或 kubernetes 问题跟踪器中记录。
</div>
<p>一些外部项目为 Kubernetes 提供使用 CNI 的 Pod 网络，其中一些还支持<a href=/zh/docs/concepts/services-networking/network-policies/>网络策略</a>。</p>
<p>请参阅实现 <a href=/zh/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>Kubernetes 网络模型</a> 的附加组件列表。</p>
<p>你可以使用以下命令在控制平面节点或具有 kubeconfig 凭据的节点上安装 Pod 网络附加组件：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f &lt;add-on.yaml&gt;
</code></pre></div>
<p>每个集群只能安装一个 Pod 网络。</p>
<p>安装 Pod 网络后，您可以通过在 <code>kubectl get pods --all-namespaces</code> 输出中检查 CoreDNS Pod 是否 <code>Running</code> 来确认其是否正常运行。
一旦 CoreDNS Pod 启用并运行，你就可以继续加入节点。</p>
<p>如果您的网络无法正常工作或 CoreDNS 不在“运行中”状态，请查看 <code>kubeadm</code> 的
<a href=/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排除指南</a>。</p>
<h3 id=控制平面节点隔离>控制平面节点隔离</h3>
<p>默认情况下，出于安全原因，你的集群不会在控制平面节点上调度 Pod。
如果你希望能够在控制平面节点上调度 Pod，
例如用于开发的单机 Kubernetes 集群，请运行：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre></div>
<p>输出看起来像：</p>
<pre><code>node &quot;test-01&quot; untainted
taint &quot;node-role.kubernetes.io/master:&quot; not found
taint &quot;node-role.kubernetes.io/master:&quot; not found
</code></pre>
<p>这将从任何拥有 <code>node-role.kubernetes.io/master</code> taint 标记的节点中移除该标记，
包括控制平面节点，这意味着调度程序将能够在任何地方调度 Pods。</p>
<h3 id=join-nodes>加入节点</h3>
<p>节点是你的工作负载（容器和 Pod 等）运行的地方。要将新节点添加到集群，请对每台计算机执行以下操作：</p>
<ul>
<li>SSH 到机器</li>
<li>成为 root （例如 <code>sudo su -</code>）</li>
<li>运行 <code>kubeadm init</code> 输出的命令。例如：</li>
</ul>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre></div>
<p>如果没有令牌，可以通过在控制平面节点上运行以下命令来获取令牌：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm token list
</code></pre></div>
<p>输出类似于以下内容：</p>
<pre><code class=language-console data-lang=console>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
                                                   signing          token generated by     bootstrappers:
                                                                    'kubeadm init'.        kubeadm:
                                                                                           default-node-token
</code></pre>
<p>默认情况下，令牌会在24小时后过期。如果要在当前令牌过期后将节点加入集群，
则可以通过在控制平面节点上运行以下命令来创建新令牌：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm token create
</code></pre></div>
<p>输出类似于以下内容：</p>
<pre><code class=language-console data-lang=console>5didvk.d09sbcov8ph2amjw
</code></pre>
<p>如果你没有 <code>--discovery-token-ca-cert-hash</code> 的值，则可以通过在控制平面节点上执行以下命令链来获取它：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   openssl dgst -sha256 -hex | sed <span style=color:#b44>&#39;s/^.* //&#39;</span>
</code></pre></div>
<p>输出类似于以下内容：</p>
<pre><code class=language-console data-lang=console>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</code></pre>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 要为 <code>&lt;control-plane-host>:&lt;control-plane-port></code> 指定 IPv6 元组，必须将 IPv6 地址括在方括号中，例如：<code>[fd00::101]:2073</code>
</div>
<p>输出应类似于：</p>
<pre><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.
</code></pre>
<p>几秒钟后，当你在控制平面节点上执行 <code>kubectl get nodes</code>，你会注意到该节点出现在输出中。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 由于集群节点通常是按顺序初始化的，CoreDNS Pods 很可能都运行在第一个控制面节点上。
为了提供更高的可用性，请在加入至少一个新节点后
使用 <code>kubectl -n kube-system rollout restart deployment coredns</code> 命令，重新平衡 CoreDNS Pods。
</div>
<h3 id=可选-从控制平面节点以外的计算机控制集群>（可选）从控制平面节点以外的计算机控制集群</h3>
<p>为了使 kubectl 在其他计算机（例如笔记本电脑）上与你的集群通信，
你需要将管理员 kubeconfig 文件从控制平面节点复制到工作站，如下所示：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes
</code></pre></div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>上面的示例假定为 root 用户启用了SSH访问。如果不是这种情况，
你可以使用 <code>scp</code> 将 admin.conf 文件复制给其他允许访问的用户。</p>
<p>admin.conf 文件为用户提供了对集群的超级用户特权。
该文件应谨慎使用。对于普通用户，建议生成一个你为其授予特权的唯一证书。
你可以使用 <code>kubeadm alpha kubeconfig user --client-name &lt;CN></code> 命令执行此操作。
该命令会将 KubeConfig 文件打印到 STDOUT，你应该将其保存到文件并分发给用户。
之后，使用 <code>kubectl create (cluster)rolebinding</code> 授予特权。</p>
</div>
<h3 id=可选-将api服务器代理到本地主机>（可选）将API服务器代理到本地主机</h3>
<p>如果要从集群外部连接到 API 服务器，则可以使用 <code>kubectl proxy</code>：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy
</code></pre></div>
<p>你现在可以在本地访问API服务器 http://localhost:8001/api/v1</p>
<h2 id=tear-down>清理</h2>
<p>如果你在集群中使用了一次性服务器进行测试，则可以关闭这些服务器，而无需进一步清理。你可以使用 <code>kubectl config delete-cluster</code> 删除对集群的本地引用。</p>
<p>但是，如果要更干净地取消配置群集，
则应首先<a href=/docs/reference/generated/kubectl/kubectl-commands#drain>清空节点</a>并确保该节点为空，
然后取消配置该节点。</p>
<h3 id=删除节点>删除节点</h3>
<p>使用适当的凭证与控制平面节点通信，运行：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl drain &lt;node name&gt; --delete-emptydir-data --force --ignore-daemonsets
</code></pre></div>
<p>在删除节点之前，请重置 <code>kubeadm</code> 安装的状态：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm reset
</code></pre></div>
<p>重置过程不会重置或清除 iptables 规则或 IPVS 表。如果你希望重置 iptables，则必须手动进行：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>iptables -F <span style=color:#666>&amp;&amp;</span> iptables -t nat -F <span style=color:#666>&amp;&amp;</span> iptables -t mangle -F <span style=color:#666>&amp;&amp;</span> iptables -X
</code></pre></div>
<p>如果要重置 IPVS 表，则必须运行以下命令：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ipvsadm -C
</code></pre></div>
<p>现在删除节点：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl delete node &lt;node name&gt;
</code></pre></div>
<p>如果你想重新开始，只需运行 <code>kubeadm init</code> 或 <code>kubeadm join</code> 并加上适当的参数。</p>
<h3 id=清理控制平面>清理控制平面</h3>
<p>你可以在控制平面主机上使用 <code>kubeadm reset</code> 来触发尽力而为的清理。</p>
<p>有关此子命令及其选项的更多信息，请参见<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-reset/><code>kubeadm reset</code></a>参考文档。</p>
<h2 id=whats-next>下一步</h2>
<ul>
<li>使用 <a href=https://github.com/heptio/sonobuoy>Sonobuoy</a> 验证集群是否正常运行。</li>
<li><a id=lifecycle>有关使用 kubeadm 升级集群的详细信息，请参阅<a href=/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>升级 kubeadm 集群</a>。</li>
<li>在 <a href=/zh/docs/reference/setup-tools/kubeadm>kubeadm 参考文档</a>中了解有关高级 <code>kubeadm</code> 用法的信息。</li>
<li>了解有关 Kubernetes <a href=/zh/docs/concepts/>概念</a>和 <a href=/zh/docs/reference/kubectl/overview/><code>kubectl</code></a> 的更多信息。</li>
<li>有关 Pod 网络附加组件的更多列表，请参见<a href=/zh/docs/concepts/cluster-administration/networking/>集群网络</a>页面。</li>
<li><a id=other-addons>请参阅<a href=/zh/docs/concepts/cluster-administration/addons/>附加组件列表</a>以探索其他附加组件，
包括用于 Kubernetes 集群的日志记录，监视，网络策略，可视化和控制的工具。</li>
<li>配置集群如何处理集群事件的日志以及
在 Pods 中运行的应用程序。
有关所涉及内容的概述，请参见<a href=/zh/docs/concepts/cluster-administration/logging/>日志架构</a>。</li>
</ul>
<h3 id=feedback>反馈</h3>
<ul>
<li>有关 bugs, 访问 <a href=https://github.com/kubernetes/kubeadm/issues>kubeadm GitHub issue tracker</a></li>
<li>有关支持, 访问
<a href=https://kubernetes.slack.com/messages/kubeadm/>#kubeadm</a> Slack 频道</li>
<li>General SIG 集群生命周期开发 Slack 频道:
<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a></li>
<li>SIG 集群生命周期 <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme>SIG information</a></li>
<li>SIG 集群生命周期邮件列表:
<a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>kubernetes-sig-cluster-lifecycle</a></li>
</ul>
<h2 id=version-skew-policy>版本倾斜政策</h2>
<p>版本 v1.27 的kubeadm 工具可以使用版本 v1.27 或 v1.26 的控制平面部署集群。kubeadm v1.27 还可以升级现有的 kubeadm 创建的 v1.26 版本的集群。</p>
<p>由于我们不能预见未来，kubeadm CLI v1.27 可能会或可能无法部署 v1.28 集群。</p>
<p>这些资源提供了有关 kubelet 与控制平面以及其他 Kubernetes 组件之间受支持的版本倾斜的更多信息：</p>
<ul>
<li>Kubernetes <a href=/zh/docs/setup/release/version-skew-policy/>版本和版本偏斜政策</a></li>
<li>Kubeadm-specific <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl>安装指南</a></li>
</ul>
<h2 id=limitations>局限性</h2>
<h3 id=resilience>集群弹性</h3>
<p>此处创建的集群具有单个控制平面节点，运行单个 etcd 数据库。
这意味着如果控制平面节点发生故障，你的集群可能会丢失数据并且可能需要从头开始重新创建。</p>
<p>解决方法:</p>
<ul>
<li>定期<a href=https://coreos.com/etcd/docs/latest/admin_guide.html>备份 etcd</a>。
kubeadm 配置的 etcd 数据目录位于控制平面节点上的 <code>/var/lib/etcd</code> 中。</li>
</ul>
<ul>
<li>使用多个控制平面节点。你可以阅读
<a href=/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/>可选的高可用性拓扑</a> 选择集群拓扑提供的
<a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>高可用性</a>.</li>
</ul>
<h3 id=multi-platform>平台兼容性</h3>
<p>kubeadm deb/rpm 软件包和二进制文件是为 amd64，arm (32-bit)，arm64，ppc64le 和 s390x 构建的遵循<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md>多平台提案</a>。</p>
<p>从 v1.12 开始还支持用于控制平面和附加组件的多平台容器镜像。</p>
<p>只有一些网络提供商为所有平台提供解决方案。请查阅上方的网络提供商清单或每个提供商的文档以确定提供商是否支持你选择的平台。</p>
<h2 id=troubleshooting>故障排除</h2>
<p>如果你在使用 kubeadm 时遇到困难，请查阅我们的<a href=/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排除文档</a>。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-4c656c5eda3e1c06ad1aedebdc04a211>1.4 - 使用 kubeadm API 定制组件</h1>
<p>本页面介绍了如何自定义 kubeadm 部署的组件。
你可以使用 <code>ClusterConfiguration</code> 结构中定义的参数，或者在每个节点上应用补丁来定制控制平面组件。
你可以使用 <code>KubeletConfiguration</code> 和 <code>KubeProxyConfiguration</code> 结构分别定制 kubelet 和 kube-proxy 组件。</p>
<p>所有这些选项都可以通过 kubeadm 配置 API 实现。
有关配置中的每个字段的详细信息，你可以导航到我们的
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/>API 参考页面</a> 。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>kubeadm 目前不支持对 CoreDNS 部署进行定制。
你必须手动更新 <code>kube-system/coredns</code> <a class=glossary-tooltip title="ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。" data-toggle=tooltip data-placement=top href=/zh/docs/tasks/configure-pod-container/configure-pod-configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a>
并在更新后重新创建 CoreDNS <a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>。
或者，你可以跳过默认的 CoreDNS 部署并部署你自己的 CoreDNS 变种。
有关更多详细信息，请参阅<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#init-phases>在 kubeadm 中使用 init phases</a>.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>要重新配置已创建的集群，请参阅<a href=/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure>重新配置 kubeadm 集群</a>。
</div>
<h2 id=customizing-the-control-plane-with-flags-in-clusterconfiguration>使用 <code>ClusterConfiguration</code> 中的标志自定义控制平面 </h2>
<p>kubeadm <code>ClusterConfiguration</code> 对象为用户提供了一种方法，
用以覆盖传递给控制平面组件（如 APIServer、ControllerManager、Scheduler 和 Etcd）的默认参数。
各组件配置使用如下字段定义：</p>
<ul>
<li><code>apiServer</code></li>
<li><code>controllerManager</code></li>
<li><code>scheduler</code></li>
<li><code>etcd</code></li>
</ul>
<p>这些结构包含一个通用的 <code>extraArgs</code> 字段，该字段由 <code>key: value</code> 组成。
要覆盖控制平面组件的参数：</p>
<ol>
<li>将适当的字段 <code>extraArgs</code> 添加到配置中。</li>
<li>向字段 <code>extraArgs</code> 添加要覆盖的参数值。</li>
<li>用 <code>--config &lt;YOUR CONFIG YAML></code> 运行 <code>kubeadm init</code>。</li>
</ol>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>你可以通过运行 <code>kubeadm config print init-defaults</code> 并将输出保存到你所选的文件中，
以默认值形式生成 <code>ClusterConfiguration</code> 对象。
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p><code>ClusterConfiguration</code> 对象目前在 kubeadm 集群中是全局的。
这意味着你添加的任何标志都将应用于同一组件在不同节点上的所有实例。
要在不同节点上为每个组件应用单独的配置，您可以使用<a href=#patches>补丁</a>。
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>当前不支持重复的参数（keys）或多次传递相同的参数 <code>--foo</code>。
要解决此问题，你必须使用<a href=#patches>补丁</a>。
</div>
<h3 id=apiserver-flags>APIServer 参数 </h3>
<p>有关详细信息，请参阅 <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver 参考文档</a>。</p>
<p>使用示例：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>anonymous-auth</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;false&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>AlwaysPullImages,DefaultStorageClass<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>audit-log-path</span>:<span style=color:#bbb> </span>/home/johndoe/audit.log<span style=color:#bbb>
</span></code></pre></div>
<h3 id=controllermanager-flags>ControllerManager 参数 </h3>
<p>有关详细信息，请参阅 <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager 参考文档</a>。</p>
<p>使用示例：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/home/johndoe/keys/ca.key<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deployment-controller-sync-period</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;50&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<h2 id=scheduler-flags>Scheduler 参数 </h2>
<p>有关详细信息，请参阅 <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler 参考文档</a>。</p>
<p>使用示例：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduler</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>config</span>:<span style=color:#bbb> </span>/etc/kubernetes/scheduler-config.yaml<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraVolumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>schedulerconfig<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb> </span>/home/johndoe/schedconfig.yaml<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/kubernetes/scheduler-config.yaml<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;File&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<h3 id=etcd-flags>Etcd 参数 </h3>
<p>有关详细信息，请参阅 <a href=https://etcd.io/docs/>etcd 服务文档</a>.</p>
<p>使用示例：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>etcd</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>local</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>election-timeout</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></code></pre></div>
<h2 id=patches>使用补丁定制控制平面 </h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>
<p>Kubeadm 允许将包含补丁文件的目录传递给各个节点上的 <code>InitConfiguration</code> 和 <code>JoinConfiguration</code>。
这些补丁可被用作控制平面组件清单写入磁盘之前的最后一个自定义步骤。</p>
<p>可以使用 <code>--config &lt;你的 YAML 格式控制文件></code> 将配置文件传递给 <code>kubeadm init</code>：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>patches</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>directory</span>:<span style=color:#bbb> </span>/home/user/somedir<span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>对于 <code>kubeadm init</code>，你可以传递一个包含 <code>ClusterConfiguration</code> 和 <code>InitConfiguration</code> 的文件，以 <code>---</code> 分隔。
</div>
<p>你可以使用 <code>--config &lt;你的 YAML 格式配置文件></code> 将配置文件传递给 <code>kubeadm join</code>：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>patches</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>directory</span>:<span style=color:#bbb> </span>/home/user/somedir<span style=color:#bbb>
</span></code></pre></div>
<p>补丁目录必须包含名为 <code>target[suffix][+patchtype].extension</code> 的文件。
例如，<code>kube-apiserver0+merge.yaml</code> 或只是 <code>etcd.json</code>。</p>
<ul>
<li><code>target</code> 可以是 <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>etcd</code> 之一。</li>
<li><code>patchtype</code> 可以是 <code>strategy</code>、<code>merge</code> 或 <code>json</code> 之一，并且这些必须匹配
<a href=/zh/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch>kubectl 支持</a> 的补丁格式。
默认补丁类型是 <code>strategic</code> 的。</li>
<li><code>extension</code> 必须是 <code>json</code> 或 <code>yaml</code>。</li>
<li><code>suffix</code> 是一个可选字符串，可用于确定首先按字母数字应用哪些补丁。</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>如果你使用 <code>kubeadm upgrade</code> 升级 kubeadm 节点，你必须再次提供相同的补丁，以便在升级后保留自定义配置。
为此，你可以使用 <code>--patches</code> 参数，该参数必须指向同一目录。 <code>kubeadm upgrade</code> 目前不支持用于相同目的的 API 结构配置。
</div>
<h2 id=customizing-the-kubelet>自定义 kubelet </h2>
<p>要自定义 kubelet，你可以在同一配置文件中的 <code>ClusterConfiguration</code> 或 <code>InitConfiguration</code>
之外添加一个 <code>KubeletConfiguration</code>，用 <code>---</code> 分隔。
然后可以将此文件传递给 <code>kubeadm init</code>。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>kubeadm 将相同的 <code>KubeletConfiguration</code> 配置应用于集群中的所有节点。
要应用节点特定设置，你可以使用 <code>kubelet</code> 参数进行覆盖，方法是将它们传递到 <code>InitConfiguration</code> 和 <code>JoinConfiguration</code>
支持的 <code>nodeRegistration.kubeletExtraArgs</code> 字段中。一些 kubelet 参数已被弃用，
因此在使用这些参数之前，请在 <a href=/zh/docs/reference/command-line-tools-reference/kubelet>kubelet 参考文档</a> 中检查它们的状态。
</div>
<p>更多详情，请参阅<a href=/zh/docs/setup/production-environment/tools/kubeadm/kubelet-integration>使用 kubeadm 配置集群中的每个 kubelet</a></p>
<h2 id=customizing-kube-proxy>自定义 kube-proxy </h2>
<p>要自定义 kube-proxy，你可以在 <code>ClusterConfiguration</code> 或 <code>InitConfiguration</code> 之外添加一个
由 <code>---</code> 分隔的 <code>KubeProxyConfiguration</code>， 传递给 <code>kubeadm init</code>。</p>
<p>可以导航到 <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>API 参考页面</a> 查看更多详情，</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>kubeadm 将 kube-proxy 部署为 <a class=glossary-tooltip title="确保 Pod 的副本在集群中的一组节点上运行。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/workloads/controllers/daemonset/ target=_blank aria-label=DaemonSet>DaemonSet</a>，
这意味着 <code>KubeProxyConfiguration</code> 将应用于集群中的所有 kube-proxy 实例。
</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-015edbc7cc688d31b1d1edce7c186135>1.5 - 高可用拓扑选项</h1>
<p>本页面介绍了配置高可用（HA） Kubernetes 集群拓扑的两个选项。</p>
<p>您可以设置 HA 集群：</p>
<ul>
<li>使用堆叠（stacked）控制平面节点，其中 etcd 节点与控制平面节点共存</li>
<li>使用外部 etcd 节点，其中 etcd 在与控制平面不同的节点上运行</li>
</ul>
<p>在设置 HA 集群之前，您应该仔细考虑每种拓扑的优缺点。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> kubeadm 静态引导 etcd 集群。 阅读 etcd <a href=https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static>集群指南</a>以获得更多详细信息。
</div>
<h2 id=堆叠-stacked-etcd-拓扑>堆叠（Stacked） etcd 拓扑</h2>
<p>堆叠（Stacked） HA 集群是一种这样的<a href=https://en.wikipedia.org/wiki/Network_topology>拓扑</a>，其中 etcd 分布式数据存储集群堆叠在 kubeadm 管理的控制平面节点上，作为控制平面的一个组件运行。</p>
<p>每个控制平面节点运行 <code>kube-apiserver</code>，<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。</p>
<p><code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。</p>
<p>每个控制平面节点创建一个本地 etcd 成员（member），这个 etcd 成员只与该节点的 <code>kube-apiserver</code> 通信。这同样适用于本地 <code>kube-controller-manager</code> 和 <code>kube-scheduler</code> 实例。</p>
<p>这种拓扑将控制平面和 etcd 成员耦合在同一节点上。相对使用外部 etcd 集群，设置起来更简单，而且更易于副本管理。</p>
<p>然而，堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 etcd 成员和控制平面实例都将丢失，并且冗余会受到影响。您可以通过添加更多控制平面节点来降低此风险。</p>
<p>因此，您应该为 HA 集群运行至少三个堆叠的控制平面节点。</p>
<p>这是 kubeadm 中的默认拓扑。当使用 <code>kubeadm init</code> 和 <code>kubeadm join --control-plane</code> 时，在控制平面节点上会自动创建本地 etcd 成员。</p>
<p><img src=/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg alt="堆叠的 etcd 拓扑"></p>
<h2 id=外部-etcd-拓扑>外部 etcd 拓扑</h2>
<p>具有外部 etcd 的 HA 集群是一种这样的<a href=https://en.wikipedia.org/wiki/Network_topology>拓扑</a>，其中 etcd 分布式数据存储集群在独立于控制平面节点的其他节点上运行。</p>
<p>就像堆叠的 etcd 拓扑一样，外部 etcd 拓扑中的每个控制平面节点都运行 <code>kube-apiserver</code>，<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。同样， <code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。但是，etcd 成员在不同的主机上运行，​​每个 etcd 主机与每个控制平面节点的 <code>kube-apiserver</code> 通信。</p>
<p>这种拓扑结构解耦了控制平面和 etcd 成员。因此，它提供了一种 HA 设置，其中失去控制平面实例或者 etcd 成员的影响较小，并且不会像堆叠的 HA 拓扑那样影响集群冗余。</p>
<p>但是，此拓扑需要两倍于堆叠 HA 拓扑的主机数量。</p>
<p>具有此拓扑的 HA 集群至少需要三个用于控制平面节点的主机和三个用于 etcd 节点的主机。</p>
<p><img src=/images/kubeadm/kubeadm-ha-topology-external-etcd.svg alt="外部 etcd 拓扑"></p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>使用 kubeadm 设置高可用集群</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-3941d5c3409342219bf7e03128b8ecb6>1.6 - 利用 kubeadm 创建高可用集群</h1>
<p>本文讲述了使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：</p>
<ul>
<li>使用具有堆叠的控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。</li>
<li>使用外部集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。</li>
</ul>
<p>在下一步之前，你应该仔细考虑哪种方法更好的满足你的应用程序和环境的需求。
<a href=/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/>高可用拓扑选项</a> 讲述了每种方法的优缺点。</p>
<p>如果你在安装 HA 集群时遇到问题，请在 kubeadm <a href=https://github.com/kubernetes/kubeadm/issues/new>问题跟踪</a>里向我们提供反馈。</p>
<p>你也可以阅读<a href=/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>升级文档</a></p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> 这篇文档没有讲述在云提供商上运行集群的问题。在云环境中，此处记录的方法不适用于类型为 LoadBalancer 的服务对象，或者具有动态的 PersistentVolumes。
</div>
<h2 id=before-you-begin>Before you begin</h2>
<p>根据集群控制平面所选择的拓扑结构不同，准备工作也有所差异：</p>
<ul class="nav nav-tabs" id=prerequisite-tabs role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#prerequisite-tabs-0 role=tab aria-controls=prerequisite-tabs-0 aria-selected=true>堆叠（Stacked） etcd 拓扑</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#prerequisite-tabs-1 role=tab aria-controls=prerequisite-tabs-1>外部 etcd 拓扑</a></li></ul>
<div class=tab-content id=prerequisite-tabs><div id=prerequisite-tabs-0 class="tab-pane show active" role=tabpanel aria-labelledby=prerequisite-tabs-0>
<p>
<p>需要准备：</p>
<ul>
<li>配置满足 <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B>kubeadm 的最低要求</a>
的三台机器作为控制面节点。奇数台控制平面节点有利于机器故障或者网络分区时进行重新选主。
<ul>
<li>机器已经安装好<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，并正常运行</li>
</ul>
</li>
<li>配置满足 <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B>kubeadm 的最低要求</a>
的三台机器作为工作节点
<ul>
<li>机器已经安装好<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，并正常运行</li>
</ul>
</li>
<li>在集群中，确保所有计算机之间存在全网络连接（公网或私网）</li>
<li>在所有机器上具有 sudo 权限
<ul>
<li>可以使用其他工具；本教程以 <code>sudo</code> 举例</li>
</ul>
</li>
<li>从某台设备通过 SSH 访问系统中所有节点的能力</li>
<li>所有机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code></li>
</ul>
<p><em>拓扑详情请参考<a href=/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/#%E5%A0%86%E5%8F%A0-stacked-etcd-%E6%8B%93%E6%89%91>堆叠（Stacked）etcd 拓扑</a>。</em></p>
</div>
<div id=prerequisite-tabs-1 class=tab-pane role=tabpanel aria-labelledby=prerequisite-tabs-1>
<p>
<p>需要准备：</p>
<ul>
<li>配置满足 <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B>kubeadm 的最低要求</a>
的三台机器作为控制面节点。奇数台控制平面节点有利于机器故障或者网络分区时进行重新选主。
<ul>
<li>机器已经安装好<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，并正常运行</li>
</ul>
</li>
<li>配置满足 <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B>kubeadm 的最低要求</a>
的三台机器作为工作节点
<ul>
<li>机器已经安装好<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，并正常运行</li>
</ul>
</li>
<li>在集群中，确保所有计算机之间存在全网络连接（公网或私网）</li>
<li>在所有机器上具有 sudo 权限
<ul>
<li>可以使用其他工具；本教程以 <code>sudo</code> 举例</li>
</ul>
</li>
<li>从某台设备通过 SSH 访问系统中所有节点的能力</li>
<li>所有机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code></li>
</ul>
<p>还需要准备：</p>
<ul>
<li>给 etcd 集群使用的另外三台及以上机器。为了分布式一致性算法达到更好的投票效果，集群必须由奇数个节点组成。
<ul>
<li>机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code>。</li>
<li>机器上同样需要安装好容器运行时，并能正常运行。</li>
</ul>
</li>
</ul>
<p><em>拓扑详情请参考<a href=/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/#%E5%A4%96%E9%83%A8-etcd-%E6%8B%93%E6%89%91>外部 etcd 拓扑</a>。</em></p>
</div></div>
<h3 id=容器镜像>容器镜像</h3>
<p>每台主机需要能够从 Kubernetes 容器镜像仓库（ <code>k8s.gcr.io</code> ）读取和拉取镜像。
想要在无法拉取 Kubernetes 仓库镜像的机器上部署高可用集群也是可行的。通过其他的手段保证主机上已经有对应的容器镜像即可。</p>
<h3 id=kubectl>命令行 </h3>
<p>一旦集群创建成功，需要在 PC 上<a href=/zh/docs/tasks/tools/#kubectl>安装 kubectl</a> 用于管理 Kubernetes。为了方便故障排查，也可以在每个控制平面节点上安装 <code>kubectl</code>。</p>
<h2 id=这两种方法的第一步>这两种方法的第一步</h2>
<h3 id=为-kube-apiserver-创建负载均衡器>为 kube-apiserver 创建负载均衡器</h3>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 使用负载均衡器需要许多配置。你的集群搭建可能需要不同的配置。
下面的例子只是其中的一方面配置。
</div>
<ol>
<li>
<p>创建一个名为 kube-apiserver 的负载均衡器解析 DNS。</p>
<ul>
<li>
<p>在云环境中，应该将控制平面节点放置在 TCP 转发负载平衡后面。
该负载均衡器将流量分配给目标列表中所有运行状况良好的控制平面节点。
API 服务器的健康检查是在 kube-apiserver 的监听端口（默认值 <code>:6443</code>）
上进行的一个 TCP 检查。</p>
</li>
<li>
<p>不建议在云环境中直接使用 IP 地址。</p>
</li>
<li>
<p>负载均衡器必须能够在 API 服务器端口上与所有控制平面节点通信。
它还必须允许其监听端口的入站流量。</p>
</li>
<li>
<p>确保负载均衡器的地址始终匹配 kubeadm 的 <code>ControlPlaneEndpoint</code> 地址。</p>
</li>
<li>
<p>阅读<a href=https://git.k8s.io/kubeadm/docs/ha-considerations.md#options-for-software-load-balancing>软件负载平衡选项指南</a>
以获取更多详细信息。</p>
</li>
</ul>
</li>
</ol>
<ol start=2>
<li>
<p>添加第一个控制平面节点到负载均衡器并测试连接：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>nc -v LOAD_BALANCER_IP PORT
</code></pre></div><p>由于 apiserver 尚未运行，预期会出现一个连接拒绝错误。
然而超时意味着负载均衡器不能和控制平面节点通信。
如果发生超时，请重新配置负载均衡器与控制平面节点进行通信。</p>
</li>
<li>
<p>将其余控制平面节点添加到负载均衡器目标组。</p>
</li>
</ol>
<h2 id=使用堆控制平面和-etcd-节点>使用堆控制平面和 etcd 节点</h2>
<h3 id=控制平面节点的第一步>控制平面节点的第一步</h3>
<ol>
<li>
<p>初始化控制平面：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo kubeadm init --control-plane-endpoint <span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span> --upload-certs
</code></pre></div><ul>
<li>你可以使用 <code>--kubernetes-version</code> 标志来设置要使用的 Kubernetes 版本。
建议将 kubeadm、kebelet、kubectl 和 Kubernetes 的版本匹配。</li>
<li>这个 <code>--control-plane-endpoint</code> 标志应该被设置成负载均衡器的地址或 DNS 和端口。</li>
<li>这个 <code>--upload-certs</code> 标志用来将在所有控制平面实例之间的共享证书上传到集群。
如果正好相反，你更喜欢手动地通过控制平面节点或者使用自动化工具复制证书，
请删除此标志并参考如下部分<a href=#manual-certs>证书分配手册</a>。</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 标志 <code>kubeadm init</code>、<code>--config</code> 和 <code>--certificate-key</code> 不能混合使用，
因此如果你要使用
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/>kubeadm 配置</a>，你必须在相应的配置结构
（位于 <code>InitConfiguration</code> 和 <code>JoinConfiguration: controlPlane</code>）添加 <code>certificateKey</code> 字段。
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 一些 CNI 网络插件如 Calico 需要 CIDR 例如 <code>192.168.0.0/16</code> 和一些像 Weave 没有。参考
<a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>CNI 网络文档</a>。
通过传递 <code>--pod-network-cidr</code> 标志添加 pod CIDR，或者你可以使用 kubeadm
配置文件，在 <code>ClusterConfiguration</code> 的 <code>networking</code> 对象下设置 <code>podSubnet</code> 字段。
</div>
<ul>
<li>
<p>输出类似于：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>...
You can now join any number of control-plane node by running the following <span style=color:#a2f>command</span> on each as a root:
kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:
  kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</code></pre></div></li>
</ul>
<ul>
<li>
<p>将此输出复制到文本文件。 稍后你将需要它来将控制平面节点和工作节点加入集群。</p>
</li>
<li>
<p>当使用 <code>--upload-certs</code> 调用 <code>kubeadm init</code> 时，主控制平面的证书被加密并上传到 <code>kubeadm-certs</code> Secret 中。</p>
</li>
<li>
<p>要重新上传证书并生成新的解密密钥，请在已加入集群节点的控制平面上使用以下命令：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo kubeadm init phase upload-certs --upload-certs
</code></pre></div></li>
</ul>
<ul>
<li>
<p>你还可以在 <code>init</code> 期间指定自定义的 <code>--certificate-key</code>，以后可以由 <code>join</code> 使用。
要生成这样的密钥，可以使用以下命令：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm certs certificate-key
</code></pre></div></li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <code>kubeadm-certs</code> Secret 和解密密钥会在两个小时后失效。
</div>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> 正如命令输出中所述，证书密钥可访问群集敏感数据。请妥善保管！
</div>
</li>
</ol>
<ol start=2>
<li>应用你所选择的 CNI 插件：
<a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>请遵循以下指示</a>
安装 CNI 驱动。如果适用，请确保配置与 kubeadm 配置文件中指定的 Pod
CIDR 相对应。
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 在进行下一步之前，必须选择并部署合适的网络插件。
否则集群不会正常运行。
</div>
</li>
</ol>
<ol start=3>
<li>
<p>输入以下内容，并查看控制平面组件的 Pods 启动：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod -n kube-system -w
</code></pre></div></li>
</ol>
<h3 id=其余控制平面节点的步骤>其余控制平面节点的步骤</h3>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 从 kubeadm 1.15 版本开始，你可以并行加入多个控制平面节点。
在此版本之前，你必须在第一个节点初始化后才能依序的增加新的控制平面节点。
</div>
<p>对于每个其他控制平面节点，你应该：</p>
<ol>
<li>
<p>执行先前由第一个节点上的 <code>kubeadm init</code> 输出提供给你的 join 命令。
它看起来应该像这样：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</code></pre></div><ul>
<li>这个 <code>--control-plane</code> 标志通知 <code>kubeadm join</code> 创建一个新的控制平面。</li>
<li><code>--certificate-key ...</code> 将导致从集群中的 <code>kubeadm-certs</code> Secret
下载控制平面证书并使用给定的密钥进行解密。</li>
</ul>
</li>
</ol>
<h2 id=外部-etcd-节点>外部 etcd 节点</h2>
<p>使用外部 etcd 节点设置集群类似于用于堆叠 etcd 的过程，
不同之处在于你应该首先设置 etcd，并在 kubeadm 配置文件中传递 etcd 信息。</p>
<h3 id=设置-ectd-集群>设置 ectd 集群</h3>
<ol>
<li>
<p>按照<a href=/zh/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>这些指示</a>
去设置 etcd 集群。</p>
</li>
<li>
<p>根据<a href=#manual-certs>这里</a> 的描述配置 SSH。</p>
</li>
<li>
<p>将以下文件从集群中的任何 etcd 节点复制到第一个控制平面节点：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>export</span> <span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu@10.0.0.7&#34;</span>
scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</code></pre></div><ul>
<li>用第一台控制平面机的 <code>user@host</code> 替换 <code>CONTROL_PLANE</code> 的值。</li>
</ul>
</li>
</ol>
<h3 id=设置第一个控制平面节点>设置第一个控制平面节点</h3>
<ol>
<li>
<p>用以下内容创建一个名为 <code>kubeadm-config.yaml</code> 的文件：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>stable<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controlPlaneEndpoint</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># change this (see below)</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>etcd</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>external</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>endpoints</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- https://ETCD_0_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># change ETCD_0_IP appropriately</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- https://ETCD_1_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># change ETCD_1_IP appropriately</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- https://ETCD_2_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># change ETCD_2_IP appropriately</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/etcd/ca.crt<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>certFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/apiserver-etcd-client.crt<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>keyFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/apiserver-etcd-client.key<span style=color:#bbb>
</span></code></pre></div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 这里的堆叠（stacked）etcd 和外部 etcd 之前的区别在于设置外部 etcd
需要一个 <code>etcd</code> 的 <code>external</code> 对象下带有 etcd 端点的配置文件。
如果是内部 etcd，是自动管理的。
</div>
<ul>
<li>
<p>在你的集群中，将配置模板中的以下变量替换为适当值：</p>
<ul>
<li><code>LOAD_BALANCER_DNS</code></li>
<li><code>LOAD_BALANCER_PORT</code></li>
<li><code>ETCD_0_IP</code></li>
<li><code>ETCD_1_IP</code></li>
<li><code>ETCD_2_IP</code></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>以下的步骤与设置内置 etcd 的集群是相似的：</p>
<ol>
<li>
<p>在节点上运行 <code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code> 命令。</p>
</li>
<li>
<p>记下输出的 join 命令，这些命令将在以后使用。</p>
</li>
<li>
<p>应用你选择的 CNI 插件。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 在进行下一步之前，必须选择并部署合适的网络插件。
否则集群不会正常运行。
</div>
</li>
</ol>
<h3 id=其他控制平面节点的步骤>其他控制平面节点的步骤</h3>
<p>步骤与设置内置 etcd 相同：</p>
<ul>
<li>确保第一个控制平面节点已完全初始化。</li>
<li>使用保存到文本文件的 join 命令将每个控制平面节点连接在一起。
建议一次加入一个控制平面节点。</li>
<li>不要忘记默认情况下，<code>--certificate-key</code> 中的解密秘钥会在两个小时后过期。</li>
</ul>
<h2 id=列举控制平面之后的常见任务>列举控制平面之后的常见任务</h2>
<h3 id=安装工作节点>安装工作节点</h3>
<p>你可以使用之前存储的 <code>kubeadm init</code> 命令的输出将工作节点加入集群中：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</code></pre></div>
<h2 id=manual-certs>手动证书分发</h2>
<p>如果你选择不将 <code>kubeadm init</code> 与 <code>--upload-certs</code> 命令一起使用，
则意味着你将必须手动将证书从主控制平面节点复制到
将要加入的控制平面节点上。</p>
<p>有许多方法可以实现这种操作。在下面的例子中我们使用 <code>ssh</code> 和 <code>scp</code>：</p>
<p>如果要在单独的一台计算机控制所有节点，则需要 SSH。</p>
<ol>
<li>
<p>在你的主设备上启用 ssh-agent，要求该设备能访问系统中的所有其他节点：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>eval</span> <span style=color:#a2f;font-weight:700>$(</span>ssh-agent<span style=color:#a2f;font-weight:700>)</span>
</code></pre></div></li>
</ol>
<ol start=2>
<li>
<p>将 SSH 身份添加到会话中：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ssh-add ~/.ssh/path_to_private_key
</code></pre></div></li>
</ol>
<ol start=3>
<li>
<p>检查节点间的 SSH 以确保连接是正常运行的</p>
<ul>
<li>
<p>SSH 到任何节点时，请确保添加 <code>-A</code> 标志：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ssh -A 10.0.0.7
</code></pre></div></li>
</ul>
<ul>
<li>
<p>当在任何节点上使用 sudo 时，请确保保持环境变量设置，以便 SSH
转发能够正常工作：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo -E -s
</code></pre></div></li>
</ul>
</li>
</ol>
<ol start=4>
<li>
<p>在所有节点上配置 SSH 之后，你应该在运行过 <code>kubeadm init</code> 命令的第一个
控制平面节点上运行以下脚本。
该脚本会将证书从第一个控制平面节点复制到另一个控制平面节点：</p>
<p>在以下示例中，用其他控制平面节点的 IP 地址替换 <code>CONTROL_PLANE_IPS</code>。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 可定制</span>
<span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#666>=</span><span style=color:#b44>&#34;10.0.0.7 10.0.0.8&#34;</span>
<span style=color:#a2f;font-weight:700>for</span> host in <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#b68;font-weight:700>}</span>; <span style=color:#a2f;font-weight:700>do</span>
    scp /etc/kubernetes/pki/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/sa.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/sa.pub <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.crt
    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.key
<span style=color:#a2f;font-weight:700>done</span>
</code></pre></div>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> 只需要复制上面列表中的证书。kubeadm 将负责生成其余证书以及加入控制平面实例所需的 SAN。
如果你错误地复制了所有证书，由于缺少所需的 SAN，创建其他节点可能会失败。
</div>
</li>
</ol>
<ol start=5>
<li>
<p>然后，在每个即将加入集群的控制平面节点上，你必须先运行以下脚本，然后
再运行 <code>kubeadm join</code>。
该脚本会将先前复制的证书从主目录移动到 <code>/etc/kubernetes/pki</code>：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 可定制</span>
mkdir -p /etc/kubernetes/pki/etcd
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.crt /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.pub /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</code></pre></div></li>
</ol>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-8160424c22d24f7d2d63c521e107dbf8>1.7 - 使用 kubeadm 创建一个高可用 etcd 集群</h1>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>在本指南中，当 kubeadm 用作为外部 etcd 节点管理工具，请注意 kubeadm 不计划支持此类节点的证书更换或升级。对于长期规划是使用 <a href=https://github.com/kubernetes-sigs/etcdadm>etcdadm</a> 增强工具来管理这方面。
</div>
<p>默认情况下，kubeadm 运行单成员的 etcd 集群，该集群由控制面节点上的 kubelet 以静态 Pod 的方式进行管理。由于 etcd 集群只包含一个成员且不能在任一成员不可用时保持运行，所以这不是一种高可用设置。本任务，将告诉你如何在使用 kubeadm 创建一个 kubernetes 集群时创建一个外部 etcd：有三个成员的高可用 etcd 集群。</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>三个可以通过 2379 和 2380 端口相互通信的主机。本文档使用这些作为默认端口。不过，它们可以通过 kubeadm 的配置文件进行自定义。</li>
</ul>
<ul>
<li>每个主机必须 <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>安装有 docker、kubelet 和 kubeadm</a>。</li>
</ul>
<ul>
<li>一些可以用来在主机间复制文件的基础设施。例如 <code>ssh</code> 和 <code>scp</code> 就可以满足需求。</li>
</ul>
<h2 id=建立集群>建立集群</h2>
<p>一般来说，是在一个节点上生成所有证书并且只分发这些<em>必要</em>的文件到其它节点上。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>kubeadm 包含生成下述证书所需的所有必要的密码学工具；在这个例子中，不需要其他加密工具。
</div>
<ol>
<li>
<p>将 kubelet 配置为 etcd 的服务管理器。</p>
<p><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 你必须在要运行 etcd 的所有主机上执行此操作。
</div>
由于 etcd 是首先创建的，因此你必须通过创建具有更高优先级的新文件来覆盖
kubeadm 提供的 kubelet 单元文件。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>cat <span style=color:#b44>&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span><span style=color:#b44>[Service]
</span><span style=color:#b44>ExecStart=
</span><span style=color:#b44># 将下面的 &#34;systemd&#34; 替换为你的容器运行时所使用的 cgroup 驱动。
</span><span style=color:#b44># kubelet 的默认值为 &#34;cgroupfs&#34;。
</span><span style=color:#b44>ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
</span><span style=color:#b44>Restart=always
</span><span style=color:#b44>EOF</span>

systemctl daemon-reload
systemctl restart kubelet
</code></pre></div>
<p>检查 kubelet 的状态以确保其处于运行状态：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>systemctl status kubelet
</code></pre></div></li>
</ol>
<ol start=2>
<li>
<p>为 kubeadm 创建配置文件。</p>
<p>使用以下脚本为每个将要运行 etcd 成员的主机生成一个 kubeadm 配置文件。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#080;font-style:italic># 使用 IP 或可解析的主机名替换 HOST0、HOST1 和 HOST2</span>
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST0</span><span style=color:#666>=</span>10.0.0.6
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST1</span><span style=color:#666>=</span>10.0.0.7
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST2</span><span style=color:#666>=</span>10.0.0.8

<span style=color:#080;font-style:italic># 创建临时目录来存储将被分发到其它主机上的文件</span>
mkdir -p /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/

<span style=color:#b8860b>ETCDHOSTS</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
<span style=color:#b8860b>NAMES</span><span style=color:#666>=(</span><span style=color:#b44>&#34;infra0&#34;</span> <span style=color:#b44>&#34;infra1&#34;</span> <span style=color:#b44>&#34;infra2&#34;</span><span style=color:#666>)</span>

<span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span>!ETCDHOSTS[@]<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>; <span style=color:#a2f;font-weight:700>do</span>
<span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCDHOSTS</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
<span style=color:#b8860b>NAME</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAMES</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
cat <span style=color:#b44>&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta3&#34;
</span><span style=color:#b44>kind: ClusterConfiguration
</span><span style=color:#b44>etcd:
</span><span style=color:#b44>    local:
</span><span style=color:#b44>        serverCertSANs:
</span><span style=color:#b44>        - &#34;${HOST}&#34;
</span><span style=color:#b44>        peerCertSANs:
</span><span style=color:#b44>        - &#34;${HOST}&#34;
</span><span style=color:#b44>        extraArgs:
</span><span style=color:#b44>            initial-cluster: infra0=https://${ETCDHOSTS[0]}:2380,infra1=https://${ETCDHOSTS[1]}:2380,infra2=https://${ETCDHOSTS[2]}:2380
</span><span style=color:#b44>            initial-cluster-state: new
</span><span style=color:#b44>            name: ${NAME}
</span><span style=color:#b44>            listen-peer-urls: https://${HOST}:2380
</span><span style=color:#b44>            listen-client-urls: https://${HOST}:2379
</span><span style=color:#b44>            advertise-client-urls: https://${HOST}:2379
</span><span style=color:#b44>            initial-advertise-peer-urls: https://${HOST}:2380
</span><span style=color:#b44>EOF</span>
<span style=color:#a2f;font-weight:700>done</span>
</code></pre></div></li>
</ol>
<ol start=3>
<li>
<p>生成证书颁发机构</p>
<p>如果你已经拥有 CA，那么唯一的操作是复制 CA 的 <code>crt</code> 和 <code>key</code> 文件到
<code>etc/kubernetes/pki/etcd/ca.crt</code> 和 <code>/etc/kubernetes/pki/etcd/ca.key</code>。
复制完这些文件后继续下一步，“为每个成员创建证书”。</p>
<p>如果你还没有 CA，则在 <code>$HOST0</code>（你为 kubeadm 生成配置文件的位置）上运行此命令。</p>
<pre><code>kubeadm init phase certs etcd-ca
</code></pre>
<p>这一操作创建如下两个文件</p>
<ul>
<li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li>
<li><code>/etc/kubernetes/pki/etcd/ca.key</code></li>
</ul>
</li>
</ol>
<ol start=4>
<li>
<p>为每个成员创建证书</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
<span style=color:#080;font-style:italic># 清理不可重复使用的证书</span>
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
<span style=color:#080;font-style:italic># 不需要移动 certs 因为它们是给 HOST0 使用的</span>

<span style=color:#080;font-style:italic># 清理不应从此主机复制的证书</span>
find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</code></pre></div></li>
</ol>
<ol start=5>
<li>
<p>复制证书和 kubeadm 配置</p>
<p>证书已生成，现在必须将它们移动到对应的主机。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu
<span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>
scp -r /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>/* <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>:
ssh <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>
USER@HOST $ sudo -Es
root@HOST $ chown -R root:root pki
root@HOST $ mv pki /etc/kubernetes/
</code></pre></div></li>
</ol>
<ol start=6>
<li>
<p>确保已经所有预期的文件都存在</p>
<p><code>$HOST0</code> 所需文件的完整列表如下：</p>
<pre><code class=language-none data-lang=none>/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre>
<p>在 <code>$HOST1</code> 上：</p>
<pre><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre>
<p>在 <code>$HOST2</code> 上：</p>
<pre><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre></li>
</ol>
<ol start=7>
<li>
<p>创建静态 Pod 清单</p>
<p>既然证书和配置已经就绪，是时候去创建清单了。
在每台主机上运行 <code>kubeadm</code> 命令来生成 etcd 使用的静态清单。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>root@HOST0 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
root@HOST1 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
root@HOST2 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</code></pre></div></li>
</ol>
<ol start=8>
<li>
<p>可选：检查群集运行状况</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>docker run --rm -it <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--net host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>-v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCD_TAG</span><span style=color:#b68;font-weight:700>}</span> etcdctl <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--cert /etc/kubernetes/pki/etcd/peer.crt <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--key /etc/kubernetes/pki/etcd/peer.key <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--cacert /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--endpoints https://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>:2379 endpoint health --cluster
...
https://<span style=color:#666>[</span>HOST0 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 16.283339ms
https://<span style=color:#666>[</span>HOST1 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 19.44402ms
https://<span style=color:#666>[</span>HOST2 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 35.926451ms
</code></pre></div>
<ul>
<li>将 <code>${ETCD_TAG}</code> 设置为你的 etcd 镜像的版本标签，例如 <code>3.4.3-0</code>。
要查看 kubeadm 使用的 etcd 镜像和标签，请执行
<code>kubeadm config images list --kubernetes-version ${K8S_VERSION}</code>，
例如，其中的 <code>${K8S_VERSION}</code> 可以是 <code>v1.17.0</code>。</li>
<li>将 <code>${HOST0}</code> 设置为要测试的主机的 IP 地址。</li>
</ul>
</li>
</ol>
<h2 id=what-s-next>What's next</h2>
<p>一旦拥有了一个正常工作的 3 成员的 etcd 集群，你就可以基于
<a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>使用 kubeadm 外部 etcd 的方法</a>，
继续部署一个高可用的控制平面。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-07709e71de6b4ac2573041c31213dbeb>1.8 - 使用 kubeadm 配置集群中的每个 kubelet</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes 1.11 [stable]</code>
</div>
<p>kubeadm CLI 工具的生命周期与 <a href=/zh/docs/reference/command-line-tools-reference/kubelet>kubelet</a>
解耦；kubelet 是一个守护程序，在 Kubernetes 集群中的每个节点上运行。
当 Kubernetes 初始化或升级时，kubeadm CLI 工具由用户执行，而 kubelet 始终在后台运行。</p>
<p>由于kubelet是守护程序，因此需要通过某种初始化系统或服务管理器进行维护。
当使用 DEB 或 RPM 安装 kubelet 时，配置系统去管理 kubelet。
你可以改用其他服务管理器，但需要手动地配置。</p>
<p>集群中涉及的所有 kubelet 的一些配置细节都必须相同，
而其他配置方面则需要基于每个 kubelet 进行设置，以适应给定机器的不同特性（例如操作系统、存储和网络）。
你可以手动地管理 kubelet 的配置，但是 kubeadm 现在提供一种 <code>KubeletConfiguration</code> API 类型
用于<a href=#configure-kubelets-using-kubeadm>集中管理 kubelet 的配置</a>。</p>
<h2 id=kubelet-配置模式>Kubelet 配置模式</h2>
<p>以下各节讲述了通过使用 kubeadm 简化 kubelet 配置模式，而不是在每个节点上手动地管理 kubelet 配置。</p>
<h3 id=将集群级配置传播到每个-kubelet-中>将集群级配置传播到每个 kubelet 中</h3>
<p>你可以通过使用 <code>kubeadm init</code> 和 <code>kubeadm join</code> 命令为 kubelet 提供默认值。
有趣的示例包括使用其他 CRI 运行时或通过服务器设置不同的默认子网。</p>
<p>如果你想使用子网 <code>10.96.0.0/12</code> 作为services的默认网段，你可以给 kubeadm 传递 <code>--service-cidr</code> 参数：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm init --service-cidr 10.96.0.0/12
</code></pre></div><p>现在，可以从该子网分配服务的虚拟 IP。
你还需要通过 kubelet 使用 <code>--cluster-dns</code> 标志设置 DNS 地址。
在集群中的每个管理器和节点上的 kubelet 的设置需要相同。
kubelet 提供了一个版本化的结构化 API 对象，该对象可以配置 kubelet 中的大多数参数，并将此配置推送到集群中正在运行的每个 kubelet 上。
此对象被称为 <a href=/zh/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>。
<code>KubeletConfiguration</code> 允许用户指定标志，例如用骆峰值代表集群的 DNS IP 地址，如下所示：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterDNS</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:#666>10.96.0.10</span><span style=color:#bbb>
</span></code></pre></div><p>有关 <code>KubeletConfiguration</code> 的更多详细信息，亲参阅<a href=#configure-kubelets-using-kubeadm>本节</a>。</p>
<h3 id=提供指定实例的详细配置信息>提供指定实例的详细配置信息</h3>
<p>由于硬件、操作系统、网络或者其他主机特定参数的差异。某些主机需要特定的 kubelet 配置。
以下列表提供了一些示例。</p>
<ul>
<li>
<p>由 kubelet 配置标志 <code>--resolv-conf</code> 指定的 DNS 解析文件的路径在操作系统之间可能有所不同，
它取决于你是否使用 <code>systemd-resolved</code>。
如果此路径错误，则在其 kubelet 配置错误的节点上 DNS 解析也将失败。</p>
</li>
<li>
<p>除非你使用云驱动，否则默认情况下 Node API 对象的 <code>.metadata.name</code> 会被设置为计算机的主机名。
如果你需要指定一个与机器的主机名不同的节点名称，你可以使用 <code>--hostname-override</code> 标志覆盖默认值。</p>
</li>
<li>
<p>当前，kubelet 无法自动检测 CRI 运行时使用的 cgroup 驱动程序，
但是值 <code>--cgroup-driver</code> 必须与 CRI 运行时使用的 cgroup 驱动程序匹配，以确保 kubelet 的健康运行状况。</p>
</li>
<li>
<p>取决于你的集群所使用的 CRI 运行时，你可能需要为 kubelet 指定不同的标志。
例如，当使用 Docker 时，你需要指定如 <code>--network-plugin=cni</code> 这类标志；但是如果你使用的是外部运行时，
则需要指定 <code>--container-runtime=remote</code> 并使用 <code>--container-runtime-endpoint=&lt;path></code> 指定 CRI 端点。</p>
</li>
</ul>
<p>你可以在服务管理器（例如 systemd）中设定某个 kubelet 的配置来指定这些参数。</p>
<h2 id=使用-kubeadm-配置-kubelet>使用 kubeadm 配置 kubelet</h2>
<p>如果自定义的 <code>KubeletConfiguration</code> API 对象使用像 <code>kubeadm ... --config some-config-file.yaml</code> 这样的配置文件进行传递，则可以配置 kubeadm 启动的 kubelet。</p>
<p>通过调用 <code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code>，
你可以看到此结构中的所有默认值。</p>
<p>也可以阅读 <a href=/zh/docs/reference/config-api/kubelet-config.v1beta1/>KubeletConfiguration 参考</a>
来获取有关各个字段的更多信息。</p>
<h3 id=当使用-kubeadm-init-时的工作流程>当使用 <code>kubeadm init</code>时的工作流程</h3>
<p>当调用 <code>kubeadm init</code> 时，kubelet 配置被编组到磁盘上的 <code>/var/lib/kubelet/config.yaml</code> 中，
并且上传到集群中的 ConfigMap。
ConfigMap 名为 <code>kubelet-config-1.X</code>，其中 <code>X</code> 是你正在初始化的 kubernetes 版本的次版本。
在集群中所有 kubelet 的基准集群范围内配置，将 kubelet 配置文件写入 <code>/etc/kubernetes/kubelet.conf</code> 中。
此配置文件指向允许 kubelet 与 API 服务器通信的客户端证书。
这解决了<a href=#propagating-cluster-level-configuration-to-each-kubelet>将集群级配置传播到每个 kubelet</a> 的需求。</p>
<p>该文档 <a href=#providing-instance-specific-configuration-details>提供特定实例的配置详细信息</a> 是第二种解决模式，
kubeadm 将环境文件写入 <code>/var/lib/kubelet/kubeadm-flags.env</code>，其中包含了一个标志列表，
当 kubelet 启动时，该标志列表会传递给 kubelet 标志在文件中的显示方式如下：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>KUBELET_KUBEADM_ARGS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</code></pre></div><p>除了启动 kubelet 时使用该标志外，该文件还包含动态参数，例如 cgroup 驱动程序以及是否使用其他 CRI 运行时 socket（<code>--cri-socket</code>）。</p>
<p>将这两个文件编组到磁盘后，如果使用 systemd，则 kubeadm 尝试运行以下两个命令：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</code></pre></div><p>如果重新加载和重新启动成功，则正常的 <code>kubeadm init</code> 工作流程将继续。</p>
<h3 id=当使用-kubeadm-join-时的工作流程>当使用 <code>kubeadm join</code>时的工作流程</h3>
<p>当运行 <code>kubeadm join</code> 时，kubeadm 使用 Bootstrap Token 证书执行 TLS 引导，该引导会获取一份证书，
该证书需要下载 <code>kubelet-config-1.X</code> ConfigMap 并把它写入 <code>/var/lib/kubelet/config.yaml</code> 中。
动态环境文件的生成方式恰好与 <code>kubeadm init</code> 完全相同。</p>
<p>接下来，<code>kubeadm</code> 运行以下两个命令将新配置加载到 kubelet 中：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</code></pre></div>
<p>在 kubelet 加载新配置后，kubeadm 将写入 <code>/etc/kubernetes/bootstrap-kubelet.conf</code> KubeConfig 文件中，
该文件包含 CA 证书和引导程序令牌。
kubelet 使用这些证书执行 TLS 引导程序并获取唯一的凭据，该凭据被存储在 <code>/etc/kubernetes/kubelet.conf</code> 中。</p>
<p>当 <code>/etc/kubernetes/kubelet.conf</code> 文件被写入后，kubelet 就完成了 TLS 引导过程。
Kubeadm 在完成 TLS 引导过程后将删除 <code>/etc/kubernetes/bootstrap-kubelet.conf</code> 文件。</p>
<h2 id=the-kubelet-drop-in-file-for-systemd>kubelet 的 systemd 文件</h2>
<p><code>kubeadm</code> 中附带了有关系统如何运行 kubelet 的 systemd 配置文件。
请注意 kubeadm CLI 命令不会修改此文件。</p>
<p>通过 <code>kubeadm</code> <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf>DEB</a>
或者 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf>RPM 包</a>
安装的配置文件被写入 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 并由系统使用。
它对原来的 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service>RPM 版本 <code>kubelet.service</code></a>
或者 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service>DEB 版本 <code>kubelet.service</code></a>
作了增强：</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 下面的内容只是一个例子。 如果您不想使用包管理器，
请遵循<a href=/zh/docs/setup/productionenvironment/tools/kubeadm/install-kubeadm/#k8s-install-2>没有包管理器</a>)
部分中叙述的指南。
</div>
<pre><code class=language-none data-lang=none>[Service]
Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;
Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;
# 这是 &quot;kubeadm init&quot; 和 &quot;kubeadm join&quot; 运行时生成的文件，动态地填充 KUBELET_KUBEADM_ARGS 变量
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# 这是一个文件，用户在不得已下可以将其用作替代 kubelet args。
# 用户最好使用 .NodeRegistration.KubeletExtraArgs 对象在配置文件中替代。
# KUBELET_EXTRA_ARGS 应该从此文件中获取。
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>该文件为 kubelet 指定由 kubeadm 管理的所有文件的默认位置。</p>
<ul>
<li>用于 TLS 引导程序的 KubeConfig 文件为 <code>/etc/kubernetes/bootstrap-kubelet.conf</code>，
但仅当 <code>/etc/kubernetes/kubelet.conf</code> 不存在时才能使用。</li>
<li>具有唯一 kubelet 标识的 KubeConfig 文件为 <code>/etc/kubernetes/kubelet.conf</code>。</li>
<li>包含 kubelet 的组件配置的文件为 <code>/var/lib/kubelet/config.yaml</code>。</li>
<li>包含的动态环境的文件 <code>KUBELET_KUBEADM_ARGS</code> 是来源于 <code>/var/lib/kubelet/kubeadm-flags.env</code>。</li>
<li>包含用户指定标志替代的文件 <code>KUBELET_EXTRA_ARGS</code> 是来源于
<code>/etc/default/kubelet</code>（对于 DEB），或者 <code>/etc/sysconfig/kubelet</code>（对于 RPM）。
<code>KUBELET_EXTRA_ARGS</code> 在标志链中排在最后，并且在设置冲突时具有最高优先级。</li>
</ul>
<h2 id=kubernetes-可执行文件和软件包内容>Kubernetes 可执行文件和软件包内容</h2>
<p>Kubernetes 版本对应的 DEB 和 RPM 软件包是：</p>
<table>
<thead>
<tr>
<th>Package name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>kubeadm</code></td>
<td>给 kubelet 安装 <code>/usr/bin/kubeadm</code> CLI 工具和 <a href=#the-kubelet-drop-in-file-for-systemd>kubelet 的 systemd 文件</a>。</td>
</tr>
<tr>
<td><code>kubelet</code></td>
<td>安装 <code>/usr/bin/kubelet</code> 可执行文件。</td>
</tr>
<tr>
<td><code>kubectl</code></td>
<td>安装 <code>/usr/bin/kubectl</code> 可执行文件。</td>
</tr>
<tr>
<td><code>cri-tools</code></td>
<td>从 <a href=https://github.com/kubernetes-sigs/cri-tools>cri-tools git 仓库</a>中安装 <code>/usr/bin/crictl</code> 可执行文件。</td>
</tr>
<tr>
<td><code>kubernetes-cni</code></td>
<td>从 <a href=https://github.com/containernetworking/plugins>plugins git 仓库</a>中安装 <code>/opt/cni/bin</code> 可执行文件。</td>
</tr>
</tbody>
</table>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-df2f3f20d404ebe2b03fcda1fcee50e7>1.9 - 使用 kubeadm 支持双协议栈</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>
<p>你的集群包含<a href=/zh/docs/concepts/services-networking/dual-stack/>双协议栈</a>组网支持，
这意味着集群网络允许你在两种地址族间任选其一。在集群中，控制面可以为同一个
<a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> 或者 <a class=glossary-tooltip title="将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>
同时赋予 IPv4 和 IPv6 地址。</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>你需要已经遵从<a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>安装 kubeadm</a>
中所给的步骤安装了 <a class=glossary-tooltip title="用来快速安装 Kubernetes 并搭建安全稳定的集群的工具。" data-toggle=tooltip data-placement=top href=/zh/docs/setup/production-environment/tools/kubeadm/ target=_blank aria-label=kubeadm>kubeadm</a> 工具。</p>
<p>针对你要作为<a class=glossary-tooltip title="Kubernetes 中的工作机器称作节点。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>使用的每台服务器，
确保其允许 IPv6 转发。在 Linux 节点上，你可以通过以 root 用户在每台服务器上运行
<code>sysctl -w net.ipv6.conf.all.forwarding=1</code> 来完成设置。</p>
<p>你需要一个可以使用的 IPv4 和 IPv6 地址范围。集群操作人员通常为 IPv4 使用
私有地址范围。对于 IPv6，集群操作人员通常会基于分配给该操作人员的地址范围，
从 <code>2000::/3</code> 中选择一个全局的单播地址块。你不需要将集群的 IP 地址范围路由
到公众互联网。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>如果你在使用 <code>kubeadm upgrade</code> 命令升级现有的集群，<code>kubeadm</code> 不允许更改 Pod
的 IP 地址范围（“集群 CIDR”），也不允许更改集群的服务地址范围（“Service CIDR”）。
</div>
<h3 id=create-a-dual-stack-cluster>创建双协议栈集群 </h3>
<p>要使用 <code>kubeadm init</code> 创建一个双协议栈集群，你可以传递与下面的例子类似的命令行参数：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># 这里的地址范围仅作示例使用</span>
kubeadm init --pod-network-cidr<span style=color:#666>=</span>10.244.0.0/16,2001:db8:42:0::/56 --service-cidr<span style=color:#666>=</span>10.96.0.0/16,2001:db8:42:1::/112
</code></pre></div>
<p>为了更便于理解，参看下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href=/zh/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>，
该文件用于双协议栈控制面的主控制节点。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>networking</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.244.0.0</span>/16,2001:db8:42:0::/56<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.96.0.0</span>/16,2001:db8:42:1::/112<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>localAPIEndpoint</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>advertiseAddress</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10.100.0.1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bindPort</span>:<span style=color:#bbb> </span><span style=color:#666>6443</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.2</span>,fd00:1:2:3::2<span style=color:#bbb>
</span></code></pre></div>
<p>InitConfiguration 中的 <code>advertiseAddress</code> 给出 API 服务器将公告自身要监听的
IP 地址。<code>advertiseAddress</code> 的取值与 <code>kubeadm init</code> 的标志
<code>--apiserver-advertise-address</code> 的取值相同。</p>
<p>运行 kubeadm 来实例化双协议栈控制面节点：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm init --config<span style=color:#666>=</span>kubeadm-config.yaml
</code></pre></div>
<p>kube-controller-manager 标志 <code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code>
是使用默认值来设置的。参见<a href=/zh/docs/concepts/services-networking/dual-stack#configure-ipv4-ipv6-dual-stack>配置 IPv4/IPv6 双协议栈</a>。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>标志 <code>--apiserver-advertise-address</code> 不支持双协议栈。
</div>
<h3 id=join-a-node-to-dual-stack-cluster>向双协议栈集群添加节点 </h3>
<p>在添加节点之前，请确保该节点具有 IPv6 可路由的网络接口并且启用了 IPv6 转发。</p>
<p>下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href=/zh/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>
示例用于向集群中添加工作节点。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>discovery</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bootstrapToken</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiServerEndpoint</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.1</span>:<span style=color:#666>6443</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;clvldh.vjjwg16ucnhp94qr&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caCertHashes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># 请更改上面的认证信息，使之与你的集群中实际使用的令牌和 CA 证书匹配</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.3</span>,fd00:1:2:3::3<span style=color:#bbb>
</span></code></pre></div>
<p>下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href=/zh/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>
示例用于向集群中添加另一个控制面节点。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controlPlane</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>localAPIEndpoint</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>advertiseAddress</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10.100.0.2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bindPort</span>:<span style=color:#bbb> </span><span style=color:#666>6443</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>discovery</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bootstrapToken</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiServerEndpoint</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.1</span>:<span style=color:#666>6443</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;clvldh.vjjwg16ucnhp94qr&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caCertHashes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># 请更改上面的认证信息，使之与你的集群中实际使用的令牌和 CA 证书匹配</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.4</span>,fd00:1:2:3::4<span style=color:#bbb>
</span></code></pre></div>
<p>JoinConfiguration.controlPlane 中的 <code>advertiseAddress</code> 设定 API 服务器将公告自身要监听的
IP 地址。<code>advertiseAddress</code> 的取值与 <code>kubeadm join</code> 的标志
<code>--apiserver-advertise-address</code> 的取值相同。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm join --config<span style=color:#666>=</span>kubeadm-config.yaml
</code></pre></div>
<h3 id=create-a-single-stack-cluster>创建单协议栈集群 </h3>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>双协议栈支持并不意味着你需要使用双协议栈来寻址。
你可以部署一个启用了双协议栈联网特性的单协议栈集群。
</div>
<p>为了更便于理解，参看下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href=/zh/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>示例，
该文件用于单协议栈控制面节点。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>networking</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.244.0.0</span>/16<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.96.0.0</span>/16<span style=color:#bbb>
</span></code></pre></div><h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=/zh/docs/tasks/network/validate-dual-stack>验证 IPv4/IPv6 双协议栈</a>联网</li>
<li>阅读<a href=/zh/docs/concepts/services-networking/dual-stack/>双协议栈</a>集群网络</li>
<li>进一步了解 kubeadm <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>配置格式</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-478acca1934b6d89a0bc00fb25bfe5b6>2 - 使用 Kops 安装 Kubernetes</h1>
<p>本篇快速入门介绍了如何在 AWS 上轻松安装 Kubernetes 集群。
本篇使用了一个名为 <a href=https://github.com/kubernetes/kops><code>kops</code></a> 的工具。</p>
<p>kops 是一个自动化的制备系统：</p>
<ul>
<li>全自动安装流程</li>
<li>使用 DNS 识别集群</li>
<li>自我修复：一切都在自动扩缩组中运行</li>
<li>支持多种操作系统（如 Debian、Ubuntu 16.04、CentOS、RHEL、Amazon Linux 和 CoreOS） - 参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/images.md>images.md</a></li>
<li>支持高可用 - 参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/high_availability.md>high_availability.md</a></li>
<li>可以直接提供或者生成 terraform 清单 - 参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/terraform.md>terraform.md</a></li>
</ul>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>你必须安装 <a href=/zh/docs/tasks/tools/>kubectl</a>。</li>
<li>你必须安装<a href=https://github.com/kubernetes/kops#installing>安装</a> <code>kops</code>
到 64 位的（AMD64 和 Intel 64）设备架构上。</li>
<li>你必须拥有一个 <a href=https://docs.aws.amazon.com/polly/latest/dg/setting-up.html>AWS 账户</a>，
生成 <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>IAM 秘钥</a>
并<a href=https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration>配置</a>
该秘钥。IAM 用户需要<a href=https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user>足够的权限许可</a>。</li>
</ul>
<h2 id=创建集群>创建集群</h2>
<h3 id=1-5-安装-kops>(1/5) 安装 kops</h3>
<h4 id=安装>安装</h4>
<p>从<a href=https://github.com/kubernetes/kops/releases>下载页面</a>下载 kops
（从源代码构建也很方便）：</p>
<ul class="nav nav-tabs" id=kops-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#kops-installation-0 role=tab aria-controls=kops-installation-0 aria-selected=true>macOS</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#kops-installation-1 role=tab aria-controls=kops-installation-1>Linux</a></li></ul>
<div class=tab-content id=kops-installation><div id=kops-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=kops-installation-0>
<p>
<p>使用下面的命令下载最新发布版本：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-darwin-amd64
</code></pre></div>
<p>要下载特定版本，使用特定的 kops 版本替换下面命令中的部分：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</code></pre></div>
<p>例如，要下载 kops v1.20.0，输入：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-darwin-amd64
</code></pre></div>
<p>令 kops 二进制文件可执行：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>chmod +x kops-darwin-amd64
</code></pre></div>
<p>将 kops 二进制文件移到你的 PATH 下：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mv kops-darwin-amd64 /usr/local/bin/kops
</code></pre></div><p>你也可以使用 <a href=https://brew.sh/>Homebrew</a> 安装 kops：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</code></pre></div></div>
<div id=kops-installation-1 class=tab-pane role=tabpanel aria-labelledby=kops-installation-1>
<p>
<p>使用命令下载最新发布版本：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-linux-amd64
</code></pre></div>
<p>要下载 kops 的特定版本，用特定的 kops 版本替换下面命令中的部分：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</code></pre></div>
<p>例如，要下载 kops v1.20 版本，输入：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64
</code></pre></div>
<p>令 kops 二进制文件可执行：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>chmod +x kops-linux-amd64
</code></pre></div>
<p>将 kops 二进制文件移到 PATH 下：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mv kops-linux-amd64 /usr/local/bin/kops
</code></pre></div><p>你也可以使用 <a href=https://docs.brew.sh/Homebrew-on-Linux>Homebrew</a>
来安装 kops：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</code></pre></div></div></div>
<h3 id=2-5-为你的集群创建一个-route53-域名>(2/5) 为你的集群创建一个 route53 域名</h3>
<p>kops 在集群内部和外部都使用 DNS 进行发现操作，这样你可以从客户端访问
kubernetes API 服务器。</p>
<p>kops 对集群名称有明显的要求：它应该是有效的 DNS 名称。这样一来，你就不会再使集群混乱，
可以与同事明确共享集群，并且无需依赖记住 IP 地址即可访问群集。</p>
<p>你可以，或许应该使用子域名来划分集群。作为示例，我们将使用域名 <code>useast1.dev.example.com</code>。
这样，API 服务器端点域名将为 <code>api.useast1.dev.example.com</code>。</p>
<p>Route53 托管区域可以服务子域名。你的托管区域可能是 <code>useast1.dev.example.com</code>，还有 <code>dev.example.com</code> 甚至 <code>example.com</code>。
kops 可以与以上任何一种配合使用，因此通常你出于组织原因选择不同的托管区域。
例如，允许你在 <code>dev.example.com</code> 下创建记录，但不能在 <code>example.com</code> 下创建记录。</p>
<p>假设你使用 <code>dev.example.com</code> 作为托管区域。你可以使用
<a href=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html>正常流程</a>
或者使用诸如 <code>aws route53 create-hosted-zone --name dev.example.com --caller-reference 1</code>
之类的命令来创建该托管区域。</p>
<p>然后，你必须在父域名中设置你的 DNS 记录，以便该域名中的记录可以被解析。
在这里，你将在 <code>example.com</code> 中为 <code>dev</code> 创建 DNS 记录。
如果它是根域名，则可以在域名注册机构配置 DNS 记录。
例如，你需要在购买 <code>example.com</code> 的地方配置 <code>example.com</code>。</p>
<p>检查你的 route53 域已经被正确设置（这是导致问题的最常见原因！）。
如果你安装了 dig 工具，则可以通过运行以下步骤再次检查集群是否配置正确：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>dig DNS dev.example.com
</code></pre></div>
<p>你应该看到 Route53 分配了你的托管区域的 4 条 DNS 记录。</p>
<h3 id=3-5-创建一个-s3-存储桶来存储集群状态>(3/5) 创建一个 S3 存储桶来存储集群状态</h3>
<p>kops 使你即使在安装后也可以管理集群。为此，它必须跟踪已创建的集群及其配置、所使用的密钥等。
此信息存储在 S3 存储桶中。S3 权限用于控制对存储桶的访问。</p>
<p>多个集群可以使用同一 S3 存储桶，并且你可以在管理同一集群的同事之间共享一个
S3 存储桶 - 这比传递 kubecfg 文件容易得多。
但是有权访问 S3 存储桶的任何人都将拥有对所有集群的管理访问权限，
因此你不想在运营团队之外共享它。</p>
<p>因此，通常每个运维团队都有一个 S3 存储桶（而且名称通常对应于上面托管区域的名称！）</p>
<p>在我们的示例中，我们选择 <code>dev.example.com</code> 作为托管区域，因此我们选择
<code>clusters.dev.example.com</code> 作为 S3 存储桶名称。</p>
<ul>
<li>导出 <code>AWS_PROFILE</code> 文件（如果你需要选择一个配置文件用来使 AWS CLI 正常工作）</li>
<li>使用 <code>aws s3 mb s3://clusters.dev.example.com</code> 创建 S3 存储桶</li>
<li>你可以进行 <code>export KOPS_STATE_STORE=s3://clusters.dev.example.com</code> 操作，
然后 kops 将默认使用此位置。
我们建议将其放入你的 bash profile 文件或类似文件中。</li>
</ul>
<h3 id=4-5-建立你的集群配置>(4/5) 建立你的集群配置</h3>
<p>运行 <code>kops create cluster</code> 以创建你的集群配置：</p>
<p><code>kops create cluster --zones=us-east-1c useast1.dev.example.com</code></p>
<p>kops 将为你的集群创建配置。请注意，它_仅_创建配置，实际上并没有创建云资源 -
你将在下一步中使用 <code>kops update cluster</code> 进行配置。
这使你有机会查看配置或进行更改。</p>
<p>它打印出可用于进一步探索的命令：</p>
<ul>
<li>使用以下命令列出集群：<code>kops get cluster</code></li>
<li>使用以下命令编辑该集群：<code>kops edit cluster useast1.dev.example.com</code></li>
<li>使用以下命令编辑你的节点实例组：<code>kops edit ig --name = useast1.dev.example.com nodes</code></li>
<li>使用以下命令编辑你的主实例组：<code>kops edit ig --name = useast1.dev.example.com master-us-east-1c</code></li>
</ul>
<p>如果这是你第一次使用 kops，请花几分钟尝试一下！ 实例组是一组实例，将被注册为 kubernetes 节点。
在 AWS 上，这是通过 auto-scaling-groups 实现的。你可以有多个实例组。
例如，如果你想要的是混合实例和按需实例的节点，或者 GPU 和非 GPU 实例。</p>
<h3 id=5-5-在-aws-中创建集群>(5/5) 在 AWS 中创建集群</h3>
<p>运行 "kops update cluster" 以在 AWS 中创建集群：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kops update cluster useast1.dev.example.com --yes
</code></pre></div>
<p>这需要几秒钟的时间才能运行，但实际上集群可能需要几分钟才能准备就绪。
每当更改集群配置时，都会使用 <code>kops update cluster</code> 工具。
它将对配置进行的更改应用于你的集群 - 根据需要重新配置 AWS 或者 kubernetes。</p>
<p>例如，在你运行 <code>kops edit ig nodes</code> 之后，然后运行 <code>kops update cluster --yes</code>
应用你的配置，有时你还必须运行 <code>kops rolling-update cluster</code> 立即回滚更新配置。</p>
<p>如果没有 <code>--yes</code> 参数，<code>kops update cluster</code> 操作将向你显示其操作的预览效果。这对于生产集群很方便！</p>
<h3 id=探索其他附加组件>探索其他附加组件</h3>
<p>请参阅<a href=/zh/docs/concepts/cluster-administration/addons/>附加组件列表</a>探索其他附加组件，
包括用于 Kubernetes 集群的日志记录、监视、网络策略、可视化和控制的工具。</p>
<h2 id=清理>清理</h2>
<ul>
<li>删除集群：<code>kops delete cluster useast1.dev.example.com --yes</code></li>
</ul>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>了解有关 Kubernetes 的<a href=/zh/docs/concepts/>概念</a> 和
<a href=/zh/docs/reference/kubectl/><code>kubectl</code></a> 有关的更多信息。</li>
<li>了解 <code>kops</code> <a href=https://github.com/kubernetes/kops>高级用法</a>。</li>
<li>请参阅 <code>kops</code> <a href=https://github.com/kubernetes/kops>文档</a> 获取教程、
最佳做法和高级配置选项。</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-f8b4964187fe973644e06ee629eff1de>3 - 使用 Kubespray 安装 Kubernetes</h1>
<p>此快速入门有助于使用 <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a>
安装在 GCE、Azure、OpenStack、AWS、vSphere、Packet（裸机）、Oracle Cloud
Infrastructure（实验性）或 Baremetal 上托管的 Kubernetes 集群。</p>
<p>Kubespray 是一个由 <a href=https://docs.ansible.com/>Ansible</a> playbooks、
<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md>清单（inventory）</a>、
制备工具和通用 OS/Kubernetes 集群配置管理任务的领域知识组成的。
Kubespray 提供：</p>
<ul>
<li>高可用性集群</li>
<li>可组合属性</li>
<li>支持大多数流行的 Linux 发行版
<ul>
<li>Ubuntu 16.04、18.04、20.04</li>
<li>CentOS / RHEL / Oracle Linux 7、8</li>
<li>Debian Buster、Jessie、Stretch、Wheezy</li>
<li>Fedora 31、32</li>
<li>Fedora CoreOS</li>
<li>openSUSE Leap 15</li>
<li>Kinvolk 的 Flatcar Container Linux</li>
</ul>
</li>
<li>持续集成测试</li>
</ul>
<p>要选择最适合你的用例的工具，请阅读
<a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a> 和
<a href=/zh/docs/setup/production-environment/tools/kops/>kops</a> 之间的
<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md>这份比较</a>。
。</p>
<h2 id=创建集群>创建集群</h2>
<h3 id=1-5-满足下层设施要求>（1/5）满足下层设施要求</h3>
<p>按以下<a href=https://github.com/kubernetes-sigs/kubespray#requirements>要求</a>来配置服务器：</p>
<ul>
<li>在将运行 Ansible 命令的计算机上安装 Ansible v2.9 和 python-netaddr</li>
<li><strong>运行 Ansible Playbook 需要 Jinja 2.11（或更高版本）</strong></li>
<li>目标服务器必须有权访问 Internet 才能拉取 Docker 镜像。否则，
需要其他配置（<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md>请参见离线环境</a>）</li>
<li>目标服务器配置为允许 IPv4 转发</li>
<li><strong>你的 SSH 密钥必须复制</strong>到部署集群的所有服务器中</li>
<li><strong>防火墙不是由 kubespray 管理的</strong>。你需要根据需求设置适当的规则策略。为了避免部署过程中出现问题，可以禁用防火墙。</li>
<li>如果从非 root 用户帐户运行 kubespray，则应在目标服务器中配置正确的特权升级方法
并指定 <code>ansible_become</code> 标志或命令参数 <code>--become</code> 或 <code>-b</code></li>
</ul>
<p>Kubespray 提供以下实用程序来帮助你设置环境：</p>
<ul>
<li>为以下云驱动提供的 <a href=https://www.terraform.io/>Terraform</a> 脚本：</li>
<li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws>AWS</a></li>
<li><a href=http://sitebeskuethree/contrigetbernform/contribeskubernform/contribeskupernform/https/sitebesku/master/>OpenStack</a></li>
<li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/packet>Packet</a></li>
</ul>
<h3 id=2-5-编写清单文件>（2/5）编写清单文件</h3>
<p>设置服务器后，请创建一个
<a href=https://docs.ansible.com/ansible/latest/network/getting_started/first_inventory.html>Ansible 的清单文件</a>。
你可以手动执行此操作，也可以通过动态清单脚本执行此操作。有关更多信息，请参阅
“<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory>建立你自己的清单</a>”。</p>
<h3 id=3-5-规划集群部署>（3/5）规划集群部署</h3>
<p>Kubespray 能够自定义部署的许多方面：</p>
<ul>
<li>选择部署模式： kubeadm 或非 kubeadm</li>
<li>CNI（网络）插件</li>
<li>DNS 配置</li>
<li>控制平面的选择：本机/可执行文件或容器化</li>
<li>组件版本</li>
<li>Calico 路由反射器</li>
<li>组件运行时选项
<ul>
<li><a class=glossary-tooltip title="Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。" data-toggle=tooltip data-placement=top href=/zh/docs/reference/kubectl/docker-cli-to-kubectl/ target=_blank aria-label=Docker>Docker</a></li>
<li><a class=glossary-tooltip title=强调简单性、健壮性和可移植性的一种容器运行时 data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></li>
<li><a class=glossary-tooltip title="专用于 Kubernetes 的轻量级容器运行时软件" data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></li>
</ul>
</li>
<li>证书生成方式</li>
</ul>
<p>可以修改<a href=https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html>变量文件</a>
以进行 Kubespray 定制。
如果你刚刚开始使用 Kubespray，请考虑使用 Kubespray 默认设置来部署你的集群
并探索 Kubernetes 。</p>
<h3 id=4-5-部署集群>（4/5）部署集群</h3>
<p>接下来，部署你的集群：</p>
<p>使用 <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment>ansible-playbook</a>
进行集群部署。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --private-key<span style=color:#666>=</span>~/.ssh/private_key
</code></pre></div>
<p>大型部署（超过 100 个节点）可能需要
<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md>特定的调整</a>，
以获得最佳效果。</p>
<h3 id=5-5-验证部署>（5/5）验证部署</h3>
<p>Kubespray 提供了一种使用
<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md>Netchecker</a>
验证 Pod 间连接和 DNS 解析的方法。
Netchecker 确保 netchecker-agents Pods 可以解析 DNS 请求，
并在默认命名空间内对每个请求执行 ping 操作。
这些 Pod 模仿其他工作负载类似的行为，并用作集群运行状况指示器。</p>
<h2 id=集群操作>集群操作</h2>
<p>Kubespray 提供了其他 Playbooks 来管理集群： <em>scale</em> 和 <em>upgrade</em>。</p>
<h3 id=扩展集群>扩展集群</h3>
<p>你可以通过运行 scale playbook 向集群中添加工作节点。有关更多信息，
请参见 “<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes>添加节点</a>”。
你可以通过运行 remove-node playbook 来从集群中删除工作节点。有关更多信息，
请参见 “<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes>删除节点</a>”。</p>
<h3 id=升级集群>升级集群</h3>
<p>你可以通过运行 upgrade-cluster Playbook 来升级集群。有关更多信息，请参见
“<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md>升级</a>”。</p>
<h2 id=清理>清理</h2>
<p>你可以通过 <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml>reset</a> Playbook
重置节点并清除所有与 Kubespray 一起安装的组件。</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> 运行 reset playbook 时，请确保不要意外地将生产集群作为目标！
</div>
<h2 id=反馈>反馈</h2>
<ul>
<li>Slack 频道：<a href=https://kubernetes.slack.com/messages/kubespray/>#kubespray</a>
（你可以在<a href=https://slack.k8s.io/>此处</a>获得邀请）</li>
<li><a href=https://github.com/kubernetes-sigs/kubespray/issues>GitHub 问题</a></li>
</ul>
<h2 id=what-s-next>What's next</h2>
<p>查看有关 Kubespray 的
<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md>路线图</a>
的计划工作。</p>
</div>
</main>
</div>
</div>
<footer class=d-print-none>
<div class=footer__links>
<nav>
<a class=text-white href=/zh/docs/home/>主页</a>
<a class=text-white href=/zh/blog/>博客</a>
<a class=text-white href=/zh/training/>培训</a>
<a class=text-white href=/zh/partners/>合作伙伴</a>
<a class=text-white href=/zh/community/>社区</a>
<a class=text-white href=/zh/case-studies/>案例分析</a>
</nav>
</div>
<div class=container-fluid>
<div class=row>
<div class="col-6 col-sm-2 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list">
<a class=text-white target=_blank href=https://discuss.kubernetes.io>
<i class="fa fa-envelope"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter>
<a class=text-white target=_blank href=https://twitter.com/kubernetesio>
<i class="fab fa-twitter"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar>
<a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
<i class="fas fa-calendar-alt"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube>
<a class=text-white target=_blank href=https://youtube.com/kubernetescommunity>
<i class="fab fa-youtube"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank href=https://slack.k8s.io>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute>
<a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide>
<i class="fas fa-edit"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow">
<a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes>
<i class="fab fa-stack-overflow"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-8 text-center order-sm-2">
<small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small>
<br>
<small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small>
<br>
<small class=text-white>ICP license: 京ICP备17074266号-3</small>
</div>
</div>
</div>
</footer>
</div>
<script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script>
<script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script>
<script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script>
</body>
</html>