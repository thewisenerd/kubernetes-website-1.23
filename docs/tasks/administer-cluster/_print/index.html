<!doctype html><html lang=en class=no-js>
<head>
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JPP6RFM2BP"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JPP6RFM2BP')</script>
<link rel=alternate hreflang=zh href=https://kubernetes.io/zh/docs/tasks/administer-cluster/>
<link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/tasks/administer-cluster/>
<link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/tasks/administer-cluster/>
<link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/tasks/administer-cluster/>
<link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/tasks/administer-cluster/>
<link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/tasks/administer-cluster/>
<link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/tasks/administer-cluster/>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.87.0">
<link rel=canonical type=text/html href=https://kubernetes.io/docs/tasks/administer-cluster/>
<link rel="shortcut icon" type=image/png href=/images/favicon.png>
<link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=manifest href=/manifest.webmanifest>
<link rel=apple-touch-icon href=/images/kubernetes-192x192.png>
<title>Administer a Cluster | Kubernetes</title><meta property="og:title" content="Administer a Cluster">
<meta property="og:description" content="Learn common tasks for administering a cluster.">
<meta property="og:type" content="website">
<meta property="og:url" content="https://kubernetes.io/docs/tasks/administer-cluster/"><meta property="og:site_name" content="Kubernetes">
<meta itemprop=name content="Administer a Cluster">
<meta itemprop=description content="Learn common tasks for administering a cluster."><meta name=twitter:card content="summary">
<meta name=twitter:title content="Administer a Cluster">
<meta name=twitter:description content="Learn common tasks for administering a cluster.">
<link href=/scss/main.css rel=stylesheet>
<script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script>
<meta name=theme-color content="#326ce5">
<link rel=stylesheet href=/css/feature-states.css>
<meta name=description content="Learn common tasks for administering a cluster.">
<meta property="og:description" content="Learn common tasks for administering a cluster.">
<meta name=twitter:description content="Learn common tasks for administering a cluster.">
<meta property="og:url" content="https://kubernetes.io/docs/tasks/administer-cluster/">
<meta property="og:title" content="Administer a Cluster">
<meta name=twitter:title content="Administer a Cluster">
<meta name=twitter:image content="https://kubernetes.io/images/favicon.png">
<meta name=twitter:image:alt content="Kubernetes">
<meta property="og:image" content="/images/kubernetes-horizontal-color.png">
<meta property="og:type" content="article">
<script src=/js/script.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary>
<a class=navbar-brand href=/></a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-2 mb-lg-0">
<a class="nav-link active" href=/docs/>Documentation</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/blog/>Kubernetes Blog</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/training/>Training</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/partners/>Partners</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/community/>Community</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/case-studies/>Case Studies</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/tasks/administer-cluster/>v1.27</a>
<a class=dropdown-item href=https://v1-26.docs.kubernetes.io/docs/tasks/administer-cluster/>v1.26</a>
<a class=dropdown-item href=https://v1-25.docs.kubernetes.io/docs/tasks/administer-cluster/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/tasks/administer-cluster/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/tasks/administer-cluster/>v1.23</a>
</div>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
English
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/zh/docs/tasks/administer-cluster/>中文 Chinese</a>
<a class=dropdown-item href=/ko/docs/tasks/administer-cluster/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/tasks/administer-cluster/>日本語 Japanese</a>
<a class=dropdown-item href=/fr/docs/tasks/administer-cluster/>Français</a>
<a class=dropdown-item href=/de/docs/tasks/administer-cluster/>Deutsch</a>
<a class=dropdown-item href=/es/docs/tasks/administer-cluster/>Español</a>
<a class=dropdown-item href=/id/docs/tasks/administer-cluster/>Bahasa Indonesia</a>
</div>
</li>
</ul>
</div>
<button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.
</p><p>
<a href=/docs/tasks/administer-cluster/>Return to the regular view of this page</a>.
</p>
</div>
<h1 class=title>Administer a Cluster</h1>
<div class=lead>Learn common tasks for administering a cluster.</div>
<ul>
<li>1: <a href=#pg-8e16d69617b175d61e2e7a6e1642c9d6>Administration with kubeadm</a></li>
<ul>
<li>1.1: <a href=#pg-f62fba1de4084f3be070785757c8079c>Certificate Management with kubeadm</a></li>
<li>1.2: <a href=#pg-6134c5061298affa145ddb801b5c29da>Configuring a cgroup driver</a></li>
<li>1.3: <a href=#pg-98530eb3653d28fef34bff4543364aa7>Reconfiguring a kubeadm cluster</a></li>
<li>1.4: <a href=#pg-2e173356df5179cab9eec90a606f0aa4>Upgrading kubeadm clusters</a></li>
<li>1.5: <a href=#pg-9133578f1e75663bb031e5a377ca896d>Adding Windows nodes</a></li>
<li>1.6: <a href=#pg-e805c7d8d4ad6195cb82dbbc843bfc29>Upgrading Windows nodes</a></li>
</ul>
<li>2: <a href=#pg-adb6c52e773f4d890595e14a9251f59b>Migrating from dockershim</a></li>
<ul>
<li>2.1: <a href=#pg-b8acce0768c2f92cdb8eaa31e8072353>Changing the Container Runtime on a Node from Docker Engine to containerd</a></li>
<li>2.2: <a href=#pg-d81ef0973a7bb4813e1797a452864742>Migrate Docker Engine nodes from dockershim to cri-dockerd</a></li>
<li>2.3: <a href=#pg-d79db9ed1698f75ec5f2228987290e49>Find Out What Container Runtime is Used on a Node</a></li>
<li>2.4: <a href=#pg-58702e4818c09c9b3d574349c1a71cb3>Check whether Dockershim deprecation affects you</a></li>
<li>2.5: <a href=#pg-eb3e279a6c5e1224e744080a52ee3f28>Migrating telemetry and security agents from dockershim</a></li>
</ul>
<li>3: <a href=#pg-7743f043c43f7b12e8654e2227dbc658>Certificates</a></li>
<li>4: <a href=#pg-47be5dd51f686017f1766e6ec7aa6f41>Manage Memory, CPU, and API Resources</a></li>
<ul>
<li>4.1: <a href=#pg-337620c76587e4aeb32009cb23be46de>Configure Default Memory Requests and Limits for a Namespace</a></li>
<li>4.2: <a href=#pg-320af95e480962c538ebef7ae205845c>Configure Default CPU Requests and Limits for a Namespace</a></li>
<li>4.3: <a href=#pg-adb489b1ab985c9215657b0d4c6ae92b>Configure Minimum and Maximum Memory Constraints for a Namespace</a></li>
<li>4.4: <a href=#pg-a87cbd1f9379dac7a48ae320da68a9ad>Configure Minimum and Maximum CPU Constraints for a Namespace</a></li>
<li>4.5: <a href=#pg-fe3283559a3df299aae3ee00ecea2fad>Configure Memory and CPU Quotas for a Namespace</a></li>
<li>4.6: <a href=#pg-40e30a9209e0c9f4153707e43243e9d7>Configure a Pod Quota for a Namespace</a></li>
</ul>
<li>5: <a href=#pg-8c31aafd38fad5b0de0bd191758d6f93>Install a Network Policy Provider</a></li>
<ul>
<li>5.1: <a href=#pg-b4418905b0c14630e4e9cb1368241534>Use Antrea for NetworkPolicy</a></li>
<li>5.2: <a href=#pg-1239a77618c6278373832a142cd85519>Use Calico for NetworkPolicy</a></li>
<li>5.3: <a href=#pg-95039241255a31df196beaa405b68eba>Use Cilium for NetworkPolicy</a></li>
<li>5.4: <a href=#pg-505a0a6a7e6eff361bbb3be81c84b2e0>Use Kube-router for NetworkPolicy</a></li>
<li>5.5: <a href=#pg-2842eac98aa0e229a5c6755c4c83d2a7>Romana for NetworkPolicy</a></li>
<li>5.6: <a href=#pg-ac075c3fdfd0d41aa753cc70e42be064>Weave Net for NetworkPolicy</a></li>
</ul>
<li>6: <a href=#pg-e77685d5b88d2db5c7631a27b9472eea>Access Clusters Using the Kubernetes API</a></li>
<li>7: <a href=#pg-a8f6511197efcd7d0db80ade49620f9d>Advertise Extended Resources for a Node</a></li>
<li>8: <a href=#pg-966cd1cc69c69410d8698b3ac74abce2>Autoscale the DNS Service in a Cluster</a></li>
<li>9: <a href=#pg-2bffd7f3571cdd609bd97fb2e1bdb2fe>Change the default StorageClass</a></li>
<li>10: <a href=#pg-fbc9136f53eccd6eb8c80f4bbea3b8f4>Change the Reclaim Policy of a PersistentVolume</a></li>
<li>11: <a href=#pg-ce4cd28c8feb9faa783e79b48af37961>Cloud Controller Manager Administration</a></li>
<li>12: <a href=#pg-5e59f5575dce11fdaed640afdbeedfc1>Configure Quotas for API Objects</a></li>
<li>13: <a href=#pg-7127e6b7344b315b30b1ce8c4d8bfc55>Control CPU Management Policies on the Node</a></li>
<li>14: <a href=#pg-8060aed5bf1172fa62199a4c306a4cd1>Control Topology Management Policies on a node</a></li>
<li>15: <a href=#pg-3d0cd7d2f13d4759094f281504cf57b8>Customizing DNS Service</a></li>
<li>16: <a href=#pg-8bcf4aeb5bbb6d6969a146e5ab97557b>Debugging DNS Resolution</a></li>
<li>17: <a href=#pg-a3790dfb57271d13517e549dffa805b9>Declare Network Policy</a></li>
<li>18: <a href=#pg-9585dc0efb0450fd68728e7511754717>Developing Cloud Controller Manager</a></li>
<li>19: <a href=#pg-09cc2cf3e0f23a3996e6cb31dc4d867c>Enable Or Disable A Kubernetes API</a></li>
<li>20: <a href=#pg-00733cc3747eb3f5fe1c9e0439262967>Enabling Service Topology</a></li>
<li>21: <a href=#pg-6b4e7ca6586f448c8533a120c29bdd25>Encrypting Secret Data at Rest</a></li>
<li>22: <a href=#pg-4a02bcca41439e16655f43fa37c81da4>Guaranteed Scheduling For Critical Add-On Pods</a></li>
<li>23: <a href=#pg-b45f024608e1b367cdacb1fd9d77278a>IP Masquerade Agent User Guide</a></li>
<li>24: <a href=#pg-a02f35804917d7a269c38d7e2c475005>Limit Storage Consumption</a></li>
<li>25: <a href=#pg-a24171610b6ea75a142cb9c8c7882390>Migrate Replicated Control Plane To Use Cloud Controller Manager</a></li>
<li>26: <a href=#pg-56de8c25b1486599777034111645b803>Namespaces Walkthrough</a></li>
<li>27: <a href=#pg-c4d0832845adc92b7ccd54aed63fc932>Operating etcd clusters for Kubernetes</a></li>
<li>28: <a href=#pg-eec61e72c300dbfbf7302400ca966432>Reconfigure a Node's Kubelet in a Live Cluster</a></li>
<li>29: <a href=#pg-b64a1d2bb3f4ed9f7021134e09a75c36>Reserve Compute Resources for System Daemons</a></li>
<li>30: <a href=#pg-f6f3b8f9789fda4286bf410b8e108f69>Running Kubernetes Node Components as a Non-root User</a></li>
<li>31: <a href=#pg-b35b8ddb9bbc15620ce9636f4346c05c>Safely Drain a Node</a></li>
<li>32: <a href=#pg-12001be83d15fcd7f3242313a55777df>Securing a Cluster</a></li>
<li>33: <a href=#pg-f58763cc9447491b6c40f939a02d441d>Set Kubelet parameters via a config file</a></li>
<li>34: <a href=#pg-1e966f5d0540bbee0876f9d0d08d54dc>Share a Cluster with Namespaces</a></li>
<li>35: <a href=#pg-fe6b50655c29ab0b7c1ee549ff64c138>Upgrade A Cluster</a></li>
<li>36: <a href=#pg-4e9de5bc3973e5d2bb8f09ff940c3319>Use Cascading Deletion in a Cluster</a></li>
<li>37: <a href=#pg-669c88964b4a9eb2b040057266e4b60d>Using a KMS provider for data encryption</a></li>
<li>38: <a href=#pg-e1afcdac8d5e8458274b3c481c5ebcda>Using CoreDNS for Service Discovery</a></li>
<li>39: <a href=#pg-9ceed97f912df7289ed8872e290cfbad>Using NodeLocal DNSCache in Kubernetes clusters</a></li>
<li>40: <a href=#pg-fe5ad73163d38596340536ec03a205f0>Using sysctls in a Kubernetes Cluster</a></li>
<li>41: <a href=#pg-778055e4a4415ca195169b42cd42ddf9>Utilizing the NUMA-aware Memory Manager</a></li>
</ul>
<div class=content>
</div>
</div>
<div class=td-content>
<h1 id=pg-8e16d69617b175d61e2e7a6e1642c9d6>1 - Administration with kubeadm</h1>
</div>
<div class=td-content>
<h1 id=pg-f62fba1de4084f3be070785757c8079c>1.1 - Certificate Management with kubeadm</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.15 [stable]</code>
</div>
<p>Client certificates generated by <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a> expire after 1 year.
This page explains how to manage certificate renewals with kubeadm. It also covers other tasks related
to kubeadm certificate management.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You should be familiar with <a href=/docs/setup/best-practices/certificates/>PKI certificates and requirements in Kubernetes</a>.</p>
<h2 id=custom-certificates>Using custom certificates</h2>
<p>By default, kubeadm generates all the certificates needed for a cluster to run.
You can override this behavior by providing your own certificates.</p>
<p>To do so, you must place them in whatever directory is specified by the
<code>--cert-dir</code> flag or the <code>certificatesDir</code> field of kubeadm's <code>ClusterConfiguration</code>.
By default this is <code>/etc/kubernetes/pki</code>.</p>
<p>If a given certificate and private key pair exists before running <code>kubeadm init</code>,
kubeadm does not overwrite them. This means you can, for example, copy an existing
CA into <code>/etc/kubernetes/pki/ca.crt</code> and <code>/etc/kubernetes/pki/ca.key</code>,
and kubeadm will use this CA for signing the rest of the certificates.</p>
<h2 id=external-ca-mode>External CA mode</h2>
<p>It is also possible to provide only the <code>ca.crt</code> file and not the
<code>ca.key</code> file (this is only available for the root CA file, not other cert pairs).
If all other certificates and kubeconfig files are in place, kubeadm recognizes
this condition and activates the "External CA" mode. kubeadm will proceed without the
CA key on disk.</p>
<p>Instead, run the controller-manager standalone with <code>--controllers=csrsigner</code> and
point to the CA certificate and key.</p>
<p><a href=/docs/setup/best-practices/certificates/>PKI certificates and requirements</a> includes guidance on
setting up a cluster to use an external CA.</p>
<h2 id=check-certificate-expiration>Check certificate expiration</h2>
<p>You can use the <code>check-expiration</code> subcommand to check when certificates expire:</p>
<pre><code>kubeadm certs check-expiration
</code></pre><p>The output is similar to this:</p>
<pre><code>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Dec 30, 2020 23:36 UTC   364d                                    no
apiserver                  Dec 30, 2020 23:36 UTC   364d            ca                      no
apiserver-etcd-client      Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
apiserver-kubelet-client   Dec 30, 2020 23:36 UTC   364d            ca                      no
controller-manager.conf    Dec 30, 2020 23:36 UTC   364d                                    no
etcd-healthcheck-client    Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
etcd-peer                  Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
etcd-server                Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
front-proxy-client         Dec 30, 2020 23:36 UTC   364d            front-proxy-ca          no
scheduler.conf             Dec 30, 2020 23:36 UTC   364d                                    no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Dec 28, 2029 23:36 UTC   9y              no
etcd-ca                 Dec 28, 2029 23:36 UTC   9y              no
front-proxy-ca          Dec 28, 2029 23:36 UTC   9y              no
</code></pre><p>The command shows expiration/residual time for the client certificates in the <code>/etc/kubernetes/pki</code> folder and for the client certificate embedded in the KUBECONFIG files used by kubeadm (<code>admin.conf</code>, <code>controller-manager.conf</code> and <code>scheduler.conf</code>).</p>
<p>Additionally, kubeadm informs the user if the certificate is externally managed; in this case, the user should take care of managing certificate renewal manually/using other tools.</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> <code>kubeadm</code> cannot manage certificates signed by an external CA.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <code>kubelet.conf</code> is not included in the list above because kubeadm configures kubelet
for <a href=/docs/tasks/tls/certificate-rotation/>automatic certificate renewal</a>
with rotatable certificates under <code>/var/lib/kubelet/pki</code>.
To repair an expired kubelet client certificate see
<a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert>Kubelet client certificate rotation fails</a>.
</div>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> <p>On nodes created with <code>kubeadm init</code>, prior to kubeadm version 1.17, there is a
<a href=https://github.com/kubernetes/kubeadm/issues/1753>bug</a> where you manually have to modify the contents of <code>kubelet.conf</code>. After <code>kubeadm init</code> finishes, you should update <code>kubelet.conf</code> to point to the
rotated kubelet client certificates, by replacing <code>client-certificate-data</code> and <code>client-key-data</code> with:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>client-certificate</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>client-key</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style=color:#bbb>
</span></code></pre></div>
</div>
<h2 id=automatic-certificate-renewal>Automatic certificate renewal</h2>
<p>kubeadm renews all the certificates during control plane <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>upgrade</a>.</p>
<p>This feature is designed for addressing the simplest use cases;
if you don't have specific requirements on certificate renewal and perform Kubernetes version upgrades regularly (less than 1 year in between each upgrade), kubeadm will take care of keeping your cluster up to date and reasonably secure.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> It is a best practice to upgrade your cluster frequently in order to stay secure.
</div>
<p>If you have more complex requirements for certificate renewal, you can opt out from the default behavior by passing <code>--certificate-renewal=false</code> to <code>kubeadm upgrade apply</code> or to <code>kubeadm upgrade node</code>.</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> Prior to kubeadm version 1.17 there is a <a href=https://github.com/kubernetes/kubeadm/issues/1818>bug</a>
where the default value for <code>--certificate-renewal</code> is <code>false</code> for the <code>kubeadm upgrade node</code>
command. In that case, you should explicitly set <code>--certificate-renewal=true</code>.
</div>
<h2 id=manual-certificate-renewal>Manual certificate renewal</h2>
<p>You can renew your certificates manually at any time with the <code>kubeadm certs renew</code> command.</p>
<p>This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in <code>/etc/kubernetes/pki</code>.</p>
<p>After running the command you should restart the control plane Pods. This is required since
dynamic certificate reload is currently not supported for all components and certificates.
<a href=/docs/tasks/configure-pod-container/static-pod/>Static Pods</a> are managed by the local kubelet
and not by the API Server, thus kubectl cannot be used to delete and restart them.
To restart a static Pod you can temporarily remove its manifest file from <code>/etc/kubernetes/manifests/</code>
and wait for 20 seconds (see the <code>fileCheckFrequency</code> value in <a href=/docs/reference/config-api/kubelet-config.v1beta1/>KubeletConfiguration struct</a>.
The kubelet will terminate the Pod if it's no longer in the manifest directory.
You can then move the file back and after another <code>fileCheckFrequency</code> period, the kubelet will recreate
the Pod and the certificate renewal for the component can complete.</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> If you are running an HA cluster, this command needs to be executed on all the control-plane nodes.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <code>certs renew</code> uses the existing certificates as the authoritative source for attributes (Common Name, Organization, SAN, etc.) instead of the kubeadm-config ConfigMap. It is strongly recommended to keep them both in sync.
</div>
<p><code>kubeadm certs renew</code> provides the following options:</p>
<p>The Kubernetes certificates normally reach their expiration date after one year.</p>
<ul>
<li>
<p><code>--csr-only</code> can be used to renew certificates with an external CA by generating certificate signing requests (without actually renewing certificates in place); see next paragraph for more information.</p>
</li>
<li>
<p>It's also possible to renew a single certificate instead of all.</p>
</li>
</ul>
<h2 id=renew-certificates-with-the-kubernetes-certificates-api>Renew certificates with the Kubernetes certificates API</h2>
<p>This section provides more details about how to execute manual certificate renewal using the Kubernetes certificates API.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> These are advanced topics for users who need to integrate their organization's certificate infrastructure into a kubeadm-built cluster. If the default kubeadm configuration satisfies your needs, you should let kubeadm manage certificates instead.
</div>
<h3 id=set-up-a-signer>Set up a signer</h3>
<p>The Kubernetes Certificate Authority does not work out of the box.
You can configure an external signer such as <a href=https://cert-manager.io/docs/configuration/ca/>cert-manager</a>, or you can use the built-in signer.</p>
<p>The built-in signer is part of <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/><code>kube-controller-manager</code></a>.</p>
<p>To activate the built-in signer, you must pass the <code>--cluster-signing-cert-file</code> and <code>--cluster-signing-key-file</code> flags.</p>
<p>If you're creating a new cluster, you can use a kubeadm <a href=https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta3>configuration file</a>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-cert-file</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/ca.crt<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/ca.key<span style=color:#bbb>
</span></code></pre></div><h3 id=create-certificate-signing-requests-csr>Create certificate signing requests (CSR)</h3>
<p>See <a href=/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatesigningrequest>Create CertificateSigningRequest</a> for creating CSRs with the Kubernetes API.</p>
<h2 id=renew-certificates-with-external-ca>Renew certificates with external CA</h2>
<p>This section provide more details about how to execute manual certificate renewal using an external CA.</p>
<p>To better integrate with external CAs, kubeadm can also produce certificate signing requests (CSRs).
A CSR represents a request to a CA for a signed certificate for a client.
In kubeadm terms, any certificate that would normally be signed by an on-disk CA can be produced as a CSR instead. A CA, however, cannot be produced as a CSR.</p>
<h3 id=create-certificate-signing-requests-csr-1>Create certificate signing requests (CSR)</h3>
<p>You can create certificate signing requests with <code>kubeadm certs renew --csr-only</code>.</p>
<p>Both the CSR and the accompanying private key are given in the output.
You can pass in a directory with <code>--csr-dir</code> to output the CSRs to the specified location.
If <code>--csr-dir</code> is not specified, the default certificate directory (<code>/etc/kubernetes/pki</code>) is used.</p>
<p>Certificates can be renewed with <code>kubeadm certs renew --csr-only</code>.
As with <code>kubeadm init</code>, an output directory can be specified with the <code>--csr-dir</code> flag.</p>
<p>A CSR contains a certificate's name, domains, and IPs, but it does not specify usages.
It is the responsibility of the CA to specify <a href=/docs/setup/best-practices/certificates/#all-certificates>the correct cert usages</a>
when issuing a certificate.</p>
<ul>
<li>In <code>openssl</code> this is done with the
<a href=https://superuser.com/questions/738612/openssl-ca-keyusage-extension><code>openssl ca</code> command</a>.</li>
<li>In <code>cfssl</code> you specify
<a href=https://github.com/cloudflare/cfssl/blob/master/doc/cmd/cfssl.txt#L170>usages in the config file</a>.</li>
</ul>
<p>After a certificate is signed using your preferred method, the certificate and the private key must be copied to the PKI directory (by default <code>/etc/kubernetes/pki</code>).</p>
<h2 id=certificate-authority-rotation>Certificate authority (CA) rotation</h2>
<p>Kubeadm does not support rotation or replacement of CA certificates out of the box.</p>
<p>For more information about manual rotation or replacement of CA, see <a href=/docs/tasks/tls/manual-rotation-of-ca-certificates/>manual rotation of CA certificates</a>.</p>
<h2 id=kubelet-serving-certs>Enabling signed kubelet serving certificates</h2>
<p>By default the kubelet serving certificate deployed by kubeadm is self-signed.
This means a connection from external services like the
<a href=https://github.com/kubernetes-sigs/metrics-server>metrics-server</a> to a
kubelet cannot be secured with TLS.</p>
<p>To configure the kubelets in a new kubeadm cluster to obtain properly signed serving
certificates you must pass the following minimal configuration to <code>kubeadm init</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>serverTLSBootstrap</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></code></pre></div><p>If you have already created the cluster you must adapt it by doing the following:</p>
<ul>
<li>Find and edit the <code>kubelet-config-1.23</code> ConfigMap in the <code>kube-system</code> namespace.
In that ConfigMap, the <code>kubelet</code> key has a
<a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>KubeletConfiguration</a>
document as its value. Edit the KubeletConfiguration document to set <code>serverTLSBootstrap: true</code>.</li>
<li>On each node, add the <code>serverTLSBootstrap: true</code> field in <code>/var/lib/kubelet/config.yaml</code>
and restart the kubelet with <code>systemctl restart kubelet</code></li>
</ul>
<p>The field <code>serverTLSBootstrap: true</code> will enable the bootstrap of kubelet serving
certificates by requesting them from the <code>certificates.k8s.io</code> API. One known limitation
is that the CSRs (Certificate Signing Requests) for these certificates cannot be automatically
approved by the default signer in the kube-controller-manager -
<a href=/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers><code>kubernetes.io/kubelet-serving</code></a>.
This will require action from the user or a third party controller.</p>
<p>These CSRs can be viewed using:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get csr
NAME        AGE     SIGNERNAME                        REQUESTOR                      CONDITION
csr-9wvgt   112s    kubernetes.io/kubelet-serving     system:node:worker-1           Pending
csr-lz97v   1m58s   kubernetes.io/kubelet-serving     system:node:control-plane-1    Pending
</code></pre></div><p>To approve them you can do the following:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl certificate approve &lt;CSR-name&gt;
</code></pre></div><p>By default, these serving certificate will expire after one year. Kubeadm sets the
<code>KubeletConfiguration</code> field <code>rotateCertificates</code> to <code>true</code>, which means that close
to expiration a new set of CSRs for the serving certificates will be created and must
be approved to complete the rotation. To understand more see
<a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#certificate-rotation>Certificate Rotation</a>.</p>
<p>If you are looking for a solution for automatic approval of these CSRs it is recommended
that you contact your cloud provider and ask if they have a CSR signer that verifies
the node identity with an out of band mechanism.</p>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div>
<p>Third party custom controllers can be used:</p>
<ul>
<li><a href=https://github.com/postfinance/kubelet-csr-approver>kubelet-csr-approver</a></li>
</ul>
<p>Such a controller is not a secure mechanism unless it not only verifies the CommonName
in the CSR but also verifies the requested IPs and domain names. This would prevent
a malicious actor that has access to a kubelet client certificate to create
CSRs requesting serving certificates for any IP or domain name.</p>
<h2 id=kubeconfig-additional-users>Generating kubeconfig files for additional users</h2>
<p>During cluster creation, kubeadm signs the certificate in the <code>admin.conf</code> to have
<code>Subject: O = system:masters, CN = kubernetes-admin</code>.
<a href=/docs/reference/access-authn-authz/rbac/#user-facing-roles><code>system:masters</code></a>
is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC).
Sharing the <code>admin.conf</code> with additional users is <strong>not recommended</strong>!</p>
<p>Instead, you can use the <a href=/docs/reference/setup-tools/kubeadm/kubeadm-kubeconfig><code>kubeadm kubeconfig user</code></a>
command to generate kubeconfig files for additional users.
The command accepts a mixture of command line flags and
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/>kubeadm configuration</a> options.
The generated kubeconfig will be written to stdout and can be piped to a file
using <code>kubeadm kubeconfig user ... > somefile.conf</code>.</p>
<p>Example configuration file that can be used with <code>--config</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#080;font-style:italic># example.yaml</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Will be used as the target &#34;cluster&#34; in the kubeconfig</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Will be used as the &#34;server&#34; (IP or DNS name) of this cluster in the kubeconfig</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controlPlaneEndpoint</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;some-dns-address:6443&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># The cluster CA key and certificate will be loaded from this local directory</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>certificatesDir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/pki&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Make sure that these settings match the desired target cluster settings.
To see the settings of an existing cluster use:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get cm kubeadm-config -n kube-system -o<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#34;{.data.ClusterConfiguration}&#34;</span>
</code></pre></div><p>The following example will generate a kubeconfig file with credentials valid for 24 hours
for a new user <code>johndoe</code> that is part of the <code>appdevs</code> group:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm kubeconfig user --config example.yaml --org appdevs --client-name johndoe --validity-period 24h
</code></pre></div><p>The following example will generate a kubeconfig file with administrator credentials valid for 1 week:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm kubeconfig user --config example.yaml --client-name admin --validity-period 168h
</code></pre></div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-6134c5061298affa145ddb801b5c29da>1.2 - Configuring a cgroup driver</h1>
<p>This page explains how to configure the kubelet cgroup driver to match the container
runtime cgroup driver for kubeadm clusters.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You should be familiar with the Kubernetes
<a href=/docs/setup/production-environment/container-runtimes>container runtime requirements</a>.</p>
<h2 id=configuring-the-container-runtime-cgroup-driver>Configuring the container runtime cgroup driver</h2>
<p>The <a href=/docs/setup/production-environment/container-runtimes>Container runtimes</a> page
explains that the <code>systemd</code> driver is recommended for kubeadm based setups instead
of the <code>cgroupfs</code> driver, because kubeadm manages the kubelet as a systemd service.</p>
<p>The page also provides details on how to setup a number of different container runtimes with the
<code>systemd</code> driver by default.</p>
<h2 id=configuring-the-kubelet-cgroup-driver>Configuring the kubelet cgroup driver</h2>
<p>kubeadm allows you to pass a <code>KubeletConfiguration</code> structure during <code>kubeadm init</code>.
This <code>KubeletConfiguration</code> can include the <code>cgroupDriver</code> field which controls the cgroup
driver of the kubelet.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> In v1.22, if the user is not setting the <code>cgroupDriver</code> field under <code>KubeletConfiguration</code>,
<code>kubeadm</code> will default it to <code>systemd</code>.
</div>
<p>A minimal example of configuring the field explicitly:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#080;font-style:italic># kubeadm-config.yaml</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.21.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>cgroupDriver</span>:<span style=color:#bbb> </span>systemd<span style=color:#bbb>
</span></code></pre></div><p>Such a configuration file can then be passed to the kubeadm command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm init --config kubeadm-config.yaml
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>Kubeadm uses the same <code>KubeletConfiguration</code> for all nodes in the cluster.
The <code>KubeletConfiguration</code> is stored in a <a href=/docs/concepts/configuration/configmap>ConfigMap</a>
object under the <code>kube-system</code> namespace.</p>
<p>Executing the sub commands <code>init</code>, <code>join</code> and <code>upgrade</code> would result in kubeadm
writing the <code>KubeletConfiguration</code> as a file under <code>/var/lib/kubelet/config.yaml</code>
and passing it to the local node kubelet.</p>
</div>
<h2 id=using-the-cgroupfs-driver>Using the <code>cgroupfs</code> driver</h2>
<p>As this guide explains using the <code>cgroupfs</code> driver with kubeadm is not recommended.</p>
<p>To continue using <code>cgroupfs</code> and to prevent <code>kubeadm upgrade</code> from modifying the
<code>KubeletConfiguration</code> cgroup driver on existing setups, you must be explicit
about its value. This applies to a case where you do not wish future versions
of kubeadm to apply the <code>systemd</code> driver by default.</p>
<p>See the below section on "Modify the kubelet ConfigMap" for details on
how to be explicit about the value.</p>
<p>If you wish to configure a container runtime to use the <code>cgroupfs</code> driver,
you must refer to the documentation of the container runtime of your choice.</p>
<h2 id=migrating-to-the-systemd-driver>Migrating to the <code>systemd</code> driver</h2>
<p>To change the cgroup driver of an existing kubeadm cluster to <code>systemd</code> in-place,
a similar procedure to a kubelet upgrade is required. This must include both
steps outlined below.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Alternatively, it is possible to replace the old nodes in the cluster with new ones
that use the <code>systemd</code> driver. This requires executing only the first step below
before joining the new nodes and ensuring the workloads can safely move to the new
nodes before deleting the old nodes.
</div>
<h3 id=modify-the-kubelet-configmap>Modify the kubelet ConfigMap</h3>
<ul>
<li>
<p>Find the kubelet ConfigMap name using <code>kubectl get cm -n kube-system | grep kubelet-config</code>.</p>
</li>
<li>
<p>Call <code>kubectl edit cm kubelet-config-x.yy -n kube-system</code> (replace <code>x.yy</code> with
the Kubernetes version).</p>
</li>
<li>
<p>Either modify the existing <code>cgroupDriver</code> value or add a new field that looks like this:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>cgroupDriver</span>:<span style=color:#bbb> </span>systemd<span style=color:#bbb>
</span></code></pre></div><p>This field must be present under the <code>kubelet:</code> section of the ConfigMap.</p>
</li>
</ul>
<h3 id=update-the-cgroup-driver-on-all-nodes>Update the cgroup driver on all nodes</h3>
<p>For each node in the cluster:</p>
<ul>
<li><a href=/docs/tasks/administer-cluster/safely-drain-node>Drain the node</a> using <code>kubectl drain &lt;node-name> --ignore-daemonsets</code></li>
<li>Stop the kubelet using <code>systemctl stop kubelet</code></li>
<li>Stop the container runtime</li>
<li>Modify the container runtime cgroup driver to <code>systemd</code></li>
<li>Set <code>cgroupDriver: systemd</code> in <code>/var/lib/kubelet/config.yaml</code></li>
<li>Start the container runtime</li>
<li>Start the kubelet using <code>systemctl start kubelet</code></li>
<li><a href=/docs/tasks/administer-cluster/safely-drain-node>Uncordon the node</a> using <code>kubectl uncordon &lt;node-name></code></li>
</ul>
<p>Execute these steps on nodes one at a time to ensure workloads
have sufficient time to schedule on different nodes.</p>
<p>Once the process is complete ensure that all nodes and workloads are healthy.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-98530eb3653d28fef34bff4543364aa7>1.3 - Reconfiguring a kubeadm cluster</h1>
<p>kubeadm does not support automated ways of reconfiguring components that
were deployed on managed nodes. One way of automating this would be
by using a custom <a href=/docs/concepts/extend-kubernetes/operator/>operator</a>.</p>
<p>To modify the components configuration you must manually edit associated cluster
objects and files on disk.</p>
<p>This guide shows the correct sequence of steps that need to be performed
to achieve kubeadm cluster reconfiguration.</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>You need a cluster that was deployed using kubeadm</li>
<li>Have administrator credentials (<code>/etc/kubernetes/admin.conf</code>) and network connectivity
to a running kube-apiserver in the cluster from a host that has kubectl installed</li>
<li>Have a text editor installed on all hosts</li>
</ul>
<h2 id=reconfiguring-the-cluster>Reconfiguring the cluster</h2>
<p>kubeadm writes a set of cluster wide component configuration options in
ConfigMaps and other objects. These objects must be manually edited. The command <code>kubectl edit</code>
can be used for that.</p>
<p>The <code>kubectl edit</code> command will open a text editor where you can edit and save the object directly.</p>
<p>You can use the environment variables <code>KUBECONFIG</code> and <code>KUBE_EDITOR</code> to specify the location of
the kubectl consumed kubeconfig file and preferred text editor.</p>
<p>For example:</p>
<pre><code>KUBECONFIG=/etc/kubernetes/admin.conf KUBE_EDITOR=nano kubectl edit &lt;parameters&gt;
</code></pre><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Upon saving any changes to these cluster objects, components running on nodes may not be
automatically updated. The steps below instruct you on how to perform that manually.
</div>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> Component configuration in ConfigMaps is stored as unstructured data (YAML string).
This means that validation will not be performed upon updating the contents of a ConfigMap.
You have to be careful to follow the documented API format for a particular
component configuration and avoid introducing typos and YAML indentation mistakes.
</div>
<h3 id=applying-cluster-configuration-changes>Applying cluster configuration changes</h3>
<h4 id=updating-the-clusterconfiguration>Updating the <code>ClusterConfiguration</code></h4>
<p>During cluster creation and upgrade, kubeadm writes its
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/><code>ClusterConfiguration</code></a>
in a ConfigMap called <code>kubeadm-config</code> in the <code>kube-system</code> namespace.</p>
<p>To change a particular option in the <code>ClusterConfiguration</code> you can edit the ConfigMap with this command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit cm -n kube-system kubeadm-config
</code></pre></div><p>The configuration is located under the <code>data.ClusterConfiguration</code> key.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The <code>ClusterConfiguration</code> includes a variety of options that affect the configuration of individual
components such as kube-apiserver, kube-scheduler, kube-controller-manager, CoreDNS, etcd and kube-proxy.
Changes to the configuration must be reflected on node components manually.
</div>
<h4 id=reflecting-clusterconfiguration-changes-on-control-plane-nodes>Reflecting <code>ClusterConfiguration</code> changes on control plane nodes</h4>
<p>kubeadm manages the control plane components as static Pod manifests located in
the directory <code>/etc/kubernetes/manifests</code>.
Any changes to the <code>ClusterConfiguration</code> under the <code>apiServer</code>, <code>controllerManager</code>, <code>scheduler</code> or <code>etcd</code>
keys must be reflected in the associated files in the manifests directory on a control plane node.</p>
<p>Such changes may include:</p>
<ul>
<li><code>extraArgs</code> - requires updating the list of flags passed to a component container</li>
<li><code>extraMounts</code> - requires updated the volume mounts for a component container</li>
<li><code>*SANs</code> - requires writing new certificates with updated Subject Alternative Names.</li>
</ul>
<p>Before proceeding with these changes, make sure you have backed up the directory <code>/etc/kubernetes/</code>.</p>
<p>To write new certificates you can use:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm init phase certs &lt;component-name&gt; --config &lt;config-file&gt;
</code></pre></div><p>To write new manifest files in <code>/etc/kubernetes/manifests</code> you can use:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm init phase control-plane &lt;component-name&gt; --config &lt;config-file&gt;
</code></pre></div><p>The <code>&lt;config-file></code> contents must match the updated <code>ClusterConfiguration</code>.
The <code>&lt;component-name></code> value must be the name of the component.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Updating a file in <code>/etc/kubernetes/manifests</code> will tell the kubelet to restart the static Pod for the corresponding component.
Try doing these changes one node at a time to leave the cluster without downtime.
</div>
<h3 id=applying-kubelet-configuration-changes>Applying kubelet configuration changes</h3>
<h4 id=updating-the-kubeletconfiguration>Updating the <code>KubeletConfiguration</code></h4>
<p>During cluster creation and upgrade, kubeadm writes its
<a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>
in a ConfigMap called <code>kubelet-config</code> in the <code>kube-system</code> namespace.</p>
<p>You can edit the ConfigMap with this command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit cm -n kube-system kubelet-config
</code></pre></div><p>The configuration is located under the <code>data.kubelet</code> key.</p>
<h4 id=reflecting-the-kubelet-changes>Reflecting the kubelet changes</h4>
<p>To reflect the change on kubeadm nodes you must do the following:</p>
<ul>
<li>Log in to a kubeadm node</li>
<li>Run <code>kubeadm upgrade node phase kubelet-config</code> to download the latest <code>kubelet-config</code>
ConfigMap contents into the local file <code>/var/lib/kubelet/config.conf</code></li>
<li>Edit the file <code>/var/lib/kubelet/kubeadm-flags.env</code> to apply additional configuration with
flags</li>
<li>Restart the kubelet service with <code>systemctl restart kubelet</code></li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Do these changes one node at a time to allow workloads to be rescheduled properly.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> During <code>kubeadm upgrade</code>, kubeadm downloads the <code>KubeletConfiguration</code> from the
<code>kubelet-config</code> ConfigMap and overwrite the contents of <code>/var/lib/kubelet/config.conf</code>.
This means that node local configuration must be applied either by flags in
<code>/var/lib/kubelet/kubeadm-flags.env</code> or by manually updating the contents of
<code>/var/lib/kubelet/config.conf</code> after <code>kubeadm upgrade</code>, and then restarting the kubelet.
</div>
<h3 id=applying-kube-proxy-configuration-changes>Applying kube-proxy configuration changes</h3>
<h4 id=updating-the-kubeproxyconfiguration>Updating the <code>KubeProxyConfiguration</code></h4>
<p>During cluster creation and upgrade, kubeadm writes its
<a href=/docs/reference/config-api/kube-proxy-config.v1alpha1/><code>KubeProxyConfiguration</code></a>
in a ConfigMap in the <code>kube-system</code> namespace called <code>kube-proxy</code>.</p>
<p>This ConfigMap is used by the <code>kube-proxy</code> DaemonSet in the <code>kube-system</code> namespace.</p>
<p>To change a particular option in the <code>KubeProxyConfiguration</code>, you can edit the ConfigMap with this command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit cm -n kube-system kube-proxy
</code></pre></div><p>The configuration is located under the <code>data.config.conf</code> key.</p>
<h4 id=reflecting-the-kube-proxy-changes>Reflecting the kube-proxy changes</h4>
<p>Once the <code>kube-proxy</code> ConfigMap is updated, you can restart all kube-proxy Pods:</p>
<p>Obtain the Pod names:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get po -n kube-system | grep kube-proxy
</code></pre></div><p>Delete a Pod with:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete po -n kube-system &lt;pod-name&gt;
</code></pre></div><p>New Pods that use the updated ConfigMap will be created.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Because kubeadm deploys kube-proxy as a DaemonSet, node specific configuration is unsupported.
</div>
<h3 id=applying-coredns-configuration-changes>Applying CoreDNS configuration changes</h3>
<h4 id=updating-the-coredns-deployment-and-service>Updating the CoreDNS Deployment and Service</h4>
<p>kubeadm deploys CoreDNS as a Deployment called <code>coredns</code> and with a Service <code>kube-dns</code>,
both in the <code>kube-system</code> namespace.</p>
<p>To update any of the CoreDNS settings, you can edit the Deployment and
Service objects:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit deployment -n kube-system coredns
kubectl edit service -n kube-system kube-dns
</code></pre></div><h4 id=reflecting-the-coredns-changes>Reflecting the CoreDNS changes</h4>
<p>Once the CoreDNS changes are applied you can delete the CoreDNS Pods:</p>
<p>Obtain the Pod names:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get po -n kube-system | grep coredns
</code></pre></div><p>Delete a Pod with:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete po -n kube-system &lt;pod-name&gt;
</code></pre></div><p>New Pods with the updated CoreDNS configuration will be created.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> kubeadm does not allow CoreDNS configuration during cluster creation and upgrade.
This means that if you execute <code>kubeadm upgrade apply</code>, your changes to the CoreDNS
objects will be lost and must be reapplied.
</div>
<h2 id=persisting-the-reconfiguration>Persisting the reconfiguration</h2>
<p>During the execution of <code>kubeadm upgrade</code> on a managed node, kubeadm might overwrite configuration
that was applied after the cluster was created (reconfiguration).</p>
<h3 id=persisting-node-object-reconfiguration>Persisting Node object reconfiguration</h3>
<p>kubeadm writes Labels, Taints, CRI socket and other information on the Node object for a particular
Kubernetes node. To change any of the contents of this Node object you can use:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit no &lt;node-name&gt;
</code></pre></div><p>During <code>kubeadm upgrade</code> the contents of such a Node might get overwritten.
If you would like to persist your modifications to the Node object after upgrade,
you can prepare a <a href=/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/>kubectl patch</a>
and apply it to the Node object:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl patch no &lt;node-name&gt; --patch-file &lt;patch-file&gt;
</code></pre></div><h4 id=persisting-control-plane-component-reconfiguration>Persisting control plane component reconfiguration</h4>
<p>The main source of control plane configuration is the <code>ClusterConfiguration</code>
object stored in the cluster. To extend the static Pod manifests configuration,
<a href=/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#patches>patches</a> can be used.</p>
<p>These patch files must remain as files on the control plane nodes to ensure that
they can be used by the <code>kubeadm upgrade ... --patches &lt;directory></code>.</p>
<p>If reconfiguration is done to the <code>ClusterConfiguration</code> and static Pod manifests on disk,
the set of node specific patches must be updated accordingly.</p>
<h4 id=persisting-kubelet-reconfiguration>Persisting kubelet reconfiguration</h4>
<p>Any changes to the <code>KubeletConfiguration</code> stored in <code>/var/lib/kubelet/config.conf</code> will be overwritten on
<code>kubeadm upgrade</code> by downloading the contents of the cluster wide <code>kubelet-config</code> ConfigMap.
To persist kubelet node specific configuration either the file <code>/var/lib/kubelet/config.conf</code>
has to be updated manually post-upgrade or the file <code>/var/lib/kubelet/kubeadm-flags.env</code> can include flags.
The kubelet flags override the associated <code>KubeletConfiguration</code> options, but note that
some of the flags are deprecated.</p>
<p>A kubelet restart will be required after changing <code>/var/lib/kubelet/config.conf</code> or
<code>/var/lib/kubelet/kubeadm-flags.env</code>.</p>
<p>What's next</p>
<ul>
<li><a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade>Upgrading kubeadm clusters</a></li>
<li><a href=/docs/setup/production-environment/tools/kubeadm/control-plane-flags>Customizing components with the kubeadm API</a></li>
<li><a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-certs>Certificate management with kubeadm</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2e173356df5179cab9eec90a606f0aa4>1.4 - Upgrading kubeadm clusters</h1>
<p>This page explains how to upgrade a Kubernetes cluster created with kubeadm from version
1.22.x to version 1.23.x, and from version
1.23.x to 1.23.y (where <code>y > x</code>). Skipping MINOR versions
when upgrading is unsupported. For more details, please visit <a href=https://kubernetes.io/releases/version-skew-policy/>Version Skew Policy</a>.</p>
<p>To see information about upgrading clusters created using older versions of kubeadm,
please refer to following pages instead:</p>
<ul>
<li><a href=https://v1-22.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading a kubeadm cluster from 1.21 to 1.22</a></li>
<li><a href=https://v1-21.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading a kubeadm cluster from 1.20 to 1.21</a></li>
<li><a href=https://v1-20.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading a kubeadm cluster from 1.19 to 1.20</a></li>
<li><a href=https://v1-19.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading a kubeadm cluster from 1.18 to 1.19</a></li>
</ul>
<p>The upgrade workflow at high level is the following:</p>
<ol>
<li>Upgrade a primary control plane node.</li>
<li>Upgrade additional control plane nodes.</li>
<li>Upgrade worker nodes.</li>
</ol>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>Make sure you read the <a href=https://git.k8s.io/kubernetes/CHANGELOG/CHANGELOG-1.27.md>release notes</a> carefully.</li>
<li>The cluster should use a static control plane and etcd pods or external etcd.</li>
<li>Make sure to back up any important components, such as app-level state stored in a database.
<code>kubeadm upgrade</code> does not touch your workloads, only components internal to Kubernetes, but backups are always a best practice.</li>
<li><a href=https://serverfault.com/questions/684771/best-way-to-disable-swap-in-linux>Swap must be disabled</a>.</li>
</ul>
<h3 id=additional-information>Additional information</h3>
<ul>
<li>The instructions below outline when to drain each node during the upgrade process.
If you are performing a <strong>minor</strong> version upgrade for any kubelet, you <strong>must</strong>
first drain the node (or nodes) that you are upgrading. In the case of control plane nodes,
they could be running CoreDNS Pods or other critical workloads. For more information see
<a href=/docs/tasks/administer-cluster/safely-drain-node/>Draining nodes</a>.</li>
<li>All containers are restarted after upgrade, because the container spec hash value is changed.</li>
<li>To verify that the kubelet service has successfully restarted after the kubelet has been upgraded,
you can execute <code>systemctl status kubelet</code> or view the service logs with <code>journalctl -xeu kubelet</code>.</li>
<li>Usage of the <code>--config</code> flag of <code>kubeadm upgrade</code> with
<a href=/docs/reference/config-api/kubeadm-config.v1beta3>kubeadm configuration API types</a>
with the purpose of reconfiguring the cluster is not recommended and can have unexpected results. Follow the steps in
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure>Reconfiguring a kubeadm cluster</a> instead.</li>
</ul>
<h2 id=determine-which-version-to-upgrade-to>Determine which version to upgrade to</h2>
<p>Find the latest patch release for Kubernetes 1.23 using the OS package manager:</p>
<ul class="nav nav-tabs" id=k8s-install-versions role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-versions-0 role=tab aria-controls=k8s-install-versions-0 aria-selected=true>Ubuntu, Debian or HypriotOS</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-versions-1 role=tab aria-controls=k8s-install-versions-1>CentOS, RHEL or Fedora</a></li></ul>
<div class=tab-content id=k8s-install-versions><div id=k8s-install-versions-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-versions-0>
<p><pre><code>apt update
apt-cache madison kubeadm
# find the latest 1.23 version in the list
# it should look like 1.23.x-00, where x is the latest patch
</code></pre>
</div>
<div id=k8s-install-versions-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-versions-1>
<p><pre><code>yum list --showduplicates kubeadm --disableexcludes=kubernetes
# find the latest 1.23 version in the list
# it should look like 1.23.x-0, where x is the latest patch
</code></pre>
</div></div>
<h2 id=upgrading-control-plane-nodes>Upgrading control plane nodes</h2>
<p>The upgrade procedure on control plane nodes should be executed one node at a time.
Pick a control plane node that you wish to upgrade first. It must have the <code>/etc/kubernetes/admin.conf</code> file.</p>
<h3 id=call-kubeadm-upgrade>Call "kubeadm upgrade"</h3>
<p><strong>For the first control plane node</strong></p>
<ul>
<li>Upgrade kubeadm:</li>
</ul>
<p><ul class="nav nav-tabs" id=k8s-install-kubeadm-first-cp role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-kubeadm-first-cp-0 role=tab aria-controls=k8s-install-kubeadm-first-cp-0 aria-selected=true>Ubuntu, Debian or HypriotOS</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-kubeadm-first-cp-1 role=tab aria-controls=k8s-install-kubeadm-first-cp-1>CentOS, RHEL or Fedora</a></li></ul>
<div class=tab-content id=k8s-install-kubeadm-first-cp><div id=k8s-install-kubeadm-first-cp-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-kubeadm-first-cp-0>
<p><pre><code># replace x in 1.23.x-00 with the latest patch version
apt-mark unhold kubeadm &amp;&amp; \
apt-get update &amp;&amp; apt-get install -y kubeadm=1.23.x-00 &amp;&amp; \
apt-mark hold kubeadm
</code></pre>
</div>
<div id=k8s-install-kubeadm-first-cp-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-kubeadm-first-cp-1>
<p><pre><code># replace x in 1.23.x-0 with the latest patch version
yum install -y kubeadm-1.23.x-0 --disableexcludes=kubernetes
</code></pre>
</div></div>
<br></p>
<ul>
<li>
<p>Verify that the download works and has the expected version:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm version
</code></pre></div></li>
<li>
<p>Verify the upgrade plan:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm upgrade plan
</code></pre></div><p>This command checks that your cluster can be upgraded, and fetches the versions you can upgrade to.
It also shows a table with the component config version states.</p>
</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <code>kubeadm upgrade</code> also automatically renews the certificates that it manages on this node.
To opt-out of certificate renewal the flag <code>--certificate-renewal=false</code> can be used.
For more information see the <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-certs>certificate management guide</a>.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If <code>kubeadm upgrade plan</code> shows any component configs that require manual upgrade, users must provide
a config file with replacement configs to <code>kubeadm upgrade apply</code> via the <code>--config</code> command line flag.
Failing to do so will cause <code>kubeadm upgrade apply</code> to exit with an error and not perform an upgrade.
</div>
<ul>
<li>
<p>Choose a version to upgrade to, and run the appropriate command. For example:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># replace x with the patch version you picked for this upgrade</span>
sudo kubeadm upgrade apply v1.23.x
</code></pre></div><p>Once the command finishes you should see:</p>
<pre><code>[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.23.x&quot;. Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
</code></pre></li>
<li>
<p>Manually upgrade your CNI provider plugin.</p>
<p>Your Container Network Interface (CNI) provider may have its own upgrade instructions to follow.
Check the <a href=/docs/concepts/cluster-administration/addons/>addons</a> page to
find your CNI provider and see whether additional upgrade steps are required.</p>
<p>This step is not required on additional control plane nodes if the CNI provider runs as a DaemonSet.</p>
</li>
</ul>
<p><strong>For the other control plane nodes</strong></p>
<p>Same as the first control plane node but use:</p>
<pre><code>sudo kubeadm upgrade node
</code></pre><p>instead of:</p>
<pre><code>sudo kubeadm upgrade apply
</code></pre><p>Also calling <code>kubeadm upgrade plan</code> and upgrading the CNI provider plugin is no longer needed.</p>
<h3 id=drain-the-node>Drain the node</h3>
<ul>
<li>
<p>Prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div></li>
</ul>
<h3 id=upgrade-kubelet-and-kubectl>Upgrade kubelet and kubectl</h3>
<ul>
<li>Upgrade the kubelet and kubectl:</li>
</ul>
<p><ul class="nav nav-tabs" id=k8s-install-kubelet role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-kubelet-0 role=tab aria-controls=k8s-install-kubelet-0 aria-selected=true>Ubuntu, Debian or HypriotOS</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-kubelet-1 role=tab aria-controls=k8s-install-kubelet-1>CentOS, RHEL or Fedora</a></li></ul>
<div class=tab-content id=k8s-install-kubelet><div id=k8s-install-kubelet-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-kubelet-0>
<p><pre><code># replace x in 1.23.x-00 with the latest patch version
apt-mark unhold kubelet kubectl &amp;&amp; \
apt-get update &amp;&amp; apt-get install -y kubelet=1.23.x-00 kubectl=1.23.x-00 &amp;&amp; \
apt-mark hold kubelet kubectl
</code></pre>
</div>
<div id=k8s-install-kubelet-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-kubelet-1>
<p><pre><code># replace x in 1.23.x-0 with the latest patch version
yum install -y kubelet-1.23.x-0 kubectl-1.23.x-0 --disableexcludes=kubernetes
</code></pre>
</div></div>
<br></p>
<ul>
<li>
<p>Restart the kubelet:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre></div></li>
</ul>
<h3 id=uncordon-the-node>Uncordon the node</h3>
<ul>
<li>
<p>Bring the node back online by marking it schedulable:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># replace &lt;node-to-drain&gt; with the name of your node</span>
kubectl uncordon &lt;node-to-drain&gt;
</code></pre></div></li>
</ul>
<h2 id=upgrade-worker-nodes>Upgrade worker nodes</h2>
<p>The upgrade procedure on worker nodes should be executed one node at a time or few nodes at a time,
without compromising the minimum required capacity for running your workloads.</p>
<h3 id=upgrade-kubeadm>Upgrade kubeadm</h3>
<ul>
<li>Upgrade kubeadm:</li>
</ul>
<ul class="nav nav-tabs" id=k8s-install-kubeadm-worker-nodes role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-kubeadm-worker-nodes-0 role=tab aria-controls=k8s-install-kubeadm-worker-nodes-0 aria-selected=true>Ubuntu, Debian or HypriotOS</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-kubeadm-worker-nodes-1 role=tab aria-controls=k8s-install-kubeadm-worker-nodes-1>CentOS, RHEL or Fedora</a></li></ul>
<div class=tab-content id=k8s-install-kubeadm-worker-nodes><div id=k8s-install-kubeadm-worker-nodes-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-kubeadm-worker-nodes-0>
<p><pre><code># replace x in 1.23.x-00 with the latest patch version
apt-mark unhold kubeadm &amp;&amp; \
apt-get update &amp;&amp; apt-get install -y kubeadm=1.23.x-00 &amp;&amp; \
apt-mark hold kubeadm
</code></pre>
</div>
<div id=k8s-install-kubeadm-worker-nodes-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-kubeadm-worker-nodes-1>
<p><pre><code># replace x in 1.23.x-0 with the latest patch version
yum install -y kubeadm-1.23.x-0 --disableexcludes=kubernetes
</code></pre>
</div></div>
<h3 id=call-kubeadm-upgrade-1>Call "kubeadm upgrade"</h3>
<ul>
<li>
<p>For worker nodes this upgrades the local kubelet configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo kubeadm upgrade node
</code></pre></div></li>
</ul>
<h3 id=drain-the-node-1>Drain the node</h3>
<ul>
<li>
<p>Prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div></li>
</ul>
<h3 id=upgrade-kubelet-and-kubectl-1>Upgrade kubelet and kubectl</h3>
<ul>
<li>Upgrade the kubelet and kubectl:</li>
</ul>
<p><ul class="nav nav-tabs" id=k8s-kubelet-and-kubectl role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-kubelet-and-kubectl-0 role=tab aria-controls=k8s-kubelet-and-kubectl-0 aria-selected=true>Ubuntu, Debian or HypriotOS</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-kubelet-and-kubectl-1 role=tab aria-controls=k8s-kubelet-and-kubectl-1>CentOS, RHEL or Fedora</a></li></ul>
<div class=tab-content id=k8s-kubelet-and-kubectl><div id=k8s-kubelet-and-kubectl-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-kubelet-and-kubectl-0>
<p><pre><code># replace x in 1.23.x-00 with the latest patch version
apt-mark unhold kubelet kubectl &amp;&amp; \
apt-get update &amp;&amp; apt-get install -y kubelet=1.23.x-00 kubectl=1.23.x-00 &amp;&amp; \
apt-mark hold kubelet kubectl
</code></pre>
</div>
<div id=k8s-kubelet-and-kubectl-1 class=tab-pane role=tabpanel aria-labelledby=k8s-kubelet-and-kubectl-1>
<p><pre><code># replace x in 1.23.x-0 with the latest patch version
yum install -y kubelet-1.23.x-0 kubectl-1.23.x-0 --disableexcludes=kubernetes
</code></pre>
</div></div>
<br></p>
<ul>
<li>
<p>Restart the kubelet:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre></div></li>
</ul>
<h3 id=uncordon-the-node-1>Uncordon the node</h3>
<ul>
<li>
<p>Bring the node back online by marking it schedulable:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># replace &lt;node-to-drain&gt; with the name of your node</span>
kubectl uncordon &lt;node-to-drain&gt;
</code></pre></div></li>
</ul>
<h2 id=verify-the-status-of-the-cluster>Verify the status of the cluster</h2>
<p>After the kubelet is upgraded on all nodes verify that all nodes are available again by running the following command
from anywhere kubectl can access the cluster:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes
</code></pre></div><p>The <code>STATUS</code> column should show <code>Ready</code> for all your nodes, and the version number should be updated.</p>
<h2 id=recovering-from-a-failure-state>Recovering from a failure state</h2>
<p>If <code>kubeadm upgrade</code> fails and does not roll back, for example because of an unexpected shutdown during execution, you can run <code>kubeadm upgrade</code> again.
This command is idempotent and eventually makes sure that the actual state is the desired state you declare.</p>
<p>To recover from a bad state, you can also run <code>kubeadm upgrade apply --force</code> without changing the version that your cluster is running.</p>
<p>During upgrade kubeadm writes the following backup folders under <code>/etc/kubernetes/tmp</code>:</p>
<ul>
<li><code>kubeadm-backup-etcd-&lt;date>-&lt;time></code></li>
<li><code>kubeadm-backup-manifests-&lt;date>-&lt;time></code></li>
</ul>
<p><code>kubeadm-backup-etcd</code> contains a backup of the local etcd member data for this control plane Node.
In case of an etcd upgrade failure and if the automatic rollback does not work, the contents of this folder
can be manually restored in <code>/var/lib/etcd</code>. In case external etcd is used this backup folder will be empty.</p>
<p><code>kubeadm-backup-manifests</code> contains a backup of the static Pod manifest files for this control plane Node.
In case of a upgrade failure and if the automatic rollback does not work, the contents of this folder can be
manually restored in <code>/etc/kubernetes/manifests</code>. If for some reason there is no difference between a pre-upgrade
and post-upgrade manifest file for a certain component, a backup file for it will not be written.</p>
<h2 id=how-it-works>How it works</h2>
<p><code>kubeadm upgrade apply</code> does the following:</p>
<ul>
<li>Checks that your cluster is in an upgradeable state:
<ul>
<li>The API server is reachable</li>
<li>All nodes are in the <code>Ready</code> state</li>
<li>The control plane is healthy</li>
</ul>
</li>
<li>Enforces the version skew policies.</li>
<li>Makes sure the control plane images are available or available to pull to the machine.</li>
<li>Generates replacements and/or uses user supplied overwrites if component configs require version upgrades.</li>
<li>Upgrades the control plane components or rollbacks if any of them fails to come up.</li>
<li>Applies the new <code>CoreDNS</code> and <code>kube-proxy</code> manifests and makes sure that all necessary RBAC rules are created.</li>
<li>Creates new certificate and key files of the API server and backs up old files if they're about to expire in 180 days.</li>
</ul>
<p><code>kubeadm upgrade node</code> does the following on additional control plane nodes:</p>
<ul>
<li>Fetches the kubeadm <code>ClusterConfiguration</code> from the cluster.</li>
<li>Optionally backups the kube-apiserver certificate.</li>
<li>Upgrades the static Pod manifests for the control plane components.</li>
<li>Upgrades the kubelet configuration for this node.</li>
</ul>
<p><code>kubeadm upgrade node</code> does the following on worker nodes:</p>
<ul>
<li>Fetches the kubeadm <code>ClusterConfiguration</code> from the cluster.</li>
<li>Upgrades the kubelet configuration for this node.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-9133578f1e75663bb031e5a377ca896d>1.5 - Adding Windows nodes</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>
<p>You can use Kubernetes to run a mixture of Linux and Windows nodes, so you can mix Pods that run on Linux on with Pods that run on Windows. This page shows how to register Windows nodes to your cluster.</p>
<h2 id=before-you-begin>Before you begin</h2>
Your Kubernetes server must be at or later than version 1.17.
To check the version, enter <code>kubectl version</code>.
<ul>
<li>
<p>Obtain a <a href=https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing>Windows Server 2019 license</a>
(or higher) in order to configure the Windows node that hosts Windows containers.
If you are using VXLAN/Overlay networking you must have also have <a href=https://support.microsoft.com/help/4489899>KB4489899</a> installed.</p>
</li>
<li>
<p>A Linux-based Kubernetes kubeadm cluster in which you have access to the control plane (see <a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>Creating a single control-plane cluster with kubeadm</a>).</p>
</li>
</ul>
<h2 id=objectives>Objectives</h2>
<ul>
<li>Register a Windows node to the cluster</li>
<li>Configure networking so Pods and Services on Linux and Windows can communicate with each other</li>
</ul>
<h2 id=getting-started-adding-a-windows-node-to-your-cluster>Getting Started: Adding a Windows Node to Your Cluster</h2>
<h3 id=networking-configuration>Networking Configuration</h3>
<p>Once you have a Linux-based Kubernetes control-plane node you are ready to choose a networking solution. This guide illustrates using Flannel in VXLAN mode for simplicity.</p>
<h4 id=configuring-flannel>Configuring Flannel</h4>
<ol>
<li>
<p>Prepare Kubernetes control plane for Flannel</p>
<p>Some minor preparation is recommended on the Kubernetes control plane in our cluster. It is recommended to enable bridged IPv4 traffic to iptables chains when using Flannel. The following command must be run on all Linux nodes:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo sysctl net.bridge.bridge-nf-call-iptables<span style=color:#666>=</span><span style=color:#666>1</span>
</code></pre></div></li>
<li>
<p>Download & configure Flannel for Linux</p>
<p>Download the most recent Flannel manifest:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre></div><p>Modify the <code>net-conf.json</code> section of the flannel manifest in order to set the VNI to 4096 and the Port to 4789. It should look as follows:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span>net-conf.json:</span> <span>|</span>
    {
      <span style=color:green;font-weight:700>&#34;Network&#34;</span>: <span style=color:#b44>&#34;10.244.0.0/16&#34;</span>,
      <span style=color:green;font-weight:700>&#34;Backend&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;Type&#34;</span>: <span style=color:#b44>&#34;vxlan&#34;</span>,
        <span style=color:green;font-weight:700>&#34;VNI&#34;</span>: <span style=color:#666>4096</span>,
        <span style=color:green;font-weight:700>&#34;Port&#34;</span>: <span style=color:#666>4789</span>
      }
    }
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The VNI must be set to 4096 and port 4789 for Flannel on Linux to interoperate with Flannel on Windows. See the <a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>VXLAN documentation</a>.
for an explanation of these fields.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> To use L2Bridge/Host-gateway mode instead change the value of <code>Type</code> to <code>"host-gw"</code> and omit <code>VNI</code> and <code>Port</code>.
</div>
</li>
<li>
<p>Apply the Flannel manifest and validate</p>
<p>Let's apply the Flannel configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f kube-flannel.yml
</code></pre></div><p>After a few minutes, you should see all the pods as running if the Flannel pod network was deployed.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pods -n kube-system
</code></pre></div><p>The output should include the Linux flannel DaemonSet as running:</p>
<pre><code>NAMESPACE     NAME                                      READY        STATUS    RESTARTS   AGE
...
kube-system   kube-flannel-ds-54954                     1/1          Running   0          1m
</code></pre></li>
<li>
<p>Add Windows Flannel and kube-proxy DaemonSets</p>
<p>Now you can add Windows-compatible versions of Flannel and kube-proxy. In order
to ensure that you get a compatible version of kube-proxy, you'll need to substitute
the tag of the image. The following example shows usage for Kubernetes v1.23.17,
but you should adjust the version for your own deployment.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed <span style=color:#b44>&#39;s/VERSION/v1.23.17/g&#39;</span> | kubectl apply -f -
kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If you're using host-gateway use <a href=https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-host-gw.yml>https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-host-gw.yml</a> instead
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>If you're using a different interface rather than Ethernet (i.e. "Ethernet0 2") on the Windows nodes, you have to modify the line:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>wins <span style=color:#a2f>cli </span><span style=color:#a2f;font-weight:700>process</span> run --path /k/flannel/setup.exe --args <span style=color:#b44>&#34;--mode=overlay --interface=Ethernet&#34;</span>
</code></pre></div><p>in the <code>flannel-host-gw.yml</code> or <code>flannel-overlay.yml</code> file and specify your interface accordingly.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Example</span>
curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml | sed <span style=color:#b44>&#39;s/Ethernet/Ethernet0 2/g&#39;</span> | kubectl apply -f -
</code></pre></div>
</div>
</li>
</ol>
<h3 id=joining-a-windows-worker-node>Joining a Windows worker node</h3>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> All code snippets in Windows sections are to be run in a PowerShell environment
with elevated permissions (Administrator) on the Windows worker node.
</div>
<ul class="nav nav-tabs" id=tab-windows-kubeadm-runtime-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-windows-kubeadm-runtime-installation-0 role=tab aria-controls=tab-windows-kubeadm-runtime-installation-0 aria-selected=true>Docker EE</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#tab-windows-kubeadm-runtime-installation-1 role=tab aria-controls=tab-windows-kubeadm-runtime-installation-1>CRI-containerD</a></li></ul>
<div class=tab-content id=tab-windows-kubeadm-runtime-installation><div id=tab-windows-kubeadm-runtime-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-windows-kubeadm-runtime-installation-0>
<p><h4 id=install-docker-ee>Install Docker EE</h4>
<p>Install the <code>Containers</code> feature</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Install-WindowsFeature</span> -Name containers
</code></pre></div><p>Install Docker
Instructions to do so are available at <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/quick-start/set-up-environment?tabs=Windows-Server#install-docker">Install Docker Engine - Enterprise on Windows Servers</a>.</p>
<h4 id=install-wins-kubelet-and-kubeadm>Install wins, kubelet, and kubeadm</h4>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-PowerShell data-lang=PowerShell>curl.exe -LO https<span>:</span>//raw.githubusercontent.com/<span style=color:#a2f>kubernetes-sigs</span>/<span style=color:#a2f>sig-windows</span>-tools/master/kubeadm/scripts/PrepareNode.ps1
.\PrepareNode.ps1 -KubernetesVersion v1.23.17
</code></pre></div><h4 id=run-kubeadm-to-join-the-node>Run <code>kubeadm</code> to join the node</h4>
<p>Use the command that was given to you when you ran <code>kubeadm init</code> on a control plane host.
If you no longer have this command, or the token has expired, you can run <code>kubeadm token create --print-join-command</code>
(on a control plane host) to generate a new token and join command.</p>
</div>
<div id=tab-windows-kubeadm-runtime-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-windows-kubeadm-runtime-installation-1>
<p><h4 id=install-containerd>Install containerD</h4>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>curl.exe -LO https<span>:</span>//github.com/<span style=color:#a2f>kubernetes-sigs</span>/<span style=color:#a2f>sig-windows</span>-tools/releases/latest/download/<span style=color:#a2f>Install-Containerd</span>.ps1
.\<span style=color:#a2f>Install-Containerd</span>.ps1
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>To install a specific version of containerD specify the version with -ContainerDVersion.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># Example</span>
.\<span style=color:#a2f>Install-Containerd</span>.ps1 -ContainerDVersion 1.4.1
</code></pre></div>
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>If you're using a different interface rather than Ethernet (i.e. "Ethernet0 2") on the Windows nodes, specify the name with <code>-netAdapterName</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># Example</span>
.\<span style=color:#a2f>Install-Containerd</span>.ps1 -netAdapterName <span style=color:#b44>&#34;Ethernet0 2&#34;</span>
</code></pre></div>
</div>
<h4 id=install-wins-kubelet-and-kubeadm>Install wins, kubelet, and kubeadm</h4>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-PowerShell data-lang=PowerShell>curl.exe -LO https<span>:</span>//raw.githubusercontent.com/<span style=color:#a2f>kubernetes-sigs</span>/<span style=color:#a2f>sig-windows</span>-tools/master/kubeadm/scripts/PrepareNode.ps1
.\PrepareNode.ps1 -KubernetesVersion v1.23.17 -ContainerRuntime containerD
</code></pre></div><h4 id=run-kubeadm-to-join-the-node>Run <code>kubeadm</code> to join the node</h4>
<p>Use the command that was given to you when you ran <code>kubeadm init</code> on a control plane host.
If you no longer have this command, or the token has expired, you can run <code>kubeadm token create --print-join-command</code>
(on a control plane host) to generate a new token and join command.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If using <strong>CRI-containerD</strong> add <code>--cri-socket "npipe:////./pipe/containerd-containerd"</code> to the kubeadm call
</div>
</div></div>
<h3 id=verifying-your-installation>Verifying your installation</h3>
<p>You should now be able to view the Windows node in your cluster by running:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get nodes -o wide
</code></pre></div><p>If your new node is in the <code>NotReady</code> state it is likely because the flannel image is still downloading.
You can check the progress as before by checking on the flannel pods in the <code>kube-system</code> namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl -n kube-system get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>flannel
</code></pre></div><p>Once the flannel Pod is running, your node should enter the <code>Ready</code> state and then be available to handle workloads.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes>Upgrading Windows kubeadm nodes</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-e805c7d8d4ad6195cb82dbbc843bfc29>1.6 - Upgrading Windows nodes</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>
<p>This page explains how to upgrade a Windows node <a href=/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes>created with kubeadm</a>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
Your Kubernetes server must be at or later than version 1.17.
To check the version, enter <code>kubectl version</code>.
</p>
<ul>
<li>Familiarize yourself with <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade>the process for upgrading the rest of your kubeadm
cluster</a>. You will want to
upgrade the control plane nodes before upgrading your Windows nodes.</li>
</ul>
<h2 id=upgrading-worker-nodes>Upgrading worker nodes</h2>
<h3 id=upgrade-kubeadm>Upgrade kubeadm</h3>
<ol>
<li>
<p>From the Windows node, upgrade kubeadm:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># replace v1.23.17 with your desired version</span>
curl.exe -Lo C:\k\kubeadm.exe https<span>:</span>//dl.k8s.io/<span style=color:#a2f>/bin/windows/amd64/kubeadm.exe
</code></pre></div></li>
</ol>
<h3 id=drain-the-node>Drain the node</h3>
<ol>
<li>
<p>From a machine with access to the Kubernetes API,
prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div><p>You should see output similar to this:</p>
<pre><code>node/ip-172-31-85-18 cordoned
node/ip-172-31-85-18 drained
</code></pre></li>
</ol>
<h3 id=upgrade-the-kubelet-configuration>Upgrade the kubelet configuration</h3>
<ol>
<li>
<p>From the Windows node, call the following command to sync new kubelet configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>kubeadm upgrade node
</code></pre></div></li>
</ol>
<h3 id=upgrade-kubelet>Upgrade kubelet</h3>
<ol>
<li>
<p>From the Windows node, upgrade and restart the kubelet:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>stop-service</span> kubelet
curl.exe -Lo C:\k\kubelet.exe https<span>:</span>//dl.k8s.io/<span style=color:#a2f>/bin/windows/amd64/kubelet.exe
<span style=color:#a2f>restart-service</span> kubelet
</code></pre></div></li>
</ol>
<h3 id=uncordon-the-node>Uncordon the node</h3>
<ol>
<li>
<p>From a machine with access to the Kubernetes API,
bring the node back online by marking it schedulable:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># replace &lt;node-to-drain&gt; with the name of your node</span>
kubectl uncordon &lt;node-to-drain&gt;
</code></pre></div></li>
</ol>
<h3 id=upgrade-kube-proxy>Upgrade kube-proxy</h3>
<ol>
<li>
<p>From a machine with access to the Kubernetes API, run the following,
again replacing v1.23.17 with your desired version:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed <span style=color:#b44>&#39;s/VERSION/v1.23.17/g&#39;</span> | kubectl apply -f -
</code></pre></div></li>
</ol>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-adb6c52e773f4d890595e14a9251f59b>2 - Migrating from dockershim</h1>
<p>This section presents information you need to know when migrating from
dockershim to other container runtimes.</p>
<p>Since the announcement of <a href=/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation>dockershim deprecation</a>
in Kubernetes 1.20, there were questions on how this will affect various workloads and Kubernetes
installations. Our <a href=/blog/2022/02/17/dockershim-faq/>Dockershim Removal FAQ</a> is there to help you
to understand the problem better.</p>
<p>Dockershim will be removed from Kubernetes following the release of v1.24.
If you use Docker via dockershim as your container runtime, and wish to upgrade to v1.24,
it is recommended that you either migrate to another runtime or find an alternative means to obtain Docker Engine support.
If you're not sure whether you are using Docker,
<a href=/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/>find out what container runtime is used on a node</a>.</p>
<p>Your cluster might have more than one kind of node, although this is not a common
configuration.</p>
<p>These tasks will help you to migrate:</p>
<ul>
<li><a href=/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/>Check whether Dockershim deprecation affects you</a></li>
<li><a href=/docs/tasks/administer-cluster/migrating-from-dockershim/>Migrating from dockershim</a></li>
<li><a href=/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/>Migrating telemetry and security agents from dockershim</a></li>
</ul>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Check out <a href=/docs/setup/production-environment/container-runtimes/>container runtimes</a>
to understand your options for a container runtime.</li>
<li>There is a
<a href=https://github.com/kubernetes/kubernetes/issues/106917>GitHub issue</a>
to track discussion about the deprecation and removal of dockershim.</li>
<li>If you found a defect or other technical concern relating to migrating away from dockershim,
you can <a href=https://github.com/kubernetes/kubernetes/issues/new/choose>report an issue</a>
to the Kubernetes project.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-b8acce0768c2f92cdb8eaa31e8072353>2.1 - Changing the Container Runtime on a Node from Docker Engine to containerd</h1>
<p>This task outlines the steps needed to update your container runtime to containerd from Docker. It
is applicable for cluster operators running Kubernetes 1.23 or earlier. Also this covers an
example scenario for migrating from dockershim to containerd and alternative container runtimes
can be picked from this <a href=/docs/setup/production-environment/container-runtimes/>page</a>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div>
<p>Install containerd. For more information see
<a href=https://containerd.io/docs/getting-started/>containerd's installation documentation</a>
and for specific prerequisite follow
<a href=/docs/setup/production-environment/container-runtimes/#containerd>the containerd guide</a>.</p>
<h2 id=drain-the-node>Drain the node</h2>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div><p>Replace <code>&lt;node-to-drain></code> with the name of your node you are draining.</p>
<h2 id=stop-the-docker-daemon>Stop the Docker daemon</h2>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>systemctl stop kubelet
systemctl disable docker.service --now
</code></pre></div><h2 id=install-containerd>Install Containerd</h2>
<p>Follow the <a href=/docs/setup/production-environment/container-runtimes/#containerd>guide</a>
for detailed steps to install containerd.</p>
<ul class="nav nav-tabs" id=tab-cri-containerd-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-cri-containerd-installation-0 role=tab aria-controls=tab-cri-containerd-installation-0 aria-selected=true>Linux</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-containerd-installation-1 role=tab aria-controls=tab-cri-containerd-installation-1>Windows (PowerShell)</a></li></ul>
<div class=tab-content id=tab-cri-containerd-installation><div id=tab-cri-containerd-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-cri-containerd-installation-0>
<p><ol>
<li>
<p>Install the <code>containerd.io</code> package from the official Docker repositories.
Instructions for setting up the Docker repository for your respective Linux distribution and
installing the <code>containerd.io</code> package can be found at
<a href=https://docs.docker.com/engine/install/#server>Install Docker Engine</a>.</p>
</li>
<li>
<p>Configure containerd:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
</code></pre></div></li>
<li>
<p>Restart containerd:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl restart containerd
</code></pre></div></li>
</ol>
</div>
<div id=tab-cri-containerd-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-cri-containerd-installation-1>
<p><p>Start a Powershell session, set <code>$Version</code> to the desired version (ex: <code>$Version="1.4.3"</code>), and
then run the following commands:</p>
<ol>
<li>
<p>Download containerd:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>curl.exe -L https<span>:</span>//github.com/containerd/containerd/releases/download/v<span style=color:#b8860b>$Version</span>/containerd-<span style=color:#b8860b>$Version</span>-windows-amd64.tar.gz -o <span style=color:#a2f>containerd-windows</span>-amd64.tar.gz
tar.exe xvf .\<span style=color:#a2f>containerd-windows</span>-amd64.tar.gz
</code></pre></div></li>
<li>
<p>Extract and configure:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Copy-Item</span> -Path <span style=color:#b44>&#34;.\bin\&#34;</span> -Destination <span style=color:#b44>&#34;$Env:ProgramFiles\containerd&#34;</span> -Recurse -Force
<span style=color:#a2f>cd </span><span style=color:#b8860b>$Env:ProgramFiles</span>\containerd\
.\containerd.exe config <span style=color:#a2f;font-weight:700>default</span> | <span style=color:#a2f>Out-File</span> config.toml -Encoding ascii

<span style=color:#080;font-style:italic># Review the configuration. Depending on setup you may want to adjust:</span>
<span style=color:#080;font-style:italic># - the sandbox_image (Kubernetes pause image)</span>
<span style=color:#080;font-style:italic># - cni bin_dir and conf_dir locations</span>
<span style=color:#a2f>Get-Content</span> config.toml

<span style=color:#080;font-style:italic># (Optional - but highly recommended) Exclude containerd from Windows Defender Scans</span>
<span style=color:#a2f>Add-MpPreference</span> -ExclusionProcess <span style=color:#b44>&#34;$Env:ProgramFiles\containerd\containerd.exe&#34;</span>
</code></pre></div></li>
<li>
<p>Start containerd:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>.\containerd.exe --register-service
<span style=color:#a2f>Start-Service</span> containerd
</code></pre></div></li>
</ol>
</div></div>
<h2 id=configure-the-kubelet-to-use-containerd-as-its-container-runtime>Configure the kubelet to use containerd as its container runtime</h2>
<p>Edit the file <code>/var/lib/kubelet/kubeadm-flags.env</code> and add the containerd runtime to the flags.
<code>--container-runtime=remote</code> and
<code>--container-runtime-endpoint=unix:///run/containerd/containerd.sock"</code>.</p>
<p>Users using kubeadm should be aware that the <code>kubeadm</code> tool stores the CRI socket for each host as
an annotation in the Node object for that host. To change it you can execute the following command
on a machine that has the kubeadm <code>/etc/kubernetes/admin.conf</code> file.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit no &lt;node-name&gt;
</code></pre></div><p>This will start a text editor where you can edit the Node object.
To choose a text editor you can set the <code>KUBE_EDITOR</code> environment variable.</p>
<ul>
<li>
<p>Change the value of <code>kubeadm.alpha.kubernetes.io/cri-socket</code> from <code>/var/run/dockershim.sock</code>
to the CRI socket path of your choice (for example <code>unix:///run/containerd/containerd.sock</code>).</p>
<p>Note that new CRI socket paths must be prefixed with <code>unix://</code> ideally.</p>
</li>
<li>
<p>Save the changes in the text editor, which will update the Node object.</p>
</li>
</ul>
<h2 id=restart-the-kubelet>Restart the kubelet</h2>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>systemctl start kubelet
</code></pre></div><h2 id=verify-that-the-node-is-healthy>Verify that the node is healthy</h2>
<p>Run <code>kubectl get nodes -o wide</code> and containerd appears as the runtime for the node we just changed.</p>
<h2 id=remove-docker-engine>Remove Docker Engine</h2>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div>
<p>Finally if everything goes well, remove Docker.</p>
<ul class="nav nav-tabs" id=tab-remove-docker-enigine role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-remove-docker-enigine-0 role=tab aria-controls=tab-remove-docker-enigine-0 aria-selected=true>CentOS</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#tab-remove-docker-enigine-1 role=tab aria-controls=tab-remove-docker-enigine-1>Debian</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#tab-remove-docker-enigine-2 role=tab aria-controls=tab-remove-docker-enigine-2>Fedora</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#tab-remove-docker-enigine-3 role=tab aria-controls=tab-remove-docker-enigine-3>Ubuntu</a></li></ul>
<div class=tab-content id=tab-remove-docker-enigine><div id=tab-remove-docker-enigine-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-remove-docker-enigine-0>
<p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo yum remove docker-ce docker-ce-cli
</code></pre></div></div>
<div id=tab-remove-docker-enigine-1 class=tab-pane role=tabpanel aria-labelledby=tab-remove-docker-enigine-1>
<p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get purge docker-ce docker-ce-cli
</code></pre></div></div>
<div id=tab-remove-docker-enigine-2 class=tab-pane role=tabpanel aria-labelledby=tab-remove-docker-enigine-2>
<p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo dnf remove docker-ce docker-ce-cli
</code></pre></div></div>
<div id=tab-remove-docker-enigine-3 class=tab-pane role=tabpanel aria-labelledby=tab-remove-docker-enigine-3>
<p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get purge docker-ce docker-ce-cli
</code></pre></div></div></div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-d81ef0973a7bb4813e1797a452864742>2.2 - Migrate Docker Engine nodes from dockershim to cri-dockerd</h1>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div>
<p>This page shows you how to migrate your Docker Engine nodes to use <code>cri-dockerd</code>
instead of dockershim. Follow these steps if your clusters run Kubernetes 1.23
or earlier and you want to continue using Docker Engine after
you upgrade to Kubernetes 1.24 and later, or if you just want to move off the
dockershim component.</p>
<h2 id=what-is-cri-dockerd>What is cri-dockerd?</h2>
<p>In Kubernetes 1.23 and earlier, Docker Engine used a component called the
dockershim to interact with Kubernetes system components such as the kubelet.
The dockershim component is deprecated and will be removed in Kubernetes 1.24. A
third-party replacement, <code>cri-dockerd</code>, is available. The <code>cri-dockerd</code> adapter
lets you use Docker Engine through the <a class=glossary-tooltip title="An API for container runtimes to integrate with kubelet" data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#container-runtime target=_blank aria-label="Container Runtime Interface">Container Runtime Interface</a>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If you already use <code>cri-dockerd</code>, you aren't affected by the dockershim removal.
Before you begin, <a href=/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/>Check whether your nodes use the dockershim</a>.
</div>
<p>If you want to migrate to <code>cri-dockerd</code> so that you can continue using Docker
Engine as your container runtime, you should do the following for each affected
node:</p>
<ol>
<li>Install <code>cri-dockerd</code>.</li>
<li>Cordon and drain the node.</li>
<li>Configure the kubelet to use <code>cri-dockerd</code>.</li>
<li>Restart the kubelet.</li>
<li>Verify that the node is healthy.</li>
</ol>
<p>Test the migration on non-critical nodes first.</p>
<p>You should perform the following steps for each node that you want to migrate
to <code>cri-dockerd</code>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li><a href=https://github.com/mirantis/cri-dockerd#build-and-install><code>cri-dockerd</code></a>
installed and started on each node.</li>
<li>A <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugin</a>.</li>
</ul>
<h2 id=cordon-and-drain-the-node>Cordon and drain the node</h2>
<ol>
<li>
<p>Cordon the node to stop new Pods scheduling on it:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl cordon &lt;NODE_NAME&gt;
</code></pre></div><p>Replace <code>&lt;NODE_NAME></code> with the name of the node.</p>
</li>
<li>
<p>Drain the node to safely evict running Pods:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl drain &lt;NODE_NAME&gt; <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    --ignore-daemonsets
</code></pre></div></li>
</ol>
<h2 id=configure-the-kubelet-to-use-cri-dockerd>Configure the kubelet to use cri-dockerd</h2>
<p>The following steps apply to clusters set up using the kubeadm tool. If you use
a different tool, you should modify the kubelet using the configuration
instructions for that tool.</p>
<ol>
<li>Open <code>/var/lib/kubelet/kubeadm-flags.env</code> on each affected node.</li>
<li>Modify the <code>--container-runtime-endpoint</code> flag to
<code>unix:///var/run/cri-dockerd.sock</code>.</li>
</ol>
<p>The kubeadm tool stores the node's socket as an annotation on the <code>Node</code> object
in the control plane. To modify this socket for each affected node:</p>
<ol>
<li>
<p>Edit the YAML representation of the <code>Node</code> object:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/path/to/admin.conf kubectl edit no &lt;NODE_NAME&gt;
</code></pre></div><p>Replace the following:</p>
<ul>
<li><code>/path/to/admin.conf</code>: the path to the kubectl configuration file,
<code>admin.conf</code>.</li>
<li><code>&lt;NODE_NAME></code>: the name of the node you want to modify.</li>
</ul>
</li>
<li>
<p>Change <code>kubeadm.alpha.kubernetes.io/cri-socket</code> from
<code>/var/run/dockershim.sock</code> to <code>unix:///var/run/cri-dockerd.sock</code>.</p>
</li>
<li>
<p>Save the changes. The <code>Node</code> object is updated on save.</p>
</li>
</ol>
<h2 id=restart-the-kubelet>Restart the kubelet</h2>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>systemctl restart kubelet
</code></pre></div><h2 id=verify-that-the-node-is-healthy>Verify that the node is healthy</h2>
<p>To check whether the node uses the <code>cri-dockerd</code> endpoint, follow the
instructions in <a href=/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/>Find out which runtime you use</a>.
The <code>--container-runtime-endpoint</code> flag for the kubelet should be <code>unix:///var/run/cri-dockerd.sock</code>.</p>
<h2 id=uncordon-the-node>Uncordon the node</h2>
<p>Uncordon the node to let Pods schedule on it:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl uncordon &lt;NODE_NAME&gt;
</code></pre></div><h2 id=what-s-next>What's next</h2>
<ul>
<li>Read the <a href=/dockershim/>dockershim removal FAQ</a>.</li>
<li><a href=/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/>Learn how to migrate from Docker Engine with dockershim to containerd</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-d79db9ed1698f75ec5f2228987290e49>2.3 - Find Out What Container Runtime is Used on a Node</h1>
<p>This page outlines steps to find out what <a href=/docs/setup/production-environment/container-runtimes/>container runtime</a>
the nodes in your cluster use.</p>
<p>Depending on the way you run your cluster, the container runtime for the nodes may
have been pre-configured or you need to configure it. If you're using a managed
Kubernetes service, there might be vendor-specific ways to check what container runtime is
configured for the nodes. The method described on this page should work whenever
the execution of <code>kubectl</code> is allowed.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>Install and configure <code>kubectl</code>. See <a href=/docs/tasks/tools/#kubectl>Install Tools</a> section for details.</p>
<h2 id=find-out-the-container-runtime-used-on-a-node>Find out the container runtime used on a Node</h2>
<p>Use <code>kubectl</code> to fetch and show node information:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes -o wide
</code></pre></div><p>The output is similar to the following. The column <code>CONTAINER-RUNTIME</code> outputs
the runtime and its version.</p>
<p>For Docker Engine, the output is similar to this:</p>
<pre><code class=language-none data-lang=none>NAME         STATUS   VERSION    CONTAINER-RUNTIME
node-1       Ready    v1.16.15   docker://19.3.1
node-2       Ready    v1.16.15   docker://19.3.1
node-3       Ready    v1.16.15   docker://19.3.1
</code></pre><p>If your runtime shows as Docker Engine, you still might not be affected by the
removal of dockershim in Kubernetes 1.24. <a href=#which-endpoint>Check the runtime
endpoint</a> to see if you use dockershim. If you don't use
dockershim, you aren't affected.</p>
<p>For containerd, the output is similar to this:</p>
<pre><code class=language-none data-lang=none>NAME         STATUS   VERSION   CONTAINER-RUNTIME
node-1       Ready    v1.19.6   containerd://1.4.1
node-2       Ready    v1.19.6   containerd://1.4.1
node-3       Ready    v1.19.6   containerd://1.4.1
</code></pre><p>Find out more information about container runtimes
on <a href=/docs/setup/production-environment/container-runtimes/>Container Runtimes</a>
page.</p>
<h2 id=which-endpoint>Find out what container runtime endpoint you use</h2>
<p>The container runtime talks to the kubelet over a Unix socket using the <a href=/docs/concepts/architecture/cri/>CRI
protocol</a>, which is based on the gRPC
framework. The kubelet acts as a client, and the runtime acts as the server.
In some cases, you might find it useful to know which socket your nodes use. For
example, with the removal of dockershim in Kubernetes 1.24 and later, you might
want to know whether you use Docker Engine with dockershim.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If you currently use Docker Engine in your nodes with <code>cri-dockerd</code>, you aren't
affected by the dockershim removal.
</div>
<p>You can check which socket you use by checking the kubelet configuration on your
nodes.</p>
<ol>
<li>
<p>Read the starting commands for the kubelet process:</p>
<pre><code>tr \\0 ' ' &lt; /proc/&quot;$(pgrep kubelet)&quot;/cmdline
</code></pre><p>If you don't have <code>tr</code> or <code>pgrep</code>, check the command line for the kubelet
process manually.</p>
</li>
<li>
<p>In the output, look for the <code>--container-runtime</code> flag and the
<code>--container-runtime-endpoint</code> flag.</p>
<ul>
<li>If your nodes use Kubernetes v1.23 and earlier and these flags aren't
present or if the <code>--container-runtime</code> flag is not <code>remote</code>,
you use the dockershim socket with Docker Engine.</li>
<li>If the <code>--container-runtime-endpoint</code> flag is present, check the socket
name to find out which runtime you use. For example,
<code>unix:///run/containerd/containerd.sock</code> is the containerd endpoint.</li>
</ul>
</li>
</ol>
<p>If you use Docker Engine with the dockershim, <a href=/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/>migrate to a different runtime</a>,
or, if you want to continue using Docker Engine in v1.24 and later, migrate to a
CRI-compatible adapter like <a href=https://github.com/Mirantis/cri-dockerd><code>cri-dockerd</code></a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-58702e4818c09c9b3d574349c1a71cb3>2.4 - Check whether Dockershim deprecation affects you</h1>
<p>The <code>dockershim</code> component of Kubernetes allows to use Docker as a Kubernetes's
<a class=glossary-tooltip title="The container runtime is the software that is responsible for running containers." data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label="container runtime">container runtime</a>.
Kubernetes' built-in <code>dockershim</code> component was
<a href=/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation>deprecated</a>
in release v1.20.</p>
<p>This page explains how your cluster could be using Docker as a container runtime,
provides details on the role that <code>dockershim</code> plays when in use, and shows steps
you can take to check whether any workloads could be affected by <code>dockershim</code> deprecation.</p>
<h2 id=find-docker-dependencies>Finding if your app has a dependencies on Docker</h2>
<p>If you are using Docker for building your application containers, you can still
run these containers on any container runtime. This use of Docker does not count
as a dependency on Docker as a container runtime.</p>
<p>When alternative container runtime is used, executing Docker commands may either
not work or yield unexpected output. This is how you can find whether you have a
dependency on Docker:</p>
<ol>
<li>Make sure no privileged Pods execute Docker commands (like <code>docker ps</code>),
restart the Docker service (commands such as <code>systemctl restart docker.service</code>),
or modify Docker-specific files such as <code>/etc/docker/daemon.json</code>.</li>
<li>Check for any private registries or image mirror settings in the Docker
configuration file (like <code>/etc/docker/daemon.json</code>). Those typically need to
be reconfigured for another container runtime.</li>
<li>Check that scripts and apps running on nodes outside of your Kubernetes
infrastructure do not execute Docker commands. It might be:
<ul>
<li>SSH to nodes to troubleshoot;</li>
<li>Node startup scripts;</li>
<li>Monitoring and security agents installed on nodes directly.</li>
</ul>
</li>
<li>Third-party tools that perform above mentioned privileged operations. See
<a href=/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents>Migrating telemetry and security agents from dockershim</a>
for more information.</li>
<li>Make sure there is no indirect dependencies on dockershim behavior.
This is an edge case and unlikely to affect your application. Some tooling may be configured
to react to Docker-specific behaviors, for example, raise alert on specific metrics or search for
a specific log message as part of troubleshooting instructions.
If you have such tooling configured, test the behavior on test
cluster before migration.</li>
</ol>
<h2 id=role-of-dockershim>Dependency on Docker explained</h2>
<p>A <a href=/docs/concepts/containers/#container-runtimes>container runtime</a> is software that can
execute the containers that make up a Kubernetes pod. Kubernetes is responsible for orchestration
and scheduling of Pods; on each node, the <a class=glossary-tooltip title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
uses the container runtime interface as an abstraction so that you can use any compatible
container runtime.</p>
<p>In its earliest releases, Kubernetes offered compatibility with one container runtime: Docker.
Later in the Kubernetes project's history, cluster operators wanted to adopt additional container runtimes.
The CRI was designed to allow this kind of flexibility - and the kubelet began supporting CRI. However,
because Docker existed before the CRI specification was invented, the Kubernetes project created an
adapter component, <code>dockershim</code>. The dockershim adapter allows the kubelet to interact with Docker as
if Docker were a CRI compatible runtime.</p>
<p>You can read about it in <a href=/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/>Kubernetes Containerd integration goes GA</a> blog post.</p>
<p><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cri-containerd.png alt="Dockershim vs. CRI with Containerd"></p>
<p>Switching to Containerd as a container runtime eliminates the middleman. All the
same containers can be run by container runtimes like Containerd as before. But
now, since containers schedule directly with the container runtime, they are not visible to Docker.
So any Docker tooling or fancy UI you might have used
before to check on these containers is no longer available.</p>
<p>You cannot get container information using <code>docker ps</code> or <code>docker inspect</code>
commands. As you cannot list containers, you cannot get logs, stop containers,
or execute something inside container using <code>docker exec</code>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If you're running workloads via Kubernetes, the best way to stop a container is through
the Kubernetes API rather than directly through the container runtime (this advice applies
for all container runtimes, not only Docker).
</div>
<p>You can still pull images or build them using <code>docker build</code> command. But images
built or pulled by Docker would not be visible to container runtime and
Kubernetes. They needed to be pushed to some registry to allow them to be used
by Kubernetes.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Read <a href=/docs/tasks/administer-cluster/migrating-from-dockershim/>Migrating from dockershim</a> to understand your next steps</li>
<li>Read the <a href=/blog/2020/12/02/dockershim-faq/>dockershim deprecation FAQ</a> article for more information.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-eb3e279a6c5e1224e744080a52ee3f28>2.5 - Migrating telemetry and security agents from dockershim</h1>
<p>Kubernetes' support for direct integration with Docker Engine is deprecated, and will be removed. Most apps do not have a direct dependency on runtime hosting containers. However, there are still a lot of telemetry and monitoring agents that has a dependency on docker to collect containers metadata, logs and metrics. This document aggregates information on how to detect these dependencies and links on how to migrate these agents to use generic tools or alternative runtimes.</p>
<h2 id=telemetry-and-security-agents>Telemetry and security agents</h2>
<p>Within a Kubernetes cluster there are a few different ways to run telemetry or security agents.
Some agents have a direct dependency on Docker Engine when they run as DaemonSets or
directly on nodes.</p>
<h3 id=why-do-some-telemetry-agents-communicate-with-docker-engine>Why do some telemetry agents communicate with Docker Engine?</h3>
<p>Historically, Kubernetes was written to work specifically with Docker Engine.
Kubernetes took care of networking and scheduling, relying on Docker Engine for launching
and running containers (within Pods) on a node. Some information that is relevant to telemetry,
such as a pod name, is only available from Kubernetes components. Other data, such as container
metrics, is not the responsibility of the container runtime. Early telemetry agents needed to query the
container runtime <strong>and</strong> Kubernetes to report an accurate picture. Over time, Kubernetes gained
the ability to support multiple runtimes, and now supports any runtime that is compatible with
the container runtime interface.</p>
<p>Some telemetry agents rely specifically on Docker Engine tooling. For example, an agent
might run a command such as
<a href=https://docs.docker.com/engine/reference/commandline/ps/><code>docker ps</code></a>
or <a href=https://docs.docker.com/engine/reference/commandline/top/><code>docker top</code></a> to list
containers and processes or <a href=https://docs.docker.com/engine/reference/commandline/logs/><code>docker logs</code></a>
to receive streamed logs. If nodes in your existing cluster use
Docker Engine, and you switch to a different container runtime,
these commands will not work any longer.</p>
<h3 id=identify-docker-dependency>Identify DaemonSets that depend on Docker Engine</h3>
<p>If a pod wants to make calls to the <code>dockerd</code> running on the node, the pod must either:</p>
<ul>
<li>mount the filesystem containing the Docker daemon's privileged socket, as a
<a class=glossary-tooltip title="A directory containing data, accessible to the containers in a pod." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a>; or</li>
<li>mount the specific path of the Docker daemon's privileged socket directly, also as a volume.</li>
</ul>
<p>For example: on COS images, Docker exposes its Unix domain socket at
<code>/var/run/docker.sock</code> This means that the pod spec will include a
<code>hostPath</code> volume mount of <code>/var/run/docker.sock</code>.</p>
<p>Here's a sample shell script to find Pods that have a mount directly mapping the
Docker socket. This script outputs the namespace and name of the pod. You can
remove the <code>grep '/var/run/docker.sock'</code> to review other mounts.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pods --all-namespaces <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>-o<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{range .items[*]}{&#34;\n&#34;}{.metadata.namespace}{&#34;:\t&#34;}{.metadata.name}{&#34;:\t&#34;}{range .spec.volumes[*]}{.hostPath.path}{&#34;, &#34;}{end}{end}&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>| sort <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>| grep <span style=color:#b44>&#39;/var/run/docker.sock&#39;</span>
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> There are alternative ways for a pod to access Docker on the host. For instance, the parent
directory <code>/var/run</code> may be mounted instead of the full path (like in <a href=https://gist.github.com/itaysk/7bc3e56d69c4d72a549286d98fd557dd>this
example</a>).
The script above only detects the most common uses.
</div>
<h3 id=detecting-docker-dependency-from-node-agents>Detecting Docker dependency from node agents</h3>
<p>In case your cluster nodes are customized and install additional security and
telemetry agents on the node, make sure to check with the vendor of the agent whether it has dependency on Docker.</p>
<h3 id=telemetry-and-security-agent-vendors>Telemetry and security agent vendors</h3>
<p>We keep the work in progress version of migration instructions for various telemetry and security agent vendors
in <a href=https://docs.google.com/document/d/1ZFi4uKit63ga5sxEiZblfb-c23lFhvy6RXVPikS8wf0/edit#>Google doc</a>.
Please contact the vendor to get up to date instructions for migrating from dockershim.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-7743f043c43f7b12e8654e2227dbc658>3 - Certificates</h1>
<p>When using client certificate authentication, you can generate certificates
manually through <code>easyrsa</code>, <code>openssl</code> or <code>cfssl</code>.</p>
<h3 id=easyrsa>easyrsa</h3>
<p><strong>easyrsa</strong> can manually generate certificates for your cluster.</p>
<ol>
<li>
<p>Download, unpack, and initialize the patched version of easyrsa3.</p>
<pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz
tar xzf easy-rsa.tar.gz
cd easy-rsa-master/easyrsa3
./easyrsa init-pki
</code></pre>
</li>
<li>
<p>Generate a new certificate authority (CA). <code>--batch</code> sets automatic mode;
<code>--req-cn</code> specifies the Common Name (CN) for the CA's new root certificate.</p>
<pre><code>./easyrsa --batch &quot;--req-cn=${MASTER_IP}@`date +%s`&quot; build-ca nopass
</code></pre>
</li>
<li>
<p>Generate server certificate and key.
The argument <code>--subject-alt-name</code> sets the possible IPs and DNS names the API server will
be accessed with. The <code>MASTER_CLUSTER_IP</code> is usually the first IP from the service CIDR
that is specified as the <code>--service-cluster-ip-range</code> argument for both the API server and
the controller manager component. The argument <code>--days</code> is used to set the number of days
after which the certificate expires.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p>
<pre><code>./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;\
&quot;IP:${MASTER_CLUSTER_IP},&quot;\
&quot;DNS:kubernetes,&quot;\
&quot;DNS:kubernetes.default,&quot;\
&quot;DNS:kubernetes.default.svc,&quot;\
&quot;DNS:kubernetes.default.svc.cluster,&quot;\
&quot;DNS:kubernetes.default.svc.cluster.local&quot; \
--days=10000 \
build-server-full server nopass
</code></pre>
</li>
<li>
<p>Copy <code>pki/ca.crt</code>, <code>pki/issued/server.crt</code>, and <code>pki/private/server.key</code> to your directory.</p>
</li>
<li>
<p>Fill in and add the following parameters into the API server start parameters:</p>
<pre><code>--client-ca-file=/yourdirectory/ca.crt
--tls-cert-file=/yourdirectory/server.crt
--tls-private-key-file=/yourdirectory/server.key
</code></pre>
</li>
</ol>
<h3 id=openssl>openssl</h3>
<p><strong>openssl</strong> can manually generate certificates for your cluster.</p>
<ol>
<li>
<p>Generate a ca.key with 2048bit:</p>
<pre><code>openssl genrsa -out ca.key 2048
</code></pre>
</li>
<li>
<p>According to the ca.key generate a ca.crt (use -days to set the certificate effective time):</p>
<pre><code>openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt
</code></pre>
</li>
<li>
<p>Generate a server.key with 2048bit:</p>
<pre><code>openssl genrsa -out server.key 2048
</code></pre>
</li>
<li>
<p>Create a config file for generating a Certificate Signing Request (CSR).
Be sure to substitute the values marked with angle brackets (e.g. <code>&lt;MASTER_IP></code>)
with real values before saving this to a file (e.g. <code>csr.conf</code>).
Note that the value for <code>MASTER_CLUSTER_IP</code> is the service cluster IP for the
API server as described in previous subsection.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p>
<pre><code>[ req ]
default_bits = 2048
prompt = no
default_md = sha256
req_extensions = req_ext
distinguished_name = dn

[ dn ]
C = &lt;country&gt;
ST = &lt;state&gt;
L = &lt;city&gt;
O = &lt;organization&gt;
OU = &lt;organization unit&gt;
CN = &lt;MASTER_IP&gt;

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
IP.1 = &lt;MASTER_IP&gt;
IP.2 = &lt;MASTER_CLUSTER_IP&gt;

[ v3_ext ]
authorityKeyIdentifier=keyid,issuer:always
basicConstraints=CA:FALSE
keyUsage=keyEncipherment,dataEncipherment
extendedKeyUsage=serverAuth,clientAuth
subjectAltName=@alt_names
</code></pre>
</li>
<li>
<p>Generate the certificate signing request based on the config file:</p>
<pre><code>openssl req -new -key server.key -out server.csr -config csr.conf
</code></pre>
</li>
<li>
<p>Generate the server certificate using the ca.key, ca.crt and server.csr:</p>
<pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out server.crt -days 10000 \
-extensions v3_ext -extfile csr.conf
</code></pre>
</li>
<li>
<p>View the certificate signing request:</p>
<pre><code>openssl req  -noout -text -in ./server.csr
</code></pre>
</li>
<li>
<p>View the certificate:</p>
<pre><code>openssl x509  -noout -text -in ./server.crt
</code></pre>
</li>
</ol>
<p>Finally, add the same parameters into the API server start parameters.</p>
<h3 id=cfssl>cfssl</h3>
<p><strong>cfssl</strong> is another tool for certificate generation.</p>
<ol>
<li>
<p>Download, unpack and prepare the command line tools as shown below.
Note that you may need to adapt the sample commands based on the hardware
architecture and cfssl version you are using.</p>
<pre><code>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o cfssl
chmod +x cfssl
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o cfssljson
chmod +x cfssljson
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -o cfssl-certinfo
chmod +x cfssl-certinfo
</code></pre>
</li>
<li>
<p>Create a directory to hold the artifacts and initialize cfssl:</p>
<pre><code>mkdir cert
cd cert
../cfssl print-defaults config &gt; config.json
../cfssl print-defaults csr &gt; csr.json
</code></pre>
</li>
<li>
<p>Create a JSON config file for generating the CA file, for example, <code>ca-config.json</code>:</p>
<pre><code>{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre>
</li>
<li>
<p>Create a JSON config file for CA certificate signing request (CSR), for example,
<code>ca-csr.json</code>. Be sure to replace the values marked with angle brackets with
real values you want to use.</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;:[{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
</li>
<li>
<p>Generate CA key (<code>ca-key.pem</code>) and certificate (<code>ca.pem</code>):</p>
<pre><code>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</code></pre>
</li>
<li>
<p>Create a JSON config file for generating keys and certificates for the API
server, for example, <code>server-csr.json</code>. Be sure to replace the values in angle brackets with
real values you want to use. The <code>MASTER_CLUSTER_IP</code> is the service cluster
IP for the API server as described in previous subsection.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;&lt;MASTER_IP&gt;&quot;,
    &quot;&lt;MASTER_CLUSTER_IP&gt;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
</li>
<li>
<p>Generate the key and certificate for the API server, which are by default
saved into file <code>server-key.pem</code> and <code>server.pem</code> respectively:</p>
<pre><code>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
--config=ca-config.json -profile=kubernetes \
server-csr.json | ../cfssljson -bare server
</code></pre>
</li>
</ol>
<h2 id=distributing-self-signed-ca-certificate>Distributing Self-Signed CA Certificate</h2>
<p>A client node may refuse to recognize a self-signed CA certificate as valid.
For a non-production deployment, or for a deployment that runs behind a company
firewall, you can distribute a self-signed CA certificate to all clients and
refresh the local list for valid certificates.</p>
<p>On each client, perform the following operations:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
sudo update-ca-certificates
</code></pre></div><pre><code>Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><h2 id=certificates-api>Certificates API</h2>
<p>You can use the <code>certificates.k8s.io</code> API to provision
x509 certificates to use for authentication as documented
<a href=/docs/tasks/tls/managing-tls-in-a-cluster>here</a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-47be5dd51f686017f1766e6ec7aa6f41>4 - Manage Memory, CPU, and API Resources</h1>
</div>
<div class=td-content>
<h1 id=pg-337620c76587e4aeb32009cb23be46de>4.1 - Configure Default Memory Requests and Limits for a Namespace</h1>
<div class=lead>Define a default memory resource limit for a namespace, so that every new Pod in that namespace has a memory resource limit configured.</div>
<p>This page shows how to configure default memory requests and limits for a
<a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>.</p>
<p>A Kubernetes cluster can be divided into namespaces. Once you have a namespace that
has a default memory
<a href=/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>limit</a>,
and you then try to create a Pod with a container that does not specify its own memory
limit, then the
<a class=glossary-tooltip title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a> assigns the default
memory limit to that container.</p>
<p>Kubernetes assigns a default memory request under certain conditions that are explained later in this topic.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>You must have access to create namespaces in your cluster.</p>
<p>Each node in your cluster must have at least 2 GiB of memory.</p>
<h2 id=create-a-namespace>Create a namespace</h2>
<p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace default-mem-example
</code></pre></div><h2 id=create-a-limitrange-and-a-pod>Create a LimitRange and a Pod</h2>
<p>Here's a manifest for an example <a class=glossary-tooltip title="Provides constraints to limit resource consumption per Containers or Pods in a namespace." data-toggle=tooltip data-placement=top href=/docs/concepts/policy/limit-range/ target=_blank aria-label=LimitRange>LimitRange</a>.
The manifest specifies a default memory
request and a default memory limit.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-defaults.yaml download=admin/resource/memory-defaults.yaml><code>admin/resource/memory-defaults.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-defaults-yaml')" title="Copy admin/resource/memory-defaults.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-defaults-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LimitRange<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mem-limit-range<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>default</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>512Mi<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>defaultRequest</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>256Mi<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Container<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the LimitRange in the default-mem-example namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace<span style=color:#666>=</span>default-mem-example
</code></pre></div><p>Now if you create a Pod in the default-mem-example namespace, and any container
within that Pod does not specify its own values for memory request and memory limit,
then the <a class=glossary-tooltip title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a>
applies default values: a memory request of 256MiB and a memory limit of 512MiB.</p>
<p>Here's an example manifest for a Pod that has one container. The container
does not specify a memory request and limit.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-defaults-pod.yaml download=admin/resource/memory-defaults-pod.yaml><code>admin/resource/memory-defaults-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-defaults-pod-yaml')" title="Copy admin/resource/memory-defaults-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-defaults-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-mem-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-mem-demo-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace<span style=color:#666>=</span>default-mem-example
</code></pre></div><p>View detailed information about the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod default-mem-demo --output<span style=color:#666>=</span>yaml --namespace<span style=color:#666>=</span>default-mem-example
</code></pre></div><p>The output shows that the Pod's container has a memory request of 256 MiB and
a memory limit of 512 MiB. These are the default values specified by the LimitRange.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>containers:
- image: nginx
  imagePullPolicy: Always
  name: default-mem-demo-ctr
  resources:
    limits:
      memory: 512Mi
    requests:
      memory: 256Mi
</code></pre></div><p>Delete your Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete pod default-mem-demo --namespace<span style=color:#666>=</span>default-mem-example
</code></pre></div><h2 id=what-if-you-specify-a-container-s-limit-but-not-its-request>What if you specify a container's limit, but not its request?</h2>
<p>Here's a manifest for a Pod that has one container. The container
specifies a memory limit, but not a request:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-defaults-pod-2.yaml download=admin/resource/memory-defaults-pod-2.yaml><code>admin/resource/memory-defaults-pod-2.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-defaults-pod-2-yaml')" title="Copy admin/resource/memory-defaults-pod-2.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-defaults-pod-2-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-mem-demo-2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-mem-demo-2-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace<span style=color:#666>=</span>default-mem-example
</code></pre></div><p>View detailed information about the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod default-mem-demo-2 --output<span style=color:#666>=</span>yaml --namespace<span style=color:#666>=</span>default-mem-example
</code></pre></div><p>The output shows that the container's memory request is set to match its memory limit.
Notice that the container was not assigned the default memory request value of 256Mi.</p>
<pre><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><h2 id=what-if-you-specify-a-container-s-request-but-not-its-limit>What if you specify a container's request, but not its limit?</h2>
<p>Here's a manifest for a Pod that has one container. The container
specifies a memory request, but not a limit:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-defaults-pod-3.yaml download=admin/resource/memory-defaults-pod-3.yaml><code>admin/resource/memory-defaults-pod-3.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-defaults-pod-3-yaml')" title="Copy admin/resource/memory-defaults-pod-3.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-defaults-pod-3-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-mem-demo-3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-mem-demo-3-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;128Mi&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace<span style=color:#666>=</span>default-mem-example
</code></pre></div><p>View the Pod's specification:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod default-mem-demo-3 --output<span style=color:#666>=</span>yaml --namespace<span style=color:#666>=</span>default-mem-example
</code></pre></div><p>The output shows that the container's memory request is set to the value specified in the
container's manifest. The container is limited to use no more than 512MiB of
memory, which matches the default memory limit for the namespace.</p>
<pre><code>resources:
  limits:
    memory: 512Mi
  requests:
    memory: 128Mi
</code></pre><h2 id=motivation-for-default-memory-limits-and-requests>Motivation for default memory limits and requests</h2>
<p>If your namespace has a memory <a class=glossary-tooltip title="Provides constraints that limit aggregate resource consumption per namespace." data-toggle=tooltip data-placement=top href=/docs/concepts/policy/resource-quotas/ target=_blank aria-label="resource quota">resource quota</a>
configured,
it is helpful to have a default value in place for memory limit.
Here are two of the restrictions that a resource quota imposes on a namespace:</p>
<ul>
<li>For every Pod that runs in the namespace, the Pod and each of its containers must have a memory limit.
(If you specify a memory limit for every container in a Pod, Kubernetes can infer the Pod-level memory
limit by adding up the limits for its containers).</li>
<li>Memory limits apply a resource reservation on the node where the Pod in question is scheduled.
The total amount of memory reserved for all Pods in the namespace must not exceed a specified limit.</li>
<li>The total amount of memory actually used by all Pods in the namespace must also not exceed a specified limit.</li>
</ul>
<p>When you add a LimitRange:</p>
<p>If any Pod in that namespace that includes a container does not specify its own memory limit,
the control plane applies the default memory limit to that container, and the Pod can be
allowed to run in a namespace that is restricted by a memory ResourceQuota.</p>
<h2 id=clean-up>Clean up</h2>
<p>Delete your namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete namespace default-mem-example
</code></pre></div><h2 id=what-s-next>What's next</h2>
<h3 id=for-cluster-administrators>For cluster administrators</h3>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>Configure Default CPU Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>Configure Minimum and Maximum Memory Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Configure Minimum and Maximum CPU Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>Configure Memory and CPU Quotas for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/>Configure a Pod Quota for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/quota-api-object/>Configure Quotas for API Objects</a></p>
</li>
</ul>
<h3 id=for-app-developers>For app developers</h3>
<ul>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-memory-resource/>Assign Memory Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>Assign CPU Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/quality-service-pod/>Configure Quality of Service for Pods</a></p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-320af95e480962c538ebef7ae205845c>4.2 - Configure Default CPU Requests and Limits for a Namespace</h1>
<div class=lead>Define a default CPU resource limits for a namespace, so that every new Pod in that namespace has a CPU resource limit configured.</div>
<p>This page shows how to configure default CPU requests and limits for a
<a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>.</p>
<p>A Kubernetes cluster can be divided into namespaces. If you create a Pod within a
namespace that has a default CPU
<a href=/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>limit</a>, and any container in that Pod does not specify
its own CPU limit, then the
<a class=glossary-tooltip title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a> assigns the default
CPU limit to that container.</p>
<p>Kubernetes assigns a default CPU
<a href=/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>request</a>,
but only under certain conditions that are explained later in this page.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>You must have access to create namespaces in your cluster.</p>
<p>If you're not already familiar with what Kubernetes means by 1.0 CPU,
read <a href=/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu>meaning of CPU</a>.</p>
<h2 id=create-a-namespace>Create a namespace</h2>
<p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace default-cpu-example
</code></pre></div><h2 id=create-a-limitrange-and-a-pod>Create a LimitRange and a Pod</h2>
<p>Here's a manifest for an example <a class=glossary-tooltip title="Provides constraints to limit resource consumption per Containers or Pods in a namespace." data-toggle=tooltip data-placement=top href=/docs/concepts/policy/limit-range/ target=_blank aria-label=LimitRange>LimitRange</a>.
The manifest specifies a default CPU request and a default CPU limit.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-defaults.yaml download=admin/resource/cpu-defaults.yaml><code>admin/resource/cpu-defaults.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-defaults-yaml')" title="Copy admin/resource/cpu-defaults.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-defaults-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LimitRange<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu-limit-range<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>default</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>defaultRequest</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>0.5</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Container<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the LimitRange in the default-cpu-example namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace<span style=color:#666>=</span>default-cpu-example
</code></pre></div><p>Now if you create a Pod in the default-cpu-example namespace, and any container
in that Pod does not specify its own values for CPU request and CPU limit,
then the control plane applies default values: a CPU request of 0.5 and a default
CPU limit of 1.</p>
<p>Here's a manifest for a Pod that has one container. The container
does not specify a CPU request and limit.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-defaults-pod.yaml download=admin/resource/cpu-defaults-pod.yaml><code>admin/resource/cpu-defaults-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-defaults-pod-yaml')" title="Copy admin/resource/cpu-defaults-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-defaults-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-cpu-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-cpu-demo-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod.yaml --namespace<span style=color:#666>=</span>default-cpu-example
</code></pre></div><p>View the Pod's specification:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod default-cpu-demo --output<span style=color:#666>=</span>yaml --namespace<span style=color:#666>=</span>default-cpu-example
</code></pre></div><p>The output shows that the Pod's only container has a CPU request of 500m <code>cpu</code>
(which you can read as “500 millicpu”), and a CPU limit of 1 <code>cpu</code>.
These are the default values specified by the LimitRange.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>containers:
- image: nginx
  imagePullPolicy: Always
  name: default-cpu-demo-ctr
  resources:
    limits:
      cpu: <span style=color:#b44>&#34;1&#34;</span>
    requests:
      cpu: 500m
</code></pre></div><h2 id=what-if-you-specify-a-container-s-limit-but-not-its-request>What if you specify a container's limit, but not its request?</h2>
<p>Here's a manifest for a Pod that has one container. The container
specifies a CPU limit, but not a request:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-defaults-pod-2.yaml download=admin/resource/cpu-defaults-pod-2.yaml><code>admin/resource/cpu-defaults-pod-2.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-defaults-pod-2-yaml')" title="Copy admin/resource/cpu-defaults-pod-2.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-defaults-pod-2-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-cpu-demo-2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-cpu-demo-2-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-2.yaml --namespace<span style=color:#666>=</span>default-cpu-example
</code></pre></div><p>View the <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status>specification</a>
of the Pod that you created:</p>
<pre><code>kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example
</code></pre><p>The output shows that the container's CPU request is set to match its CPU limit.
Notice that the container was not assigned the default CPU request value of 0.5 <code>cpu</code>:</p>
<pre><code>resources:
  limits:
    cpu: &quot;1&quot;
  requests:
    cpu: &quot;1&quot;
</code></pre><h2 id=what-if-you-specify-a-container-s-request-but-not-its-limit>What if you specify a container's request, but not its limit?</h2>
<p>Here's an example manifest for a Pod that has one container. The container
specifies a CPU request, but not a limit:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-defaults-pod-3.yaml download=admin/resource/cpu-defaults-pod-3.yaml><code>admin/resource/cpu-defaults-pod-3.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-defaults-pod-3-yaml')" title="Copy admin/resource/cpu-defaults-pod-3.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-defaults-pod-3-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-cpu-demo-3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-cpu-demo-3-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0.75&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-3.yaml --namespace<span style=color:#666>=</span>default-cpu-example
</code></pre></div><p>View the specification of the Pod that you created:</p>
<pre><code>kubectl get pod default-cpu-demo-3 --output=yaml --namespace=default-cpu-example
</code></pre><p>The output shows that the container's CPU request is set to the value you specified at
the time you created the Pod (in other words: it matches the manifest).
However, the same container's CPU limit is set to 1 <code>cpu</code>, which is the default CPU limit
for that namespace.</p>
<pre><code>resources:
  limits:
    cpu: &quot;1&quot;
  requests:
    cpu: 750m
</code></pre><h2 id=motivation-for-default-cpu-limits-and-requests>Motivation for default CPU limits and requests</h2>
<p>If your namespace has a CPU <a class=glossary-tooltip title="Provides constraints that limit aggregate resource consumption per namespace." data-toggle=tooltip data-placement=top href=/docs/concepts/policy/resource-quotas/ target=_blank aria-label="resource quota">resource quota</a>
configured,
it is helpful to have a default value in place for CPU limit.
Here are two of the restrictions that a CPU resource quota imposes on a namespace:</p>
<ul>
<li>For every Pod that runs in the namespace, each of its containers must have a CPU limit.</li>
<li>CPU limits apply a resource reservation on the node where the Pod in question is scheduled.
The total amount of CPU that is reserved for use by all Pods in the namespace must not
exceed a specified limit.</li>
</ul>
<p>When you add a LimitRange:</p>
<p>If any Pod in that namespace that includes a container does not specify its own CPU limit,
the control plane applies the default CPU limit to that container, and the Pod can be
allowed to run in a namespace that is restricted by a CPU ResourceQuota.</p>
<h2 id=clean-up>Clean up</h2>
<p>Delete your namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete namespace default-cpu-example
</code></pre></div><h2 id=what-s-next>What's next</h2>
<h3 id=for-cluster-administrators>For cluster administrators</h3>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>Configure Minimum and Maximum Memory Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Configure Minimum and Maximum CPU Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>Configure Memory and CPU Quotas for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/>Configure a Pod Quota for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/quota-api-object/>Configure Quotas for API Objects</a></p>
</li>
</ul>
<h3 id=for-app-developers>For app developers</h3>
<ul>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-memory-resource/>Assign Memory Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>Assign CPU Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/quality-service-pod/>Configure Quality of Service for Pods</a></p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-adb489b1ab985c9215657b0d4c6ae92b>4.3 - Configure Minimum and Maximum Memory Constraints for a Namespace</h1>
<div class=lead>Define a range of valid memory resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</div>
<p>This page shows how to set minimum and maximum values for memory used by containers
running in a namespace. You specify minimum and maximum memory values in a
<a href=/docs/reference/generated/kubernetes-api/v1.23/#limitrange-v1-core>LimitRange</a>
object. If a Pod does not meet the constraints imposed by the LimitRange,
it cannot be created in the namespace.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>You must have access to create namespaces in your cluster.</p>
<p>Each node in your cluster must have at least 1 GiB of memory available for Pods.</p>
<h2 id=create-a-namespace>Create a namespace</h2>
<p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace constraints-mem-example
</code></pre></div><h2 id=create-a-limitrange-and-a-pod>Create a LimitRange and a Pod</h2>
<p>Here's an example manifest for a LimitRange:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-constraints.yaml download=admin/resource/memory-constraints.yaml><code>admin/resource/memory-constraints.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-constraints-yaml')" title="Copy admin/resource/memory-constraints.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-constraints-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LimitRange<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mem-min-max-demo-lr<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>min</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>500Mi<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Container<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the LimitRange:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><p>View detailed information about the LimitRange:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get limitrange mem-min-max-demo-lr --namespace<span style=color:#666>=</span>constraints-mem-example --output<span style=color:#666>=</span>yaml
</code></pre></div><p>The output shows the minimum and maximum memory constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.</p>
<pre><code>  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
</code></pre><p>Now whenever you define a Pod within the constraints-mem-example namespace, Kubernetes
performs these steps:</p>
<ul>
<li>
<p>If any container in that Pod does not specify its own memory request and limit, assign
the default memory request and limit to that container.</p>
</li>
<li>
<p>Verify that every container in that Pod requests at least 500 MiB of memory.</p>
</li>
<li>
<p>Verify that every container in that Pod requests no more than 1024 MiB (1 GiB)
of memory.</p>
</li>
</ul>
<p>Here's a manifest for a Pod that has one container. Within the Pod spec, the sole
container specifies a memory request of 600 MiB and a memory limit of 800 MiB. These satisfy the
minimum and maximum memory constraints imposed by the LimitRange.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-constraints-pod.yaml download=admin/resource/memory-constraints-pod.yaml><code>admin/resource/memory-constraints-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-constraints-pod-yaml')" title="Copy admin/resource/memory-constraints-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-constraints-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;600Mi&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><p>Verify that the Pod is running and that its container is healthy:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod constraints-mem-demo --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><p>View detailed information about the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod constraints-mem-demo --output<span style=color:#666>=</span>yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><p>The output shows that the container within that Pod has a memory request of 600 MiB and
a memory limit of 800 MiB. These satisfy the constraints imposed by the LimitRange for
this namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>800Mi<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>600Mi<span style=color:#bbb>
</span></code></pre></div><p>Delete your Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete pod constraints-mem-demo --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><h2 id=attempt-to-create-a-pod-that-exceeds-the-maximum-memory-constraint>Attempt to create a Pod that exceeds the maximum memory constraint</h2>
<p>Here's a manifest for a Pod that has one container. The container specifies a
memory request of 800 MiB and a memory limit of 1.5 GiB.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-constraints-pod-2.yaml download=admin/resource/memory-constraints-pod-2.yaml><code>admin/resource/memory-constraints-pod-2.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-constraints-pod-2-yaml')" title="Copy admin/resource/memory-constraints-pod-2.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-constraints-pod-2-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-2-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1.5Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800Mi&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Attempt to create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><p>The output shows that the Pod does not get created, because it defines a container that
requests more memory than is allowed:</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/memory-constraints-pod-2.yaml&quot;:
pods &quot;constraints-mem-demo-2&quot; is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi.
</code></pre><h2 id=attempt-to-create-a-pod-that-does-not-meet-the-minimum-memory-request>Attempt to create a Pod that does not meet the minimum memory request</h2>
<p>Here's a manifest for a Pod that has one container. That container specifies a
memory request of 100 MiB and a memory limit of 800 MiB.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-constraints-pod-3.yaml download=admin/resource/memory-constraints-pod-3.yaml><code>admin/resource/memory-constraints-pod-3.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-constraints-pod-3-yaml')" title="Copy admin/resource/memory-constraints-pod-3.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-constraints-pod-3-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-3-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Mi&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Attempt to create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><p>The output shows that the Pod does not get created, because it defines a container
that requests less memory than the enforced minimum:</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/memory-constraints-pod-3.yaml&quot;:
pods &quot;constraints-mem-demo-3&quot; is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi.
</code></pre><h2 id=create-a-pod-that-does-not-specify-any-memory-request-or-limit>Create a Pod that does not specify any memory request or limit</h2>
<p>Here's a manifest for a Pod that has one container. The container does not
specify a memory request, and it does not specify a memory limit.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/memory-constraints-pod-4.yaml download=admin/resource/memory-constraints-pod-4.yaml><code>admin/resource/memory-constraints-pod-4.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-memory-constraints-pod-4-yaml')" title="Copy admin/resource/memory-constraints-pod-4.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-memory-constraints-pod-4-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-4<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-4-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><p>View detailed information about the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod constraints-mem-demo-4 --namespace<span style=color:#666>=</span>constraints-mem-example --output<span style=color:#666>=</span>yaml
</code></pre></div><p>The output shows that the Pod's only container has a memory request of 1 GiB and a memory limit of 1 GiB.
How did that container get those values?</p>
<pre><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><p>Because your Pod did not define any memory request and limit for that container, the cluster
applied a
<a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>default memory request and limit</a>
from the LimitRange.</p>
<p>This means that the definition of that Pod shows those values. You can check it using
<code>kubectl describe</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># Look for the &#34;Requests:&#34; section of the output</span>
kubectl describe pod constraints-mem-demo-4 --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><p>At this point, your Pod might be running or it might not be running. Recall that a prerequisite
for this task is that your Nodes have at least 1 GiB of memory. If each of your Nodes has only
1 GiB of memory, then there is not enough allocatable memory on any Node to accommodate a memory
request of 1 GiB. If you happen to be using Nodes with 2 GiB of memory, then you probably have
enough space to accommodate the 1 GiB request.</p>
<p>Delete your Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete pod constraints-mem-demo-4 --namespace<span style=color:#666>=</span>constraints-mem-example
</code></pre></div><h2 id=enforcement-of-minimum-and-maximum-memory-constraints>Enforcement of minimum and maximum memory constraints</h2>
<p>The maximum and minimum memory constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.</p>
<h2 id=motivation-for-minimum-and-maximum-memory-constraints>Motivation for minimum and maximum memory constraints</h2>
<p>As a cluster administrator, you might want to impose restrictions on the amount of memory that Pods can use.
For example:</p>
<ul>
<li>
<p>Each Node in a cluster has 2 GiB of memory. You do not want to accept any Pod that requests
more than 2 GiB of memory, because no Node in the cluster can support the request.</p>
</li>
<li>
<p>A cluster is shared by your production and development departments.
You want to allow production workloads to consume up to 8 GiB of memory, but
you want development workloads to be limited to 512 MiB. You create separate namespaces
for production and development, and you apply memory constraints to each namespace.</p>
</li>
</ul>
<h2 id=clean-up>Clean up</h2>
<p>Delete your namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete namespace constraints-mem-example
</code></pre></div><h2 id=what-s-next>What's next</h2>
<h3 id=for-cluster-administrators>For cluster administrators</h3>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>Configure Default CPU Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Configure Minimum and Maximum CPU Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>Configure Memory and CPU Quotas for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/>Configure a Pod Quota for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/quota-api-object/>Configure Quotas for API Objects</a></p>
</li>
</ul>
<h3 id=for-app-developers>For app developers</h3>
<ul>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-memory-resource/>Assign Memory Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>Assign CPU Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/quality-service-pod/>Configure Quality of Service for Pods</a></p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-a87cbd1f9379dac7a48ae320da68a9ad>4.4 - Configure Minimum and Maximum CPU Constraints for a Namespace</h1>
<div class=lead>Define a range of valid CPU resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</div>
<p>This page shows how to set minimum and maximum values for the CPU resources used by containers
and Pods in a <a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>. You specify minimum
and maximum CPU values in a
<a href=/docs/reference/kubernetes-api/policy-resources/limit-range-v1/>LimitRange</a>
object. If a Pod does not meet the constraints imposed by the LimitRange, it cannot be created
in the namespace.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>You must have access to create namespaces in your cluster.</p>
<p>Your cluster must have at least 1.0 CPU available for use to run the task examples.
See <a href=/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu>meaning of CPU</a>
to learn what Kubernetes means by “1 CPU”.</p>
<h2 id=create-a-namespace>Create a namespace</h2>
<p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace constraints-cpu-example
</code></pre></div><h2 id=create-a-limitrange-and-a-pod>Create a LimitRange and a Pod</h2>
<p>Here's an example manifest for a LimitRange:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-constraints.yaml download=admin/resource/cpu-constraints.yaml><code>admin/resource/cpu-constraints.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-constraints-yaml')" title="Copy admin/resource/cpu-constraints.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-constraints-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LimitRange<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu-min-max-demo-lr<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>min</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Container<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the LimitRange:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><p>View detailed information about the LimitRange:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get limitrange cpu-min-max-demo-lr --output<span style=color:#666>=</span>yaml --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><p>The output shows the minimum and maximum CPU constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>default</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>800m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>defaultRequest</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>800m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>800m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>min</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>200m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Container<span style=color:#bbb>
</span></code></pre></div><p>Now whenever you create a Pod in the constraints-cpu-example namespace (or some other client
of the Kubernetes API creates an equivalent Pod), Kubernetes performs these steps:</p>
<ul>
<li>
<p>If any container in that Pod does not specify its own CPU request and limit, the control plane
assigns the default CPU request and limit to that container.</p>
</li>
<li>
<p>Verify that every container in that Pod specifies a CPU request that is greater than or equal to 200 millicpu.</p>
</li>
<li>
<p>Verify that every container in that Pod specifies a CPU limit that is less than or equal to 800 millicpu.</p>
</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> When creating a <code>LimitRange</code> object, you can specify limits on huge-pages
or GPUs as well. However, when both <code>default</code> and <code>defaultRequest</code> are specified
on these resources, the two values must be the same.
</div>
<p>Here's a manifest for a Pod that has one container. The container manifest
specifies a CPU request of 500 millicpu and a CPU limit of 800 millicpu. These satisfy the
minimum and maximum CPU constraints imposed by the LimitRange.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-constraints-pod.yaml download=admin/resource/cpu-constraints-pod.yaml><code>admin/resource/cpu-constraints-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-constraints-pod-yaml')" title="Copy admin/resource/cpu-constraints-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-constraints-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-cpu-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-cpu-demo-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><p>Verify that the Pod is running and that its container is healthy:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod constraints-cpu-demo --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><p>View detailed information about the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod constraints-cpu-demo --output<span style=color:#666>=</span>yaml --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><p>The output shows that the Pod's only container has a CPU request of 500 millicpu and CPU limit
of 800 millicpu. These satisfy the constraints imposed by the LimitRange.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>800m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span></code></pre></div><h2 id=delete-the-pod>Delete the Pod</h2>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete pod constraints-cpu-demo --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><h2 id=attempt-to-create-a-pod-that-exceeds-the-maximum-cpu-constraint>Attempt to create a Pod that exceeds the maximum CPU constraint</h2>
<p>Here's a manifest for a Pod that has one container. The container specifies a
CPU request of 500 millicpu and a cpu limit of 1.5 cpu.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-constraints-pod-2.yaml download=admin/resource/cpu-constraints-pod-2.yaml><code>admin/resource/cpu-constraints-pod-2.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-constraints-pod-2-yaml')" title="Copy admin/resource/cpu-constraints-pod-2.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-constraints-pod-2-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-cpu-demo-2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-cpu-demo-2-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1.5&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Attempt to create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><p>The output shows that the Pod does not get created, because it defines an unacceptable container.
That container is not acceptable because it specifies a CPU limit that is too large:</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/cpu-constraints-pod-2.yaml&quot;:
pods &quot;constraints-cpu-demo-2&quot; is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m.
</code></pre><h2 id=attempt-to-create-a-pod-that-does-not-meet-the-minimum-cpu-request>Attempt to create a Pod that does not meet the minimum CPU request</h2>
<p>Here's a manifest for a Pod that has one container. The container specifies a
CPU request of 100 millicpu and a CPU limit of 800 millicpu.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-constraints-pod-3.yaml download=admin/resource/cpu-constraints-pod-3.yaml><code>admin/resource/cpu-constraints-pod-3.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-constraints-pod-3-yaml')" title="Copy admin/resource/cpu-constraints-pod-3.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-constraints-pod-3-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-cpu-demo-3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-cpu-demo-3-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100m&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Attempt to create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><p>The output shows that the Pod does not get created, because it defines an unacceptable container.
That container is not acceptable because it specifies a CPU request that is lower than the
enforced minimum:</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/cpu-constraints-pod-3.yaml&quot;:
pods &quot;constraints-cpu-demo-3&quot; is forbidden: minimum cpu usage per Container is 200m, but request is 100m.
</code></pre><h2 id=create-a-pod-that-does-not-specify-any-cpu-request-or-limit>Create a Pod that does not specify any CPU request or limit</h2>
<p>Here's a manifest for a Pod that has one container. The container does not
specify a CPU request, nor does it specify a CPU limit.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/cpu-constraints-pod-4.yaml download=admin/resource/cpu-constraints-pod-4.yaml><code>admin/resource/cpu-constraints-pod-4.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-cpu-constraints-pod-4-yaml')" title="Copy admin/resource/cpu-constraints-pod-4.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-cpu-constraints-pod-4-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-cpu-demo-4<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-cpu-demo-4-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>vish/stress<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace<span style=color:#666>=</span>constraints-cpu-example
</code></pre></div><p>View detailed information about the Pod:</p>
<pre><code>kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml
</code></pre><p>The output shows that the Pod's single container has a CPU request of 800 millicpu and a
CPU limit of 800 millicpu.
How did that container get those values?</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>800m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>800m<span style=color:#bbb>
</span></code></pre></div><p>Because that container did not specify its own CPU request and limit, the control plane
applied the
<a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>default CPU request and limit</a>
from the LimitRange for this namespace.</p>
<p>At this point, your Pod might be running or it might not be running. Recall that a prerequisite for this task is that your cluster must have at least 1 CPU available for use. If each of your Nodes has only 1 CPU, then there might not be enough allocatable CPU on any Node to accommodate a request of 800 millicpu. If you happen to be using Nodes with 2 CPU, then you probably have enough CPU to accommodate the 800 millicpu request.</p>
<p>Delete your Pod:</p>
<pre><code>kubectl delete pod constraints-cpu-demo-4 --namespace=constraints-cpu-example
</code></pre><h2 id=enforcement-of-minimum-and-maximum-cpu-constraints>Enforcement of minimum and maximum CPU constraints</h2>
<p>The maximum and minimum CPU constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.</p>
<h2 id=motivation-for-minimum-and-maximum-cpu-constraints>Motivation for minimum and maximum CPU constraints</h2>
<p>As a cluster administrator, you might want to impose restrictions on the CPU resources that Pods can use.
For example:</p>
<ul>
<li>
<p>Each Node in a cluster has 2 CPU. You do not want to accept any Pod that requests
more than 2 CPU, because no Node in the cluster can support the request.</p>
</li>
<li>
<p>A cluster is shared by your production and development departments.
You want to allow production workloads to consume up to 3 CPU, but you want development workloads to be limited
to 1 CPU. You create separate namespaces for production and development, and you apply CPU constraints to
each namespace.</p>
</li>
</ul>
<h2 id=clean-up>Clean up</h2>
<p>Delete your namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete namespace constraints-cpu-example
</code></pre></div><h2 id=what-s-next>What's next</h2>
<h3 id=for-cluster-administrators>For cluster administrators</h3>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>Configure Default CPU Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>Configure Minimum and Maximum Memory Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>Configure Memory and CPU Quotas for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/>Configure a Pod Quota for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/quota-api-object/>Configure Quotas for API Objects</a></p>
</li>
</ul>
<h3 id=for-app-developers>For app developers</h3>
<ul>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-memory-resource/>Assign Memory Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>Assign CPU Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/quality-service-pod/>Configure Quality of Service for Pods</a></p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-fe3283559a3df299aae3ee00ecea2fad>4.5 - Configure Memory and CPU Quotas for a Namespace</h1>
<div class=lead>Define overall memory and CPU resource limits for a namespace.</div>
<p>This page shows how to set quotas for the total amount memory and CPU that
can be used by all Pods running in a <a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>.
You specify quotas in a
<a href=/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/>ResourceQuota</a>
object.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>You must have access to create namespaces in your cluster.</p>
<p>Each node in your cluster must have at least 1 GiB of memory.</p>
<h2 id=create-a-namespace>Create a namespace</h2>
<p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace quota-mem-cpu-example
</code></pre></div><h2 id=create-a-resourcequota>Create a ResourceQuota</h2>
<p>Here is a manifest for an example ResourceQuota:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/quota-mem-cpu.yaml download=admin/resource/quota-mem-cpu.yaml><code>admin/resource/quota-mem-cpu.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-quota-mem-cpu-yaml')" title="Copy admin/resource/quota-mem-cpu.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-quota-mem-cpu-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mem-cpu-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests.cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests.memory</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>limits.cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>limits.memory</span>:<span style=color:#bbb> </span>2Gi<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the ResourceQuota:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace<span style=color:#666>=</span>quota-mem-cpu-example
</code></pre></div><p>View detailed information about the ResourceQuota:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get resourcequota mem-cpu-demo --namespace<span style=color:#666>=</span>quota-mem-cpu-example --output<span style=color:#666>=</span>yaml
</code></pre></div><p>The ResourceQuota places these requirements on the quota-mem-cpu-example namespace:</p>
<ul>
<li>For every Pod in the namespace, each container must have a memory request, memory limit, cpu request, and cpu limit.</li>
<li>The memory request total for all Pods in that namespace must not exceed 1 GiB.</li>
<li>The memory limit total for all Pods in that namespace must not exceed 2 GiB.</li>
<li>The CPU request total for all Pods in that namespace must not exceed 1 cpu.</li>
<li>The CPU limit total for all Pods in that namespace must not exceed 2 cpu.</li>
</ul>
<p>See <a href=/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu>meaning of CPU</a>
to learn what Kubernetes means by “1 CPU”.</p>
<h2 id=create-a-pod>Create a Pod</h2>
<p>Here is a manifest for an example Pod:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/quota-mem-cpu-pod.yaml download=admin/resource/quota-mem-cpu-pod.yaml><code>admin/resource/quota-mem-cpu-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-quota-mem-cpu-pod-yaml')" title="Copy admin/resource/quota-mem-cpu-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-quota-mem-cpu-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>quota-mem-cpu-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>quota-mem-cpu-demo-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;600Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;400m&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace<span style=color:#666>=</span>quota-mem-cpu-example
</code></pre></div><p>Verify that the Pod is running and that its (only) container is healthy:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pod quota-mem-cpu-demo --namespace<span style=color:#666>=</span>quota-mem-cpu-example
</code></pre></div><p>Once again, view detailed information about the ResourceQuota:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get resourcequota mem-cpu-demo --namespace<span style=color:#666>=</span>quota-mem-cpu-example --output<span style=color:#666>=</span>yaml
</code></pre></div><p>The output shows the quota along with how much of the quota has been used.
You can see that the memory and CPU requests and limits for your Pod do not
exceed the quota.</p>
<pre><code>status:
  hard:
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
  used:
    limits.cpu: 800m
    limits.memory: 800Mi
    requests.cpu: 400m
    requests.memory: 600Mi
</code></pre><p>If you have the <code>jq</code> tool, you can also query (using <a href=/docs/reference/kubectl/jsonpath/>JSONPath</a>)
for just the <code>used</code> values, <strong>and</strong> pretty-print that that of the output. For example:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get resourcequota mem-cpu-demo --namespace<span style=color:#666>=</span>quota-mem-cpu-example -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{ .status.used }&#39;</span> | jq .
</code></pre></div><h2 id=attempt-to-create-a-second-pod>Attempt to create a second Pod</h2>
<p>Here is a manifest for a second Pod:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/quota-mem-cpu-pod-2.yaml download=admin/resource/quota-mem-cpu-pod-2.yaml><code>admin/resource/quota-mem-cpu-pod-2.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-quota-mem-cpu-pod-2-yaml')" title="Copy admin/resource/quota-mem-cpu-pod-2.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-quota-mem-cpu-pod-2-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>quota-mem-cpu-demo-2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>quota-mem-cpu-demo-2-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;700Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;400m&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>In the manifest, you can see that the Pod has a memory request of 700 MiB.
Notice that the sum of the used memory request and this new memory
request exceeds the memory request quota: 600 MiB + 700 MiB > 1 GiB.</p>
<p>Attempt to create the Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace<span style=color:#666>=</span>quota-mem-cpu-example
</code></pre></div><p>The second Pod does not get created. The output shows that creating the second Pod
would cause the memory request total to exceed the memory request quota.</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/quota-mem-cpu-pod-2.yaml&quot;:
pods &quot;quota-mem-cpu-demo-2&quot; is forbidden: exceeded quota: mem-cpu-demo,
requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi
</code></pre><h2 id=discussion>Discussion</h2>
<p>As you have seen in this exercise, you can use a ResourceQuota to restrict
the memory request total for all Pods running in a namespace.
You can also restrict the totals for memory limit, cpu request, and cpu limit.</p>
<p>Instead of managing total resource use within a namespace, you might want to restrict
individual Pods, or the containers in those Pods. To achieve that kind of limiting, use a
<a href=/docs/concepts/policy/limit-range/>LimitRange</a>.</p>
<h2 id=clean-up>Clean up</h2>
<p>Delete your namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete namespace quota-mem-cpu-example
</code></pre></div><h2 id=what-s-next>What's next</h2>
<h3 id=for-cluster-administrators>For cluster administrators</h3>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>Configure Default CPU Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>Configure Minimum and Maximum Memory Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Configure Minimum and Maximum CPU Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/>Configure a Pod Quota for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/quota-api-object/>Configure Quotas for API Objects</a></p>
</li>
</ul>
<h3 id=for-app-developers>For app developers</h3>
<ul>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-memory-resource/>Assign Memory Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>Assign CPU Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/quality-service-pod/>Configure Quality of Service for Pods</a></p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-40e30a9209e0c9f4153707e43243e9d7>4.6 - Configure a Pod Quota for a Namespace</h1>
<div class=lead>Restrict how many Pods you can create within a namespace.</div>
<p>This page shows how to set a quota for the total number of Pods that can run
in a <a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=Namespace>Namespace</a>. You specify quotas in a
<a href=/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/>ResourceQuota</a>
object.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>You must have access to create namespaces in your cluster.</p>
<h2 id=create-a-namespace>Create a namespace</h2>
<p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace quota-pod-example
</code></pre></div><h2 id=create-a-resourcequota>Create a ResourceQuota</h2>
<p>Here is an example manifest for a ResourceQuota:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/quota-pod.yaml download=admin/resource/quota-pod.yaml><code>admin/resource/quota-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-quota-pod-yaml')" title="Copy admin/resource/quota-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-quota-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the ResourceQuota:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace<span style=color:#666>=</span>quota-pod-example
</code></pre></div><p>View detailed information about the ResourceQuota:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get resourcequota pod-demo --namespace<span style=color:#666>=</span>quota-pod-example --output<span style=color:#666>=</span>yaml
</code></pre></div><p>The output shows that the namespace has a quota of two Pods, and that currently there are
no Pods; that is, none of the quota is used.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>used</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Here is an example manifest for a <a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/quota-pod-deployment.yaml download=admin/resource/quota-pod-deployment.yaml><code>admin/resource/quota-pod-deployment.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-quota-pod-deployment-yaml')" title="Copy admin/resource/quota-pod-deployment.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-quota-pod-deployment-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod-quota-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>purpose</span>:<span style=color:#bbb> </span>quota-demo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>purpose</span>:<span style=color:#bbb> </span>quota-demo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod-quota-demo<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>In that manifest, <code>replicas: 3</code> tells Kubernetes to attempt to create three new Pods, all
running the same application.</p>
<p>Create the Deployment:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace<span style=color:#666>=</span>quota-pod-example
</code></pre></div><p>View detailed information about the Deployment:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment pod-quota-demo --namespace<span style=color:#666>=</span>quota-pod-example --output<span style=color:#666>=</span>yaml
</code></pre></div><p>The output shows that even though the Deployment specifies three replicas, only two
Pods were created because of the quota you defined earlier:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>availableReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>lastUpdateTime</span>:<span style=color:#bbb> </span>2021-04-02T20:57:05Z<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>message: &#39;unable to create pods</span>:<span style=color:#bbb> </span>pods &#34;pod-quota-demo-1650323038-&#34; is forbidden:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited</span>:<span style=color:#bbb> </span>pods=2&#39;<span style=color:#bbb>
</span></code></pre></div><h3 id=choice-of-resource>Choice of resource</h3>
<p>In this task you have defined a ResourceQuota that limited the total number of Pods, but
you could also limit the total number of other kinds of object. For example, you
might decide to limit how many <a class=glossary-tooltip title="A repeating task (a Job) that runs on a regular schedule." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/cron-jobs/ target=_blank aria-label=CronJobs>CronJobs</a>
that can live in a single namespace.</p>
<h2 id=clean-up>Clean up</h2>
<p>Delete your namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete namespace quota-pod-example
</code></pre></div><h2 id=what-s-next>What's next</h2>
<h3 id=for-cluster-administrators>For cluster administrators</h3>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>Configure Default CPU Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>Configure Minimum and Maximum Memory Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Configure Minimum and Maximum CPU Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>Configure Memory and CPU Quotas for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/quota-api-object/>Configure Quotas for API Objects</a></p>
</li>
</ul>
<h3 id=for-app-developers>For app developers</h3>
<ul>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-memory-resource/>Assign Memory Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>Assign CPU Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/quality-service-pod/>Configure Quality of Service for Pods</a></p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-8c31aafd38fad5b0de0bd191758d6f93>5 - Install a Network Policy Provider</h1>
</div>
<div class=td-content>
<h1 id=pg-b4418905b0c14630e4e9cb1368241534>5.1 - Use Antrea for NetworkPolicy</h1>
<p>This page shows how to install and use Antrea CNI plugin on Kubernetes.
For background on Project Antrea, read the <a href=https://antrea.io/docs/>Introduction to Antrea</a>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster. Follow the
<a href=/docs/reference/setup-tools/kubeadm/>kubeadm getting started guide</a> to bootstrap one.</p>
<h2 id=deploying-antrea-with-kubeadm>Deploying Antrea with kubeadm</h2>
<p>Follow <a href=https://github.com/vmware-tanzu/antrea/blob/main/docs/getting-started.md>Getting Started</a> guide to deploy Antrea for kubeadm.</p>
<h2 id=what-s-next>What's next</h2>
<p>Once your cluster is running, you can follow the <a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1239a77618c6278373832a142cd85519>5.2 - Use Calico for NetworkPolicy</h1>
<p>This page shows a couple of quick ways to create a Calico cluster on Kubernetes.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>Decide whether you want to deploy a <a href=#creating-a-calico-cluster-with-google-kubernetes-engine-gke>cloud</a> or <a href=#creating-a-local-calico-cluster-with-kubeadm>local</a> cluster.</p>
<h2 id=creating-a-calico-cluster-with-google-kubernetes-engine-gke>Creating a Calico cluster with Google Kubernetes Engine (GKE)</h2>
<p><strong>Prerequisite</strong>: <a href=https://cloud.google.com/sdk/docs/quickstarts>gcloud</a>.</p>
<ol>
<li>
<p>To launch a GKE cluster with Calico, include the <code>--enable-network-policy</code> flag.</p>
<p><strong>Syntax</strong></p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>gcloud container clusters create <span style=color:#666>[</span>CLUSTER_NAME<span style=color:#666>]</span> --enable-network-policy
</code></pre></div><p><strong>Example</strong></p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>gcloud container clusters create my-calico-cluster --enable-network-policy
</code></pre></div></li>
<li>
<p>To verify the deployment, use the following command.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>The Calico pods begin with <code>calico</code>. Check to make sure each one has a status of <code>Running</code>.</p>
</li>
</ol>
<h2 id=creating-a-local-calico-cluster-with-kubeadm>Creating a local Calico cluster with kubeadm</h2>
<p>To get a local single-host Calico cluster in fifteen minutes using kubeadm, refer to the
<a href=https://docs.projectcalico.org/latest/getting-started/kubernetes/>Calico Quickstart</a>.</p>
<h2 id=what-s-next>What's next</h2>
<p>Once your cluster is running, you can follow the <a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-95039241255a31df196beaa405b68eba>5.3 - Use Cilium for NetworkPolicy</h1>
<p>This page shows how to use Cilium for NetworkPolicy.</p>
<p>For background on Cilium, read the <a href=https://docs.cilium.io/en/stable/intro>Introduction to Cilium</a>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=deploying-cilium-on-minikube-for-basic-testing>Deploying Cilium on Minikube for Basic Testing</h2>
<p>To get familiar with Cilium easily you can follow the
<a href=https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/>Cilium Kubernetes Getting Started Guide</a>
to perform a basic DaemonSet installation of Cilium in minikube.</p>
<p>To start minikube, minimal version required is >= v1.5.2, run the with the
following arguments:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>minikube version
</code></pre></div><pre><code>minikube version: v1.5.2
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>minikube start --network-plugin<span style=color:#666>=</span>cni
</code></pre></div><p>For minikube you can install Cilium using its CLI tool. Cilium will
automatically detect the cluster configuration and will install the appropriate
components for a successful installation:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz
cilium install
</code></pre></div><pre><code>🔮 Auto-detected Kubernetes kind: minikube
✨ Running &quot;minikube&quot; validation checks
✅ Detected minikube version &quot;1.20.0&quot;
ℹ️  Cilium version not set, using default version &quot;v1.10.0&quot;
🔮 Auto-detected cluster name: minikube
🔮 Auto-detected IPAM mode: cluster-pool
🔮 Auto-detected datapath mode: tunnel
🔑 Generating CA...
2021/05/27 02:54:44 [INFO] generate received request
2021/05/27 02:54:44 [INFO] received CSR
2021/05/27 02:54:44 [INFO] generating key: ecdsa-256
2021/05/27 02:54:44 [INFO] encoded CSR
2021/05/27 02:54:44 [INFO] signed certificate with serial number 48713764918856674401136471229482703021230538642
🔑 Generating certificates for Hubble...
2021/05/27 02:54:44 [INFO] generate received request
2021/05/27 02:54:44 [INFO] received CSR
2021/05/27 02:54:44 [INFO] generating key: ecdsa-256
2021/05/27 02:54:44 [INFO] encoded CSR
2021/05/27 02:54:44 [INFO] signed certificate with serial number 3514109734025784310086389188421560613333279574
🚀 Creating Service accounts...
🚀 Creating Cluster roles...
🚀 Creating ConfigMap...
🚀 Creating Agent DaemonSet...
🚀 Creating Operator Deployment...
⌛ Waiting for Cilium to be installed...
</code></pre><p>The remainder of the Getting Started Guide explains how to enforce both L3/L4
(i.e., IP address + port) security policies, as well as L7 (e.g., HTTP) security
policies using an example application.</p>
<h2 id=deploying-cilium-for-production-use>Deploying Cilium for Production Use</h2>
<p>For detailed instructions around deploying Cilium for production, see:
<a href=https://docs.cilium.io/en/stable/concepts/kubernetes/intro/>Cilium Kubernetes Installation Guide</a>
This documentation includes detailed requirements, instructions and example
production DaemonSet files.</p>
<h2 id=understanding-cilium-components>Understanding Cilium components</h2>
<p>Deploying a cluster with Cilium adds Pods to the <code>kube-system</code> namespace. To see
this list of Pods run:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods --namespace<span style=color:#666>=</span>kube-system -l k8s-app<span style=color:#666>=</span>cilium
</code></pre></div><p>You'll see a list of Pods similar to this:</p>
<pre><code class=language-console data-lang=console>NAME           READY   STATUS    RESTARTS   AGE
cilium-kkdhz   1/1     Running   0          3m23s
...
</code></pre><p>A <code>cilium</code> Pod runs on each node in your cluster and enforces network policy
on the traffic to/from Pods on that node using Linux BPF.</p>
<h2 id=what-s-next>What's next</h2>
<p>Once your cluster is running, you can follow the
<a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare Network Policy</a>
to try out Kubernetes NetworkPolicy with Cilium.
Have fun, and if you have questions, contact us using the
<a href=https://cilium.herokuapp.com/>Cilium Slack Channel</a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-505a0a6a7e6eff361bbb3be81c84b2e0>5.4 - Use Kube-router for NetworkPolicy</h1>
<p>This page shows how to use <a href=https://github.com/cloudnativelabs/kube-router>Kube-router</a> for NetworkPolicy.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster running. If you do not already have a cluster, you can create one by using any of the cluster installers like Kops, Bootkube, Kubeadm etc.</p>
<h2 id=installing-kube-router-addon>Installing Kube-router addon</h2>
<p>The Kube-router Addon comes with a Network Policy Controller that watches Kubernetes API server for any NetworkPolicy and pods updated and configures iptables rules and ipsets to allow or block traffic as directed by the policies. Please follow the <a href=https://www.kube-router.io/docs/user-guide/#try-kube-router-with-cluster-installers>trying Kube-router with cluster installers</a> guide to install Kube-router addon.</p>
<h2 id=what-s-next>What's next</h2>
<p>Once you have installed the Kube-router addon, you can follow the <a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2842eac98aa0e229a5c6755c4c83d2a7>5.5 - Romana for NetworkPolicy</h1>
<p>This page shows how to use Romana for NetworkPolicy.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>Complete steps 1, 2, and 3 of the <a href=/docs/reference/setup-tools/kubeadm/>kubeadm getting started guide</a>.</p>
<h2 id=installing-romana-with-kubeadm>Installing Romana with kubeadm</h2>
<p>Follow the <a href=https://github.com/romana/romana/tree/master/containerize>containerized installation guide</a> for kubeadm.</p>
<h2 id=applying-network-policies>Applying network policies</h2>
<p>To apply network policies use one of the following:</p>
<ul>
<li><a href=https://github.com/romana/romana/wiki/Romana-policies>Romana network policies</a>.
<ul>
<li><a href=https://github.com/romana/core/blob/master/doc/policy.md>Example of Romana network policy</a>.</li>
</ul>
</li>
<li>The NetworkPolicy API.</li>
</ul>
<h2 id=what-s-next>What's next</h2>
<p>Once you have installed Romana, you can follow the
<a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare Network Policy</a>
to try out Kubernetes NetworkPolicy.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ac075c3fdfd0d41aa753cc70e42be064>5.6 - Weave Net for NetworkPolicy</h1>
<p>This page shows how to use Weave Net for NetworkPolicy.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster. Follow the
<a href=/docs/reference/setup-tools/kubeadm/>kubeadm getting started guide</a> to bootstrap one.</p>
<h2 id=install-the-weave-net-addon>Install the Weave Net addon</h2>
<p>Follow the <a href=https://www.weave.works/docs/net/latest/kubernetes/kube-addon/>Integrating Kubernetes via the Addon</a> guide.</p>
<p>The Weave Net addon for Kubernetes comes with a
<a href=https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#npc>Network Policy Controller</a>
that automatically monitors Kubernetes for any NetworkPolicy annotations on all
namespaces and configures <code>iptables</code> rules to allow or block traffic as directed by the policies.</p>
<h2 id=test-the-installation>Test the installation</h2>
<p>Verify that the weave works.</p>
<p>Enter the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -n kube-system -o wide
</code></pre></div><p>The output is similar to this:</p>
<pre><code>NAME                                    READY     STATUS    RESTARTS   AGE       IP              NODE
weave-net-1t1qg                         2/2       Running   0          9d        192.168.2.10    worknode3
weave-net-231d7                         2/2       Running   1          7d        10.2.0.17       worknodegpu
weave-net-7nmwt                         2/2       Running   3          9d        192.168.2.131   masternode
weave-net-pmw8w                         2/2       Running   0          9d        192.168.2.216   worknode2
</code></pre><p>Each Node has a weave Pod, and all Pods are <code>Running</code> and <code>2/2 READY</code>. (<code>2/2</code> means that each Pod has <code>weave</code> and <code>weave-npc</code>.)</p>
<h2 id=what-s-next>What's next</h2>
<p>Once you have installed the Weave Net addon, you can follow the
<a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare Network Policy</a>
to try out Kubernetes NetworkPolicy. If you have any question, contact us at
<a href=https://github.com/weaveworks/weave#getting-help>#weave-community on Slack or Weave User Group</a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-e77685d5b88d2db5c7631a27b9472eea>6 - Access Clusters Using the Kubernetes API</h1>
<p>This page shows how to access clusters using the Kubernetes API.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=accessing-the-kubernetes-api>Accessing the Kubernetes API</h2>
<h3 id=accessing-for-the-first-time-with-kubectl>Accessing for the first time with kubectl</h3>
<p>When accessing the Kubernetes API for the first time, use the
Kubernetes command-line tool, <code>kubectl</code>.</p>
<p>To access a cluster, you need to know the location of the cluster and have credentials
to access it. Typically, this is automatically set-up when you work through
a <a href=/docs/setup/>Getting started guide</a>,
or someone else setup the cluster and provided you with credentials and a location.</p>
<p>Check the location and credentials that kubectl knows about with this command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config view
</code></pre></div><p>Many of the <a href=https://github.com/kubernetes/examples/tree/master/>examples</a> provide an introduction to using
kubectl. Complete documentation is found in the <a href=/docs/reference/kubectl/>kubectl manual</a>.</p>
<h3 id=directly-accessing-the-rest-api>Directly accessing the REST API</h3>
<p>kubectl handles locating and authenticating to the API server. If you want to directly access the REST API with an http client like
<code>curl</code> or <code>wget</code>, or a browser, there are multiple ways you can locate and authenticate against the API server:</p>
<ol>
<li>Run kubectl in proxy mode (recommended). This method is recommended, since it uses the stored apiserver location and verifies the identity of the API server using a self-signed cert. No man-in-the-middle (MITM) attack is possible using this method.</li>
<li>Alternatively, you can provide the location and credentials directly to the http client. This works with client code that is confused by proxies. To protect against man in the middle attacks, you'll need to import a root cert into your browser.</li>
</ol>
<p>Using the Go or Python client libraries provides accessing kubectl in proxy mode.</p>
<h4 id=using-kubectl-proxy>Using kubectl proxy</h4>
<p>The following command runs kubectl in a mode where it acts as a reverse proxy. It handles
locating the API server and authenticating.</p>
<p>Run it like this:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span> &amp;
</code></pre></div><p>See <a href=/docs/reference/generated/kubectl/kubectl-commands/#proxy>kubectl proxy</a> for more details.</p>
<p>Then you can explore the API with curl, wget, or a browser, like so:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl http://localhost:8080/api/
</code></pre></div><p>The output is similar to this:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;versions&#34;</span>: [
    <span style=color:#b44>&#34;v1&#34;</span>
  ],
  <span style=color:green;font-weight:700>&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style=color:green;font-weight:700>&#34;clientCIDR&#34;</span>: <span style=color:#b44>&#34;0.0.0.0/0&#34;</span>,
      <span style=color:green;font-weight:700>&#34;serverAddress&#34;</span>: <span style=color:#b44>&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><h4 id=without-kubectl-proxy>Without kubectl proxy</h4>
<p>It is possible to avoid using kubectl proxy by passing an authentication token
directly to the API server, like this:</p>
<p>Using <code>grep/cut</code> approach:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># Check all possible clusters, as your .KUBECONFIG may have multiple contexts:</span>
kubectl config view -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{&#34;Cluster name\tServer\n&#34;}{range .clusters[*]}{.name}{&#34;\t&#34;}{.cluster.server}{&#34;\n&#34;}{end}&#39;</span>

<span style=color:#080;font-style:italic># Select name of cluster you want to interact with from above output:</span>
<span style=color:#a2f>export</span> <span style=color:#b8860b>CLUSTER_NAME</span><span style=color:#666>=</span><span style=color:#b44>&#34;some_server_name&#34;</span>

<span style=color:#080;font-style:italic># Point to the API server referring the cluster name</span>
<span style=color:#b8860b>APISERVER</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl config view -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#34;{.clusters[?(@.name==\&#34;</span><span style=color:#b8860b>$CLUSTER_NAME</span><span style=color:#b44>\&#34;)].cluster.server}&#34;</span><span style=color:#a2f;font-weight:700>)</span>

<span style=color:#080;font-style:italic># Create a secret to hold a token for the default service account</span>
kubectl apply -f - <span style=color:#b44>&lt;&lt;EOF
</span><span style=color:#b44>apiVersion: v1
</span><span style=color:#b44>kind: Secret
</span><span style=color:#b44>metadata:
</span><span style=color:#b44>  name: default-token
</span><span style=color:#b44>  annotations:
</span><span style=color:#b44>    kubernetes.io/service-account.name: default
</span><span style=color:#b44>type: kubernetes.io/service-account-token
</span><span style=color:#b44>EOF</span>

<span style=color:#080;font-style:italic># Wait for the token controller to populate the secret with a token:</span>
<span style=color:#a2f;font-weight:700>while</span> ! kubectl describe secret default-token | grep -E <span style=color:#b44>&#39;^token&#39;</span> &gt;/dev/null; <span style=color:#a2f;font-weight:700>do</span>
  <span style=color:#a2f>echo</span> <span style=color:#b44>&#34;waiting for token...&#34;</span> &gt;&amp;<span style=color:#666>2</span>
  sleep <span style=color:#666>1</span>
<span style=color:#a2f;font-weight:700>done</span>

<span style=color:#080;font-style:italic># Get the token value</span>
<span style=color:#b8860b>TOKEN</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get secret default-token -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.data.token}&#39;</span> | base64 --decode<span style=color:#a2f;font-weight:700>)</span>

<span style=color:#080;font-style:italic># Explore the API with TOKEN</span>
curl -X GET <span style=color:#b8860b>$APISERVER</span>/api --header <span style=color:#b44>&#34;Authorization: Bearer </span><span style=color:#b8860b>$TOKEN</span><span style=color:#b44>&#34;</span> --insecure
</code></pre></div><p>The output is similar to this:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;APIVersions&#34;</span>,
  <span style=color:green;font-weight:700>&#34;versions&#34;</span>: [
    <span style=color:#b44>&#34;v1&#34;</span>
  ],
  <span style=color:green;font-weight:700>&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style=color:green;font-weight:700>&#34;clientCIDR&#34;</span>: <span style=color:#b44>&#34;0.0.0.0/0&#34;</span>,
      <span style=color:green;font-weight:700>&#34;serverAddress&#34;</span>: <span style=color:#b44>&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><p>The above example uses the <code>--insecure</code> flag. This leaves it subject to MITM
attacks. When kubectl accesses the cluster it uses a stored root certificate
and client certificates to access the server. (These are installed in the
<code>~/.kube</code> directory). Since cluster certificates are typically self-signed, it
may take special configuration to get your http client to use root
certificate.</p>
<p>On some clusters, the API server does not require authentication; it may serve
on localhost, or be protected by a firewall. There is not a standard
for this. <a href=/docs/concepts/security/controlling-access>Controlling Access to the Kubernetes API</a>
describes how you can configure this as a cluster administrator.</p>
<h3 id=programmatic-access-to-the-api>Programmatic access to the API</h3>
<p>Kubernetes officially supports client libraries for <a href=#go-client>Go</a>, <a href=#python-client>Python</a>, <a href=#java-client>Java</a>, <a href=#dotnet-client>dotnet</a>, <a href=#javascript-client>JavaScript</a>, and <a href=#haskell-client>Haskell</a>. There are other client libraries that are provided and maintained by their authors, not the Kubernetes team. See <a href=/docs/reference/using-api/client-libraries/>client libraries</a> for accessing the API from other languages and how they authenticate.</p>
<h4 id=go-client>Go client</h4>
<ul>
<li>To get the library, run the following command: <code>go get k8s.io/client-go@kubernetes-&lt;kubernetes-version-number></code> See <a href=https://github.com/kubernetes/client-go/releases>https://github.com/kubernetes/client-go/releases</a> to see which versions are supported.</li>
<li>Write an application atop of the client-go clients.</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> client-go defines its own API objects, so if needed, import API definitions from client-go rather than from the main repository. For example, <code>import "k8s.io/client-go/kubernetes"</code> is correct.
</div>
<p>The Go client can use the same <a href=/docs/concepts/configuration/organize-cluster-access-kubeconfig/>kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this <a href=https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go>example</a>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang><span style=color:#a2f;font-weight:700>package</span> main

<span style=color:#a2f;font-weight:700>import</span> (
  <span style=color:#b44>&#34;context&#34;</span>
  <span style=color:#b44>&#34;fmt&#34;</span>
  <span style=color:#b44>&#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;</span>
  <span style=color:#b44>&#34;k8s.io/client-go/kubernetes&#34;</span>
  <span style=color:#b44>&#34;k8s.io/client-go/tools/clientcmd&#34;</span>
)

<span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>main</span>() {
  <span style=color:#080;font-style:italic>// uses the current context in kubeconfig
</span><span style=color:#080;font-style:italic></span>  <span style=color:#080;font-style:italic>// path-to-kubeconfig -- for example, /root/.kube/config
</span><span style=color:#080;font-style:italic></span>  config, _ <span style=color:#666>:=</span> clientcmd.<span style=color:#00a000>BuildConfigFromFlags</span>(<span style=color:#b44>&#34;&#34;</span>, <span style=color:#b44>&#34;&lt;path-to-kubeconfig&gt;&#34;</span>)
  <span style=color:#080;font-style:italic>// creates the clientset
</span><span style=color:#080;font-style:italic></span>  clientset, _ <span style=color:#666>:=</span> kubernetes.<span style=color:#00a000>NewForConfig</span>(config)
  <span style=color:#080;font-style:italic>// access the API to list pods
</span><span style=color:#080;font-style:italic></span>  pods, _ <span style=color:#666>:=</span> clientset.<span style=color:#00a000>CoreV1</span>().<span style=color:#00a000>Pods</span>(<span style=color:#b44>&#34;&#34;</span>).<span style=color:#00a000>List</span>(context.<span style=color:#00a000>TODO</span>(), v1.ListOptions{})
  fmt.<span style=color:#00a000>Printf</span>(<span style=color:#b44>&#34;There are %d pods in the cluster\n&#34;</span>, <span style=color:#a2f>len</span>(pods.Items))
}
</code></pre></div><p>If the application is deployed as a Pod in the cluster, see <a href=/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod>Accessing the API from within a Pod</a>.</p>
<h4 id=python-client>Python client</h4>
<p>To use <a href=https://github.com/kubernetes-client/python>Python client</a>, run the following command: <code>pip install kubernetes</code> See <a href=https://github.com/kubernetes-client/python>Python Client Library page</a> for more installation options.</p>
<p>The Python client can use the same <a href=/docs/concepts/configuration/organize-cluster-access-kubeconfig/>kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this <a href=https://github.com/kubernetes-client/python/blob/master/examples/out_of_cluster_config.py>example</a>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>kubernetes</span> <span style=color:#a2f;font-weight:700>import</span> client, config

config<span style=color:#666>.</span>load_kube_config()

v1<span style=color:#666>=</span>client<span style=color:#666>.</span>CoreV1Api()
<span style=color:#a2f>print</span>(<span style=color:#b44>&#34;Listing pods with their IPs:&#34;</span>)
ret <span style=color:#666>=</span> v1<span style=color:#666>.</span>list_pod_for_all_namespaces(watch<span style=color:#666>=</span><span style=color:#a2f;font-weight:700>False</span>)
<span style=color:#a2f;font-weight:700>for</span> i <span style=color:#a2f;font-weight:700>in</span> ret<span style=color:#666>.</span>items:
    <span style=color:#a2f>print</span>(<span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>%s</span><span style=color:#b62;font-weight:700>\t</span><span style=color:#b68;font-weight:700>%s</span><span style=color:#b62;font-weight:700>\t</span><span style=color:#b68;font-weight:700>%s</span><span style=color:#b44>&#34;</span> <span style=color:#666>%</span> (i<span style=color:#666>.</span>status<span style=color:#666>.</span>pod_ip, i<span style=color:#666>.</span>metadata<span style=color:#666>.</span>namespace, i<span style=color:#666>.</span>metadata<span style=color:#666>.</span>name))
</code></pre></div><h4 id=java-client>Java client</h4>
<p>To install the <a href=https://github.com/kubernetes-client/java>Java Client</a>, run:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># Clone java library</span>
git clone --recursive https://github.com/kubernetes-client/java

<span style=color:#080;font-style:italic># Installing project artifacts, POM etc:</span>
<span style=color:#a2f>cd</span> java
mvn install
</code></pre></div><p>See <a href=https://github.com/kubernetes-client/java/releases>https://github.com/kubernetes-client/java/releases</a> to see which versions are supported.</p>
<p>The Java client can use the same <a href=/docs/concepts/configuration/organize-cluster-access-kubeconfig/>kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this <a href=https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java>example</a>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=color:#a2f;font-weight:700>package</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.examples</span><span style=color:#666>;</span>

<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.ApiClient</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.ApiException</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.Configuration</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.apis.CoreV1Api</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.models.V1Pod</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.models.V1PodList</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.util.ClientBuilder</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>io.kubernetes.client.util.KubeConfig</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>java.io.FileReader</span><span style=color:#666>;</span>
<span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>java.io.IOException</span><span style=color:#666>;</span>

<span style=color:#080;font-style:italic>/**
</span><span style=color:#080;font-style:italic> * A simple example of how to use the Java API from an application outside a kubernetes cluster
</span><span style=color:#080;font-style:italic> *
</span><span style=color:#080;font-style:italic> * &lt;p&gt;Easiest way to run this: mvn exec:java
</span><span style=color:#080;font-style:italic> * -Dexec.mainClass=&#34;io.kubernetes.client.examples.KubeConfigFileClientExample&#34;
</span><span style=color:#080;font-style:italic> *
</span><span style=color:#080;font-style:italic> */</span>
<span style=color:#a2f;font-weight:700>public</span> <span style=color:#a2f;font-weight:700>class</span> <span style=color:#00f>KubeConfigFileClientExample</span> <span style=color:#666>{</span>
  <span style=color:#a2f;font-weight:700>public</span> <span style=color:#a2f;font-weight:700>static</span> <span style=color:#0b0;font-weight:700>void</span> <span style=color:#00a000>main</span><span style=color:#666>(</span>String<span style=color:#666>[]</span> args<span style=color:#666>)</span> <span style=color:#a2f;font-weight:700>throws</span> IOException<span style=color:#666>,</span> ApiException <span style=color:#666>{</span>

    <span style=color:#080;font-style:italic>// file path to your KubeConfig
</span><span style=color:#080;font-style:italic></span>    String kubeConfigPath <span style=color:#666>=</span> <span style=color:#b44>&#34;~/.kube/config&#34;</span><span style=color:#666>;</span>

    <span style=color:#080;font-style:italic>// loading the out-of-cluster config, a kubeconfig from file-system
</span><span style=color:#080;font-style:italic></span>    ApiClient client <span style=color:#666>=</span>
        ClientBuilder<span style=color:#666>.</span><span style=color:#b44>kubeconfig</span><span style=color:#666>(</span>KubeConfig<span style=color:#666>.</span><span style=color:#b44>loadKubeConfig</span><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>new</span> FileReader<span style=color:#666>(</span>kubeConfigPath<span style=color:#666>))).</span><span style=color:#b44>build</span><span style=color:#666>();</span>

    <span style=color:#080;font-style:italic>// set the global default api-client to the in-cluster one from above
</span><span style=color:#080;font-style:italic></span>    Configuration<span style=color:#666>.</span><span style=color:#b44>setDefaultApiClient</span><span style=color:#666>(</span>client<span style=color:#666>);</span>

    <span style=color:#080;font-style:italic>// the CoreV1Api loads default api-client from global configuration.
</span><span style=color:#080;font-style:italic></span>    CoreV1Api api <span style=color:#666>=</span> <span style=color:#a2f;font-weight:700>new</span> CoreV1Api<span style=color:#666>();</span>

    <span style=color:#080;font-style:italic>// invokes the CoreV1Api client
</span><span style=color:#080;font-style:italic></span>    V1PodList list <span style=color:#666>=</span> api<span style=color:#666>.</span><span style=color:#b44>listPodForAllNamespaces</span><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>null</span><span style=color:#666>,</span> <span style=color:#a2f;font-weight:700>null</span><span style=color:#666>,</span> <span style=color:#a2f;font-weight:700>null</span><span style=color:#666>,</span> <span style=color:#a2f;font-weight:700>null</span><span style=color:#666>,</span> <span style=color:#a2f;font-weight:700>null</span><span style=color:#666>,</span> <span style=color:#a2f;font-weight:700>null</span><span style=color:#666>,</span> <span style=color:#a2f;font-weight:700>null</span><span style=color:#666>,</span> <span style=color:#a2f;font-weight:700>null</span><span style=color:#666>,</span> <span style=color:#a2f;font-weight:700>null</span><span style=color:#666>);</span>
    System<span style=color:#666>.</span><span style=color:#b44>out</span><span style=color:#666>.</span><span style=color:#b44>println</span><span style=color:#666>(</span><span style=color:#b44>&#34;Listing all pods: &#34;</span><span style=color:#666>);</span>
    <span style=color:#a2f;font-weight:700>for</span> <span style=color:#666>(</span>V1Pod item <span style=color:#666>:</span> list<span style=color:#666>.</span><span style=color:#b44>getItems</span><span style=color:#666>())</span> <span style=color:#666>{</span>
      System<span style=color:#666>.</span><span style=color:#b44>out</span><span style=color:#666>.</span><span style=color:#b44>println</span><span style=color:#666>(</span>item<span style=color:#666>.</span><span style=color:#b44>getMetadata</span><span style=color:#666>().</span><span style=color:#b44>getName</span><span style=color:#666>());</span>
    <span style=color:#666>}</span>
  <span style=color:#666>}</span>
<span style=color:#666>}</span>
</code></pre></div><h4 id=dotnet-client>dotnet client</h4>
<p>To use <a href=https://github.com/kubernetes-client/csharp>dotnet client</a>, run the following command: <code>dotnet add package KubernetesClient --version 1.6.1</code> See <a href=https://github.com/kubernetes-client/csharp>dotnet Client Library page</a> for more installation options. See <a href=https://github.com/kubernetes-client/csharp/releases>https://github.com/kubernetes-client/csharp/releases</a> to see which versions are supported.</p>
<p>The dotnet client can use the same <a href=/docs/concepts/configuration/organize-cluster-access-kubeconfig/>kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this <a href=https://github.com/kubernetes-client/csharp/blob/master/examples/simple/PodList.cs>example</a>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-csharp data-lang=csharp><span style=color:#a2f;font-weight:700>using</span> <span style=color:#00f;font-weight:700>System</span>;
<span style=color:#a2f;font-weight:700>using</span> <span style=color:#00f;font-weight:700>k8s</span>;

<span style=color:#a2f;font-weight:700>namespace</span> <span style=color:#00f;font-weight:700>simple</span>
{
    <span style=color:#a2f;font-weight:700>internal</span> <span style=color:#a2f;font-weight:700>class</span> <span style=color:#00f>PodList</span>
    {
        <span style=color:#a2f;font-weight:700>private</span> <span style=color:#a2f;font-weight:700>static</span> <span style=color:#a2f;font-weight:700>void</span> Main(<span style=color:#0b0;font-weight:700>string</span>[] args)
        {
            <span style=color:#0b0;font-weight:700>var</span> config = KubernetesClientConfiguration.BuildDefaultConfig();
            IKubernetes client = <span style=color:#a2f;font-weight:700>new</span> Kubernetes(config);
            Console.WriteLine(<span style=color:#b44>&#34;Starting Request!&#34;</span>);

            <span style=color:#0b0;font-weight:700>var</span> list = client.ListNamespacedPod(<span style=color:#b44>&#34;default&#34;</span>);
            <span style=color:#a2f;font-weight:700>foreach</span> (<span style=color:#0b0;font-weight:700>var</span> item <span style=color:#a2f;font-weight:700>in</span> list.Items)
            {
                Console.WriteLine(item.Metadata.Name);
            }
            <span style=color:#a2f;font-weight:700>if</span> (list.Items.Count == <span style=color:#666>0</span>)
            {
                Console.WriteLine(<span style=color:#b44>&#34;Empty!&#34;</span>);
            }
        }
    }
}
</code></pre></div><h4 id=javascript-client>JavaScript client</h4>
<p>To install <a href=https://github.com/kubernetes-client/javascript>JavaScript client</a>, run the following command: <code>npm install @kubernetes/client-node</code>. See <a href=https://github.com/kubernetes-client/javascript/releases>https://github.com/kubernetes-client/javascript/releases</a> to see which versions are supported.</p>
<p>The JavaScript client can use the same <a href=/docs/concepts/configuration/organize-cluster-access-kubeconfig/>kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this <a href=https://github.com/kubernetes-client/javascript/blob/master/examples/example.js>example</a>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=color:#a2f;font-weight:700>const</span> k8s <span style=color:#666>=</span> require(<span style=color:#b44>&#39;@kubernetes/client-node&#39;</span>);

<span style=color:#a2f;font-weight:700>const</span> kc <span style=color:#666>=</span> <span style=color:#a2f;font-weight:700>new</span> k8s.KubeConfig();
kc.loadFromDefault();

<span style=color:#a2f;font-weight:700>const</span> k8sApi <span style=color:#666>=</span> kc.makeApiClient(k8s.CoreV1Api);

k8sApi.listNamespacedPod(<span style=color:#b44>&#39;default&#39;</span>).then((res) =&gt; {
    console.log(res.body);
});
</code></pre></div><h4 id=haskell-client>Haskell client</h4>
<p>See <a href=https://github.com/kubernetes-client/haskell/releases>https://github.com/kubernetes-client/haskell/releases</a> to see which versions are supported.</p>
<p>The <a href=https://github.com/kubernetes-client/haskell>Haskell client</a> can use the same <a href=/docs/concepts/configuration/organize-cluster-access-kubeconfig/>kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this <a href=https://github.com/kubernetes-client/haskell/blob/master/kubernetes-client/example/App.hs>example</a>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-haskell data-lang=haskell><span style=color:#00a000>exampleWithKubeConfig</span> <span style=color:#a2f;font-weight:700>::</span> <span style=color:#0b0;font-weight:700>IO</span> <span style=color:#a2f>()</span>
<span style=color:#00a000>exampleWithKubeConfig</span> <span style=color:#a2f;font-weight:700>=</span> <span style=color:#a2f;font-weight:700>do</span>
    oidcCache <span style=color:#a2f;font-weight:700>&lt;-</span> atomically <span style=color:#666>$</span> newTVar <span style=color:#666>$</span> <span style=color:#0b0;font-weight:700>Map</span><span style=color:#666>.</span>fromList <span style=color:#0b0;font-weight:700>[]</span>
    (mgr, kcfg) <span style=color:#a2f;font-weight:700>&lt;-</span> mkKubeClientConfig oidcCache <span style=color:#666>$</span> <span style=color:#0b0;font-weight:700>KubeConfigFile</span> <span style=color:#b44>&#34;/path/to/kubeconfig&#34;</span>
    dispatchMime
            mgr
            kcfg
            (<span style=color:#0b0;font-weight:700>CoreV1</span><span style=color:#666>.</span>listPodForAllNamespaces (<span style=color:#0b0;font-weight:700>Accept</span> <span style=color:#0b0;font-weight:700>MimeJSON</span>))
        <span style=color:#666>&gt;&gt;=</span> print
</code></pre></div><h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=/docs/tasks/run-application/access-api-from-pod/>Accessing the Kubernetes API from a Pod</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-a8f6511197efcd7d0db80ade49620f9d>7 - Advertise Extended Resources for a Node</h1>
<p>This page shows how to specify extended resources for a Node.
Extended resources allow cluster administrators to advertise node-level
resources that would otherwise be unknown to Kubernetes.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=get-the-names-of-your-nodes>Get the names of your Nodes</h2>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes
</code></pre></div><p>Choose one of your Nodes to use for this exercise.</p>
<h2 id=advertise-a-new-extended-resource-on-one-of-your-nodes>Advertise a new extended resource on one of your Nodes</h2>
<p>To advertise a new extended resource on a Node, send an HTTP PATCH request to
the Kubernetes API server. For example, suppose one of your Nodes has four dongles
attached. Here's an example of a PATCH request that advertises four dongle resources
for your Node.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

<span style=color:#666>[</span>
  <span style=color:#666>{</span>
    <span style=color:#b44>&#34;op&#34;</span>: <span style=color:#b44>&#34;add&#34;</span>,
    <span style=color:#b44>&#34;path&#34;</span>: <span style=color:#b44>&#34;/status/capacity/example.com~1dongle&#34;</span>,
    <span style=color:#b44>&#34;value&#34;</span>: <span style=color:#b44>&#34;4&#34;</span>
  <span style=color:#666>}</span>
<span style=color:#666>]</span>
</code></pre></div><p>Note that Kubernetes does not need to know what a dongle is or what a dongle is for.
The preceding PATCH request tells Kubernetes that your Node has four things that
you call dongles.</p>
<p>Start a proxy, so that you can easily send requests to the Kubernetes API server:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy
</code></pre></div><p>In another command window, send the HTTP PATCH request.
Replace <code>&lt;your-node-name></code> with the name of your Node:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl --header <span style=color:#b44>&#34;Content-Type: application/json-patch+json&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--request PATCH <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--data <span style=color:#b44>&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1dongle&#34;, &#34;value&#34;: &#34;4&#34;}]&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> In the preceding request, <code>~1</code> is the encoding for the character / in
the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
<a href=https://tools.ietf.org/html/rfc6901>IETF RFC 6901</a>, section 3.
</div>
<p>The output shows that the Node has a capacity of 4 dongles:</p>
<pre><code>&quot;capacity&quot;: {
  &quot;cpu&quot;: &quot;2&quot;,
  &quot;memory&quot;: &quot;2049008Ki&quot;,
  &quot;example.com/dongle&quot;: &quot;4&quot;,
</code></pre><p>Describe your Node:</p>
<pre><code>kubectl describe node &lt;your-node-name&gt;
</code></pre><p>Once again, the output shows the dongle resource:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>Capacity</span>:<span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb>  </span><span style=color:#666>2</span><span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb>  </span>2049008Ki<span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>example.com/dongle</span>:<span style=color:#bbb>  </span><span style=color:#666>4</span><span style=color:#bbb>
</span></code></pre></div><p>Now, application developers can create Pods that request a certain
number of dongles. See
<a href=/docs/tasks/configure-pod-container/extended-resource/>Assign Extended Resources to a Container</a>.</p>
<h2 id=discussion>Discussion</h2>
<p>Extended resources are similar to memory and CPU resources. For example,
just as a Node has a certain amount of memory and CPU to be shared by all components
running on the Node, it can have a certain number of dongles to be shared
by all components running on the Node. And just as application developers
can create Pods that request a certain amount of memory and CPU, they can
create Pods that request a certain number of dongles.</p>
<p>Extended resources are opaque to Kubernetes; Kubernetes does not
know anything about what they are. Kubernetes knows only that a Node
has a certain number of them. Extended resources must be advertised in integer
amounts. For example, a Node can advertise four dongles, but not 4.5 dongles.</p>
<h3 id=storage-example>Storage example</h3>
<p>Suppose a Node has 800 GiB of a special kind of disk storage. You could
create a name for the special storage, say example.com/special-storage.
Then you could advertise it in chunks of a certain size, say 100 GiB. In that case,
your Node would advertise that it has eight resources of type
example.com/special-storage.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>Capacity</span>:<span style=color:#bbb>
</span><span style=color:#bbb> </span>...<span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>example.com/special-storage</span>:<span style=color:#bbb> </span><span style=color:#666>8</span><span style=color:#bbb>
</span></code></pre></div><p>If you want to allow arbitrary requests for special storage, you
could advertise special storage in chunks of size 1 byte. In that case, you would advertise
800Gi resources of type example.com/special-storage.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>Capacity</span>:<span style=color:#bbb>
</span><span style=color:#bbb> </span>...<span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>example.com/special-storage</span>:<span style=color:#bbb>  </span>800Gi<span style=color:#bbb>
</span></code></pre></div><p>Then a Container could request any number of bytes of special storage, up to 800Gi.</p>
<h2 id=clean-up>Clean up</h2>
<p>Here is a PATCH request that removes the dongle advertisement from a Node.</p>
<pre><code>PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    &quot;op&quot;: &quot;remove&quot;,
    &quot;path&quot;: &quot;/status/capacity/example.com~1dongle&quot;,
  }
]
</code></pre><p>Start a proxy, so that you can easily send requests to the Kubernetes API server:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy
</code></pre></div><p>In another command window, send the HTTP PATCH request.
Replace <code>&lt;your-node-name></code> with the name of your Node:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl --header <span style=color:#b44>&#34;Content-Type: application/json-patch+json&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--request PATCH <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--data <span style=color:#b44>&#39;[{&#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1dongle&#34;}]&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</code></pre></div><p>Verify that the dongle advertisement has been removed:</p>
<pre><code>kubectl describe node &lt;your-node-name&gt; | grep dongle
</code></pre><p>(you should not see any output)</p>
<h2 id=what-s-next>What's next</h2>
<h3 id=for-application-developers>For application developers</h3>
<ul>
<li><a href=/docs/tasks/configure-pod-container/extended-resource/>Assign Extended Resources to a Container</a></li>
</ul>
<h3 id=for-cluster-administrators>For cluster administrators</h3>
<ul>
<li><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>Configure Minimum and Maximum Memory Constraints for a Namespace</a></li>
<li><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Configure Minimum and Maximum CPU Constraints for a Namespace</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-966cd1cc69c69410d8698b3ac74abce2>8 - Autoscale the DNS Service in a Cluster</h1>
<p>This page shows how to enable and configure autoscaling of the DNS service in
your Kubernetes cluster.</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
</li>
<li>
<p>This guide assumes your nodes use the AMD64 or Intel 64 CPU architecture.</p>
</li>
<li>
<p>Make sure <a href=/docs/concepts/services-networking/dns-pod-service/>Kubernetes DNS</a> is enabled.</p>
</li>
</ul>
<h2 id=determining-whether-dns-horizontal-autoscaling-is-already-enabled>Determine whether DNS horizontal autoscaling is already enabled</h2>
<p>List the <a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployments>Deployments</a>
in your cluster in the kube-system <a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>The output is similar to this:</p>
<pre><code>NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
...
dns-autoscaler            1/1     1            1           ...
...
</code></pre>
<p>If you see "dns-autoscaler" in the output, DNS horizontal autoscaling is
already enabled, and you can skip to
<a href=#tuning-autoscaling-parameters>Tuning autoscaling parameters</a>.</p>
<h2 id=find-scaling-target>Get the name of your DNS Deployment</h2>
<p>List the DNS deployments in your cluster in the kube-system namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment -l k8s-app<span style=color:#666>=</span>kube-dns --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>The output is similar to this:</p>
<pre><code>NAME      READY   UP-TO-DATE   AVAILABLE   AGE
...
coredns   2/2     2            2           ...
...
</code></pre>
<p>If you don't see a Deployment for DNS services, you can also look for it by name:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>and look for a deployment named <code>coredns</code> or <code>kube-dns</code>.</p>
<p>Your scale target is</p>
<pre><code>Deployment/&lt;your-deployment-name&gt;
</code></pre>
<p>where <code>&lt;your-deployment-name></code> is the name of your DNS Deployment. For example, if
the name of your Deployment for DNS is coredns, your scale target is Deployment/coredns.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> CoreDNS is the default DNS service for Kubernetes. CoreDNS sets the label
<code>k8s-app=kube-dns</code> so that it can work in clusters that originally used
kube-dns.
</div>
<h2 id=enablng-dns-horizontal-autoscaling>Enable DNS horizontal autoscaling</h2>
<p>In this section, you create a new Deployment. The Pods in the Deployment run a
container based on the <code>cluster-proportional-autoscaler-amd64</code> image.</p>
<p>Create a file named <code>dns-horizontal-autoscaler.yaml</code> with this content:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/dns/dns-horizontal-autoscaler.yaml download=admin/dns/dns-horizontal-autoscaler.yaml><code>admin/dns/dns-horizontal-autoscaler.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-dns-dns-horizontal-autoscaler-yaml')" title="Copy admin/dns/dns-horizontal-autoscaler.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-dns-dns-horizontal-autoscaler-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ServiceAccount<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;nodes&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;list&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;watch&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;replicationcontrollers/scale&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;get&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;update&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;apps&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;deployments/scale&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;replicasets/scale&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;get&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;update&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Remove the configmaps rule once below issue is fixed:</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># kubernetes-incubator/cluster-proportional-autoscaler#16</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;configmaps&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;get&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;create&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRoleBinding<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ServiceAccount<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>roleRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/cluster-service</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>priorityClassName</span>:<span style=color:#bbb> </span>system-cluster-critical<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>securityContext</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>seccompProfile</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>RuntimeDefault<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>supplementalGroups</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#666>65534</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fsGroup</span>:<span style=color:#bbb> </span><span style=color:#666>65534</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>linux<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/cpa/cluster-proportional-autoscaler:1.8.4<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;20m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- /cluster-proportional-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- --namespace=kube-system<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- --configmap=kube-dns-autoscaler<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:#080;font-style:italic># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span>- --target=&lt;SCALE_TARGET&gt;<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:#080;font-style:italic># When cluster is using large nodes(with more cores), &#34;coresPerReplica&#34; should dominate.</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:#080;font-style:italic># If using small nodes, &#34;nodesPerReplica&#34; should dominate.</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span>- --default-params={&#34;linear&#34;:{&#34;coresPerReplica&#34;:256,&#34;nodesPerReplica&#34;:16,&#34;preventSinglePointFailure&#34;:true,&#34;includeUnschedulableNodes&#34;:true}}<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- --logtostderr=true<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- --v=2<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;CriticalAddonsOnly&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>serviceAccountName</span>:<span style=color:#bbb> </span>kube-dns-autoscaler<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>In the file, replace <code>&lt;SCALE_TARGET></code> with your scale target.</p>
<p>Go to the directory that contains your configuration file, and enter this
command to create the Deployment:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dns-horizontal-autoscaler.yaml
</code></pre></div><p>The output of a successful command is:</p>
<pre><code>deployment.apps/dns-autoscaler created
</code></pre>
<p>DNS horizontal autoscaling is now enabled.</p>
<h2 id=tuning-autoscaling-parameters>Tune DNS autoscaling parameters</h2>
<p>Verify that the dns-autoscaler <a class=glossary-tooltip title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a> exists:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get configmap --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>The output is similar to this:</p>
<pre><code>NAME                  DATA      AGE
...
dns-autoscaler        1         ...
...
</code></pre>
<p>Modify the data in the ConfigMap:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit configmap dns-autoscaler --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>Look for this line:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>linear</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;{&#34;coresPerReplica&#34;:256,&#34;min&#34;:1,&#34;nodesPerReplica&#34;:16}&#39;</span><span style=color:#bbb>
</span></code></pre></div><p>Modify the fields according to your needs. The "min" field indicates the
minimal number of DNS backends. The actual number of backends is
calculated using this equation:</p>
<pre><code>replicas = max( ceil( cores × 1/coresPerReplica ) , ceil( nodes × 1/nodesPerReplica ) )
</code></pre>
<p>Note that the values of both <code>coresPerReplica</code> and <code>nodesPerReplica</code> are
floats.</p>
<p>The idea is that when a cluster is using nodes that have many cores,
<code>coresPerReplica</code> dominates. When a cluster is using nodes that have fewer
cores, <code>nodesPerReplica</code> dominates.</p>
<p>There are other supported scaling patterns. For details, see
<a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>cluster-proportional-autoscaler</a>.</p>
<h2 id=disable-dns-horizontal-autoscaling>Disable DNS horizontal autoscaling</h2>
<p>There are a few options for tuning DNS horizontal autoscaling. Which option to
use depends on different conditions.</p>
<h3 id=option-1-scale-down-the-dns-autoscaler-deployment-to-0-replicas>Option 1: Scale down the dns-autoscaler deployment to 0 replicas</h3>
<p>This option works for all situations. Enter this command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl scale deployment --replicas<span style=color:#666>=</span><span style=color:#666>0</span> dns-autoscaler --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>The output is:</p>
<pre><code>deployment.apps/dns-autoscaler scaled
</code></pre>
<p>Verify that the replica count is zero:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get rs --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>The output displays 0 in the DESIRED and CURRENT columns:</p>
<pre><code>NAME                                 DESIRED   CURRENT   READY   AGE
...
dns-autoscaler-6b59789fc8            0         0         0       ...
...
</code></pre>
<h3 id=option-2-delete-the-dns-autoscaler-deployment>Option 2: Delete the dns-autoscaler deployment</h3>
<p>This option works if dns-autoscaler is under your own control, which means
no one will re-create it:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployment dns-autoscaler --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>The output is:</p>
<pre><code>deployment.apps &quot;dns-autoscaler&quot; deleted
</code></pre>
<h3 id=option-3-delete-the-dns-autoscaler-manifest-file-from-the-master-node>Option 3: Delete the dns-autoscaler manifest file from the master node</h3>
<p>This option works if dns-autoscaler is under control of the (deprecated)
<a href=https://git.k8s.io/kubernetes/cluster/addons/README.md>Addon Manager</a>,
and you have write access to the master node.</p>
<p>Sign in to the master node and delete the corresponding manifest file.
The common path for this dns-autoscaler is:</p>
<pre><code>/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
</code></pre>
<p>After the manifest file is deleted, the Addon Manager will delete the
dns-autoscaler Deployment.</p>
<h2 id=understanding-how-dns-horizontal-autoscaling-works>Understanding how DNS horizontal autoscaling works</h2>
<ul>
<li>
<p>The cluster-proportional-autoscaler application is deployed separately from
the DNS service.</p>
</li>
<li>
<p>An autoscaler Pod runs a client that polls the Kubernetes API server for the
number of nodes and cores in the cluster.</p>
</li>
<li>
<p>A desired replica count is calculated and applied to the DNS backends based on
the current schedulable nodes and cores and the given scaling parameters.</p>
</li>
<li>
<p>The scaling parameters and data points are provided via a ConfigMap to the
autoscaler, and it refreshes its parameters table every poll interval to be up
to date with the latest desired scaling parameters.</p>
</li>
<li>
<p>Changes to the scaling parameters are allowed without rebuilding or restarting
the autoscaler Pod.</p>
</li>
<li>
<p>The autoscaler provides a controller interface to support two control
patterns: <em>linear</em> and <em>ladder</em>.</p>
</li>
</ul>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Read about <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/>Guaranteed Scheduling For Critical Add-On Pods</a>.</li>
<li>Learn more about the
<a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>implementation of cluster-proportional-autoscaler</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2bffd7f3571cdd609bd97fb2e1bdb2fe>9 - Change the default StorageClass</h1>
<p>This page shows how to change the default Storage Class that is used to
provision volumes for PersistentVolumeClaims that have no special requirements.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=why-change-the-default-storage-class>Why change the default storage class?</h2>
<p>Depending on the installation method, your Kubernetes cluster may be deployed with
an existing StorageClass that is marked as default. This default StorageClass
is then used to dynamically provision storage for PersistentVolumeClaims
that do not require any specific storage class. See
<a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>PersistentVolumeClaim documentation</a>
for details.</p>
<p>The pre-installed default StorageClass may not fit well with your expected workload;
for example, it might provision storage that is too expensive. If this is the case,
you can either change the default StorageClass or disable it completely to avoid
dynamic provisioning of storage.</p>
<p>Deleting the default StorageClass may not work, as it may be re-created
automatically by the addon manager running in your cluster. Please consult the docs for your installation
for details about addon manager and how to disable individual addons.</p>
<h2 id=changing-the-default-storageclass>Changing the default StorageClass</h2>
<ol>
<li>
<p>List the StorageClasses in your cluster:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get storageclass
</code></pre></div><p>The output is similar to this:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>NAME                 PROVISIONER               AGE
standard <span style=color:#666>(</span>default<span style=color:#666>)</span>   kubernetes.io/gce-pd      1d
gold                 kubernetes.io/gce-pd      1d
</code></pre></div><p>The default StorageClass is marked by <code>(default)</code>.</p>
</li>
<li>
<p>Mark the default StorageClass as non-default:</p>
<p>The default StorageClass has an annotation
<code>storageclass.kubernetes.io/is-default-class</code> set to <code>true</code>. Any other value
or absence of the annotation is interpreted as <code>false</code>.</p>
<p>To mark a StorageClass as non-default, you need to change its value to <code>false</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl patch storageclass standard -p <span style=color:#b44>&#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;false&#34;}}}&#39;</span>
</code></pre></div><p>where <code>standard</code> is the name of your chosen StorageClass.</p>
</li>
<li>
<p>Mark a StorageClass as default:</p>
<p>Similar to the previous step, you need to add/set the annotation
<code>storageclass.kubernetes.io/is-default-class=true</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl patch storageclass gold -p <span style=color:#b44>&#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;true&#34;}}}&#39;</span>
</code></pre></div><p>Please note that at most one StorageClass can be marked as default. If two
or more of them are marked as default, a <code>PersistentVolumeClaim</code> without <code>storageClassName</code> explicitly specified cannot be created.</p>
</li>
<li>
<p>Verify that your chosen StorageClass is default:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get storageclass
</code></pre></div><p>The output is similar to this:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>NAME             PROVISIONER               AGE
standard         kubernetes.io/gce-pd      1d
gold <span style=color:#666>(</span>default<span style=color:#666>)</span>   kubernetes.io/gce-pd      1d
</code></pre></div></li>
</ol>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Learn more about <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-fbc9136f53eccd6eb8c80f4bbea3b8f4>10 - Change the Reclaim Policy of a PersistentVolume</h1>
<p>This page shows how to change the reclaim policy of a Kubernetes
PersistentVolume.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=why-change-reclaim-policy-of-a-persistentvolume>Why change reclaim policy of a PersistentVolume</h2>
<p>PersistentVolumes can have various reclaim policies, including "Retain",
"Recycle", and "Delete". For dynamically provisioned PersistentVolumes,
the default reclaim policy is "Delete". This means that a dynamically provisioned
volume is automatically deleted when a user deletes the corresponding
PersistentVolumeClaim. This automatic behavior might be inappropriate if the volume
contains precious data. In that case, it is more appropriate to use the "Retain"
policy. With the "Retain" policy, if a user deletes a PersistentVolumeClaim,
the corresponding PersistentVolume will not be deleted. Instead, it is moved to the
Released phase, where all of its data can be manually recovered.</p>
<h2 id=changing-the-reclaim-policy-of-a-persistentvolume>Changing the reclaim policy of a PersistentVolume</h2>
<ol>
<li>
<p>List the PersistentVolumes in your cluster:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pv
</code></pre></div><p>The output is similar to this:</p>
<pre><code class=language-none data-lang=none>NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM             STORAGECLASS     REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1    manual                     10s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2    manual                     6s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim3    manual                     3s
</code></pre><p>This list also includes the name of the claims that are bound to each volume
for easier identification of dynamically provisioned volumes.</p>
</li>
<li>
<p>Choose one of your PersistentVolumes and change its reclaim policy:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl patch pv &lt;your-pv-name&gt; -p <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;persistentVolumeReclaimPolicy&#34;:&#34;Retain&#34;}}&#39;</span>
</code></pre></div><p>where <code>&lt;your-pv-name></code> is the name of your chosen PersistentVolume.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>On Windows, you must <em>double</em> quote any JSONPath template that contains spaces (not single
quote as shown above for bash). This in turn means that you must use a single quote or escaped
double quote around any literals in the template. For example:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd>kubectl patch pv &lt;your-pv-name&gt; -p <span style=color:#b44>&#34;{\&#34;</span>spec\<span style=color:#b44>&#34;:{\&#34;</span>persistentVolumeReclaimPolicy\<span style=color:#b44>&#34;:\&#34;</span>Retain\<span style=color:#b44>&#34;}}&#34;</span>
</code></pre></div>
</div>
</li>
<li>
<p>Verify that your chosen PersistentVolume has the right policy:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pv
</code></pre></div><p>The output is similar to this:</p>
<pre><code class=language-none data-lang=none>NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM             STORAGECLASS     REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1    manual                     40s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2    manual                     36s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Retain          Bound     default/claim3    manual                     33s
</code></pre><p>In the preceding output, you can see that the volume bound to claim
<code>default/claim3</code> has reclaim policy <code>Retain</code>. It will not be automatically
deleted when a user deletes claim <code>default/claim3</code>.</p>
</li>
</ol>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Learn more about <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a>.</li>
<li>Learn more about <a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>PersistentVolumeClaims</a>.</li>
</ul>
<h3 id=reference>References</h3>
<ul>
<li>
<a href=/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/>PersistentVolume</a>
<ul>
<li>Pay attention to the <code>.spec.persistentVolumeReclaimPolicy</code>
<a href=/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec>field</a>
of PersistentVolume.</li>
</ul>
</li>
<li>
<a href=/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/>PersistentVolumeClaim</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ce4cd28c8feb9faa783e79b48af37961>11 - Cloud Controller Manager Administration</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>
<p>Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the <code><a class=glossary-tooltip title="Control plane component that integrates Kubernetes with third-party cloud providers." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/cloud-controller/ target=_blank aria-label=cloud-controller-manager>cloud-controller-manager</a></code> binary allows cloud vendors to evolve independently from the core Kubernetes code.</p>
<p>The <code>cloud-controller-manager</code> can be linked to any cloud provider that satisfies <a href=https://github.com/kubernetes/cloud-provider/blob/master/cloud.go>cloudprovider.Interface</a>. For backwards compatibility, the <a href=https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager>cloud-controller-manager</a> provided in the core Kubernetes project uses the same cloud libraries as <code>kube-controller-manager</code>. Cloud providers already supported in Kubernetes core are expected to use the in-tree cloud-controller-manager to transition out of Kubernetes core.</p>
<h2 id=administration>Administration</h2>
<h3 id=requirements>Requirements</h3>
<p>Every cloud has their own set of requirements for running their own cloud provider integration, it should not be too different from the requirements when running <code>kube-controller-manager</code>. As a general rule of thumb you'll need:</p>
<ul>
<li>cloud authentication/authorization: your cloud may require a token or IAM rules to allow access to their APIs</li>
<li>kubernetes authentication/authorization: cloud-controller-manager may need RBAC rules set to speak to the kubernetes apiserver</li>
<li>high availability: like kube-controller-manager, you may want a high available setup for cloud controller manager using leader election (on by default).</li>
</ul>
<h3 id=running-cloud-controller-manager>Running cloud-controller-manager</h3>
<p>Successfully running cloud-controller-manager requires some changes to your cluster configuration.</p>
<ul>
<li><code>kube-apiserver</code> and <code>kube-controller-manager</code> MUST NOT specify the <code>--cloud-provider</code> flag. This ensures that it does not run any cloud specific loops that would be run by cloud controller manager. In the future, this flag will be deprecated and removed.</li>
<li><code>kubelet</code> must run with <code>--cloud-provider=external</code>. This is to ensure that the kubelet is aware that it must be initialized by the cloud controller manager before it is scheduled any work.</li>
</ul>
<p>Keep in mind that setting up your cluster to use cloud controller manager will change your cluster behaviour in a few ways:</p>
<ul>
<li>kubelets specifying <code>--cloud-provider=external</code> will add a taint <code>node.cloudprovider.kubernetes.io/uninitialized</code> with an effect <code>NoSchedule</code> during initialization. This marks the node as needing a second initialization from an external controller before it can be scheduled work. Note that in the event that cloud controller manager is not available, new nodes in the cluster will be left unschedulable. The taint is important since the scheduler may require cloud specific information about nodes such as their region or type (high cpu, gpu, high memory, spot instance, etc).</li>
<li>cloud information about nodes in the cluster will no longer be retrieved using local metadata, but instead all API calls to retrieve node information will go through cloud controller manager. This may mean you can restrict access to your cloud API on the kubelets for better security. For larger clusters you may want to consider if cloud controller manager will hit rate limits since it is now responsible for almost all API calls to your cloud from within the cluster.</li>
</ul>
<p>The cloud controller manager can implement:</p>
<ul>
<li>Node controller - responsible for updating kubernetes nodes using cloud APIs and deleting kubernetes nodes that were deleted on your cloud.</li>
<li>Service controller - responsible for loadbalancers on your cloud against services of type LoadBalancer.</li>
<li>Route controller - responsible for setting up network routes on your cloud</li>
<li>any other features you would like to implement if you are running an out-of-tree provider.</li>
</ul>
<h2 id=examples>Examples</h2>
<p>If you are using a cloud that is currently supported in Kubernetes core and would like to adopt cloud controller manager, see the <a href=https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager>cloud controller manager in kubernetes core</a>.</p>
<p>For cloud controller managers not in Kubernetes core, you can find the respective projects in repositories maintained by cloud vendors or by SIGs.</p>
<p>For providers already in Kubernetes core, you can run the in-tree cloud controller manager as a DaemonSet in your cluster, use the following as a guideline:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/cloud/ccm-example.yaml download=admin/cloud/ccm-example.yaml><code>admin/cloud/ccm-example.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-cloud-ccm-example-yaml')" title="Copy admin/cloud/ccm-example.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-cloud-ccm-example-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#080;font-style:italic># This is an example of how to setup cloud-controller-manager as a Daemonset in your cluster.</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># It assumes that your masters can run pods and has the role node-role.kubernetes.io/master</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Note that this Daemonset will not work straight out of the box for your cloud, this is</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># meant to be a guideline.</span><span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ServiceAccount<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRoleBinding<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>roleRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cluster-admin<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ServiceAccount<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DaemonSet<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>serviceAccountName</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># for in-tree providers we use k8s.gcr.io/cloud-controller-manager</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># this can be replaced with any other image for out-of-tree providers</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/cloud-controller-manager:v1.8.0<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- /usr/local/bin/cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- --cloud-provider=[YOUR_CLOUD_PROVIDER] <span style=color:#bbb> </span><span style=color:#080;font-style:italic># Add your own cloud provider here!</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span>- --leader-elect=true<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- --use-service-account-credentials<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># these flags will vary for every cloud provider</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span>- --allocate-node-cidrs=true<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- --configure-cloud-routes=true<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- --cluster-cidr=172.17.0.0/16<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># this is required so CCM can bootstrap itself</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node.cloudprovider.kubernetes.io/uninitialized<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># this is to have the daemonset runnable on master nodes</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># the taint may vary depending on your cluster setup</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node-role.kubernetes.io/master<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># this is to restrict CCM to only run on master nodes</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># the node selector may vary depending on your cluster setup</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>node-role.kubernetes.io/master</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<h2 id=limitations>Limitations</h2>
<p>Running cloud controller manager comes with a few possible limitations. Although these limitations are being addressed in upcoming releases, it's important that you are aware of these limitations for production workloads.</p>
<h3 id=support-for-volumes>Support for Volumes</h3>
<p>Cloud controller manager does not implement any of the volume controllers found in <code>kube-controller-manager</code> as the volume integrations also require coordination with kubelets. As we evolve CSI (container storage interface) and add stronger support for flex volume plugins, necessary support will be added to cloud controller manager so that clouds can fully integrate with volumes. Learn more about out-of-tree CSI volume plugins <a href=https://github.com/kubernetes/features/issues/178>here</a>.</p>
<h3 id=scalability>Scalability</h3>
<p>The cloud-controller-manager queries your cloud provider's APIs to retrieve information for all nodes. For very large clusters, consider possible bottlenecks such as resource requirements and API rate limiting.</p>
<h3 id=chicken-and-egg>Chicken and Egg</h3>
<p>The goal of the cloud controller manager project is to decouple development of cloud features from the core Kubernetes project. Unfortunately, many aspects of the Kubernetes project has assumptions that cloud provider features are tightly integrated into the project. As a result, adopting this new architecture can create several situations where a request is being made for information from a cloud provider, but the cloud controller manager may not be able to return that information without the original request being complete.</p>
<p>A good example of this is the TLS bootstrapping feature in the Kubelet. TLS bootstrapping assumes that the Kubelet has the ability to ask the cloud provider (or a local metadata service) for all its address types (private, public, etc) but cloud controller manager cannot set a node's address types without being initialized in the first place which requires that the kubelet has TLS certificates to communicate with the apiserver.</p>
<p>As this initiative evolves, changes will be made to address these issues in upcoming releases.</p>
<h2 id=what-s-next>What's next</h2>
<p>To build and develop your own cloud controller manager, read <a href=/docs/tasks/administer-cluster/developing-cloud-controller-manager/>Developing Cloud Controller Manager</a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-5e59f5575dce11fdaed640afdbeedfc1>12 - Configure Quotas for API Objects</h1>
<p>This page shows how to configure quotas for API objects, including
PersistentVolumeClaims and Services. A quota restricts the number of
objects, of a particular type, that can be created in a namespace.
You specify quotas in a
<a href=/docs/reference/generated/kubernetes-api/v1.23/#resourcequota-v1-core>ResourceQuota</a>
object.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=create-a-namespace>Create a namespace</h2>
<p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace quota-object-example
</code></pre></div><h2 id=create-a-resourcequota>Create a ResourceQuota</h2>
<p>Here is the configuration file for a ResourceQuota object:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/quota-objects.yaml download=admin/resource/quota-objects.yaml><code>admin/resource/quota-objects.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-quota-objects-yaml')" title="Copy admin/resource/quota-objects.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-quota-objects-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>object-quota-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>persistentvolumeclaims</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>services.loadbalancers</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>services.nodeports</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the ResourceQuota:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace<span style=color:#666>=</span>quota-object-example
</code></pre></div><p>View detailed information about the ResourceQuota:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get resourcequota object-quota-demo --namespace<span style=color:#666>=</span>quota-object-example --output<span style=color:#666>=</span>yaml
</code></pre></div><p>The output shows that in the quota-object-example namespace, there can be at most
one PersistentVolumeClaim, at most two Services of type LoadBalancer, and no Services
of type NodePort.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>persistentvolumeclaims</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>services.loadbalancers</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>services.nodeports</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>used</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>persistentvolumeclaims</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>services.loadbalancers</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>services.nodeports</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span></code></pre></div><h2 id=create-a-persistentvolumeclaim>Create a PersistentVolumeClaim</h2>
<p>Here is the configuration file for a PersistentVolumeClaim object:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/quota-objects-pvc.yaml download=admin/resource/quota-objects-pvc.yaml><code>admin/resource/quota-objects-pvc.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-quota-objects-pvc-yaml')" title="Copy admin/resource/quota-objects-pvc.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-quota-objects-pvc-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pvc-quota-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>manual<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>3Gi<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Create the PersistentVolumeClaim:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc.yaml --namespace<span style=color:#666>=</span>quota-object-example
</code></pre></div><p>Verify that the PersistentVolumeClaim was created:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get persistentvolumeclaims --namespace<span style=color:#666>=</span>quota-object-example
</code></pre></div><p>The output shows that the PersistentVolumeClaim exists and has status Pending:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME             STATUS
pvc-quota-demo   Pending
</code></pre></div><h2 id=attempt-to-create-a-second-persistentvolumeclaim>Attempt to create a second PersistentVolumeClaim</h2>
<p>Here is the configuration file for a second PersistentVolumeClaim:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/resource/quota-objects-pvc-2.yaml download=admin/resource/quota-objects-pvc-2.yaml><code>admin/resource/quota-objects-pvc-2.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-resource-quota-objects-pvc-2-yaml')" title="Copy admin/resource/quota-objects-pvc-2.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-resource-quota-objects-pvc-2-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pvc-quota-demo-2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>manual<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>4Gi<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Attempt to create the second PersistentVolumeClaim:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc-2.yaml --namespace<span style=color:#666>=</span>quota-object-example
</code></pre></div><p>The output shows that the second PersistentVolumeClaim was not created,
because it would have exceeded the quota for the namespace.</p>
<pre><code>persistentvolumeclaims &quot;pvc-quota-demo-2&quot; is forbidden:
exceeded quota: object-quota-demo, requested: persistentvolumeclaims=1,
used: persistentvolumeclaims=1, limited: persistentvolumeclaims=1
</code></pre><h2 id=notes>Notes</h2>
<p>These are the strings used to identify API resources that can be constrained
by quotas:</p>
<table>
<tr><th>String</th><th>API Object</th></tr>
<tr><td>"pods"</td><td>Pod</td></tr>
<tr><td>"services"</td><td>Service</td></tr>
<tr><td>"replicationcontrollers"</td><td>ReplicationController</td></tr>
<tr><td>"resourcequotas"</td><td>ResourceQuota</td></tr>
<tr><td>"secrets"</td><td>Secret</td></tr>
<tr><td>"configmaps"</td><td>ConfigMap</td></tr>
<tr><td>"persistentvolumeclaims"</td><td>PersistentVolumeClaim</td></tr>
<tr><td>"services.nodeports"</td><td>Service of type NodePort</td></tr>
<tr><td>"services.loadbalancers"</td><td>Service of type LoadBalancer</td></tr>
</table>
<h2 id=clean-up>Clean up</h2>
<p>Delete your namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete namespace quota-object-example
</code></pre></div><h2 id=what-s-next>What's next</h2>
<h3 id=for-cluster-administrators>For cluster administrators</h3>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>Configure Default CPU Requests and Limits for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>Configure Minimum and Maximum Memory Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Configure Minimum and Maximum CPU Constraints for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>Configure Memory and CPU Quotas for a Namespace</a></p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/>Configure a Pod Quota for a Namespace</a></p>
</li>
</ul>
<h3 id=for-app-developers>For app developers</h3>
<ul>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-memory-resource/>Assign Memory Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>Assign CPU Resources to Containers and Pods</a></p>
</li>
<li>
<p><a href=/docs/tasks/configure-pod-container/quality-service-pod/>Configure Quality of Service for Pods</a></p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-7127e6b7344b315b30b1ce8c4d8bfc55>13 - Control CPU Management Policies on the Node</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.12 [beta]</code>
</div>
<p>Kubernetes keeps many aspects of how pods execute on nodes abstracted
from the user. This is by design.  However, some workloads require
stronger guarantees in terms of latency and/or performance in order to operate
acceptably. The kubelet provides methods to enable more complex workload
placement policies while keeping the abstraction free from explicit placement
directives.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=cpu-management-policies>CPU Management Policies</h2>
<p>By default, the kubelet uses <a href=https://en.wikipedia.org/wiki/Completely_Fair_Scheduler>CFS quota</a>
to enforce pod CPU limits.  When the node runs many CPU-bound pods,
the workload can move to different CPU cores depending on
whether the pod is throttled and which CPU cores are available at
scheduling time. Many workloads are not sensitive to this migration and thus
work fine without any intervention.</p>
<p>However, in workloads where CPU cache affinity and scheduling latency
significantly affect workload performance, the kubelet allows alternative CPU
management policies to determine some placement preferences on the node.</p>
<h3 id=configuration>Configuration</h3>
<p>The CPU Manager policy is set with the <code>--cpu-manager-policy</code> kubelet
flag or the <code>cpuManagerPolicy</code> field in <a href=/docs/reference/config-api/kubelet-config.v1beta1/>KubeletConfiguration</a>.
There are two supported policies:</p>
<ul>
<li><a href=#none-policy><code>none</code></a>: the default policy.</li>
<li><a href=#static-policy><code>static</code></a>: allows pods with certain resource characteristics to be
granted increased CPU affinity and exclusivity on the node.</li>
</ul>
<p>The CPU manager periodically writes resource updates through the CRI in
order to reconcile in-memory CPU assignments with cgroupfs. The reconcile
frequency is set through a new Kubelet configuration value
<code>--cpu-manager-reconcile-period</code>. If not specified, it defaults to the same
duration as <code>--node-status-update-frequency</code>.</p>
<p>The behavior of the static policy can be fine-tuned using the <code>--cpu-manager-policy-options</code> flag.
The flag takes a comma-separated list of <code>key=value</code> policy options.
This feature can be disabled completely using the <code>CPUManagerPolicyOptions</code> feature gate.</p>
<p>The policy options are split into two groups: alpha quality (hidden by default) and beta quality
(visible by default). The groups are guarded respectively by the <code>CPUManagerPolicyAlphaOptions</code>
and <code>CPUManagerPolicyBetaOptions</code> feature gates. Diverging from the Kubernetes standard, these
feature gates guard groups of options, because it would have been too cumbersome to add a feature
gate for each individual option.</p>
<h3 id=changing-the-cpu-manager-policy>Changing the CPU Manager Policy</h3>
<p>Since the CPU manger policy can only be applied when kubelet spawns new pods, simply changing from
"none" to "static" won't apply to existing pods. So in order to properly change the CPU manager
policy on a node, perform the following steps:</p>
<ol>
<li><a href=/docs/tasks/administer-cluster/safely-drain-node>Drain</a> the node.</li>
<li>Stop kubelet.</li>
<li>Remove the old CPU manager state file. The path to this file is
<code>/var/lib/kubelet/cpu_manager_state</code> by default. This clears the state maintained by the
CPUManager so that the cpu-sets set up by the new policy won’t conflict with it.</li>
<li>Edit the kubelet configuration to change the CPU manager policy to the desired value.</li>
<li>Start kubelet.</li>
</ol>
<p>Repeat this process for every node that needs its CPU manager policy changed. Skipping this
process will result in kubelet crashlooping with the following error:</p>
<pre><code>could not restore state from checkpoint: configured policy &quot;static&quot; differs from state checkpoint policy &quot;none&quot;, please drain this node and delete the CPU manager checkpoint file &quot;/var/lib/kubelet/cpu_manager_state&quot; before restarting Kubelet
</code></pre><h3 id=none-policy>None policy</h3>
<p>The <code>none</code> policy explicitly enables the existing default CPU
affinity scheme, providing no affinity beyond what the OS scheduler does
automatically.  Limits on CPU usage for
<a href=/docs/tasks/configure-pod-container/quality-service-pod/>Guaranteed pods</a> and
<a href=/docs/tasks/configure-pod-container/quality-service-pod/>Burstable pods</a>
are enforced using CFS quota.</p>
<h3 id=static-policy>Static policy</h3>
<p>The <code>static</code> policy allows containers in <code>Guaranteed</code> pods with integer CPU
<code>requests</code> access to exclusive CPUs on the node. This exclusivity is enforced
using the <a href=https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt>cpuset cgroup controller</a>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> System services such as the container runtime and the kubelet itself can continue to run on these exclusive CPUs.  The exclusivity only extends to other pods.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> CPU Manager doesn't support offlining and onlining of
CPUs at runtime. Also, if the set of online CPUs changes on the node,
the node must be drained and CPU manager manually reset by deleting the
state file <code>cpu_manager_state</code> in the kubelet root directory.
</div>
<p>This policy manages a shared pool of CPUs that initially contains all CPUs in the
node. The amount of exclusively allocatable CPUs is equal to the total
number of CPUs in the node minus any CPU reservations by the kubelet <code>--kube-reserved</code> or
<code>--system-reserved</code> options. From 1.17, the CPU reservation list can be specified
explicitly by kubelet <code>--reserved-cpus</code> option. The explicit CPU list specified by
<code>--reserved-cpus</code> takes precedence over the CPU reservation specified by
<code>--kube-reserved</code> and <code>--system-reserved</code>. CPUs reserved by these options are taken, in
integer quantity, from the initial shared pool in ascending order by physical
core ID.  This shared pool is the set of CPUs on which any containers in
<code>BestEffort</code> and <code>Burstable</code> pods run. Containers in <code>Guaranteed</code> pods with fractional
CPU <code>requests</code> also run on CPUs in the shared pool. Only containers that are
both part of a <code>Guaranteed</code> pod and have integer CPU <code>requests</code> are assigned
exclusive CPUs.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The kubelet requires a CPU reservation greater than zero be made
using either <code>--kube-reserved</code> and/or <code>--system-reserved</code> or <code>--reserved-cpus</code> when
the static policy is enabled. This is because zero CPU reservation would allow the shared
pool to become empty.
</div>
<p>As <code>Guaranteed</code> pods whose containers fit the requirements for being statically
assigned are scheduled to the node, CPUs are removed from the shared pool and
placed in the cpuset for the container. CFS quota is not used to bound
the CPU usage of these containers as their usage is bound by the scheduling domain
itself. In others words, the number of CPUs in the container cpuset is equal to the integer
CPU <code>limit</code> specified in the pod spec. This static assignment increases CPU
affinity and decreases context switches due to throttling for the CPU-bound
workload.</p>
<p>Consider the containers in the following pod specs:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>BestEffort</code> QoS class because no resource <code>requests</code> or
<code>limits</code> are specified. It runs in the shared pool.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Mi&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>Burstable</code> QoS class because resource <code>requests</code> do not
equal <code>limits</code> and the <code>cpu</code> quantity is not specified. It runs in the shared
pool.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>Burstable</code> QoS class because resource <code>requests</code> do not
equal <code>limits</code>. It runs in the shared pool.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal to <code>limits</code>.
And the container's resource limit for the CPU resource is an integer greater than
or equal to one. The <code>nginx</code> container is granted 2 exclusive CPUs.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1.5&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1.5&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal to <code>limits</code>.
But the container's resource limit for the CPU resource is a fraction. It runs in
the shared pool.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>Guaranteed</code> QoS class because only <code>limits</code> are specified
and <code>requests</code> are set equal to <code>limits</code> when not explicitly specified. And the
container's resource limit for the CPU resource is an integer greater than or
equal to one. The <code>nginx</code> container is granted 2 exclusive CPUs.</p>
<h4 id=static-policy-options>Static policy options</h4>
<p>You can toggle groups of options on and off based upon their maturity level
using the following feature gates:</p>
<ul>
<li><code>CPUManagerPolicyBetaOptions</code> default enabled. Disable to hide beta-level options.</li>
<li><code>CPUManagerPolicyAlphaOptions</code> default disabled. Enable to show alpha-level options.
You will still have to enable each option using the <code>CPUManagerPolicyOptions</code> kubelet option.</li>
</ul>
<p>The following policy options exist for the static <code>CPUManager</code> policy:</p>
<ul>
<li><code>full-pcpus-only</code> (beta, visible by default)</li>
<li><code>distribute-cpus-across-numa</code> (alpha, hidden by default)</li>
</ul>
<p>If the <code>full-pcpus-only</code> policy option is specified, the static policy will always allocate full physical cores.
By default, without this option, the static policy allocates CPUs using a topology-aware best-fit allocation.
On SMT enabled systems, the policy can allocate individual virtual cores, which correspond to hardware threads.
This can lead to different containers sharing the same physical cores; this behaviour in turn contributes
to the <a href=https://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors>noisy neighbours problem</a>.
With the option enabled, the pod will be admitted by the kubelet only if the CPU request of all its containers
can be fulfilled by allocating full physical cores.
If the pod does not pass the admission, it will be put in Failed state with the message <code>SMTAlignmentError</code>.</p>
<p>If the <code>distribute-cpus-across-numa</code>policy option is specified, the static
policy will evenly distribute CPUs across NUMA nodes in cases where more than
one NUMA node is required to satisfy the allocation.
By default, the <code>CPUManager</code> will pack CPUs onto one NUMA node until it is
filled, with any remaining CPUs simply spilling over to the next NUMA node.
This can cause undesired bottlenecks in parallel code relying on barriers (and
similar synchronization primitives), as this type of code tends to run only as
fast as its slowest worker (which is slowed down by the fact that fewer CPUs
are available on at least one NUMA node).
By distributing CPUs evenly across NUMA nodes, application developers can more
easily ensure that no single worker suffers from NUMA effects more than any
other, improving the overall performance of these types of applications.</p>
<p>The <code>full-pcpus-only</code> option can be enabled by adding <code>full-pcups-only=true</code> to
the CPUManager policy options.
Likewise, the <code>distribute-cpus-across-numa</code> option can be enabled by adding
<code>distribute-cpus-across-numa=true</code> to the CPUManager policy options.
When both are set, they are "additive" in the sense that CPUs will be
distributed across NUMA nodes in chunks of full-pcpus rather than individual
cores.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-8060aed5bf1172fa62199a4c306a4cd1>14 - Control Topology Management Policies on a node</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>
<p>An increasing number of systems leverage a combination of CPUs and hardware accelerators to support latency-critical execution and high-throughput parallel computation. These include workloads in fields such as telecommunications, scientific computing, machine learning, financial services and data analytics. Such hybrid systems comprise a high performance environment.</p>
<p>In order to extract the best performance, optimizations related to CPU isolation, memory and device locality are required. However, in Kubernetes, these optimizations are handled by a disjoint set of components.</p>
<p><em>Topology Manager</em> is a Kubelet component that aims to co-ordinate the set of components that are responsible for these optimizations.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
Your Kubernetes server must be at or later than version v1.18.
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=how-topology-manager-works>How Topology Manager Works</h2>
<p>Prior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make resource allocation decisions independently of each other.
This can result in undesirable allocations on multiple-socketed systems, performance/latency sensitive applications will suffer due to these undesirable allocations.
Undesirable in this case meaning for example, CPUs and devices being allocated from different NUMA Nodes thus, incurring additional latency.</p>
<p>The Topology Manager is a Kubelet component, which acts as a source of truth so that other Kubelet components can make topology aligned resource allocation choices.</p>
<p>The Topology Manager provides an interface for components, called <em>Hint Providers</em>, to send and receive topology information. Topology Manager has a set of node level policies which are explained below.</p>
<p>The Topology manager receives Topology information from the <em>Hint Providers</em> as a bitmask denoting NUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform a set of operations on the hints provided and converge on the hint determined by the policy to give the optimal result, if an undesirable hint is stored the preferred field for the hint will be set to false. In the current policies preferred is the narrowest preferred mask.
The selected hint is stored as part of the Topology Manager. Depending on the policy configured the pod can be accepted or rejected from the node based on the selected hint.
The hint is then stored in the Topology Manager for use by the <em>Hint Providers</em> when making the resource allocation decisions.</p>
<h3 id=enable-the-topology-manager-feature>Enable the Topology Manager feature</h3>
<p>Support for the Topology Manager requires <code>TopologyManager</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> to be enabled. It is enabled by default starting with Kubernetes 1.18.</p>
<h2 id=topology-manager-scopes-and-policies>Topology Manager Scopes and Policies</h2>
<p>The Topology Manager currently:</p>
<ul>
<li>Aligns Pods of all QoS classes.</li>
<li>Aligns the requested resources that Hint Provider provides topology hints for.</li>
</ul>
<p>If these conditions are met, the Topology Manager will align the requested resources.</p>
<p>In order to customise how this alignment is carried out, the Topology Manager provides two distinct knobs: <code>scope</code> and <code>policy</code>.</p>
<p>The <code>scope</code> defines the granularity at which you would like resource alignment to be performed (e.g. at the <code>pod</code> or <code>container</code> level). And the <code>policy</code> defines the actual strategy used to carry out the alignment (e.g. <code>best-effort</code>, <code>restricted</code>, <code>single-numa-node</code>, etc.).</p>
<p>Details on the various <code>scopes</code> and <code>policies</code> available today can be found below.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> To align CPU resources with other requested resources in a Pod Spec, the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node. See <a href=/docs/tasks/administer-cluster/cpu-management-policies/>control CPU Management Policies</a>.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> To align memory (and hugepages) resources with other requested resources in a Pod Spec, the Memory Manager should be enabled and proper Memory Manager policy should be configured on a Node. Examine <a href=/docs/tasks/administer-cluster/memory-manager/>Memory Manager</a> documentation.
</div>
<h3 id=topology-manager-scopes>Topology Manager Scopes</h3>
<p>The Topology Manager can deal with the alignment of resources in a couple of distinct scopes:</p>
<ul>
<li><code>container</code> (default)</li>
<li><code>pod</code></li>
</ul>
<p>Either option can be selected at a time of the kubelet startup, with <code>--topology-manager-scope</code> flag.</p>
<h3 id=container-scope>container scope</h3>
<p>The <code>container</code> scope is used by default.</p>
<p>Within this scope, the Topology Manager performs a number of sequential resource alignments, i.e., for each container (in a pod) a separate alignment is computed. In other words, there is no notion of grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect, the Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.</p>
<p>The notion of grouping the containers was endorsed and implemented on purpose in the following scope, for example the <code>pod</code> scope.</p>
<h3 id=pod-scope>pod scope</h3>
<p>To select the <code>pod</code> scope, start the kubelet with the command line option <code>--topology-manager-scope=pod</code>.</p>
<p>This scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the Topology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers) to either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the alignments produced by the Topology Manager on different occasions:</p>
<ul>
<li>all containers can be and are allocated to a single NUMA node;</li>
<li>all containers can be and are allocated to a shared set of NUMA nodes.</li>
</ul>
<p>The total amount of particular resource demanded for the entire pod is calculated according to <a href=/docs/concepts/workloads/pods/init-containers/#resources>effective requests/limits</a> formula, and thus, this total value is equal to the maximum of:</p>
<ul>
<li>the sum of all app container requests,</li>
<li>the maximum of init container requests,
for a resource.</li>
</ul>
<p>Using the <code>pod</code> scope in tandem with <code>single-numa-node</code> Topology Manager policy is specifically valuable for workloads that are latency sensitive or for high-throughput applications that perform IPC. By combining both options, you are able to place all containers in a pod onto a single NUMA node; hence, the inter-NUMA communication overhead can be eliminated for that pod.</p>
<p>In the case of <code>single-numa-node</code> policy, a pod is accepted only if a suitable set of NUMA nodes is present among possible allocations. Reconsider the example above:</p>
<ul>
<li>a set containing only a single NUMA node - it leads to pod being admitted,</li>
<li>whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one NUMA node, two or more NUMA nodes are required to satisfy the allocation).</li>
</ul>
<p>To recap, Topology Manager first computes a set of NUMA nodes and then tests it against Topology Manager policy, which either leads to the rejection or admission of the pod.</p>
<h3 id=topology-manager-policies>Topology Manager Policies</h3>
<p>Topology Manager supports four allocation policies. You can set a policy via a Kubelet flag, <code>--topology-manager-policy</code>.
There are four supported policies:</p>
<ul>
<li><code>none</code> (default)</li>
<li><code>best-effort</code></li>
<li><code>restricted</code></li>
<li><code>single-numa-node</code></li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If Topology Manager is configured with the <strong>pod</strong> scope, the container, which is considered by the policy, is reflecting requirements of the entire pod, and thus each container from the pod will result with <strong>the same</strong> topology alignment decision.
</div>
<h3 id=policy-none>none policy</h3>
<p>This is the default policy and does not perform any topology alignment.</p>
<h3 id=policy-best-effort>best-effort policy</h3>
<p>For each container in a Pod, the kubelet, with <code>best-effort</code> topology
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager stores the
preferred NUMA Node affinity for that container. If the affinity is not preferred,
Topology Manager will store this and admit the pod to the node anyway.</p>
<p>The <em>Hint Providers</em> can then use this information when making the
resource allocation decision.</p>
<h3 id=policy-restricted>restricted policy</h3>
<p>For each container in a Pod, the kubelet, with <code>restricted</code> topology
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager stores the
preferred NUMA Node affinity for that container. If the affinity is not preferred,
Topology Manager will reject this pod from the node. This will result in a pod in a <code>Terminated</code> state with a pod admission failure.</p>
<p>Once the pod is in a <code>Terminated</code> state, the Kubernetes scheduler will <strong>not</strong> attempt to reschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeploy of the pod.
An external control loop could be also implemented to trigger a redeployment of pods that have the <code>Topology Affinity</code> error.</p>
<p>If the pod is admitted, the <em>Hint Providers</em> can then use this information when making the
resource allocation decision.</p>
<h3 id=policy-single-numa-node>single-numa-node policy</h3>
<p>For each container in a Pod, the kubelet, with <code>single-numa-node</code> topology
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager determines if a single NUMA Node affinity is possible.
If it is, Topology Manager will store this and the <em>Hint Providers</em> can then use this information when making the
resource allocation decision.
If, however, this is not possible then the Topology Manager will reject the pod from the node. This will result in a pod in a <code>Terminated</code> state with a pod admission failure.</p>
<p>Once the pod is in a <code>Terminated</code> state, the Kubernetes scheduler will <strong>not</strong> attempt to reschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeploy of the Pod.
An external control loop could be also implemented to trigger a redeployment of pods that have the <code>Topology Affinity</code> error.</p>
<h3 id=pod-interactions-with-topology-manager-policies>Pod Interactions with Topology Manager Policies</h3>
<p>Consider the containers in the following pod specs:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>BestEffort</code> QoS class because no resource <code>requests</code> or
<code>limits</code> are specified.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Mi&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>Burstable</code> QoS class because requests are less than limits.</p>
<p>If the selected policy is anything other than <code>none</code>, Topology Manager would consider these Pod specifications. The Topology Manager would consult the Hint Providers to get topology hints. In the case of the <code>static</code>, the CPU Manager policy would return default topology hint, because these Pods do not have explicitly request CPU resources.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/device</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/device</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod with integer CPU request runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal to <code>limits</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;300m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/device</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;300m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/device</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod with sharing CPU request runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal to <code>limits</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/deviceA</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/deviceB</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/deviceA</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/deviceB</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>This pod runs in the <code>BestEffort</code> QoS class because there are no CPU and memory requests.</p>
<p>The Topology Manager would consider the above pods. The Topology Manager would consult the Hint Providers, which are CPU and Device Manager to get topology hints for the pods.</p>
<p>In the case of the <code>Guaranteed</code> pod with integer CPU request, the <code>static</code> CPU Manager policy would return topology hints relating to the exclusive CPU and the Device Manager would send back hints for the requested device.</p>
<p>In the case of the <code>Guaranteed</code> pod with sharing CPU request, the <code>static</code> CPU Manager policy would return default topology hint as there is no exclusive CPU request and the Device Manager would send back hints for the requested device.</p>
<p>In the above two cases of the <code>Guaranteed</code> pod, the <code>none</code> CPU Manager policy would return default topology hint.</p>
<p>In the case of the <code>BestEffort</code> pod, the <code>static</code> CPU Manager policy would send back the default topology hint as there is no CPU request and the Device Manager would send back the hints for each of the requested devices.</p>
<p>Using this information the Topology Manager calculates the optimal hint for the pod and stores this information, which will be used by the Hint Providers when they are making their resource assignments.</p>
<h3 id=known-limitations>Known Limitations</h3>
<ol>
<li>
<p>The maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes there will be a state explosion when trying to enumerate the possible NUMA affinities and generating their hints.</p>
</li>
<li>
<p>The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail on the node due to the Topology Manager.</p>
</li>
</ol>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-3d0cd7d2f13d4759094f281504cf57b8>15 - Customizing DNS Service</h1>
<p>This page explains how to configure your DNS
<a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod(s)>Pod(s)</a> and customize the
DNS resolution process in your cluster.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>Your cluster must be running the CoreDNS add-on.
<a href=/docs/tasks/administer-cluster/coredns/#migrating-to-coredns>Migrating to CoreDNS</a>
explains how to use <code>kubeadm</code> to migrate from <code>kube-dns</code>.</p>
<p>Your Kubernetes server must be at or later than version v1.12.
To check the version, enter <code>kubectl version</code>.</p>
<h2 id=introduction>Introduction</h2>
<p>DNS is a built-in Kubernetes service launched automatically
using the <em>addon manager</em>
<a href=http://releases.k8s.io/master/cluster/addons/README.md>cluster add-on</a>.</p>
<p>As of Kubernetes v1.12, CoreDNS is the recommended DNS Server, replacing kube-dns. If your cluster
originally used kube-dns, you may still have <code>kube-dns</code> deployed rather than CoreDNS.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The CoreDNS Service is named <code>kube-dns</code> in the <code>metadata.name</code> field.<br>
This is so that there is greater interoperability with workloads that relied on the legacy <code>kube-dns</code> Service name to resolve addresses internal to the cluster. Using a Service named <code>kube-dns</code> abstracts away the implementation detail of which DNS provider is running behind that common name.
</div>
<p>If you are running CoreDNS as a Deployment, it will typically be exposed as a Kubernetes Service with a static IP address.
The kubelet passes DNS resolver information to each container with the <code>--cluster-dns=&lt;dns-service-ip></code> flag.</p>
<p>DNS names also need domains. You configure the local domain in the kubelet
with the flag <code>--cluster-domain=&lt;default-local-domain></code>.</p>
<p>The DNS server supports forward lookups (A and AAAA records), port lookups (SRV records), reverse IP address lookups (PTR records),
and more. For more information, see <a href=/docs/concepts/services-networking/dns-pod-service/>DNS for Services and Pods</a>.</p>
<p>If a Pod's <code>dnsPolicy</code> is set to <code>default</code>, it inherits the name resolution
configuration from the node that the Pod runs on. The Pod's DNS resolution
should behave the same as the node.
But see <a href=/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues>Known issues</a>.</p>
<p>If you don't want this, or if you want a different DNS config for pods, you can
use the kubelet's <code>--resolv-conf</code> flag. Set this flag to "" to prevent Pods from
inheriting DNS. Set it to a valid file path to specify a file other than
<code>/etc/resolv.conf</code> for DNS inheritance.</p>
<h2 id=coredns>CoreDNS</h2>
<p>CoreDNS is a general-purpose authoritative DNS server that can serve as cluster DNS, complying with the <a href=https://github.com/kubernetes/dns/blob/master/docs/specification.md>dns specifications</a>.</p>
<h3 id=coredns-configmap-options>CoreDNS ConfigMap options</h3>
<p>CoreDNS is a DNS server that is modular and pluggable, and each plugin adds new functionality to CoreDNS.
This can be configured by maintaining a <a href=https://coredns.io/2017/07/23/corefile-explained/>Corefile</a>, which is the CoreDNS
configuration file. As a cluster administrator, you can modify the
<a class=glossary-tooltip title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a> for the CoreDNS Corefile to change how DNS service discovery
behaves for that cluster.</p>
<p>In Kubernetes, CoreDNS is installed with the following default Corefile configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>coredns<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>Corefile</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    .:53 {
</span><span style=color:#b44;font-style:italic>        errors
</span><span style=color:#b44;font-style:italic>        health {
</span><span style=color:#b44;font-style:italic>            lameduck 5s
</span><span style=color:#b44;font-style:italic>        }
</span><span style=color:#b44;font-style:italic>        ready
</span><span style=color:#b44;font-style:italic>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style=color:#b44;font-style:italic>            pods insecure
</span><span style=color:#b44;font-style:italic>            fallthrough in-addr.arpa ip6.arpa
</span><span style=color:#b44;font-style:italic>            ttl 30
</span><span style=color:#b44;font-style:italic>        }
</span><span style=color:#b44;font-style:italic>        prometheus :9153
</span><span style=color:#b44;font-style:italic>        forward . /etc/resolv.conf
</span><span style=color:#b44;font-style:italic>        cache 30
</span><span style=color:#b44;font-style:italic>        loop
</span><span style=color:#b44;font-style:italic>        reload
</span><span style=color:#b44;font-style:italic>        loadbalance
</span><span style=color:#b44;font-style:italic>    }</span><span style=color:#bbb>    
</span></code></pre></div><p>The Corefile configuration includes the following <a href=https://coredns.io/plugins/>plugins</a> of CoreDNS:</p>
<ul>
<li><a href=https://coredns.io/plugins/errors/>errors</a>: Errors are logged to stdout.</li>
<li><a href=https://coredns.io/plugins/health/>health</a>: Health of CoreDNS is reported to <code>http://localhost:8080/health</code>. In this extended syntax <code>lameduck</code> will make the process unhealthy then wait for 5 seconds before the process is shut down.</li>
<li><a href=https://coredns.io/plugins/ready/>ready</a>: An HTTP endpoint on port 8181 will return 200 OK, when all plugins that are able to signal readiness have done so.</li>
<li><a href=https://coredns.io/plugins/kubernetes/>kubernetes</a>: CoreDNS will reply to DNS queries based on IP of the services and pods of Kubernetes. You can find <a href=https://coredns.io/plugins/kubernetes/>more details</a> about that plugin on the CoreDNS website. <code>ttl</code> allows you to set a custom TTL for responses. The default is 5 seconds. The minimum TTL allowed is 0 seconds, and the maximum is capped at 3600 seconds. Setting TTL to 0 will prevent records from being cached.<br>
The <code>pods insecure</code> option is provided for backward compatibility with <em>kube-dns</em>. You can use the <code>pods verified</code> option, which returns an A record only if there exists a pod in same namespace with matching IP. The <code>pods disabled</code> option can be used if you don't use pod records.</li>
<li><a href=https://coredns.io/plugins/metrics/>prometheus</a>: Metrics of CoreDNS are available at <code>http://localhost:9153/metrics</code> in <a href=https://prometheus.io/>Prometheus</a> format (also known as OpenMetrics).</li>
<li><a href=https://coredns.io/plugins/forward/>forward</a>: Any queries that are not within the cluster domain of Kubernetes will be forwarded to predefined resolvers (/etc/resolv.conf).</li>
<li><a href=https://coredns.io/plugins/cache/>cache</a>: This enables a frontend cache.</li>
<li><a href=https://coredns.io/plugins/loop/>loop</a>: Detects simple forwarding loops and halts the CoreDNS process if a loop is found.</li>
<li><a href=https://coredns.io/plugins/reload>reload</a>: Allows automatic reload of a changed Corefile. After you edit the ConfigMap configuration, allow two minutes for your changes to take effect.</li>
<li><a href=https://coredns.io/plugins/loadbalance>loadbalance</a>: This is a round-robin DNS loadbalancer that randomizes the order of A, AAAA, and MX records in the answer.</li>
</ul>
<p>You can modify the default CoreDNS behavior by modifying the ConfigMap.</p>
<h3 id=configuration-of-stub-domain-and-upstream-nameserver-using-coredns>Configuration of Stub-domain and upstream nameserver using CoreDNS</h3>
<p>CoreDNS has the ability to configure stubdomains and upstream nameservers using the <a href=https://coredns.io/plugins/forward/>forward plugin</a>.</p>
<h4 id=example>Example</h4>
<p>If a cluster operator has a <a href=https://www.consul.io/>Consul</a> domain server located at 10.150.0.1, and all Consul names have the suffix .consul.local. To configure it in CoreDNS, the cluster administrator creates the following stanza in the CoreDNS ConfigMap.</p>
<pre><code>consul.local:53 {
        errors
        cache 30
        forward . 10.150.0.1
    }
</code></pre><p>To explicitly force all non-cluster DNS lookups to go through a specific nameserver at 172.16.0.1, point the <code>forward</code> to the nameserver instead of <code>/etc/resolv.conf</code></p>
<pre><code>forward .  172.16.0.1
</code></pre><p>The final ConfigMap along with the default <code>Corefile</code> configuration looks like:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>coredns<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>Corefile</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    .:53 {
</span><span style=color:#b44;font-style:italic>        errors
</span><span style=color:#b44;font-style:italic>        health
</span><span style=color:#b44;font-style:italic>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style=color:#b44;font-style:italic>           pods insecure
</span><span style=color:#b44;font-style:italic>           fallthrough in-addr.arpa ip6.arpa
</span><span style=color:#b44;font-style:italic>        }
</span><span style=color:#b44;font-style:italic>        prometheus :9153
</span><span style=color:#b44;font-style:italic>        forward . 172.16.0.1
</span><span style=color:#b44;font-style:italic>        cache 30
</span><span style=color:#b44;font-style:italic>        loop
</span><span style=color:#b44;font-style:italic>        reload
</span><span style=color:#b44;font-style:italic>        loadbalance
</span><span style=color:#b44;font-style:italic>    }
</span><span style=color:#b44;font-style:italic>    consul.local:53 {
</span><span style=color:#b44;font-style:italic>        errors
</span><span style=color:#b44;font-style:italic>        cache 30
</span><span style=color:#b44;font-style:italic>        forward . 10.150.0.1
</span><span style=color:#b44;font-style:italic>    }</span><span style=color:#bbb>    
</span></code></pre></div><p>The <code>kubeadm</code> tool supports automatic translation from the kube-dns ConfigMap
to the equivalent CoreDNS ConfigMap.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> While kube-dns accepts an FQDN for stubdomain and nameserver (eg: ns.foo.com), CoreDNS does not support this feature.
During translation, all FQDN nameservers will be omitted from the CoreDNS config.
</div>
<h2 id=coredns-configuration-equivalent-to-kube-dns>CoreDNS configuration equivalent to kube-dns</h2>
<p>CoreDNS supports the features of kube-dns and more.
A ConfigMap created for kube-dns to support <code>StubDomains</code>and <code>upstreamNameservers</code> translates to the <code>forward</code> plugin in CoreDNS.</p>
<h3 id=example-1>Example</h3>
<p>This example ConfigMap for kube-dns specifies stubdomains and upstreamnameservers:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>stubDomains</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>{<span style=color:#b44>&#34;abc.com&#34;</span><span style=color:#bbb> </span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;1.2.3.4&#34;</span>],<span style=color:#bbb> </span><span style=color:#b44>&#34;my.cluster.local&#34;</span><span style=color:#bbb> </span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;2.3.4.5&#34;</span>]}<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>upstreamNameservers</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>[<span style=color:#b44>&#34;8.8.8.8&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;8.8.4.4&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></code></pre></div><p>The equivalent configuration in CoreDNS creates a Corefile:</p>
<ul>
<li>For stubDomains:</li>
</ul>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>abc.com:53 {<span style=color:#bbb>
</span><span style=color:#bbb>    </span>errors<span style=color:#bbb>
</span><span style=color:#bbb>    </span>cache 30<span style=color:#bbb>
</span><span style=color:#bbb>    </span>forward . 1.2.3.4<span style=color:#bbb>
</span><span style=color:#bbb></span>}<span style=color:#bbb>
</span><span style=color:#bbb></span>my.cluster.local:53 {<span style=color:#bbb>
</span><span style=color:#bbb>    </span>errors<span style=color:#bbb>
</span><span style=color:#bbb>    </span>cache 30<span style=color:#bbb>
</span><span style=color:#bbb>    </span>forward . 2.3.4.5<span style=color:#bbb>
</span><span style=color:#bbb></span>}<span style=color:#bbb>
</span></code></pre></div><p>The complete Corefile with the default plugins:</p>
<pre><code>.:53 {
    errors
    health
    kubernetes cluster.local in-addr.arpa ip6.arpa {
        pods insecure
        fallthrough in-addr.arpa ip6.arpa
    }
    federation cluster.local {
        foo foo.feddomain.com
    }
    prometheus :9153
    forward . 8.8.8.8 8.8.4.4
    cache 30
}
abc.com:53 {
    errors
    cache 30
    forward . 1.2.3.4
}
my.cluster.local:53 {
    errors
    cache 30
    forward . 2.3.4.5
}
</code></pre><h2 id=migration-to-coredns>Migration to CoreDNS</h2>
<p>To migrate from kube-dns to CoreDNS, a detailed
<a href=https://coredns.io/2018/05/21/migration-from-kube-dns-to-coredns/>blog article</a>
is available to help users adapt CoreDNS in place of kube-dns.</p>
<p>You can also migrate using the official CoreDNS
<a href=https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh>deploy script</a>.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Read <a href=/docs/tasks/administer-cluster/dns-debugging-resolution/>Debugging DNS Resolution</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-8bcf4aeb5bbb6d6969a146e5ab97557b>16 - Debugging DNS Resolution</h1>
<p>This page provides hints on diagnosing DNS problems.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<br>
Your cluster must be configured to use the CoreDNS
<a class=glossary-tooltip title="Resources that extend the functionality of Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=addon>addon</a> or its precursor,
kube-dns.</p>
<p>Your Kubernetes server must be at or later than version v1.6.
To check the version, enter <code>kubectl version</code>.</p>
<h3 id=create-a-simple-pod-to-use-as-a-test-environment>Create a simple Pod to use as a test environment</h3>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/dns/dnsutils.yaml download=admin/dns/dnsutils.yaml><code>admin/dns/dnsutils.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-dns-dnsutils-yaml')" title="Copy admin/dns/dnsutils.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-dns-dnsutils-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dnsutils<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dnsutils<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- sleep<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;3600&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Always<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> This example creates a pod in the <code>default</code> namespace. DNS name resolution for
services depends on the namespace of the pod. For more information, review
<a href=/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names>DNS for Services and Pods</a>.
</div>
<p>Use that manifest to create a Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
</code></pre></div><pre><code>pod/dnsutils created
</code></pre><p>…and verify its status:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods dnsutils
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
dnsutils   1/1       Running   0          &lt;some-time&gt;
</code></pre><p>Once that Pod is running, you can exec <code>nslookup</code> in that environment.
If you see something like the following, DNS is working correctly.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl <span style=color:#a2f>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</code></pre></div><pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      kubernetes.default
Address 1: 10.0.0.1
</code></pre><p>If the <code>nslookup</code> command fails, check the following:</p>
<h3 id=check-the-local-dns-configuration-first>Check the local DNS configuration first</h3>
<p>Take a look inside the resolv.conf file.
(See <a href=/docs/tasks/administer-cluster/dns-custom-nameservers>Customizing DNS Service</a> and
<a href=#known-issues>Known issues</a> below for more information)</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl <span style=color:#a2f>exec</span> -ti dnsutils -- cat /etc/resolv.conf
</code></pre></div><p>Verify that the search path and name server are set up like the following
(note that search path may vary for different cloud providers):</p>
<pre><code>search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
nameserver 10.0.0.10
options ndots:5
</code></pre><p>Errors such as the following indicate a problem with the CoreDNS (or kube-dns)
add-on or with associated Services:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl <span style=color:#a2f>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</code></pre></div><pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes.default'
</code></pre><p>or</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl <span style=color:#a2f>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</code></pre></div><pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'
</code></pre><h3 id=check-if-the-dns-pod-is-running>Check if the DNS pod is running</h3>
<p>Use the <code>kubectl get pods</code> command to verify that the DNS pod is running.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods --namespace<span style=color:#666>=</span>kube-system -l k8s-app<span style=color:#666>=</span>kube-dns
</code></pre></div><pre><code>NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...
</code></pre><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The value for label <code>k8s-app</code> is <code>kube-dns</code> for both CoreDNS and kube-dns deployments.
</div>
<p>If you see that no CoreDNS Pod is running or that the Pod has failed/completed,
the DNS add-on may not be deployed by default in your current environment and you
will have to deploy it manually.</p>
<h3 id=check-for-errors-in-the-dns-pod>Check for errors in the DNS pod</h3>
<p>Use the <code>kubectl logs</code> command to see logs for the DNS containers.</p>
<p>For CoreDNS:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs --namespace<span style=color:#666>=</span>kube-system -l k8s-app<span style=color:#666>=</span>kube-dns
</code></pre></div><p>Here is an example of a healthy CoreDNS log:</p>
<pre><code>.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c
</code></pre><p>See if there are any suspicious or unexpected messages in the logs.</p>
<h3 id=is-dns-service-up>Is DNS service up?</h3>
<p>Verify that the DNS service is up by using the <code>kubectl get service</code> command.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get svc --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><pre><code>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      &lt;none&gt;        53/UDP,53/TCP        1h
...
</code></pre><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The service name is <code>kube-dns</code> for both CoreDNS and kube-dns deployments.
</div>
<p>If you have created the Service or in the case it should be created by default
but it does not appear, see
<a href=/docs/tasks/debug/debug-application/debug-service/>debugging Services</a> for
more information.</p>
<h3 id=are-dns-endpoints-exposed>Are DNS endpoints exposed?</h3>
<p>You can verify that DNS endpoints are exposed by using the <code>kubectl get endpoints</code>
command.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get endpoints kube-dns --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><pre><code>NAME       ENDPOINTS                       AGE
kube-dns   10.180.3.17:53,10.180.3.17:53    1h
</code></pre><p>If you do not see the endpoints, see the endpoints section in the
<a href=/docs/tasks/debug/debug-application/debug-service/>debugging Services</a> documentation.</p>
<p>For additional Kubernetes DNS examples, see the
<a href=https://github.com/kubernetes/examples/tree/master/staging/cluster-dns>cluster-dns examples</a>
in the Kubernetes GitHub repository.</p>
<h3 id=are-dns-queries-being-received-processed>Are DNS queries being received/processed?</h3>
<p>You can verify if queries are being received by CoreDNS by adding the <code>log</code> plugin to the CoreDNS configuration (aka Corefile).
The CoreDNS Corefile is held in a <a class=glossary-tooltip title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a> named <code>coredns</code>. To edit it, use the command:</p>
<pre><code>kubectl -n kube-system edit configmap coredns
</code></pre><p>Then add <code>log</code> in the Corefile section per the example below:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>coredns<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>Corefile</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    .:53 {
</span><span style=color:#b44;font-style:italic>        log
</span><span style=color:#b44;font-style:italic>        errors
</span><span style=color:#b44;font-style:italic>        health
</span><span style=color:#b44;font-style:italic>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style=color:#b44;font-style:italic>          pods insecure
</span><span style=color:#b44;font-style:italic>          upstream
</span><span style=color:#b44;font-style:italic>          fallthrough in-addr.arpa ip6.arpa
</span><span style=color:#b44;font-style:italic>        }
</span><span style=color:#b44;font-style:italic>        prometheus :9153
</span><span style=color:#b44;font-style:italic>        forward . /etc/resolv.conf
</span><span style=color:#b44;font-style:italic>        cache 30
</span><span style=color:#b44;font-style:italic>        loop
</span><span style=color:#b44;font-style:italic>        reload
</span><span style=color:#b44;font-style:italic>        loadbalance
</span><span style=color:#b44;font-style:italic>    }</span><span style=color:#bbb>    
</span></code></pre></div><p>After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods.</p>
<p>Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs.</p>
<p>Here is an example of a query in the log:</p>
<pre><code>.:53
2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0
2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.0
linux/amd64, go1.10.3, 2e322f6
2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f
2018/09/07 15:29:04 [INFO] Reloading complete
172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 &quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd,ra 106 0.000066649s
</code></pre><h3 id=are-you-in-the-right-namespace-for-the-service>Are you in the right namespace for the service?</h3>
<p>DNS queries that don't specify a namespace are limited to the pod's
namespace.</p>
<p>If the namespace of the pod and service differ, the DNS query must include
the namespace of the service.</p>
<p>This query is limited to the pod's namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl <span style=color:#a2f>exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;
</code></pre></div><p>This query specifies the namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl <span style=color:#a2f>exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;.&lt;namespace&gt;
</code></pre></div><p>To learn more about name resolution, see
<a href=/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names>DNS for Services and Pods</a>.</p>
<h2 id=known-issues>Known issues</h2>
<p>Some Linux distributions (e.g. Ubuntu) use a local DNS resolver by default (systemd-resolved).
Systemd-resolved moves and replaces <code>/etc/resolv.conf</code> with a stub file that can cause a fatal forwarding
loop when resolving names in upstream servers. This can be fixed manually by using kubelet's <code>--resolv-conf</code> flag
to point to the correct <code>resolv.conf</code> (With <code>systemd-resolved</code>, this is <code>/run/systemd/resolve/resolv.conf</code>).
kubeadm automatically detects <code>systemd-resolved</code>, and adjusts the kubelet flags accordingly.</p>
<p>Kubernetes installs do not configure the nodes' <code>resolv.conf</code> files to use the
cluster DNS by default, because that process is inherently distribution-specific.
This should probably be implemented eventually.</p>
<p>Linux's libc (a.k.a. glibc) has a limit for the DNS <code>nameserver</code> records to 3 by default. What's more, for the glibc versions which are older than glibc-2.17-222 (<a href=https://access.redhat.com/solutions/58028>the new versions update see this issue</a>), the allowed number of DNS <code>search</code> records has been limited to 6 (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=168253">see this bug from 2005</a>). Kubernetes needs to consume 1 <code>nameserver</code> record and 3 <code>search</code> records. This means that if a local installation already uses 3 <code>nameserver</code>s or uses more than 3 <code>search</code>es while your glibc version is in the affected list, some of those settings will be lost. To work around the DNS <code>nameserver</code> records limit, the node can run <code>dnsmasq</code>, which will provide more <code>nameserver</code> entries. You can also use kubelet's <code>--resolv-conf</code> flag. To fix the DNS <code>search</code> records limit, consider upgrading your linux distribution or upgrading to an unaffected version of glibc.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> With <a href=/docs/concepts/services-networking/dns-pod-service/#expanded-dns-configuration>Expanded DNS Configuration</a>,
Kubernetes allows more DNS <code>search</code> records.
</div>
<p>If you are using Alpine version 3.3 or earlier as your base image, DNS may not
work properly due to a known issue with Alpine.
Kubernetes <a href=https://github.com/kubernetes/kubernetes/issues/30215>issue 30215</a>
details more information on this.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>See <a href=/docs/tasks/administer-cluster/dns-horizontal-autoscaling/>Autoscaling the DNS Service in a Cluster</a>.</li>
<li>Read <a href=/docs/concepts/services-networking/dns-pod-service/>DNS for Services and Pods</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-a3790dfb57271d13517e549dffa805b9>17 - Declare Network Policy</h1>
<p>This document helps you get started using the Kubernetes <a href=/docs/concepts/services-networking/network-policies/>NetworkPolicy API</a> to declare network policies that govern how pods communicate with each other.</p>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
Your Kubernetes server must be at or later than version v1.8.
To check the version, enter <code>kubectl version</code>.
</p>
<p>Make sure you've configured a network provider with network policy support. There are a number of network providers that support NetworkPolicy, including:</p>
<ul>
<li><a href=/docs/tasks/administer-cluster/network-policy-provider/antrea-network-policy/>Antrea</a></li>
<li><a href=/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/>Calico</a></li>
<li><a href=/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/>Cilium</a></li>
<li><a href=/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/>Kube-router</a></li>
<li><a href=/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/>Romana</a></li>
<li><a href=/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/>Weave Net</a></li>
</ul>
<h2 id=create-an-nginx-deployment-and-expose-it-via-a-service>Create an <code>nginx</code> deployment and expose it via a service</h2>
<p>To see how Kubernetes network policy works, start off by creating an <code>nginx</code> Deployment.</p>
<pre><code class=language-console data-lang=console>kubectl create deployment nginx --image=nginx
</code></pre><pre><code class=language-none data-lang=none>deployment.apps/nginx created
</code></pre><p>Expose the Deployment through a Service called <code>nginx</code>.</p>
<pre><code class=language-console data-lang=console>kubectl expose deployment nginx --port=80
</code></pre><pre><code class=language-none data-lang=none>service/nginx exposed
</code></pre><p>The above commands create a Deployment with an nginx Pod and expose the Deployment through a Service named <code>nginx</code>. The <code>nginx</code> Pod and Deployment are found in the <code>default</code> namespace.</p>
<pre><code class=language-console data-lang=console>kubectl get svc,pod
</code></pre><pre><code class=language-none data-lang=none>NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
service/kubernetes          10.100.0.1    &lt;none&gt;        443/TCP    46m
service/nginx               10.100.0.16   &lt;none&gt;        80/TCP     33s

NAME                        READY         STATUS        RESTARTS   AGE
pod/nginx-701339712-e0qfq   1/1           Running       0          35s
</code></pre><h2 id=test-the-service-by-accessing-it-from-another-pod>Test the service by accessing it from another Pod</h2>
<p>You should be able to access the new <code>nginx</code> service from other Pods. To access the <code>nginx</code> Service from another Pod in the <code>default</code> namespace, start a busybox container:</p>
<pre><code class=language-console data-lang=console>kubectl run busybox --rm -ti --image=busybox:1.28 -- /bin/sh
</code></pre><p>In your shell, run the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>wget --spider --timeout<span style=color:#666>=</span><span style=color:#666>1</span> nginx
</code></pre></div><pre><code class=language-none data-lang=none>Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre><h2 id=limit-access-to-the-nginx-service>Limit access to the <code>nginx</code> service</h2>
<p>To limit the access to the <code>nginx</code> service so that only Pods with the label <code>access: true</code> can query it, create a NetworkPolicy object as follows:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/service/networking/nginx-policy.yaml download=service/networking/nginx-policy.yaml><code>service/networking/nginx-policy.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('service-networking-nginx-policy-yaml')" title="Copy service/networking/nginx-policy.yaml to clipboard">
</img>
</div>
<div class=includecode id=service-networking-nginx-policy-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>access-nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>access</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>The name of a NetworkPolicy object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> NetworkPolicy includes a <code>podSelector</code> which selects the grouping of Pods to which the policy applies. You can see this policy selects Pods with the label <code>app=nginx</code>. The label was automatically added to the Pod in the <code>nginx</code> Deployment. An empty <code>podSelector</code> selects all pods in the namespace.
</div>
<h2 id=assign-the-policy-to-the-service>Assign the policy to the service</h2>
<p>Use kubectl to create a NetworkPolicy from the above <code>nginx-policy.yaml</code> file:</p>
<pre><code class=language-console data-lang=console>kubectl apply -f https://k8s.io/examples/service/networking/nginx-policy.yaml
</code></pre><pre><code class=language-none data-lang=none>networkpolicy.networking.k8s.io/access-nginx created
</code></pre><h2 id=test-access-to-the-service-when-access-label-is-not-defined>Test access to the service when access label is not defined</h2>
<p>When you attempt to access the <code>nginx</code> Service from a Pod without the correct labels, the request times out:</p>
<pre><code class=language-console data-lang=console>kubectl run busybox --rm -ti --image=busybox:1.28 -- /bin/sh
</code></pre><p>In your shell, run the command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>wget --spider --timeout<span style=color:#666>=</span><span style=color:#666>1</span> nginx
</code></pre></div><pre><code class=language-none data-lang=none>Connecting to nginx (10.100.0.16:80)
wget: download timed out
</code></pre><h2 id=define-access-label-and-test-again>Define access label and test again</h2>
<p>You can create a Pod with the correct labels to see that the request is allowed:</p>
<pre><code class=language-console data-lang=console>kubectl run busybox --rm -ti --labels=&quot;access=true&quot; --image=busybox:1.28 -- /bin/sh
</code></pre><p>In your shell, run the command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>wget --spider --timeout<span style=color:#666>=</span><span style=color:#666>1</span> nginx
</code></pre></div><pre><code class=language-none data-lang=none>Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-9585dc0efb0450fd68728e7511754717>18 - Developing Cloud Controller Manager</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>
<p><p>The cloud-controller-manager is a Kubernetes <a class=glossary-tooltip title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.</p></p>
<p>By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.</p>
<h2 id=background>Background</h2>
<p>Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the <code>cloud-controller-manager</code> binary allows cloud vendors to evolve independently from the core Kubernetes code.</p>
<p>The Kubernetes project provides skeleton cloud-controller-manager code with Go interfaces to allow you (or your cloud provider) to plug in your own implementations. This means that a cloud provider can implement a cloud-controller-manager by importing packages from Kubernetes core; each cloudprovider will register their own code by calling <code>cloudprovider.RegisterCloudProvider</code> to update a global variable of available cloud providers.</p>
<h2 id=developing>Developing</h2>
<h3 id=out-of-tree>Out of tree</h3>
<p>To build an out-of-tree cloud-controller-manager for your cloud:</p>
<ol>
<li>Create a go package with an implementation that satisfies <a href=https://github.com/kubernetes/cloud-provider/blob/master/cloud.go>cloudprovider.Interface</a>.</li>
<li>Use <a href=https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/main.go><code>main.go</code> in cloud-controller-manager</a> from Kubernetes core as a template for your <code>main.go</code>. As mentioned above, the only difference should be the cloud package that will be imported.</li>
<li>Import your cloud package in <code>main.go</code>, ensure your package has an <code>init</code> block to run <a href=https://github.com/kubernetes/cloud-provider/blob/master/plugins.go><code>cloudprovider.RegisterCloudProvider</code></a>.</li>
</ol>
<p>Many cloud providers publish their controller manager code as open source. If you are creating
a new cloud-controller-manager from scratch, you could take an existing out-of-tree cloud
controller manager as your starting point.</p>
<h3 id=in-tree>In tree</h3>
<p>For in-tree cloud providers, you can run the in-tree cloud controller manager as a <a class=glossary-tooltip title="Ensures a copy of a Pod is running across a set of nodes in a cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a> in your cluster. See <a href=/docs/tasks/administer-cluster/running-cloud-controller/>Cloud Controller Manager Administration</a> for more details.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-09cc2cf3e0f23a3996e6cb31dc4d867c>19 - Enable Or Disable A Kubernetes API</h1>
<p>This page shows how to enable or disable an API version from your cluster's
<a class=glossary-tooltip title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a>.</p>
<p>Specific API versions can be turned on or off by passing <code>--runtime-config=api/&lt;version></code> as a
command line argument to the API server. The values for this argument are a comma-separated
list of API versions. Later values override earlier values.</p>
<p>The <code>runtime-config</code> command line argument also supports 2 special keys:</p>
<ul>
<li><code>api/all</code>, representing all known APIs</li>
<li><code>api/legacy</code>, representing only legacy APIs. Legacy APIs are any APIs that have been
explicitly <a href=/docs/reference/using-api/deprecation-policy/>deprecated</a>.</li>
</ul>
<p>For example, to turning off all API versions except v1, pass <code>--runtime-config=api/all=false,api/v1=true</code>
to the <code>kube-apiserver</code>.</p>
<h2 id=what-s-next>What's next</h2>
<p>Read the <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>full documentation</a>
for the <code>kube-apiserver</code> component.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-00733cc3747eb3f5fe1c9e0439262967>20 - Enabling Service Topology</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>
<p>This feature, specifically the alpha <code>topologyKeys</code> field, is deprecated since
Kubernetes v1.21.
<a href=/docs/concepts/services-networking/topology-aware-hints/>Topology Aware Hints</a>,
introduced in Kubernetes v1.21, provide similar functionality.</p>
<p><em>Service Topology</em> enables a <a class=glossary-tooltip title="A way to expose an application running on a set of Pods as a network service." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> to route traffic based upon the Node
topology of the cluster. For example, a service can specify that traffic be
preferentially routed to endpoints that are on the same Node as the client, or
in the same availability zone.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
Your Kubernetes server must be at or later than version 1.17.
To check the version, enter <code>kubectl version</code>.
</p>
<p>The following prerequisites are needed in order to enable topology aware service
routing:</p>
<ul>
<li>Kubernetes v1.17 or later</li>
<li>Configure <a class=glossary-tooltip title="kube-proxy is a network proxy that runs on each node in the cluster." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a> to run in iptables mode or IPVS mode</li>
</ul>
<h2 id=enable-service-topology>Enable Service Topology</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>
<p>To enable service topology, enable the <code>ServiceTopology</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> for all Kubernetes components:</p>
<pre><code>--feature-gates=&quot;ServiceTopology=true`
</code></pre><h2 id=what-s-next>What's next</h2>
<ul>
<li>Read about <a href=/docs/concepts/services-networking/topology-aware-hints/>Topology Aware Hints</a>, the replacement for the <code>topologyKeys</code> field.</li>
<li>Read about <a href=/docs/concepts/services-networking/endpoint-slices/>EndpointSlices</a></li>
<li>Read about the <a href=/docs/concepts/services-networking/service-topology/>Service Topology</a> concept</li>
<li>Read <a href=/docs/concepts/services-networking/connect-applications-service/>Connecting Applications with Services</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-6b4e7ca6586f448c8533a120c29bdd25>21 - Encrypting Secret Data at Rest</h1>
<p>This page shows how to enable and configure encryption of secret data at rest.</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
Your Kubernetes server must be at or later than version 1.13.
To check the version, enter <code>kubectl version</code>.
</p>
</li>
<li>
<p>etcd v3.0 or later is required</p>
</li>
</ul>
<h2 id=configuration-and-determining-whether-encryption-at-rest-is-already-enabled>Configuration and determining whether encryption at rest is already enabled</h2>
<p>The <code>kube-apiserver</code> process accepts an argument <code>--encryption-provider-config</code>
that controls how API data is encrypted in etcd.
The configuration is provided as an API named
<a href=/docs/reference/config-api/apiserver-encryption.v1/><code>EncryptionConfiguration</code></a>.
An example configuration is provided below.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> <strong>IMPORTANT:</strong> For high-availability configurations (with two or more control plane nodes), the
encryption configuration file must be the same! Otherwise, the <code>kube-apiserver</code> component cannot
decrypt data stored in the etcd.
</div>
<h2 id=understanding-the-encryption-at-rest-configuration>Understanding the encryption at rest configuration.</h2>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EncryptionConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- secrets<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>providers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>identity</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>aesgcm</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>keys</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>key1<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>key2<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb> </span>dGhpcyBpcyBwYXNzd29yZA==<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>aescbc</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>keys</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>key1<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>key2<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb> </span>dGhpcyBpcyBwYXNzd29yZA==<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>secretbox</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>keys</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>key1<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb> </span>YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=<span style=color:#bbb>
</span></code></pre></div><p>Each <code>resources</code> array item is a separate config and contains a complete configuration. The
<code>resources.resources</code> field is an array of Kubernetes resource names (<code>resource</code> or <code>resource.group</code>)
that should be encrypted. The <code>providers</code> array is an ordered list of the possible encryption
providers.</p>
<p>Only one provider type may be specified per entry (<code>identity</code> or <code>aescbc</code> may be provided,
but not both in the same item).
The first provider in the list is used to encrypt resources written into the storage. When reading
resources from storage, each provider that matches the stored data attempts in order to decrypt the
data. If no provider can read the stored data due to a mismatch in format or secret key, an error
is returned which prevents clients from accessing that resource.</p>
<p>For more detailed information about the <code>EncryptionConfiguration</code> struct, please refer to the
<a href=/docs/reference/config-api/apiserver-encryption.v1/>encryption configuration API</a>.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> If any resource is not readable via the encryption config (because keys were changed),
the only recourse is to delete that key from the underlying etcd directly. Calls that attempt to
read that resource will fail until it is deleted or a valid decryption key is provided.
</div>
<h3 id=providers>Providers:</h3>
<table><caption style=display:none>Providers for Kubernetes encryption at rest</caption>
<thead>
<tr>
<th>Name</th>
<th>Encryption</th>
<th>Strength</th>
<th>Speed</th>
<th>Key Length</th>
<th>Other Considerations</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>identity</code></td>
<td>None</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>Resources written as-is without encryption. When set as the first provider, the resource will be decrypted as new values are written.</td>
</tr>
<tr>
<td><code>secretbox</code></td>
<td>XSalsa20 and Poly1305</td>
<td>Strong</td>
<td>Faster</td>
<td>32-byte</td>
<td>A newer standard and may not be considered acceptable in environments that require high levels of review.</td>
</tr>
<tr>
<td><code>aesgcm</code></td>
<td>AES-GCM with random nonce</td>
<td>Must be rotated every 200k writes</td>
<td>Fastest</td>
<td>16, 24, or 32-byte</td>
<td>Is not recommended for use except when an automated key rotation scheme is implemented.</td>
</tr>
<tr>
<td><code>aescbc</code></td>
<td>AES-CBC with PKCS#7 padding</td>
<td>Weak</td>
<td>Fast</td>
<td>32-byte</td>
<td>Not recommended due to CBC's vulnerability to padding oracle attacks.</td>
</tr>
<tr>
<td><code>kms</code></td>
<td>Uses envelope encryption scheme: Data is encrypted by data encryption keys (DEKs) using AES-CBC with PKCS#7 padding, DEKs are encrypted by key encryption keys (KEKs) according to configuration in Key Management Service (KMS)</td>
<td>Strongest</td>
<td>Fast</td>
<td>32-bytes</td>
<td>The recommended choice for using a third party tool for key management. Simplifies key rotation, with a new DEK generated for each encryption, and KEK rotation controlled by the user. <a href=/docs/tasks/administer-cluster/kms-provider/>Configure the KMS provider</a></td>
</tr>
</tbody>
</table>
<p>Each provider supports multiple keys - the keys are tried in order for decryption, and if the provider
is the first provider, the first key is used for encryption.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> Storing the raw encryption key in the EncryptionConfig only moderately improves your security
posture, compared to no encryption. Please use <code>kms</code> provider for additional security.
</div>
<p>By default, the <code>identity</code> provider is used to protect Secrets in etcd, which provides no
encryption. <code>EncryptionConfiguration</code> was introduced to encrypt Secrets locally, with a locally
managed key.</p>
<p>Encrypting Secrets with a locally managed key protects against an etcd compromise, but it fails to
protect against a host compromise. Since the encryption keys are stored on the host in the
EncryptionConfiguration YAML file, a skilled attacker can access that file and extract the encryption
keys.</p>
<p>Envelope encryption creates dependence on a separate key, not stored in Kubernetes. In this case,
an attacker would need to compromise etcd, the <code>kubeapi-server</code>, and the third-party KMS provider to
retrieve the plaintext values, providing a higher level of security than locally stored encryption keys.</p>
<h2 id=encrypting-your-data>Encrypting your data</h2>
<p>Create a new encryption config file:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EncryptionConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- secrets<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>providers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>aescbc</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>keys</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>key1<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>identity</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></code></pre></div><p>To create a new Secret, perform the following steps:</p>
<ol>
<li>
<p>Generate a 32-byte random key and base64 encode it. If you're on Linux or macOS, run the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>head -c <span style=color:#666>32</span> /dev/urandom | base64
</code></pre></div></li>
<li>
<p>Place that value in the <code>secret</code> field of the <code>EncryptionConfiguration</code> struct.</p>
</li>
<li>
<p>Set the <code>--encryption-provider-config</code> flag on the <code>kube-apiserver</code> to point to
the location of the config file.</p>
</li>
<li>
<p>Restart your API server.</p>
</li>
</ol>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> Your config file contains keys that can decrypt the contents in etcd, so you must properly restrict
permissions on your control-plane nodes so only the user who runs the <code>kube-apiserver</code> can read it.
</div>
<h2 id=verifying-that-data-is-encrypted>Verifying that data is encrypted</h2>
<p>Data is encrypted when written to etcd. After restarting your <code>kube-apiserver</code>, any newly created or
updated Secret should be encrypted when stored. To check this, you can use the <code>etcdctl</code> command line
program to retrieve the contents of your Secret.</p>
<ol>
<li>
<p>Create a new Secret called <code>secret1</code> in the <code>default</code> namespace:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret generic secret1 -n default --from-literal<span style=color:#666>=</span><span style=color:#b8860b>mykey</span><span style=color:#666>=</span>mydata
</code></pre></div></li>
<li>
<p>Using the <code>etcdctl</code> command line, read that Secret out of etcd:</p>
<p><code>ETCDCTL_API=3 etcdctl get /registry/secrets/default/secret1 [...] | hexdump -C</code></p>
<p>where <code>[...]</code> must be the additional arguments for connecting to the etcd server.</p>
</li>
<li>
<p>Verify the stored Secret is prefixed with <code>k8s:enc:aescbc:v1:</code> which indicates
the <code>aescbc</code> provider has encrypted the resulting data.</p>
</li>
<li>
<p>Verify the Secret is correctly decrypted when retrieved via the API:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe secret secret1 -n default
</code></pre></div><p>The output should contain <code>mykey: bXlkYXRh</code>, with contents of <code>mydata</code> encoded, check
<a href=/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret>decoding a Secret</a>
to completely decode the Secret.</p>
</li>
</ol>
<h2 id=ensure-all-secrets-are-encrypted>Ensure all Secrets are encrypted</h2>
<p>Since Secrets are encrypted on write, performing an update on a Secret will encrypt that content.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre></div><p>The command above reads all Secrets and then updates them to apply server side encryption.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If an error occurs due to a conflicting write, retry the command.
For larger clusters, you may wish to subdivide the secrets by namespace or script an update.
</div>
<h2 id=rotating-a-decryption-key>Rotating a decryption key</h2>
<p>Changing a Secret without incurring downtime requires a multi-step operation, especially in
the presence of a highly-available deployment where multiple <code>kube-apiserver</code> processes are running.</p>
<ol>
<li>Generate a new key and add it as the second key entry for the current provider on all servers</li>
<li>Restart all <code>kube-apiserver</code> processes to ensure each server can decrypt using the new key</li>
<li>Make the new key the first entry in the <code>keys</code> array so that it is used for encryption in the config</li>
<li>Restart all <code>kube-apiserver</code> processes to ensure each server now encrypts using the new key</li>
<li>Run <code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -</code> to encrypt all
existing Secrets with the new key</li>
<li>Remove the old decryption key from the config after you have backed up etcd with the new key in use
and updated all Secrets</li>
</ol>
<p>When running a single <code>kube-apiserver</code> instance, step 2 may be skipped.</p>
<h2 id=decrypting-all-data>Decrypting all data</h2>
<p>To disable encryption at rest, place the <code>identity</code> provider as the first entry in the config
and restart all <code>kube-apiserver</code> processes.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EncryptionConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- secrets<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>providers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>identity</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>aescbc</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>keys</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>key1<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style=color:#bbb>
</span></code></pre></div><p>Then run the following command to force decrypt
all Secrets:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre></div><h2 id=what-s-next>What's next</h2>
<ul>
<li>Learn more about the <a href=/docs/reference/config-api/apiserver-encryption.v1/>EncryptionConfiguration configuration API (v1)</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-4a02bcca41439e16655f43fa37c81da4>22 - Guaranteed Scheduling For Critical Add-On Pods</h1>
<p>Kubernetes core components such as the API server, scheduler, and controller-manager run on a control plane node. However, add-ons must run on a regular cluster node.
Some of these add-ons are critical to a fully functional cluster, such as metrics-server, DNS, and UI.
A cluster may stop working properly if a critical add-on is evicted (either manually or as a side effect of another operation like upgrade)
and becomes pending (for example when the cluster is highly utilized and either there are other pending pods that schedule into the space
vacated by the evicted critical add-on pod or the amount of resources available on the node changed for some other reason).</p>
<p>Note that marking a pod as critical is not meant to prevent evictions entirely; it only prevents the pod from becoming permanently unavailable.
A static pod marked as critical, can't be evicted. However, a non-static pods marked as critical are always rescheduled.</p>
<h3 id=marking-pod-as-critical>Marking pod as critical</h3>
<p>To mark a Pod as critical, set priorityClassName for that Pod to <code>system-cluster-critical</code> or <code>system-node-critical</code>. <code>system-node-critical</code> is the highest available priority, even higher than <code>system-cluster-critical</code>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-b45f024608e1b367cdacb1fd9d77278a>23 - IP Masquerade Agent User Guide</h1>
<p>This page shows how to configure and enable the <code>ip-masq-agent</code>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=ip-masquerade-agent-user-guide>IP Masquerade Agent User Guide</h2>
<p>The <code>ip-masq-agent</code> configures iptables rules to hide a pod's IP address behind the cluster node's IP address. This is typically done when sending traffic to destinations outside the cluster's pod <a href=https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing>CIDR</a> range.</p>
<h3 id=key-terms><strong>Key Terms</strong></h3>
<ul>
<li><strong>NAT (Network Address Translation)</strong>
Is a method of remapping one IP address to another by modifying either the source and/or destination address information in the IP header. Typically performed by a device doing IP routing.</li>
<li><strong>Masquerading</strong>
A form of NAT that is typically used to perform a many to one address translation, where multiple source IP addresses are masked behind a single address, which is typically the device doing the IP routing. In Kubernetes this is the Node's IP address.</li>
<li><strong>CIDR (Classless Inter-Domain Routing)</strong>
Based on the variable-length subnet masking, allows specifying arbitrary-length prefixes. CIDR introduced a new method of representation for IP addresses, now commonly known as <strong>CIDR notation</strong>, in which an address or routing prefix is written with a suffix indicating the number of bits of the prefix, such as 192.168.2.0/24.</li>
<li><strong>Link Local</strong>
A link-local address is a network address that is valid only for communications within the network segment or the broadcast domain that the host is connected to. Link-local addresses for IPv4 are defined in the address block 169.254.0.0/16 in CIDR notation.</li>
</ul>
<p>The ip-masq-agent configures iptables rules to handle masquerading node/pod IP addresses when sending traffic to destinations outside the cluster node's IP and the Cluster IP range. This essentially hides pod IP addresses behind the cluster node's IP address. In some environments, traffic to "external" addresses must come from a known machine address. For example, in Google Cloud, any traffic to the internet must come from a VM's IP. When containers are used, as in Google Kubernetes Engine, the Pod IP will be rejected for egress. To avoid this, we must hide the Pod IP behind the VM's own IP address - generally known as "masquerade". By default, the agent is configured to treat the three private IP ranges specified by <a href=https://tools.ietf.org/html/rfc1918>RFC 1918</a> as non-masquerade <a href=https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing>CIDR</a>. These ranges are 10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16. The agent will also treat link-local (169.254.0.0/16) as a non-masquerade CIDR by default. The agent is configured to reload its configuration from the location <em>/etc/config/ip-masq-agent</em> every 60 seconds, which is also configurable.</p>
<p><img src=/images/docs/ip-masq.png alt="masq/non-masq example"></p>
<p>The agent configuration file must be written in YAML or JSON syntax, and may contain three optional keys:</p>
<ul>
<li><code>nonMasqueradeCIDRs</code>: A list of strings in
<a href=https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing>CIDR</a> notation that specify the non-masquerade ranges.</li>
<li><code>masqLinkLocal</code>: A Boolean (true/false) which indicates whether to masquerade traffic to the
link local prefix <code>169.254.0.0/16</code>. False by default.</li>
<li><code>resyncInterval</code>: A time interval at which the agent attempts to reload config from disk.
For example: '30s', where 's' means seconds, 'ms' means milliseconds.</li>
</ul>
<p>Traffic to 10.0.0.0/8, 172.16.0.0/12 and 192.168.0.0/16) ranges will NOT be masqueraded. Any other traffic (assumed to be internet) will be masqueraded. An example of a local destination from a pod could be its Node's IP address as well as another node's address or one of the IP addresses in Cluster's IP range. Any other traffic will be masqueraded by default. The below entries show the default set of rules that are applied by the ip-masq-agent:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>iptables -t nat -L IP-MASQ-AGENT
</code></pre></div><pre><code class=language-none data-lang=none>RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL

</code></pre><p>By default, in GCE/Google Kubernetes Engine, if network policy is enabled or
you are using a cluster CIDR not in the 10.0.0.0/8 range, the <code>ip-masq-agent</code>
will run in your cluster. If you are running in another environment,
you can add the <code>ip-masq-agent</code> <a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a>
to your cluster.</p>
<h2 id=create-an-ip-masq-agent>Create an ip-masq-agent</h2>
<p>To create an ip-masq-agent, run the following kubectl command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</code></pre></div><p>You must also apply the appropriate node label to any nodes in your cluster that you want the agent to run on.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl label nodes my-node node.kubernetes.io/masq-agent-ds-ready<span style=color:#666>=</span><span style=color:#a2f>true</span>
</code></pre></div><p>More information can be found in the ip-masq-agent documentation <a href=https://github.com/kubernetes-sigs/ip-masq-agent>here</a></p>
<p>In most cases, the default set of rules should be sufficient; however, if this is not the case for your cluster, you can create and apply a <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a> to customize the IP ranges that are affected. For example, to allow only 10.0.0.0/8 to be considered by the ip-masq-agent, you can create the following <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a> in a file called "config".</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>It is important that the file is called config since, by default, that will be used as the key for lookup by the <code>ip-masq-agent</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>nonMasqueradeCIDRs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#666>10.0.0.0</span>/8<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resyncInterval</span>:<span style=color:#bbb> </span>60s<span style=color:#bbb>
</span></code></pre></div>
</div>
<p>Run the following command to add the config map to your cluster:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create configmap ip-masq-agent --from-file<span style=color:#666>=</span>config --namespace<span style=color:#666>=</span>kube-system
</code></pre></div><p>This will update a file located at <code>/etc/config/ip-masq-agent</code> which is periodically checked every <code>resyncInterval</code> and applied to the cluster node.
After the resync interval has expired, you should see the iptables rules reflect your changes:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>iptables -t nat -L IP-MASQ-AGENT
</code></pre></div><pre><code class=language-none data-lang=none>Chain IP-MASQ-AGENT (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre><p>By default, the link local range (169.254.0.0/16) is also handled by the ip-masq agent, which sets up the appropriate iptables rules. To have the ip-masq-agent ignore link local, you can set <code>masqLinkLocal</code> to true in the ConfigMap.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>nonMasqueradeCIDRs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#666>10.0.0.0</span>/8<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resyncInterval</span>:<span style=color:#bbb> </span>60s<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>masqLinkLocal</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></code></pre></div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-a02f35804917d7a269c38d7e2c475005>24 - Limit Storage Consumption</h1>
<p>This example demonstrates how to limit the amount of storage consumed in a namespace.</p>
<p>The following resources are used in the demonstration: <a href=/docs/concepts/policy/resource-quotas/>ResourceQuota</a>,
<a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>LimitRange</a>,
and <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumeClaim</a>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</li>
</ul>
<h2 id=scenario-limiting-storage-consumption>Scenario: Limiting Storage Consumption</h2>
<p>The cluster-admin is operating a cluster on behalf of a user population and the admin wants to control
how much storage a single namespace can consume in order to control cost.</p>
<p>The admin would like to limit:</p>
<ol>
<li>The number of persistent volume claims in a namespace</li>
<li>The amount of storage each claim can request</li>
<li>The amount of cumulative storage the namespace can have</li>
</ol>
<h2 id=limitrange-to-limit-requests-for-storage>LimitRange to limit requests for storage</h2>
<p>Adding a <code>LimitRange</code> to a namespace enforces storage request sizes to a minimum and maximum. Storage is requested
via <code>PersistentVolumeClaim</code>. The admission controller that enforces limit ranges will reject any PVC that is above or below
the values set by the admin.</p>
<p>In this example, a PVC requesting 10Gi of storage would be rejected because it exceeds the 2Gi max.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LimitRange<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>storagelimits<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>2Gi<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>min</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></code></pre></div><p>Minimum storage requests are used when the underlying storage provider requires certain minimums. For example,
AWS EBS volumes have a 1Gi minimum requirement.</p>
<h2 id=storagequota-to-limit-pvc-count-and-cumulative-storage-capacity>StorageQuota to limit PVC count and cumulative storage capacity</h2>
<p>Admins can limit the number of PVCs in a namespace as well as the cumulative capacity of those PVCs. New PVCs that exceed
either maximum value will be rejected.</p>
<p>In this example, a 6th PVC in the namespace would be rejected because it exceeds the maximum count of 5. Alternatively,
a 5Gi maximum quota when combined with the 2Gi max limit above, cannot have 3 PVCs where each has 2Gi. That would be 6Gi requested
for a namespace capped at 5Gi.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>storagequota<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>persistentvolumeclaims</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests.storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5Gi&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<h2 id=summary>Summary</h2>
<p>A limit range can put a ceiling on how much storage is requested while a resource quota can effectively cap the storage
consumed by a namespace through claim counts and cumulative storage capacity. The allows a cluster-admin to plan their
cluster's storage budget without risk of any one project going over their allotment.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-a24171610b6ea75a142cb9c8c7882390>25 - Migrate Replicated Control Plane To Use Cloud Controller Manager</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>
<p><p>The cloud-controller-manager is a Kubernetes <a class=glossary-tooltip title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.</p></p>
<p>By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.</p>
<h2 id=background>Background</h2>
<p>As part of the <a href=https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/>cloud provider extraction effort</a>, all cloud specific controllers must be moved out of the <code>kube-controller-manager</code>. All existing clusters that run cloud controllers in the <code>kube-controller-manager</code> must migrate to instead run the controllers in a cloud provider specific <code>cloud-controller-manager</code>.</p>
<p>Leader Migration provides a mechanism in which HA clusters can safely migrate "cloud specific" controllers between the <code>kube-controller-manager</code> and the <code>cloud-controller-manager</code> via a shared resource lock between the two components while upgrading the replicated control plane. For a single-node control plane, or if unavailability of controller managers can be tolerated during the upgrade, Leader Migration is not needed and this guide can be ignored.</p>
<p>Leader Migration can be enabled by setting <code>--enable-leader-migration</code> on <code>kube-controller-manager</code> or <code>cloud-controller-manager</code>. Leader Migration only applies during the upgrade and can be safely disabled or left enabled after the upgrade is complete.</p>
<p>This guide walks you through the manual process of upgrading the control plane from <code>kube-controller-manager</code> with built-in cloud provider to running both <code>kube-controller-manager</code> and <code>cloud-controller-manager</code>. If you use a tool to administrator the cluster, please refer to the documentation of the tool and the cloud provider for more details.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>It is assumed that the control plane is running Kubernetes version N and to be upgraded to version N + 1. Although it is possible to migrate within the same version, ideally the migration should be performed as part of an upgrade so that changes of configuration can be aligned to each release. The exact versions of N and N + 1 depend on each cloud provider. For example, if a cloud provider builds a <code>cloud-controller-manager</code> to work with Kubernetes 1.22, then N can be 1.21 and N + 1 can be 1.22.</p>
<p>The control plane nodes should run <code>kube-controller-manager</code> with Leader Election enabled through <code>--leader-elect=true</code>. As of version N, an in-tree cloud privider must be set with <code>--cloud-provider</code> flag and <code>cloud-controller-manager</code> should not yet be deployed.</p>
<p>The out-of-tree cloud provider must have built a <code>cloud-controller-manager</code> with Leader Migration implementation. If the cloud provider imports <code>k8s.io/cloud-provider</code> and <code>k8s.io/controller-manager</code> of version v0.21.0 or later, Leader Migration will be available. However, for version before v0.22.0, Leader Migration is alpha and requires feature gate <code>ControllerManagerLeaderMigration</code> to be enabled.</p>
<p>This guide assumes that kubelet of each control plane node starts <code>kube-controller-manager</code> and <code>cloud-controller-manager</code> as static pods defined by their manifests. If the components run in a different setting, please adjust the steps accordingly.</p>
<p>For authorization, this guide assumes that the cluster uses RBAC. If another authorization mode grants permissions to <code>kube-controller-manager</code> and <code>cloud-controller-manager</code> components, please grant the needed access in a way that matches the mode.</p>
<h3 id=grant-access-to-migration-lease>Grant access to Migration Lease</h3>
<p>The default permissions of the controller manager allow only accesses to their main Lease. In order for the migration to work, accesses to another Lease are required.</p>
<p>You can grant <code>kube-controller-manager</code> full access to the leases API by modifying the <code>system::leader-locking-kube-controller-manager</code> role. This task guide assumes that the name of the migration lease is <code>cloud-provider-extraction-migration</code>.</p>
<p><code>kubectl patch -n kube-system role 'system::leader-locking-kube-controller-manager' -p '{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}' --type=merge</code></p>
<p>Do the same to the <code>system::leader-locking-cloud-controller-manager</code> role.</p>
<p><code>kubectl patch -n kube-system role 'system::leader-locking-cloud-controller-manager' -p '{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}' --type=merge</code></p>
<h3 id=initial-leader-migration-configuration>Initial Leader Migration configuration</h3>
<p>Leader Migration optionally takes a configuration file representing the state of controller-to-manager assignment. At this moment, with in-tree cloud provider, <code>kube-controller-manager</code> runs <code>route</code>, <code>service</code>, and <code>cloud-node-lifecycle</code>. The following example configuration shows the assignment.</p>
<p>Leader Migration can be enabled without a configuration. Please see <a href=#default-configuration>Default Configuration</a> for details.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LeaderMigrationConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>controllermanager.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>leaderName</span>:<span style=color:#bbb> </span>cloud-provider-extraction-migration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resourceLock</span>:<span style=color:#bbb> </span>leases<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerLeaders</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>route<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>kube-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>kube-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-node-lifecycle<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>kube-controller-manager<span style=color:#bbb>
</span></code></pre></div><p>On each control plane node, save the content to <code>/etc/leadermigration.conf</code>, and update the manifest of <code>kube-controller-manager</code> so that the file is mounted inside the container at the same location. Also, update the same manifest to add the following arguments:</p>
<ul>
<li><code>--enable-leader-migration</code> to enable Leader Migration on the controller manager</li>
<li><code>--leader-migration-config=/etc/leadermigration.conf</code> to set configuration file</li>
</ul>
<p>Restart <code>kube-controller-manager</code> on each node. At this moment, <code>kube-controller-manager</code> has leader migration enabled and is ready for the migration.</p>
<h3 id=deploy-cloud-controller-manager>Deploy Cloud Controller Manager</h3>
<p>In version N + 1, the desired state of controller-to-manager assignment can be represented by a new configuration file, shown as follows. Please note <code>component</code> field of each <code>controllerLeaders</code> changing from <code>kube-controller-manager</code> to <code>cloud-controller-manager</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LeaderMigrationConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>controllermanager.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>leaderName</span>:<span style=color:#bbb> </span>cloud-provider-extraction-migration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resourceLock</span>:<span style=color:#bbb> </span>leases<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerLeaders</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>route<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-node-lifecycle<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span></code></pre></div><p>When creating control plane nodes of version N + 1, the content should be deploy to <code>/etc/leadermigration.conf</code>. The manifest of <code>cloud-controller-manager</code> should be updated to mount the configuration file in the same manner as <code>kube-controller-manager</code> of version N. Similarly, add <code>--feature-gates=ControllerManagerLeaderMigration=true</code>,<code>--enable-leader-migration</code>, and <code>--leader-migration-config=/etc/leadermigration.conf</code> to the arguments of <code>cloud-controller-manager</code>.</p>
<p>Create a new control plane node of version N + 1 with the updated <code>cloud-controller-manager</code> manifest, and with the <code>--cloud-provider</code> flag unset for <code>kube-controller-manager</code>. <code>kube-controller-manager</code> of version N + 1 MUST NOT have Leader Migration enabled because, with an external cloud provider, it does not run the migrated controllers anymore and thus it is not involved in the migration.</p>
<p>Please refer to <a href=/docs/tasks/administer-cluster/running-cloud-controller/>Cloud Controller Manager Administration</a> for more detail on how to deploy <code>cloud-controller-manager</code>.</p>
<h3 id=upgrade-control-plane>Upgrade Control Plane</h3>
<p>The control plane now contains nodes of both version N and N + 1. The nodes of version N run <code>kube-controller-manager</code> only, and these of version N + 1 run both <code>kube-controller-manager</code> and <code>cloud-controller-manager</code>. The migrated controllers, as specified in the configuration, are running under either <code>kube-controller-manager</code> of version N or <code>cloud-controller-manager</code> of version N + 1 depending on which controller manager holds the migration lease. No controller will ever be running under both controller managers at any time.</p>
<p>In a rolling manner, create a new control plane node of version N + 1 and bring down one of version N + 1 until the control plane contains only nodes of version N + 1.
If a rollback from version N + 1 to N is required, add nodes of version N with Leader Migration enabled for <code>kube-controller-manager</code> back to the control plane, replacing one of version N + 1 each time until there are only nodes of version N.</p>
<h3 id=disable-leader-migration>(Optional) Disable Leader Migration</h3>
<p>Now that the control plane has been upgraded to run both <code>kube-controller-manager</code> and <code>cloud-controller-manager</code> of version N + 1, Leader Migration has finished its job and can be safely disabled to save one Lease resource. It is safe to re-enable Leader Migration for the rollback in the future.</p>
<p>In a rolling manager, update manifest of <code>cloud-controller-manager</code> to unset both <code>--enable-leader-migration</code> and <code>--leader-migration-config=</code> flag, also remove the mount of <code>/etc/leadermigration.conf</code>, and finally remove <code>/etc/leadermigration.conf</code>. To re-enable Leader Migration, recreate the configuration file and add its mount and the flags that enable Leader Migration back to <code>cloud-controller-manager</code>.</p>
<h3 id=default-configuration>Default Configuration</h3>
<p>Starting Kubernetes 1.22, Leader Migration provides a default configuration suitable for the default controller-to-manager assignment.
The default configuration can be enabled by setting <code>--enable-leader-migration</code> but without <code>--leader-migration-config=</code>.</p>
<p>For <code>kube-controller-manager</code> and <code>cloud-controller-manager</code>, if there are no flags that enable any in-tree cloud provider or change ownership of controllers, the default configuration can be used to avoid manual creation of the configuration file.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Read the <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2436-controller-manager-leader-migration>Controller Manager Leader Migration</a> enhancement proposal</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-56de8c25b1486599777034111645b803>26 - Namespaces Walkthrough</h1>
<p>Kubernetes <a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespaces>namespaces</a>
help different projects, teams, or customers to share a Kubernetes cluster.</p>
<p>It does this by providing the following:</p>
<ol>
<li>A scope for <a href=/docs/concepts/overview/working-with-objects/names/>Names</a>.</li>
<li>A mechanism to attach authorization and policy to a subsection of the cluster.</li>
</ol>
<p>Use of multiple namespaces is optional.</p>
<p>This example demonstrates how to use Kubernetes namespaces to subdivide your cluster.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=prerequisites>Prerequisites</h2>
<p>This example assumes the following:</p>
<ol>
<li>You have an <a href=/docs/setup/>existing Kubernetes cluster</a>.</li>
<li>You have a basic understanding of Kubernetes <a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>, <a class=glossary-tooltip title="A way to expose an application running on a set of Pods as a network service." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Services>Services</a>, and <a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployments>Deployments</a>.</li>
</ol>
<h2 id=understand-the-default-namespace>Understand the default namespace</h2>
<p>By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods,
Services, and Deployments used by the cluster.</p>
<p>Assuming you have a fresh cluster, you can inspect the available namespaces by doing the following:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get namespaces
</code></pre></div><pre><code>NAME      STATUS    AGE
default   Active    13m
</code></pre><h2 id=create-new-namespaces>Create new namespaces</h2>
<p>For this exercise, we will create two additional Kubernetes namespaces to hold our content.</p>
<p>Let's imagine a scenario where an organization is using a shared Kubernetes cluster for development and production use cases.</p>
<p>The development team would like to maintain a space in the cluster where they can get a view on the list of Pods, Services, and Deployments
they use to build and run their application. In this space, Kubernetes resources come and go, and the restrictions on who can or cannot modify resources
are relaxed to enable agile development.</p>
<p>The operations team would like to maintain a space in the cluster where they can enforce strict procedures on who can or cannot manipulate the set of
Pods, Services, and Deployments that run the production site.</p>
<p>One pattern this organization could follow is to partition the Kubernetes cluster into two namespaces: <code>development</code> and <code>production</code>.</p>
<p>Let's create two new namespaces to hold our work.</p>
<p>Use the file <a href=/examples/admin/namespace-dev.json><code>namespace-dev.json</code></a> which describes a <code>development</code> namespace:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/namespace-dev.json download=admin/namespace-dev.json><code>admin/namespace-dev.json</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-namespace-dev-json')" title="Copy admin/namespace-dev.json to clipboard">
</img>
</div>
<div class=includecode id=admin-namespace-dev-json>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Namespace&#34;</span>,
  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;development&#34;</span>,
    <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;development&#34;</span>
    }
  }
}
</code></pre></div>
</div>
</div>
<p>Create the <code>development</code> namespace using kubectl.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</code></pre></div><p>Save the following contents into file <a href=/examples/admin/namespace-prod.json><code>namespace-prod.json</code></a> which describes a <code>production</code> namespace:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/namespace-prod.json download=admin/namespace-prod.json><code>admin/namespace-prod.json</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-namespace-prod-json')" title="Copy admin/namespace-prod.json to clipboard">
</img>
</div>
<div class=includecode id=admin-namespace-prod-json>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Namespace&#34;</span>,
  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;production&#34;</span>,
    <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;production&#34;</span>
    }
  }
}
</code></pre></div>
</div>
</div>
<p>And then let's create the <code>production</code> namespace using kubectl.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</code></pre></div><p>To be sure things are right, let's list all of the namespaces in our cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get namespaces --show-labels
</code></pre></div><pre><code>NAME          STATUS    AGE       LABELS
default       Active    32m       &lt;none&gt;
development   Active    29s       name=development
production    Active    23s       name=production
</code></pre><h2 id=create-pods-in-each-namespace>Create pods in each namespace</h2>
<p>A Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster.</p>
<p>Users interacting with one namespace do not see the content in another namespace.</p>
<p>To demonstrate this, let's spin up a simple Deployment and Pods in the <code>development</code> namespace.</p>
<p>We first check what is the current context:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config view
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusters</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>cluster</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>certificate-authority-data</span>:<span style=color:#bbb> </span>REDACTED<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>server</span>:<span style=color:#bbb> </span>https://130.211.122.180<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>contexts</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>context</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>current-context</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Config<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>preferences</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>users</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>client-certificate-data</span>:<span style=color:#bbb> </span>REDACTED<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>client-key-data</span>:<span style=color:#bbb> </span>REDACTED<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span>65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes-basic-auth<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>password</span>:<span style=color:#bbb> </span>h5M0FtUUIflBSdI7<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>admin<span style=color:#bbb>
</span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config current-context
</code></pre></div><pre><code>lithe-cocoa-92103_kubernetes
</code></pre><p>The next step is to define a context for the kubectl client to work in each namespace. The value of "cluster" and "user" fields are copied from the current context.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config set-context dev --namespace<span style=color:#666>=</span>development <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --cluster<span style=color:#666>=</span>lithe-cocoa-92103_kubernetes <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --user<span style=color:#666>=</span>lithe-cocoa-92103_kubernetes

kubectl config set-context prod --namespace<span style=color:#666>=</span>production <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --cluster<span style=color:#666>=</span>lithe-cocoa-92103_kubernetes <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --user<span style=color:#666>=</span>lithe-cocoa-92103_kubernetes
</code></pre></div><p>By default, the above commands adds two contexts that are saved into file
<code>.kube/config</code>. You can now view the contexts and alternate against the two
new request contexts depending on which namespace you wish to work against.</p>
<p>To view the new contexts:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config view
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusters</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>cluster</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>certificate-authority-data</span>:<span style=color:#bbb> </span>REDACTED<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>server</span>:<span style=color:#bbb> </span>https://130.211.122.180<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>contexts</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>context</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>context</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>development<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dev<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>context</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>production<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>current-context</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Config<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>preferences</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>users</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>client-certificate-data</span>:<span style=color:#bbb> </span>REDACTED<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>client-key-data</span>:<span style=color:#bbb> </span>REDACTED<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span>65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>lithe-cocoa-92103_kubernetes-basic-auth<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>password</span>:<span style=color:#bbb> </span>h5M0FtUUIflBSdI7<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>admin<span style=color:#bbb>
</span></code></pre></div><p>Let's switch to operate in the <code>development</code> namespace.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config use-context dev
</code></pre></div><p>You can verify your current context by doing the following:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config current-context
</code></pre></div><pre><code>dev
</code></pre><p>At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the <code>development</code> namespace.</p>
<p>Let's create some contents.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/admin/snowflake-deployment.yaml download=admin/snowflake-deployment.yaml><code>admin/snowflake-deployment.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-snowflake-deployment-yaml')" title="Copy admin/snowflake-deployment.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-snowflake-deployment-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>snowflake<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>snowflake<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>snowflake<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>snowflake<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/serve_hostname<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>Always<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>snowflake<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Apply the manifest to create a Deployment</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/admin/snowflake-deployment.yaml
</code></pre></div><p>We have created a deployment whose replica size is 2 that is running the pod called <code>snowflake</code> with a basic container that serves the hostname.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment
</code></pre></div><pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
snowflake    2/2     2            2           2m
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>snowflake
</code></pre></div><pre><code>NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
</code></pre><p>And this is great, developers are able to do what they want, and they do not have to worry about affecting content in the <code>production</code> namespace.</p>
<p>Let's switch to the <code>production</code> namespace and show how resources in one namespace are hidden from the other.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config use-context prod
</code></pre></div><p>The <code>production</code> namespace should be empty, and the following commands should return nothing.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment
kubectl get pods
</code></pre></div><p>Production likes to run cattle, so let's create some cattle pods.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create deployment cattle --image<span style=color:#666>=</span>k8s.gcr.io/serve_hostname --replicas<span style=color:#666>=</span><span style=color:#666>5</span>

kubectl get deployment
</code></pre></div><pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
cattle       5/5     5            5           10s
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>cattle
</code></pre></div><pre><code>NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
</code></pre><p>At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.</p>
<p>As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different
authorization rules for each namespace.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c4d0832845adc92b7ccd54aed63fc932>27 - Operating etcd clusters for Kubernetes</h1>
<p><p>etcd is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p></p>
<p>If your Kubernetes cluster uses etcd as its backing store, make sure you have a
<a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster>back up</a> plan
for those data.</p>
<p>You can find in-depth information about etcd in the official <a href=https://etcd.io/docs/>documentation</a>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=prerequisites>Prerequisites</h2>
<ul>
<li>
<p>Run etcd as a cluster of odd members.</p>
</li>
<li>
<p>etcd is a leader-based distributed system. Ensure that the leader
periodically send heartbeats on time to all followers to keep the cluster
stable.</p>
</li>
<li>
<p>Ensure that no resource starvation occurs.</p>
<p>Performance and stability of the cluster is sensitive to network and disk
I/O. Any resource starvation can lead to heartbeat timeout, causing instability
of the cluster. An unstable etcd indicates that no leader is elected. Under
such circumstances, a cluster cannot make any changes to its current state,
which implies no new pods can be scheduled.</p>
</li>
<li>
<p>Keeping etcd clusters stable is critical to the stability of Kubernetes
clusters. Therefore, run etcd clusters on dedicated machines or isolated
environments for <a href=https://etcd.io/docs/current/op-guide/hardware/>guaranteed resource requirements</a>.</p>
</li>
<li>
<p>The minimum recommended version of etcd to run in production is <code>3.2.10+</code>.</p>
</li>
</ul>
<h2 id=resource-requirements>Resource requirements</h2>
<p>Operating etcd with limited resources is suitable only for testing purposes.
For deploying in production, advanced hardware configuration is required.
Before deploying etcd in production, see
<a href=https://etcd.io/docs/current/op-guide/hardware/#example-hardware-configurations>resource requirement reference</a>.</p>
<h2 id=starting-etcd-clusters>Starting etcd clusters</h2>
<p>This section covers starting a single-node and multi-node etcd cluster.</p>
<h3 id=single-node-etcd-cluster>Single-node etcd cluster</h3>
<p>Use a single-node etcd cluster only for testing purpose.</p>
<ol>
<li>
<p>Run the following:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>etcd --listen-client-urls<span style=color:#666>=</span>http://<span style=color:#b8860b>$PRIVATE_IP</span>:2379 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   --advertise-client-urls<span style=color:#666>=</span>http://<span style=color:#b8860b>$PRIVATE_IP</span>:2379
</code></pre></div></li>
<li>
<p>Start the Kubernetes API server with the flag
<code>--etcd-servers=$PRIVATE_IP:2379</code>.</p>
<p>Make sure <code>PRIVATE_IP</code> is set to your etcd client IP.</p>
</li>
</ol>
<h3 id=multi-node-etcd-cluster>Multi-node etcd cluster</h3>
<p>For durability and high availability, run etcd as a multi-node cluster in
production and back it up periodically. A five-member cluster is recommended
in production. For more information, see
<a href=https://etcd.io/docs/current/faq/#what-is-failure-tolerance>FAQ documentation</a>.</p>
<p>Configure an etcd cluster either by static member information or by dynamic
discovery. For more information on clustering, see
<a href=https://etcd.io/docs/current/op-guide/clustering/>etcd clustering documentation</a>.</p>
<p>For an example, consider a five-member etcd cluster running with the following
client URLs: <code>http://$IP1:2379</code>, <code>http://$IP2:2379</code>, <code>http://$IP3:2379</code>,
<code>http://$IP4:2379</code>, and <code>http://$IP5:2379</code>. To start a Kubernetes API server:</p>
<ol>
<li>
<p>Run the following:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>etcd --listen-client-urls<span style=color:#666>=</span>http://<span style=color:#b8860b>$IP1</span>:2379,http://<span style=color:#b8860b>$IP2</span>:2379,http://<span style=color:#b8860b>$IP3</span>:2379,http://<span style=color:#b8860b>$IP4</span>:2379,http://<span style=color:#b8860b>$IP5</span>:2379 --advertise-client-urls<span style=color:#666>=</span>http://<span style=color:#b8860b>$IP1</span>:2379,http://<span style=color:#b8860b>$IP2</span>:2379,http://<span style=color:#b8860b>$IP3</span>:2379,http://<span style=color:#b8860b>$IP4</span>:2379,http://<span style=color:#b8860b>$IP5</span>:2379
</code></pre></div></li>
<li>
<p>Start the Kubernetes API servers with the flag
<code>--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379</code>.</p>
<p>Make sure the <code>IP&lt;n></code> variables are set to your client IP addresses.</p>
</li>
</ol>
<h3 id=multi-node-etcd-cluster-with-load-balancer>Multi-node etcd cluster with load balancer</h3>
<p>To run a load balancing etcd cluster:</p>
<ol>
<li>Set up an etcd cluster.</li>
<li>Configure a load balancer in front of the etcd cluster.
For example, let the address of the load balancer be <code>$LB</code>.</li>
<li>Start Kubernetes API Servers with the flag <code>--etcd-servers=$LB:2379</code>.</li>
</ol>
<h2 id=securing-etcd-clusters>Securing etcd clusters</h2>
<p>Access to etcd is equivalent to root permission in the cluster so ideally only
the API server should have access to it. Considering the sensitivity of the
data, it is recommended to grant permission to only those nodes that require
access to etcd clusters.</p>
<p>To secure etcd, either set up firewall rules or use the security features
provided by etcd. etcd security features depend on x509 Public Key
Infrastructure (PKI). To begin, establish secure communication channels by
generating a key and certificate pair. For example, use key pairs <code>peer.key</code>
and <code>peer.cert</code> for securing communication between etcd members, and
<code>client.key</code> and <code>client.cert</code> for securing communication between etcd and its
clients. See the <a href=https://github.com/coreos/etcd/tree/master/hack/tls-setup>example scripts</a>
provided by the etcd project to generate key pairs and CA files for client
authentication.</p>
<h3 id=securing-communication>Securing communication</h3>
<p>To configure etcd with secure peer communication, specify flags
<code>--peer-key-file=peer.key</code> and <code>--peer-cert-file=peer.cert</code>, and use HTTPS as
the URL schema.</p>
<p>Similarly, to configure etcd with secure client communication, specify flags
<code>--key-file=k8sclient.key</code> and <code>--cert-file=k8sclient.cert</code>, and use HTTPS as
the URL schema. Here is an example on a client command that uses secure
communication:</p>
<pre><code>ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
</code></pre><h3 id=limiting-access-of-etcd-clusters>Limiting access of etcd clusters</h3>
<p>After configuring secure communication, restrict the access of etcd cluster to
only the Kubernetes API servers. Use TLS authentication to do so.</p>
<p>For example, consider key pairs <code>k8sclient.key</code> and <code>k8sclient.cert</code> that are
trusted by the CA <code>etcd.ca</code>. When etcd is configured with <code>--client-cert-auth</code>
along with TLS, it verifies the certificates from clients by using system CAs
or the CA passed in by <code>--trusted-ca-file</code> flag. Specifying flags
<code>--client-cert-auth=true</code> and <code>--trusted-ca-file=etcd.ca</code> will restrict the
access to clients with the certificate <code>k8sclient.cert</code>.</p>
<p>Once etcd is configured correctly, only clients with valid certificates can
access it. To give Kubernetes API servers the access, configure them with the
flags <code>--etcd-certfile=k8sclient.cert</code>,<code>--etcd-keyfile=k8sclient.key</code> and
<code>--etcd-cafile=ca.cert</code>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> etcd authentication is not currently supported by Kubernetes. For more
information, see the related issue
<a href=https://github.com/kubernetes/kubernetes/issues/23398>Support Basic Auth for Etcd v2</a>.
</div>
<h2 id=replacing-a-failed-etcd-member>Replacing a failed etcd member</h2>
<p>etcd cluster achieves high availability by tolerating minor member failures.
However, to improve the overall health of the cluster, replace failed members
immediately. When multiple members fail, replace them one by one. Replacing a
failed member involves two steps: removing the failed member and adding a new
member.</p>
<p>Though etcd keeps unique member IDs internally, it is recommended to use a
unique name for each member to avoid human errors. For example, consider a
three-member etcd cluster. Let the URLs be, <code>member1=http://10.0.0.1</code>,
<code>member2=http://10.0.0.2</code>, and <code>member3=http://10.0.0.3</code>. When <code>member1</code> fails,
replace it with <code>member4=http://10.0.0.4</code>.</p>
<ol>
<li>
<p>Get the member ID of the failed <code>member1</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>etcdctl --endpoints<span style=color:#666>=</span>http://10.0.0.2,http://10.0.0.3 member list
</code></pre></div><p>The following message is displayed:</p>
<pre><code class=language-console data-lang=console>8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
</code></pre></li>
<li>
<p>Remove the failed member:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>etcdctl member remove 8211f1d0f64f3269
</code></pre></div><p>The following message is displayed:</p>
<pre><code class=language-console data-lang=console>Removed member 8211f1d0f64f3269 from cluster
</code></pre></li>
<li>
<p>Add the new member:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>etcdctl member add member4 --peer-urls<span style=color:#666>=</span>http://10.0.0.4:2380
</code></pre></div><p>The following message is displayed:</p>
<pre><code class=language-console data-lang=console>Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
</code></pre></li>
<li>
<p>Start the newly added member on a machine with the IP <code>10.0.0.4</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>export</span> <span style=color:#b8860b>ETCD_NAME</span><span style=color:#666>=</span><span style=color:#b44>&#34;member4&#34;</span>
<span style=color:#a2f>export</span> <span style=color:#b8860b>ETCD_INITIAL_CLUSTER</span><span style=color:#666>=</span><span style=color:#b44>&#34;member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380&#34;</span>
<span style=color:#a2f>export</span> <span style=color:#b8860b>ETCD_INITIAL_CLUSTER_STATE</span><span style=color:#666>=</span>existing
etcd <span style=color:#666>[</span>flags<span style=color:#666>]</span>
</code></pre></div></li>
<li>
<p>Do either of the following:</p>
<ol>
<li>Update the <code>--etcd-servers</code> flag for the Kubernetes API servers to make
Kubernetes aware of the configuration changes, then restart the
Kubernetes API servers.</li>
<li>Update the load balancer configuration if a load balancer is used in the
deployment.</li>
</ol>
</li>
</ol>
<p>For more information on cluster reconfiguration, see
<a href=https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member>etcd reconfiguration documentation</a>.</p>
<h2 id=backing-up-an-etcd-cluster>Backing up an etcd cluster</h2>
<p>All Kubernetes objects are stored on etcd. Periodically backing up the etcd
cluster data is important to recover Kubernetes clusters under disaster
scenarios, such as losing all control plane nodes. The snapshot file contains
all the Kubernetes states and critical information. In order to keep the
sensitive Kubernetes data safe, encrypt the snapshot files.</p>
<p>Backing up an etcd cluster can be accomplished in two ways: etcd built-in
snapshot and volume snapshot.</p>
<h3 id=built-in-snapshot>Built-in snapshot</h3>
<p>etcd supports built-in snapshot. A snapshot may either be taken from a live
member with the <code>etcdctl snapshot save</code> command or by copying the
<code>member/snap/db</code> file from an etcd
<a href=https://etcd.io/docs/current/op-guide/configuration/#--data-dir>data directory</a>
that is not currently used by an etcd process. Taking the snapshot will
not affect the performance of the member.</p>
<p>Below is an example for taking a snapshot of the keyspace served by
<code>$ENDPOINT</code> to the file <code>snapshotdb</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>ETCDCTL_API</span><span style=color:#666>=</span><span style=color:#666>3</span> etcdctl --endpoints <span style=color:#b8860b>$ENDPOINT</span> snapshot save snapshotdb
</code></pre></div><p>Verify the snapshot:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>ETCDCTL_API</span><span style=color:#666>=</span><span style=color:#666>3</span> etcdctl --write-out<span style=color:#666>=</span>table snapshot status snapshotdb
</code></pre></div><pre><code class=language-console data-lang=console>+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fe01cf57 |       10 |          7 | 2.1 MB     |
+----------+----------+------------+------------+
</code></pre><h3 id=volume-snapshot>Volume snapshot</h3>
<p>If etcd is running on a storage volume that supports backup, such as Amazon
Elastic Block Store, back up etcd data by taking a snapshot of the storage
volume.</p>
<h3 id=snapshot-using-etcdctl-options>Snapshot using etcdctl options</h3>
<p>We can also take the snapshot using various options given by etcdctl. For example</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>ETCDCTL_API</span><span style=color:#666>=</span><span style=color:#666>3</span> etcdctl -h 
</code></pre></div><p>will list various options available from etcdctl. For example, you can take a snapshot by specifying
the endpoint, certificates etc as shown below:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>ETCDCTL_API</span><span style=color:#666>=</span><span style=color:#666>3</span> etcdctl --endpoints<span style=color:#666>=</span>https://127.0.0.1:2379 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --cacert<span style=color:#666>=</span>&lt;trusted-ca-file&gt; --cert<span style=color:#666>=</span>&lt;cert-file&gt; --key<span style=color:#666>=</span>&lt;key-file&gt; <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  snapshot save &lt;backup-file-location&gt;
</code></pre></div><p>where <code>trusted-ca-file</code>, <code>cert-file</code> and <code>key-file</code> can be obtained from the description of the etcd Pod.</p>
<h2 id=scaling-up-etcd-clusters>Scaling up etcd clusters</h2>
<p>Scaling up etcd clusters increases availability by trading off performance.
Scaling does not increase cluster performance nor capability. A general rule
is not to scale up or down etcd clusters. Do not configure any auto scaling
groups for etcd clusters. It is highly recommended to always run a static
five-member etcd cluster for production Kubernetes clusters at any officially
supported scale.</p>
<p>A reasonable scaling is to upgrade a three-member cluster to a five-member
one, when more reliability is desired. See
<a href=https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member>etcd reconfiguration documentation</a>
for information on how to add members into an existing cluster.</p>
<h2 id=restoring-an-etcd-cluster>Restoring an etcd cluster</h2>
<p>etcd supports restoring from snapshots that are taken from an etcd process of
the <a href=http://semver.org/>major.minor</a> version. Restoring a version from a
different patch version of etcd also is supported. A restore operation is
employed to recover the data of a failed cluster.</p>
<p>Before starting the restore operation, a snapshot file must be present. It can
either be a snapshot file from a previous backup operation, or from a remaining
<a href=https://etcd.io/docs/current/op-guide/configuration/#--data-dir>data directory</a>.
Here is an example:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>ETCDCTL_API</span><span style=color:#666>=</span><span style=color:#666>3</span> etcdctl --endpoints 10.2.0.9:2379 snapshot restore snapshotdb
</code></pre></div><p>Another example for restoring using etcdctl options:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>ETCDCTL_API</span><span style=color:#666>=</span><span style=color:#666>3</span> etcdctl --data-dir &lt;data-dir-location&gt; snapshot restore snapshotdb
</code></pre></div><p>For more information and examples on restoring a cluster from a snapshot file, see
<a href=https://etcd.io/docs/current/op-guide/recovery/#restoring-a-cluster>etcd disaster recovery documentation</a>.</p>
<p>If the access URLs of the restored cluster is changed from the previous
cluster, the Kubernetes API server must be reconfigured accordingly. In this
case, restart Kubernetes API servers with the flag
<code>--etcd-servers=$NEW_ETCD_CLUSTER</code> instead of the flag
<code>--etcd-servers=$OLD_ETCD_CLUSTER</code>. Replace <code>$NEW_ETCD_CLUSTER</code> and
<code>$OLD_ETCD_CLUSTER</code> with the respective IP addresses. If a load balancer is
used in front of an etcd cluster, you might need to update the load balancer
instead.</p>
<p>If the majority of etcd members have permanently failed, the etcd cluster is
considered failed. In this scenario, Kubernetes cannot make any changes to its
current state. Although the scheduled pods might continue to run, no new pods
can be scheduled. In such cases, recover the etcd cluster and potentially
reconfigure Kubernetes API servers to fix the issue.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>If any API servers are running in your cluster, you should not attempt to
restore instances of etcd. Instead, follow these steps to restore etcd:</p>
<ul>
<li>stop <em>all</em> API server instances</li>
<li>restore state in all etcd instances</li>
<li>restart all API server instances</li>
</ul>
<p>We also recommend restarting any components (e.g. <code>kube-scheduler</code>,
<code>kube-controller-manager</code>, <code>kubelet</code>) to ensure that they don't rely on some
stale data. Note that in practice, the restore takes a bit of time. During the
restoration, critical components will lose leader lock and restart themselves.</p>
</div>
<h2 id=upgrading-etcd-clusters>Upgrading etcd clusters</h2>
<p>For more details on etcd upgrade, please refer to the <a href=https://etcd.io/docs/latest/upgrades/>etcd upgrades</a> documentation.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Before you start an upgrade, please back up your etcd cluster first.
</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-eec61e72c300dbfbf7302400ca966432>28 - Reconfigure a Node's Kubelet in a Live Cluster</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [deprecated]</code>
</div>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> The <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/281-dynamic-kubelet-configuration>Dynamic Kubelet Configuration</a>
feature is deprecated and should not be used.
Please switch to alternative means distributing configuration to the Nodes of your cluster.
</div>
<p><a href=https://github.com/kubernetes/enhancements/issues/281>Dynamic Kubelet Configuration</a>
allows you to change the configuration of each
<a class=glossary-tooltip title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> in a running Kubernetes cluster,
by deploying a <a class=glossary-tooltip title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a> and configuring
each <a class=glossary-tooltip title="A node is a worker machine in Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> to use it.</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> All kubelet configuration parameters can be changed dynamically,
but this is unsafe for some parameters. Before deciding to change a parameter
dynamically, you need a strong understanding of how that change will affect your
cluster's behavior. Always carefully test configuration changes on a small set
of nodes before rolling them out cluster-wide. Advice on configuring specific
fields is available in the inline
<a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>.
</div>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster.
You also need <code>kubectl</code>, <a href=/docs/tasks/tools/#kubectl>installed</a> and configured to communicate with your cluster.
Make sure that you are using a version of <code>kubectl</code> that is
<a href=/releases/version-skew-policy/>compatible</a> with your cluster.
Your Kubernetes server must be at or later than version v1.11.
To check the version, enter <code>kubectl version</code>.
</p>
<p>Some of the examples use the command line tool
<a href=https://stedolan.github.io/jq/>jq</a>. You do not need <code>jq</code> to complete the task,
because there are manual alternatives.</p>
<p>For each node that you're reconfiguring, you must set the kubelet
<code>--dynamic-config-dir</code> flag to a writable directory.</p>
<h2 id=reconfiguring-the-kubelet-on-a-running-node-in-your-cluster>Reconfiguring the kubelet on a running node in your cluster</h2>
<h3 id=basic-workflow-overview>Basic workflow overview</h3>
<p>The basic workflow for configuring a kubelet in a live cluster is as follows:</p>
<ol>
<li>Write a YAML or JSON configuration file containing the
kubelet's configuration.</li>
<li>Wrap this file in a ConfigMap and save it to the Kubernetes control plane.</li>
<li>Update the kubelet's corresponding Node object to use this ConfigMap.</li>
</ol>
<p>Each kubelet watches a configuration reference on its respective Node object.
When this reference changes, the kubelet downloads the new configuration,
updates a local reference to refer to the file, and exits.
For the feature to work correctly, you must be running an OS-level service
manager (such as systemd), which will restart the kubelet if it exits. When the
kubelet is restarted, it will begin using the new configuration.</p>
<p>The new configuration completely overrides configuration provided by <code>--config</code>,
and is overridden by command-line flags. Unspecified values in the new configuration
will receive default values appropriate to the configuration version
(e.g. <code>kubelet.config.k8s.io/v1beta1</code>), unless overridden by flags.</p>
<p>The status of the Node's kubelet configuration is reported via
<code>Node.Status.Config</code>. Once you have updated a Node to use the new
ConfigMap, you can observe this status to confirm that the Node is using the
intended configuration.</p>
<p>This document describes editing Nodes using <code>kubectl edit</code>.
There are other ways to modify a Node's spec, including <code>kubectl patch</code>, for
example, which facilitate scripted workflows.</p>
<p>This document only describes a single Node consuming each ConfigMap. Keep in
mind that it is also valid for multiple Nodes to consume the same ConfigMap.</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> While it is <em>possible</em> to change the configuration by
updating the ConfigMap in-place, this causes all kubelets configured with
that ConfigMap to update simultaneously. It is much safer to treat ConfigMaps
as immutable by convention, aided by <code>kubectl</code>'s <code>--append-hash</code> option,
and incrementally roll out updates to <code>Node.Spec.ConfigSource</code>.
</div>
<h3 id=automatic-rbac-rules-for-node-authorizer>Automatic RBAC rules for Node Authorizer</h3>
<p>Previously, you were required to manually create RBAC rules
to allow Nodes to access their assigned ConfigMaps. The Node Authorizer now
automatically configures these rules.</p>
<h3 id=generating-a-file-that-contains-the-current-configuration>Generating a file that contains the current configuration</h3>
<p>The Dynamic Kubelet Configuration feature allows you to provide an override for
the entire configuration object, rather than a per-field overlay. This is a
simpler model that makes it easier to trace the source of configuration values
and debug issues. The compromise, however, is that you must start with knowledge
of the existing configuration to ensure that you only change the fields you
intend to change.</p>
<p>The kubelet loads settings from its configuration file, but you can set command
line flags to override the configuration in the file. This means that if you
only know the contents of the configuration file, and you don't know the
command line overrides, then you do not know the running configuration either.</p>
<p>Because you need to know the running configuration in order to override it,
you can fetch the running configuration from the kubelet. You can generate a
config file containing a Node's current configuration by accessing the kubelet's
<code>configz</code> endpoint, through <code>kubectl proxy</code>. The next section explains how to
do this.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> The kubelet's <code>configz</code> endpoint is there to help with debugging, and is not
a stable part of kubelet behavior.
Do not rely on the behavior of this endpoint for production scenarios or for
use with automated tools.
</div>
<p>For more information on configuring the kubelet via a configuration file, see
<a href=/docs/tasks/administer-cluster/kubelet-config-file>Set kubelet parameters via a config file</a>).</p>
<h4 id=generate-the-configuration-file>Generate the configuration file</h4>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The steps below use the <code>jq</code> command to streamline working with JSON.
To follow the tasks as written, you need to have <code>jq</code> installed. You can
adapt the steps if you prefer to extract the <code>kubeletconfig</code> subobject manually.
</div>
<ol>
<li>
<p>Choose a Node to reconfigure. In this example, the name of this Node is
referred to as <code>NODE_NAME</code>.</p>
</li>
<li>
<p>Start the kubectl proxy in the background using the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8001</span> &amp;
</code></pre></div></li>
<li>
<p>Run the following command to download and unpack the configuration from the
<code>configz</code> endpoint. The command is long, so be careful when copying and
pasting. <strong>If you use zsh</strong>, note that common zsh configurations add backslashes
to escape the opening and closing curly braces around the variable name in the URL.
For example: <code>${NODE_NAME}</code> will be rewritten as <code>$\{NODE_NAME\}</code> during the paste.
You must remove the backslashes before running the command, or the command will fail.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>NODE_NAME</span><span style=color:#666>=</span><span style=color:#b44>&#34;the-name-of-the-node-you-are-reconfiguring&#34;</span>; curl -sSL <span style=color:#b44>&#34;http://localhost:8001/api/v1/nodes/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NODE_NAME</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/proxy/configz&#34;</span> | jq <span style=color:#b44>&#39;.kubeletconfig|.kind=&#34;KubeletConfiguration&#34;|.apiVersion=&#34;kubelet.config.k8s.io/v1beta1&#34;&#39;</span> &gt; kubelet_configz_<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NODE_NAME</span><span style=color:#b68;font-weight:700>}</span>
</code></pre></div></li>
</ol>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> You need to manually add the <code>kind</code> and <code>apiVersion</code> to the downloaded
object, because those fields are not reported by the <code>configz</code> endpoint.
</div>
<h4 id=edit-the-configuration-file>Edit the configuration file</h4>
<p>Using a text editor, change one of the parameters in the
file generated by the previous procedure. For example, you
might edit the parameter <code>eventRecordQPS</code>, that controls
rate limiting for event recording.</p>
<h4 id=push-the-configuration-file-to-the-control-plane>Push the configuration file to the control plane</h4>
<p>Push the edited configuration file to the control plane with the
following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl -n kube-system create configmap my-node-config --from-file<span style=color:#666>=</span><span style=color:#b8860b>kubelet</span><span style=color:#666>=</span>kubelet_configz_<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NODE_NAME</span><span style=color:#b68;font-weight:700>}</span> --append-hash -o yaml
</code></pre></div><p>This is an example of a valid response:</p>
<pre><code class=language-none data-lang=none>apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: 2017-09-14T20:23:33Z
  name: my-node-config-gkt4c2m4b2
  namespace: kube-system
  resourceVersion: &quot;119980&quot;
  uid: 946d785e-998a-11e7-a8dd-42010a800006
data:
  kubelet: |
    {...}
</code></pre><p>You created that ConfigMap inside the <code>kube-system</code> namespace because the kubelet
is a Kubernetes system component.</p>
<p>The <code>--append-hash</code> option appends a short checksum of the ConfigMap contents
to the name. This is convenient for an edit-then-push workflow, because it
automatically, yet deterministically, generates new names for new resources.
The name that includes this generated hash is referred to as <code>CONFIG_MAP_NAME</code>
in the following examples.</p>
<h4 id=set-the-node-to-use-the-new-configuration>Set the Node to use the new configuration</h4>
<p>Edit the Node's reference to point to the new ConfigMap with the
following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl edit node <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NODE_NAME</span><span style=color:#b68;font-weight:700>}</span>
</code></pre></div><p>In your text editor, add the following YAML under <code>spec</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>configSource</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>CONFIG_MAP_NAME<span style=color:#bbb> </span><span style=color:#080;font-style:italic># replace CONFIG_MAP_NAME with the name of the ConfigMap</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kubeletConfigKey</span>:<span style=color:#bbb> </span>kubelet<span style=color:#bbb>
</span></code></pre></div><p>You must specify all three of <code>name</code>, <code>namespace</code>, and <code>kubeletConfigKey</code>.
The <code>kubeletConfigKey</code> parameter shows the kubelet which key of the ConfigMap
contains its config.</p>
<h4 id=observe-that-the-node-begins-using-the-new-configuration>Observe that the Node begins using the new configuration</h4>
<p>Retrieve the Node using the <code>kubectl get node ${NODE_NAME} -o yaml</code> command and inspect
<code>Node.Status.Config</code>. The config sources corresponding to the <code>active</code>,
<code>assigned</code>, and <code>lastKnownGood</code> configurations are reported in the status.</p>
<ul>
<li>The <code>active</code> configuration is the version the kubelet is currently running with.</li>
<li>The <code>assigned</code> configuration is the latest version the kubelet has resolved based on
<code>Node.Spec.ConfigSource</code>.</li>
<li>The <code>lastKnownGood</code> configuration is the version the
kubelet will fall back to if an invalid config is assigned in <code>Node.Spec.ConfigSource</code>.</li>
</ul>
<p>The<code>lastKnownGood</code> configuration might not be present if it is set to its default value,
the local config deployed with the node. The status will update <code>lastKnownGood</code> to
match a valid <code>assigned</code> config after the kubelet becomes comfortable with the config.
The details of how the kubelet determines a config should become the <code>lastKnownGood</code> are
not guaranteed by the API, but is currently implemented as a 10-minute grace period.</p>
<p>You can use the following command (using <code>jq</code>) to filter down
to the config status:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get no <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NODE_NAME</span><span style=color:#b68;font-weight:700>}</span> -o json | jq <span style=color:#b44>&#39;.status.config&#39;</span>
</code></pre></div><p>The following is an example response:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;active&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;configMap&#34;</span>: {
      <span style=color:green;font-weight:700>&#34;kubeletConfigKey&#34;</span>: <span style=color:#b44>&#34;kubelet&#34;</span>,
      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;kube-system&#34;</span>,
      <span style=color:green;font-weight:700>&#34;resourceVersion&#34;</span>: <span style=color:#b44>&#34;1326&#34;</span>,
      <span style=color:green;font-weight:700>&#34;uid&#34;</span>: <span style=color:#b44>&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  },
  <span style=color:green;font-weight:700>&#34;assigned&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;configMap&#34;</span>: {
      <span style=color:green;font-weight:700>&#34;kubeletConfigKey&#34;</span>: <span style=color:#b44>&#34;kubelet&#34;</span>,
      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;kube-system&#34;</span>,
      <span style=color:green;font-weight:700>&#34;resourceVersion&#34;</span>: <span style=color:#b44>&#34;1326&#34;</span>,
      <span style=color:green;font-weight:700>&#34;uid&#34;</span>: <span style=color:#b44>&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  },
  <span style=color:green;font-weight:700>&#34;lastKnownGood&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;configMap&#34;</span>: {
      <span style=color:green;font-weight:700>&#34;kubeletConfigKey&#34;</span>: <span style=color:#b44>&#34;kubelet&#34;</span>,
      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;kube-system&#34;</span>,
      <span style=color:green;font-weight:700>&#34;resourceVersion&#34;</span>: <span style=color:#b44>&#34;1326&#34;</span>,
      <span style=color:green;font-weight:700>&#34;uid&#34;</span>: <span style=color:#b44>&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  }
}

</code></pre></div><p>(if you do not have <code>jq</code>, you can look at the whole response and find <code>Node.Status.Config</code>
by eye).</p>
<p>If an error occurs, the kubelet reports it in the <code>Node.Status.Config.Error</code>
structure. Possible errors are listed in
<a href=#understanding-node-config-status-errors>Understanding Node.Status.Config.Error messages</a>.
You can search for the identical text in the kubelet log for additional details
and context about the error.</p>
<h4 id=make-more-changes>Make more changes</h4>
<p>Follow the workflow above to make more changes and push them again. Each time
you push a ConfigMap with new contents, the <code>--append-hash</code> kubectl option creates
the ConfigMap with a new name. The safest rollout strategy is to first create a
new ConfigMap, and then update the Node to use the new ConfigMap.</p>
<h4 id=reset-the-node-to-use-its-local-default-configuration>Reset the Node to use its local default configuration</h4>
<p>To reset the Node to use the configuration it was provisioned with, edit the
Node using <code>kubectl edit node ${NODE_NAME}</code> and remove the
<code>Node.Spec.ConfigSource</code> field.</p>
<h4 id=observe-that-the-node-is-using-its-local-default-configuration>Observe that the Node is using its local default configuration</h4>
<p>After removing this subfield, <code>Node.Status.Config</code> eventually becomes
empty, since all config sources have been reset to <code>nil</code>, which indicates that
the local default config is <code>assigned</code>, <code>active</code>, and <code>lastKnownGood</code>, and no
error is reported.</p>
<h2 id=kubectl-patch-example><code>kubectl patch</code> example</h2>
<p>You can change a Node's configSource using several different mechanisms.
This example uses <code>kubectl patch</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl patch node <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NODE_NAME</span><span style=color:#b68;font-weight:700>}</span> -p <span style=color:#b44>&#34;{\&#34;spec\&#34;:{\&#34;configSource\&#34;:{\&#34;configMap\&#34;:{\&#34;name\&#34;:\&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONFIG_MAP_NAME</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>\&#34;,\&#34;namespace\&#34;:\&#34;kube-system\&#34;,\&#34;kubeletConfigKey\&#34;:\&#34;kubelet\&#34;}}}}&#34;</span>
</code></pre></div><h2 id=understanding-how-the-kubelet-checkpoints-config>Understanding how the kubelet checkpoints config</h2>
<p>When a new config is assigned to the Node, the kubelet downloads and unpacks the
config payload as a set of files on the local disk. The kubelet also records metadata
that locally tracks the assigned and last-known-good config sources, so that the
kubelet knows which config to use across restarts, even if the API server becomes
unavailable. After checkpointing a config and the relevant metadata, the kubelet
exits if it detects that the assigned config has changed. When the kubelet is
restarted by the OS-level service manager (such as <code>systemd</code>), it reads the new
metadata and uses the new config.</p>
<p>The recorded metadata is fully resolved, meaning that it contains all necessary
information to choose a specific config version - typically a <code>UID</code> and <code>ResourceVersion</code>.
This is in contrast to <code>Node.Spec.ConfigSource</code>, where the intended config is declared
via the idempotent <code>namespace/name</code> that identifies the target ConfigMap; the kubelet
tries to use the latest version of this ConfigMap.</p>
<p>When you are debugging problems on a node, you can inspect the kubelet's config
metadata and checkpoints. The structure of the kubelet's checkpointing directory is:</p>
<pre><code class=language-none data-lang=none>- --dynamic-config-dir (root for managing dynamic config)
| - meta
  | - assigned (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the assigned config)
  | - last-known-good (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the last-known-good config)
| - checkpoints
  | - uid1 (dir for versions of object identified by uid1)
    | - resourceVersion1 (dir for unpacked files from resourceVersion1 of object with uid1)
    | - ...
  | - ...
</code></pre><h2 id=understanding-node-config-status-errors>Understanding <code>Node.Status.Config.Error</code> messages</h2>
<p>The following table describes error messages that can occur
when using Dynamic Kubelet Config. You can search for the identical text
in the Kubelet log for additional details and context about the error.</p>
<table><caption style=display:none>Understanding Node.Status.Config.Error messages</caption>
<thead>
<tr>
<th style=text-align:left>Error Message</th>
<th style=text-align:left>Possible Causes</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left>failed to load config, see Kubelet log for details</td>
<td style=text-align:left>The kubelet likely could not parse the downloaded config payload, or encountered a filesystem error attempting to load the payload from disk.</td>
</tr>
<tr>
<td style=text-align:left>failed to validate config, see Kubelet log for details</td>
<td style=text-align:left>The configuration in the payload, combined with any command-line flag overrides, and the sum of feature gates from flags, the config file, and the remote payload, was determined to be invalid by the kubelet.</td>
</tr>
<tr>
<td style=text-align:left>invalid NodeConfigSource, exactly one subfield must be non-nil, but all were nil</td>
<td style=text-align:left>Since Node.Spec.ConfigSource is validated by the API server to contain at least one non-nil subfield, this likely means that the kubelet is older than the API server and does not recognize a newer source type.</td>
</tr>
<tr>
<td style=text-align:left>failed to sync: failed to download config, see Kubelet log for details</td>
<td style=text-align:left>The kubelet could not download the config. It is possible that Node.Spec.ConfigSource could not be resolved to a concrete API object, or that network errors disrupted the download attempt. The kubelet will retry the download when in this error state.</td>
</tr>
<tr>
<td style=text-align:left>failed to sync: internal failure, see Kubelet log for details</td>
<td style=text-align:left>The kubelet encountered some internal problem and failed to update its config as a result. Examples include filesystem errors and reading objects from the internal informer cache.</td>
</tr>
<tr>
<td style=text-align:left>internal failure, see Kubelet log for details</td>
<td style=text-align:left>The kubelet encountered some internal problem while manipulating config, outside of the configuration sync loop.</td>
</tr>
</tbody>
</table>
<h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=/docs/tasks/administer-cluster/kubelet-config-file>Set kubelet parameters via a config file</a>
explains the supported way to configure a kubelet.</li>
<li>See the reference documentation for Node, including the <code>configSource</code> field within
the Node's <a href=/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeSpec>.spec</a></li>
<li>Learn more about kubelet configuration by checking the
<a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>
reference.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-b64a1d2bb3f4ed9f7021134e09a75c36>29 - Reserve Compute Resources for System Daemons</h1>
<p>Kubernetes nodes can be scheduled to <code>Capacity</code>. Pods can consume all the
available capacity on a node by default. This is an issue because nodes
typically run quite a few system daemons that power the OS and Kubernetes
itself. Unless resources are set aside for these system daemons, pods and system
daemons compete for resources and lead to resource starvation issues on the
node.</p>
<p>The <code>kubelet</code> exposes a feature named 'Node Allocatable' that helps to reserve
compute resources for system daemons. Kubernetes recommends cluster
administrators to configure 'Node Allocatable' based on their workload density
on each node.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
Your Kubernetes server must be at or later than version 1.8.
To check the version, enter <code>kubectl version</code>.
Your Kubernetes server must be at or later than version 1.17 to use
the kubelet command line option <code>--reserved-cpus</code> to set an
<a href=#explicitly-reserved-cpu-list>explicitly reserved CPU list</a>.</p>
<h2 id=node-allocatable>Node Allocatable</h2>
<p><img src=/images/docs/node-capacity.svg alt="node capacity"></p>
<p>'Allocatable' on a Kubernetes node is defined as the amount of compute resources
that are available for pods. The scheduler does not over-subscribe
'Allocatable'. 'CPU', 'memory' and 'ephemeral-storage' are supported as of now.</p>
<p>Node Allocatable is exposed as part of <code>v1.Node</code> object in the API and as part
of <code>kubectl describe node</code> in the CLI.</p>
<p>Resources can be reserved for two categories of system daemons in the <code>kubelet</code>.</p>
<h3 id=enabling-qos-and-pod-level-cgroups>Enabling QoS and Pod level cgroups</h3>
<p>To properly enforce node allocatable constraints on the node, you must
enable the new cgroup hierarchy via the <code>--cgroups-per-qos</code> flag. This flag is
enabled by default. When enabled, the <code>kubelet</code> will parent all end-user pods
under a cgroup hierarchy managed by the <code>kubelet</code>.</p>
<h3 id=configuring-a-cgroup-driver>Configuring a cgroup driver</h3>
<p>The <code>kubelet</code> supports manipulation of the cgroup hierarchy on
the host using a cgroup driver. The driver is configured via the
<code>--cgroup-driver</code> flag.</p>
<p>The supported values are the following:</p>
<ul>
<li><code>cgroupfs</code> is the default driver that performs direct manipulation of the
cgroup filesystem on the host in order to manage cgroup sandboxes.</li>
<li><code>systemd</code> is an alternative driver that manages cgroup sandboxes using
transient slices for resources that are supported by that init system.</li>
</ul>
<p>Depending on the configuration of the associated container runtime,
operators may have to choose a particular cgroup driver to ensure
proper system behavior. For example, if operators use the <code>systemd</code>
cgroup driver provided by the <code>containerd</code> runtime, the <code>kubelet</code> must
be configured to use the <code>systemd</code> cgroup driver.</p>
<h3 id=kube-reserved>Kube Reserved</h3>
<ul>
<li><strong>Kubelet Flag</strong>: <code>--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]</code></li>
<li><strong>Kubelet Flag</strong>: <code>--kube-reserved-cgroup=</code></li>
</ul>
<p><code>kube-reserved</code> is meant to capture resource reservation for kubernetes system
daemons like the <code>kubelet</code>, <code>container runtime</code>, <code>node problem detector</code>, etc.
It is not meant to reserve resources for system daemons that are run as pods.
<code>kube-reserved</code> is typically a function of <code>pod density</code> on the nodes.</p>
<p>In addition to <code>cpu</code>, <code>memory</code>, and <code>ephemeral-storage</code>, <code>pid</code> may be
specified to reserve the specified number of process IDs for
kubernetes system daemons.</p>
<p>To optionally enforce <code>kube-reserved</code> on kubernetes system daemons, specify the parent
control group for kube daemons as the value for <code>--kube-reserved-cgroup</code> kubelet
flag.</p>
<p>It is recommended that the kubernetes system daemons are placed under a top
level control group (<code>runtime.slice</code> on systemd machines for example). Each
system daemon should ideally run within its own child control group. Refer to
<a href=https://git.k8s.io/design-proposals-archive/node/node-allocatable.md#recommended-cgroups-setup>the design proposal</a>
for more details on recommended control group hierarchy.</p>
<p>Note that Kubelet <strong>does not</strong> create <code>--kube-reserved-cgroup</code> if it doesn't
exist. Kubelet will fail if an invalid cgroup is specified.</p>
<h3 id=system-reserved>System Reserved</h3>
<ul>
<li><strong>Kubelet Flag</strong>: <code>--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]</code></li>
<li><strong>Kubelet Flag</strong>: <code>--system-reserved-cgroup=</code></li>
</ul>
<p><code>system-reserved</code> is meant to capture resource reservation for OS system daemons
like <code>sshd</code>, <code>udev</code>, etc. <code>system-reserved</code> should reserve <code>memory</code> for the
<code>kernel</code> too since <code>kernel</code> memory is not accounted to pods in Kubernetes at this time.
Reserving resources for user login sessions is also recommended (<code>user.slice</code> in
systemd world).</p>
<p>In addition to <code>cpu</code>, <code>memory</code>, and <code>ephemeral-storage</code>, <code>pid</code> may be
specified to reserve the specified number of process IDs for OS system
daemons.</p>
<p>To optionally enforce <code>system-reserved</code> on system daemons, specify the parent
control group for OS system daemons as the value for <code>--system-reserved-cgroup</code>
kubelet flag.</p>
<p>It is recommended that the OS system daemons are placed under a top level
control group (<code>system.slice</code> on systemd machines for example).</p>
<p>Note that <code>kubelet</code> <strong>does not</strong> create <code>--system-reserved-cgroup</code> if it doesn't
exist. <code>kubelet</code> will fail if an invalid cgroup is specified.</p>
<h3 id=explicitly-reserved-cpu-list>Explicitly Reserved CPU List</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>
<p><strong>Kubelet Flag</strong>: <code>--reserved-cpus=0-3</code></p>
<p><code>reserved-cpus</code> is meant to define an explicit CPU set for OS system daemons and
kubernetes system daemons. <code>reserved-cpus</code> is for systems that do not intend to
define separate top level cgroups for OS system daemons and kubernetes system daemons
with regard to cpuset resource.
If the Kubelet <strong>does not</strong> have <code>--system-reserved-cgroup</code> and <code>--kube-reserved-cgroup</code>,
the explicit cpuset provided by <code>reserved-cpus</code> will take precedence over the CPUs
defined by <code>--kube-reserved</code> and <code>--system-reserved</code> options.</p>
<p>This option is specifically designed for Telco/NFV use cases where uncontrolled
interrupts/timers may impact the workload performance. you can use this option
to define the explicit cpuset for the system/kubernetes daemons as well as the
interrupts/timers, so the rest CPUs on the system can be used exclusively for
workloads, with less impact from uncontrolled interrupts/timers. To move the
system daemon, kubernetes daemons and interrupts/timers to the explicit cpuset
defined by this option, other mechanism outside Kubernetes should be used.
For example: in Centos, you can do this using the tuned toolset.</p>
<h3 id=eviction-thresholds>Eviction Thresholds</h3>
<p><strong>Kubelet Flag</strong>: <code>--eviction-hard=[memory.available&lt;500Mi]</code></p>
<p>Memory pressure at the node level leads to System OOMs which affects the entire
node and all pods running on it. Nodes can go offline temporarily until memory
has been reclaimed. To avoid (or reduce the probability of) system OOMs kubelet
provides <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>out of resource</a>
management. Evictions are
supported for <code>memory</code> and <code>ephemeral-storage</code> only. By reserving some memory via
<code>--eviction-hard</code> flag, the <code>kubelet</code> attempts to evict pods whenever memory
availability on the node drops below the reserved value. Hypothetically, if
system daemons did not exist on a node, pods cannot use more than <code>capacity - eviction-hard</code>. For this reason, resources reserved for evictions are not
available for pods.</p>
<h3 id=enforcing-node-allocatable>Enforcing Node Allocatable</h3>
<p><strong>Kubelet Flag</strong>: <code>--enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]</code></p>
<p>The scheduler treats 'Allocatable' as the available <code>capacity</code> for pods.</p>
<p><code>kubelet</code> enforce 'Allocatable' across pods by default. Enforcement is performed
by evicting pods whenever the overall usage across all pods exceeds
'Allocatable'. More details on eviction policy can be found
on the <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>node pressure eviction</a>
page. This enforcement is controlled by
specifying <code>pods</code> value to the kubelet flag <code>--enforce-node-allocatable</code>.</p>
<p>Optionally, <code>kubelet</code> can be made to enforce <code>kube-reserved</code> and
<code>system-reserved</code> by specifying <code>kube-reserved</code> & <code>system-reserved</code> values in
the same flag. Note that to enforce <code>kube-reserved</code> or <code>system-reserved</code>,
<code>--kube-reserved-cgroup</code> or <code>--system-reserved-cgroup</code> needs to be specified
respectively.</p>
<h2 id=general-guidelines>General Guidelines</h2>
<p>System daemons are expected to be treated similar to
<a href=/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed>Guaranteed pods</a>.
System daemons can burst within their bounding control groups and this behavior needs
to be managed as part of kubernetes deployments. For example, <code>kubelet</code> should
have its own control group and share <code>kube-reserved</code> resources with the
container runtime. However, Kubelet cannot burst and use up all available Node
resources if <code>kube-reserved</code> is enforced.</p>
<p>Be extra careful while enforcing <code>system-reserved</code> reservation since it can lead
to critical system services being CPU starved, OOM killed, or unable
to fork on the node. The
recommendation is to enforce <code>system-reserved</code> only if a user has profiled their
nodes exhaustively to come up with precise estimates and is confident in their
ability to recover if any process in that group is oom-killed.</p>
<ul>
<li>To begin with enforce 'Allocatable' on <code>pods</code>.</li>
<li>Once adequate monitoring and alerting is in place to track kube system
daemons, attempt to enforce <code>kube-reserved</code> based on usage heuristics.</li>
<li>If absolutely necessary, enforce <code>system-reserved</code> over time.</li>
</ul>
<p>The resource requirements of kube system daemons may grow over time as more and
more features are added. Over time, kubernetes project will attempt to bring
down utilization of node system daemons, but that is not a priority as of now.
So expect a drop in <code>Allocatable</code> capacity in future releases.</p>
<h2 id=example-scenario>Example Scenario</h2>
<p>Here is an example to illustrate Node Allocatable computation:</p>
<ul>
<li>Node has <code>32Gi</code> of <code>memory</code>, <code>16 CPUs</code> and <code>100Gi</code> of <code>Storage</code></li>
<li><code>--kube-reserved</code> is set to <code>cpu=1,memory=2Gi,ephemeral-storage=1Gi</code></li>
<li><code>--system-reserved</code> is set to <code>cpu=500m,memory=1Gi,ephemeral-storage=1Gi</code></li>
<li><code>--eviction-hard</code> is set to <code>memory.available&lt;500Mi,nodefs.available&lt;10%</code></li>
</ul>
<p>Under this scenario, 'Allocatable' will be 14.5 CPUs, 28.5Gi of memory and
<code>88Gi</code> of local storage.
Scheduler ensures that the total memory <code>requests</code> across all pods on this node does
not exceed 28.5Gi and storage doesn't exceed 88Gi.
Kubelet evicts pods whenever the overall memory usage across pods exceeds 28.5Gi,
or if overall disk usage exceeds 88Gi If all processes on the node consume as
much CPU as they can, pods together cannot consume more than 14.5 CPUs.</p>
<p>If <code>kube-reserved</code> and/or <code>system-reserved</code> is not enforced and system daemons
exceed their reservation, <code>kubelet</code> evicts pods whenever the overall node memory
usage is higher than 31.5Gi or <code>storage</code> is greater than 90Gi.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-f6f3b8f9789fda4286bf410b8e108f69>30 - Running Kubernetes Node Components as a Non-root User</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>
<p>This document describes how to run Kubernetes Node components such as kubelet, CRI, OCI, and CNI
without root privileges, by using a <a class=glossary-tooltip title="A Linux kernel feature to emulate superuser privilege for unprivileged users." data-toggle=tooltip data-placement=top href=https://man7.org/linux/man-pages/man7/user_namespaces.7.html target=_blank aria-label="user namespace">user namespace</a>.</p>
<p>This technique is also known as <em>rootless mode</em>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>This document describes how to run Kubernetes Node components (and hence pods) as a non-root user.</p>
<p>If you are just looking for how to run a pod as a non-root user, see <a href=/docs/tasks/configure-pod-container/security-context/>SecurityContext</a>.</p>
</div>
<h2 id=before-you-begin>Before you begin</h2>
<p>Your Kubernetes server must be at or later than version 1.22.
To check the version, enter <code>kubectl version</code>.</p>
<ul>
<li><a href=https://rootlesscontaine.rs/getting-started/common/cgroup2/>Enable Cgroup v2</a></li>
<li><a href=https://rootlesscontaine.rs/getting-started/common/login/>Enable systemd with user session</a></li>
<li><a href=https://rootlesscontaine.rs/getting-started/common/sysctl/>Configure several sysctl values, depending on host Linux distribution</a></li>
<li><a href=https://rootlesscontaine.rs/getting-started/common/subuid/>Ensure that your unprivileged user is listed in <code>/etc/subuid</code> and <code>/etc/subgid</code></a></li>
<li>Enable the <code>KubeletInUserNamespace</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a></li>
</ul>
<h2 id=running-kubernetes-inside-rootless-docker-podman>Running Kubernetes inside Rootless Docker/Podman</h2>
<h3 id=kind>kind</h3>
<p><a href=https://kind.sigs.k8s.io/>kind</a> supports running Kubernetes inside Rootless Docker or Rootless Podman.</p>
<p>See <a href=https://kind.sigs.k8s.io/docs/user/rootless/>Running kind with Rootless Docker</a>.</p>
<h3 id=minikube>minikube</h3>
<p><a href=https://minikube.sigs.k8s.io/>minikube</a> also supports running Kubernetes inside Rootless Docker.</p>
<p>See the page about the <a href=https://minikube.sigs.k8s.io/docs/drivers/docker/>docker</a> driver in the Minikube documentation.</p>
<p>Rootless Podman is not supported.</p>
<h2 id=running-kubernetes-inside-unprivileged-containers>Running Kubernetes inside Unprivileged Containers</h2>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div>
<h3 id=sysbox>sysbox</h3>
<p><a href=https://github.com/nestybox/sysbox>Sysbox</a> is an open-source container runtime
(similar to "runc") that supports running system-level workloads such as Docker
and Kubernetes inside unprivileged containers isolated with the Linux user
namespace.</p>
<p>See <a href=https://github.com/nestybox/sysbox/blob/master/docs/quickstart/kind.md>Sysbox Quick Start Guide: Kubernetes-in-Docker</a> for more info.</p>
<p>Sysbox supports running Kubernetes inside unprivileged containers without
requiring Cgroup v2 and without the <code>KubeletInUserNamespace</code> feature gate. It
does this by exposing specially crafted <code>/proc</code> and <code>/sys</code> filesystems inside
the container plus several other advanced OS virtualization techniques.</p>
<h2 id=running-rootless-kubernetes-directly-on-a-host>Running Rootless Kubernetes directly on a host</h2>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div>
<h3 id=k3s>K3s</h3>
<p><a href=https://k3s.io/>K3s</a> experimentally supports rootless mode.</p>
<p>See <a href=https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental>Running K3s with Rootless mode</a> for the usage.</p>
<h3 id=usernetes>Usernetes</h3>
<p><a href=https://github.com/rootless-containers/usernetes>Usernetes</a> is a reference distribution of Kubernetes that can be installed under <code>$HOME</code> directory without the root privilege.</p>
<p>Usernetes supports both containerd and CRI-O as CRI runtimes.
Usernetes supports multi-node clusters using Flannel (VXLAN).</p>
<p>See <a href=https://github.com/rootless-containers/usernetes>the Usernetes repo</a> for the usage.</p>
<h2 id=userns-the-hard-way>Manually deploy a node that runs the kubelet in a user namespace</h2>
<p>This section provides hints for running Kubernetes in a user namespace manually.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> This section is intended to be read by developers of Kubernetes distributions, not by end users.
</div>
<h3 id=creating-a-user-namespace>Creating a user namespace</h3>
<p>The first step is to create a <a class=glossary-tooltip title="A Linux kernel feature to emulate superuser privilege for unprivileged users." data-toggle=tooltip data-placement=top href=https://man7.org/linux/man-pages/man7/user_namespaces.7.html target=_blank aria-label="user namespace">user namespace</a>.</p>
<p>If you are trying to run Kubernetes in a user-namespaced container such as
Rootless Docker/Podman or LXC/LXD, you are all set, and you can go to the next subsection.</p>
<p>Otherwise you have to create a user namespace by yourself, by calling <code>unshare(2)</code> with <code>CLONE_NEWUSER</code>.</p>
<p>A user namespace can be also unshared by using command line tools such as:</p>
<ul>
<li><a href=https://man7.org/linux/man-pages/man1/unshare.1.html><code>unshare(1)</code></a></li>
<li><a href=https://github.com/rootless-containers/rootlesskit>RootlessKit</a></li>
<li><a href=https://github.com/giuseppe/become-root>become-root</a></li>
</ul>
<p>After unsharing the user namespace, you will also have to unshare other namespaces such as mount namespace.</p>
<p>You do <em>not</em> need to call <code>chroot()</code> nor <code>pivot_root()</code> after unsharing the mount namespace,
however, you have to mount writable filesystems on several directories <em>in</em> the namespace.</p>
<p>At least, the following directories need to be writable <em>in</em> the namespace (not <em>outside</em> the namespace):</p>
<ul>
<li><code>/etc</code></li>
<li><code>/run</code></li>
<li><code>/var/logs</code></li>
<li><code>/var/lib/kubelet</code></li>
<li><code>/var/lib/cni</code></li>
<li><code>/var/lib/containerd</code> (for containerd)</li>
<li><code>/var/lib/containers</code> (for CRI-O)</li>
</ul>
<h3 id=creating-a-delegated-cgroup-tree>Creating a delegated cgroup tree</h3>
<p>In addition to the user namespace, you also need to have a writable cgroup tree with cgroup v2.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Kubernetes support for running Node components in user namespaces requires cgroup v2.
Cgroup v1 is not supported.
</div>
<p>If you are trying to run Kubernetes in Rootless Docker/Podman or LXC/LXD on a systemd-based host, you are all set.</p>
<p>Otherwise you have to create a systemd unit with <code>Delegate=yes</code> property to delegate a cgroup tree with writable permission.</p>
<p>On your node, systemd must already be configured to allow delegation; for more details, see
<a href=https://rootlesscontaine.rs/getting-started/common/cgroup2/>cgroup v2</a> in the Rootless
Containers documentation.</p>
<h3 id=configuring-network>Configuring network</h3>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div>
<p>The network namespace of the Node components has to have a non-loopback interface, which can be for example configured with
<a href=https://github.com/rootless-containers/slirp4netns>slirp4netns</a>,
<a href=https://github.com/moby/vpnkit>VPNKit</a>, or
<a href=https://www.man7.org/linux/man-pages/man1/lxc-user-nic.1.html>lxc-user-nic(1)</a>.</p>
<p>The network namespaces of the Pods can be configured with regular CNI plugins.
For multi-node networking, Flannel (VXLAN, 8472/UDP) is known to work.</p>
<p>Ports such as the kubelet port (10250/TCP) and <code>NodePort</code> service ports have to be exposed from the Node network namespace to
the host with an external port forwarder, such as RootlessKit, slirp4netns, or
<a href=https://linux.die.net/man/1/socat>socat(1)</a>.</p>
<p>You can use the port forwarder from K3s.
See <a href=https://rancher.com/docs/k3s/latest/en/advanced/#known-issues-with-rootless-mode>Running K3s in Rootless Mode</a>
for more details.
The implementation can be found in <a href=https://github.com/k3s-io/k3s/blob/v1.22.3+k3s1/pkg/rootlessports/controller.go>the <code>pkg/rootlessports</code> package</a> of k3s.</p>
<h3 id=configuring-cri>Configuring CRI</h3>
<p>The kubelet relies on a container runtime. You should deploy a container runtime such as
containerd or CRI-O and ensure that it is running within the user namespace before the kubelet starts.</p>
<ul class="nav nav-tabs" id=cri role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#cri-0 role=tab aria-controls=cri-0 aria-selected=true>containerd</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#cri-1 role=tab aria-controls=cri-1>CRI-O</a></li></ul>
<div class=tab-content id=cri><div id=cri-0 class="tab-pane show active" role=tabpanel aria-labelledby=cri-0>
<p><p>Running CRI plugin of containerd in a user namespace is supported since containerd 1.4.</p>
<p>Running containerd within a user namespace requires the following configurations.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml>version = <span style=color:#666>2</span>

[plugins.<span style=color:#b44>&#34;io.containerd.grpc.v1.cri&#34;</span>]
<span style=color:#080;font-style:italic># Disable AppArmor</span>
  disable_apparmor = <span style=color:#a2f;font-weight:700>true</span>
<span style=color:#080;font-style:italic># Ignore an error during setting oom_score_adj</span>
  restrict_oom_score_adj = <span style=color:#a2f;font-weight:700>true</span>
<span style=color:#080;font-style:italic># Disable hugetlb cgroup v2 controller (because systemd does not support delegating hugetlb controller)</span>
  disable_hugetlb_controller = <span style=color:#a2f;font-weight:700>true</span>

[plugins.<span style=color:#b44>&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd]
<span style=color:#080;font-style:italic># Using non-fuse overlayfs is also possible for kernel &gt;= 5.11, but requires SELinux to be disabled</span>
  snapshotter = <span style=color:#b44>&#34;fuse-overlayfs&#34;</span>

[plugins.<span style=color:#b44>&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc.options]
<span style=color:#080;font-style:italic># We use cgroupfs that is delegated by systemd, so we do not use SystemdCgroup driver</span>
<span style=color:#080;font-style:italic># (unless you run another systemd in the namespace)</span>
  SystemdCgroup = <span style=color:#a2f;font-weight:700>false</span>
</code></pre></div><p>The default path of the configuration file is <code>/etc/containerd/config.toml</code>.
The path can be specified with <code>containerd -c /path/to/containerd/config.toml</code>.</p>
</div>
<div id=cri-1 class=tab-pane role=tabpanel aria-labelledby=cri-1>
<p><p>Running CRI-O in a user namespace is supported since CRI-O 1.22.</p>
<p>CRI-O requires an environment variable <code>_CRIO_ROOTLESS=1</code> to be set.</p>
<p>The following configurations are also recommended:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml>[crio]
  storage_driver = <span style=color:#b44>&#34;overlay&#34;</span>
<span style=color:#080;font-style:italic># Using non-fuse overlayfs is also possible for kernel &gt;= 5.11, but requires SELinux to be disabled</span>
  storage_option = [<span style=color:#b44>&#34;overlay.mount_program=/usr/local/bin/fuse-overlayfs&#34;</span>]

[crio.runtime]
<span style=color:#080;font-style:italic># We use cgroupfs that is delegated by systemd, so we do not use &#34;systemd&#34; driver</span>
<span style=color:#080;font-style:italic># (unless you run another systemd in the namespace)</span>
  cgroup_manager = <span style=color:#b44>&#34;cgroupfs&#34;</span>
</code></pre></div><p>The default path of the configuration file is <code>/etc/crio/crio.conf</code>.
The path can be specified with <code>crio --config /path/to/crio/crio.conf</code>.</p>
</div></div>
<h3 id=configuring-kubelet>Configuring kubelet</h3>
<p>Running kubelet in a user namespace requires the following configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>featureGates</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>KubeletInUserNamespace</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># We use cgroupfs that is delegated by systemd, so we do not use &#34;systemd&#34; driver</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># (unless you run another systemd in the namespace)</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>cgroupDriver</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;cgroupfs&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>When the <code>KubeletInUserNamespace</code> feature gate is enabled, the kubelet ignores errors
that may happen during setting the following sysctl values on the node.</p>
<ul>
<li><code>vm.overcommit_memory</code></li>
<li><code>vm.panic_on_oom</code></li>
<li><code>kernel.panic</code></li>
<li><code>kernel.panic_on_oops</code></li>
<li><code>kernel.keys.root_maxkeys</code></li>
<li><code>kernel.keys.root_maxbytes</code>.</li>
</ul>
<p>Within a user namespace, the kubelet also ignores any error raised from trying to open <code>/dev/kmsg</code>.
This feature gate also allows kube-proxy to ignore an error during setting <code>RLIMIT_NOFILE</code>.</p>
<p>The <code>KubeletInUserNamespace</code> feature gate was introduced in Kubernetes v1.22 with "alpha" status.</p>
<p>Running kubelet in a user namespace without using this feature gate is also possible
by mounting a specially crafted proc filesystem (as done by <a href=https://github.com/nestybox/sysbox>Sysbox</a>), but not officially supported.</p>
<h3 id=configuring-kube-proxy>Configuring kube-proxy</h3>
<p>Running kube-proxy in a user namespace requires the following configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeproxy.config.k8s.io/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeProxyConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>mode</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;iptables&#34;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># or &#34;userspace&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>conntrack</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Skip setting sysctl value &#34;net.netfilter.nf_conntrack_max&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>maxPerCore</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Skip setting &#34;net.netfilter.nf_conntrack_tcp_timeout_established&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tcpEstablishedTimeout</span>:<span style=color:#bbb> </span>0s<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Skip setting &#34;net.netfilter.nf_conntrack_tcp_timeout_close&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tcpCloseWaitTimeout</span>:<span style=color:#bbb> </span>0s<span style=color:#bbb>
</span></code></pre></div><h2 id=caveats>Caveats</h2>
<ul>
<li>
<p>Most of "non-local" volume drivers such as <code>nfs</code> and <code>iscsi</code> do not work.
Local volumes like <code>local</code>, <code>hostPath</code>, <code>emptyDir</code>, <code>configMap</code>, <code>secret</code>, and <code>downwardAPI</code> are known to work.</p>
</li>
<li>
<p>Some CNI plugins may not work. Flannel (VXLAN) is known to work.</p>
</li>
</ul>
<p>For more on this, see the <a href=https://rootlesscontaine.rs/caveats/>Caveats and Future work</a> page
on the rootlesscontaine.rs website.</p>
<h2 id=see-also>See Also</h2>
<ul>
<li><a href=https://rootlesscontaine.rs/>rootlesscontaine.rs</a></li>
<li><a href=https://www.slideshare.net/AkihiroSuda/kubecon-na-2020-containerd-rootless-containers-2020>Rootless Containers 2020 (KubeCon NA 2020)</a></li>
<li><a href=https://kind.sigs.k8s.io/docs/user/rootless/>Running kind with Rootless Docker</a></li>
<li><a href=https://github.com/rootless-containers/usernetes>Usernetes</a></li>
<li><a href=https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental>Running K3s with rootless mode</a></li>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2033-kubelet-in-userns-aka-rootless>KEP-2033: Kubelet-in-UserNS (aka Rootless mode)</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-b35b8ddb9bbc15620ce9636f4346c05c>31 - Safely Drain a Node</h1>
<p>This page shows how to safely drain a <a class=glossary-tooltip title="A node is a worker machine in Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a>,
optionally respecting the PodDisruptionBudget you have defined.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>Your Kubernetes server must be at or later than version 1.5.
To check the version, enter <code>kubectl version</code>.</p>
<p>This task also assumes that you have met the following prerequisites:</p>
<ol>
<li>You do not require your applications to be highly available during the
node drain, or</li>
<li>You have read about the <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> concept,
and have <a href=/docs/tasks/run-application/configure-pdb/>configured PodDisruptionBudgets</a> for
applications that need them.</li>
</ol>
<h2 id=configure-poddisruptionbudget>(Optional) Configure a disruption budget</h2>
<p>To ensure that your workloads remain available during maintenance, you can
configure a <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a>.</p>
<p>If availability is important for any applications that run or could run on the node(s)
that you are draining, <a href=/docs/tasks/run-application/configure-pdb/>configure a PodDisruptionBudgets</a>
first and then continue following this guide.</p>
<h2 id=use-kubectl-drain-to-remove-a-node-from-service>Use <code>kubectl drain</code> to remove a node from service</h2>
<p>You can use <code>kubectl drain</code> to safely evict all of your pods from a
node before you perform maintenance on the node (e.g. kernel upgrade,
hardware maintenance, etc.). Safe evictions allow the pod's containers
to <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>gracefully terminate</a>
and will respect the PodDisruptionBudgets you have specified.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> By default <code>kubectl drain</code> ignores certain system pods on the node
that cannot be killed; see
the <a href=/docs/reference/generated/kubectl/kubectl-commands/#drain>kubectl drain</a>
documentation for more details.
</div>
<p>When <code>kubectl drain</code> returns successfully, that indicates that all of
the pods (except the ones excluded as described in the previous paragraph)
have been safely evicted (respecting the desired graceful termination period,
and respecting the PodDisruptionBudget you have defined). It is then safe to
bring down the node by powering down its physical machine or, if running on a
cloud platform, deleting its virtual machine.</p>
<p>First, identify the name of the node you wish to drain. You can list all of the nodes in your cluster with</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes
</code></pre></div><p>Next, tell Kubernetes to drain the node:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl drain &lt;node name&gt;
</code></pre></div><p>Once it returns (without giving an error), you can power down the node
(or equivalently, if on a cloud platform, delete the virtual machine backing the node).
If you leave the node in the cluster during the maintenance operation, you need to run</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl uncordon &lt;node name&gt;
</code></pre></div><p>afterwards to tell Kubernetes that it can resume scheduling new pods onto the node.</p>
<h2 id=draining-multiple-nodes-in-parallel>Draining multiple nodes in parallel</h2>
<p>The <code>kubectl drain</code> command should only be issued to a single node at a
time. However, you can run multiple <code>kubectl drain</code> commands for
different nodes in parallel, in different terminals or in the
background. Multiple drain commands running concurrently will still
respect the PodDisruptionBudget you specify.</p>
<p>For example, if you have a StatefulSet with three replicas and have
set a PodDisruptionBudget for that set specifying <code>minAvailable: 2</code>,
<code>kubectl drain</code> only evicts a pod from the StatefulSet if all three
replicas pods are ready; if then you issue multiple drain commands in
parallel, Kubernetes respects the PodDisruptionBudget and ensure
that only 1 (calculated as <code>replicas - minAvailable</code>) Pod is unavailable
at any given time. Any drains that would cause the number of ready
replicas to fall below the specified budget are blocked.</p>
<h2 id=eviction-api>The Eviction API</h2>
<p>If you prefer not to use <a href=/docs/reference/generated/kubectl/kubectl-commands/#drain>kubectl drain</a> (such as
to avoid calling to an external command, or to get finer control over the pod
eviction process), you can also programmatically cause evictions using the
eviction API.</p>
<p>For more information, see <a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated eviction</a>.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Follow steps to protect your application by <a href=/docs/tasks/run-application/configure-pdb/>configuring a Pod Disruption Budget</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-12001be83d15fcd7f3242313a55777df>32 - Securing a Cluster</h1>
<p>This document covers topics related to protecting a cluster from accidental or malicious access
and provides recommendations on overall security.</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</li>
</ul>
<h2 id=controlling-access-to-the-kubernetes-api>Controlling access to the Kubernetes API</h2>
<p>As Kubernetes is entirely API-driven, controlling and limiting who can access the cluster and what actions
they are allowed to perform is the first line of defense.</p>
<h3 id=use-transport-layer-security-tls-for-all-api-traffic>Use Transport Layer Security (TLS) for all API traffic</h3>
<p>Kubernetes expects that all API communication in the cluster is encrypted by default with TLS, and the
majority of installation methods will allow the necessary certificates to be created and distributed to
the cluster components. Note that some components and installation methods may enable local ports over
HTTP and administrators should familiarize themselves with the settings of each component to identify
potentially unsecured traffic.</p>
<h3 id=api-authentication>API Authentication</h3>
<p>Choose an authentication mechanism for the API servers to use that matches the common access patterns
when you install a cluster. For instance, small, single-user clusters may wish to use a simple certificate
or static Bearer token approach. Larger clusters may wish to integrate an existing OIDC or LDAP server that
allow users to be subdivided into groups.</p>
<p>All API clients must be authenticated, even those that are part of the infrastructure like nodes,
proxies, the scheduler, and volume plugins. These clients are typically <a href=/docs/reference/access-authn-authz/service-accounts-admin/>service accounts</a> or use x509 client certificates, and they are created automatically at cluster startup or are setup as part of the cluster installation.</p>
<p>Consult the <a href=/docs/reference/access-authn-authz/authentication/>authentication reference document</a> for more information.</p>
<h3 id=api-authorization>API Authorization</h3>
<p>Once authenticated, every API call is also expected to pass an authorization check. Kubernetes ships
an integrated <a href=/docs/reference/access-authn-authz/rbac/>Role-Based Access Control (RBAC)</a> component that matches an incoming user or group to a
set of permissions bundled into roles. These permissions combine verbs (get, create, delete) with
resources (pods, services, nodes) and can be namespace-scoped or cluster-scoped. A set of out-of-the-box
roles are provided that offer reasonable default separation of responsibility depending on what
actions a client might want to perform. It is recommended that you use the
<a href=/docs/reference/access-authn-authz/node/>Node</a> and
<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a> authorizers together, in combination with the
<a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction</a> admission plugin.</p>
<p>As with authentication, simple and broad roles may be appropriate for smaller clusters, but as
more users interact with the cluster, it may become necessary to separate teams into separate
<a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespaces>namespaces</a> with more limited roles.</p>
<p>With authorization, it is important to understand how updates on one object may cause actions in
other places. For instance, a user may not be able to create pods directly, but allowing them to
create a deployment, which creates pods on their behalf, will let them create those pods
indirectly. Likewise, deleting a node from the API will result in the pods scheduled to that node
being terminated and recreated on other nodes. The out-of-the box roles represent a balance
between flexibility and common use cases, but more limited roles should be carefully reviewed
to prevent accidental escalation. You can make roles specific to your use case if the out-of-box ones don't meet your needs.</p>
<p>Consult the <a href=/docs/reference/access-authn-authz/authorization/>authorization reference section</a> for more information.</p>
<h2 id=controlling-access-to-the-kubelet>Controlling access to the Kubelet</h2>
<p>Kubelets expose HTTPS endpoints which grant powerful control over the node and containers. By default Kubelets allow unauthenticated access to this API.</p>
<p>Production clusters should enable Kubelet authentication and authorization.</p>
<p>Consult the <a href=/docs/reference/command-line-tools-reference/kubelet-authentication-authorization>Kubelet authentication/authorization reference</a> for more information.</p>
<h2 id=controlling-the-capabilities-of-a-workload-or-user-at-runtime>Controlling the capabilities of a workload or user at runtime</h2>
<p>Authorization in Kubernetes is intentionally high level, focused on coarse actions on resources.
More powerful controls exist as <strong>policies</strong> to limit by use case how those objects act on the
cluster, themselves, and other resources.</p>
<h3 id=limiting-resource-usage-on-a-cluster>Limiting resource usage on a cluster</h3>
<p><a href=/docs/concepts/policy/resource-quotas/>Resource quota</a> limits the number or capacity of
resources granted to a namespace. This is most often used to limit the amount of CPU, memory,
or persistent disk a namespace can allocate, but can also control how many pods, services, or
volumes exist in each namespace.</p>
<p><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>Limit ranges</a> restrict the maximum or minimum size of some of the
resources above, to prevent users from requesting unreasonably high or low values for commonly
reserved resources like memory, or to provide default limits when none are specified.</p>
<h3 id=controlling-what-privileges-containers-run-with>Controlling what privileges containers run with</h3>
<p>A pod definition contains a <a href=/docs/tasks/configure-pod-container/security-context/>security context</a>
that allows it to request access to run as a specific Linux user on a node (like root),
access to run privileged or access the host network, and other controls that would otherwise
allow it to run unfettered on a hosting node.</p>
<p>You can configure <a href=/docs/concepts/security/pod-security-admission/>Pod security admission</a>
to enforce use of a particular <a href=/docs/concepts/security/pod-security-standards/>Pod Security Standard</a>
in a <a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>, or to detect breaches.</p>
<p>Generally, most application workloads need limited access to host resources so they can
successfully run as a root process (uid 0) without access to host information. However,
considering the privileges associated with the root user, you should write application
containers to run as a non-root user. Similarly, administrators who wish to prevent
client applications from escaping their containers should apply the <strong>Baseline</strong>
or <strong>Restricted</strong> Pod Security Standard.</p>
<h3 id=preventing-containers-from-loading-unwanted-kernel-modules>Preventing containers from loading unwanted kernel modules</h3>
<p>The Linux kernel automatically loads kernel modules from disk if needed in certain
circumstances, such as when a piece of hardware is attached or a filesystem is mounted. Of
particular relevance to Kubernetes, even unprivileged processes can cause certain
network-protocol-related kernel modules to be loaded, just by creating a socket of the
appropriate type. This may allow an attacker to exploit a security hole in a kernel module
that the administrator assumed was not in use.</p>
<p>To prevent specific modules from being automatically loaded, you can uninstall them from
the node, or add rules to block them. On most Linux distributions, you can do that by
creating a file such as <code>/etc/modprobe.d/kubernetes-blacklist.conf</code> with contents like:</p>
<pre><code># DCCP is unlikely to be needed, has had multiple serious
# vulnerabilities, and is not well-maintained.
blacklist dccp

# SCTP is not used in most Kubernetes clusters, and has also had
# vulnerabilities in the past.
blacklist sctp
</code></pre><p>To block module loading more generically, you can use a Linux Security Module (such as
SELinux) to completely deny the <code>module_request</code> permission to containers, preventing the
kernel from loading modules for containers under any circumstances. (Pods would still be
able to use modules that had been loaded manually, or modules that were loaded by the
kernel on behalf of some more-privileged process.)</p>
<h3 id=restricting-network-access>Restricting network access</h3>
<p>The <a href=/docs/tasks/administer-cluster/declare-network-policy/>network policies</a> for a namespace
allows application authors to restrict which pods in other namespaces may access pods and ports
within their namespaces. Many of the supported <a href=/docs/concepts/cluster-administration/networking/>Kubernetes networking providers</a>
now respect network policy.</p>
<p>Quota and limit ranges can also be used to control whether users may request node ports or
load-balanced services, which on many clusters can control whether those users applications
are visible outside of the cluster.</p>
<p>Additional protections may be available that control network rules on a per-plugin or per-
environment basis, such as per-node firewalls, physically separating cluster nodes to
prevent cross talk, or advanced networking policy.</p>
<h3 id=restricting-cloud-metadata-api-access>Restricting cloud metadata API access</h3>
<p>Cloud platforms (AWS, Azure, GCE, etc.) often expose metadata services locally to instances.
By default these APIs are accessible by pods running on an instance and can contain cloud
credentials for that node, or provisioning data such as kubelet credentials. These credentials
can be used to escalate within the cluster or to other cloud services under the same account.</p>
<p>When running Kubernetes on a cloud platform, limit permissions given to instance credentials, use
<a href=/docs/tasks/administer-cluster/declare-network-policy/>network policies</a> to restrict pod access
to the metadata API, and avoid using provisioning data to deliver secrets.</p>
<h3 id=controlling-which-nodes-pods-may-access>Controlling which nodes pods may access</h3>
<p>By default, there are no restrictions on which nodes may run a pod. Kubernetes offers a
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/>rich set of policies for controlling placement of pods onto nodes</a>
and the <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taint-based pod placement and eviction</a>
that are available to end users. For many clusters use of these policies to separate workloads
can be a convention that authors adopt or enforce via tooling.</p>
<p>As an administrator, a beta admission plugin <code>PodNodeSelector</code> can be used to force pods
within a namespace to default or require a specific node selector, and if end users cannot
alter namespaces, this can strongly limit the placement of all of the pods in a specific workload.</p>
<h2 id=protecting-cluster-components-from-compromise>Protecting cluster components from compromise</h2>
<p>This section describes some common patterns for protecting clusters from compromise.</p>
<h3 id=restrict-access-to-etcd>Restrict access to etcd</h3>
<p>Write access to the etcd backend for the API is equivalent to gaining root on the entire cluster,
and read access can be used to escalate fairly quickly. Administrators should always use strong
credentials from the API servers to their etcd server, such as mutual auth via TLS client certificates,
and it is often recommended to isolate the etcd servers behind a firewall that only the API servers
may access.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> Allowing other components within the cluster to access the master etcd instance with
read or write access to the full keyspace is equivalent to granting cluster-admin access. Using
separate etcd instances for non-master components or using etcd ACLs to restrict read and write
access to a subset of the keyspace is strongly recommended.
</div>
<h3 id=enable-audit-logging>Enable audit logging</h3>
<p>The <a href=/docs/tasks/debug/debug-cluster/audit/>audit logger</a> is a beta feature that records actions taken by the
API for later analysis in the event of a compromise. It is recommended to enable audit logging
and archive the audit file on a secure server.</p>
<h3 id=restrict-access-to-alpha-or-beta-features>Restrict access to alpha or beta features</h3>
<p>Alpha and beta Kubernetes features are in active development and may have limitations or bugs
that result in security vulnerabilities. Always assess the value an alpha or beta feature may
provide against the possible risk to your security posture. When in doubt, disable features you
do not use.</p>
<h3 id=rotate-infrastructure-credentials-frequently>Rotate infrastructure credentials frequently</h3>
<p>The shorter the lifetime of a secret or credential the harder it is for an attacker to make
use of that credential. Set short lifetimes on certificates and automate their rotation. Use
an authentication provider that can control how long issued tokens are available and use short
lifetimes where possible. If you use service-account tokens in external integrations, plan to
rotate those tokens frequently. For example, once the bootstrap phase is complete, a bootstrap
token used for setting up nodes should be revoked or its authorization removed.</p>
<h3 id=review-third-party-integrations-before-enabling-them>Review third party integrations before enabling them</h3>
<p>Many third party integrations to Kubernetes may alter the security profile of your cluster. When
enabling an integration, always review the permissions that an extension requests before granting
it access. For example, many security integrations may request access to view all secrets on
your cluster which is effectively making that component a cluster admin. When in doubt,
restrict the integration to functioning in a single namespace if possible.</p>
<p>Components that create pods may also be unexpectedly powerful if they can do so inside namespaces
like the <code>kube-system</code> namespace, because those pods can gain access to service account secrets
or run with elevated permissions if those service accounts are granted access to permissive
<a href=/docs/concepts/security/pod-security-policy/>PodSecurityPolicies</a>.</p>
<p>If you use <a href=/docs/concepts/security/pod-security-admission/>Pod Security admission</a> and allow
any component to create Pods within a namespace that permits privileged Pods, those Pods may
be able to escape their containers and use this widened access to elevate their privileges.</p>
<p>You should not allow untrusted components to create Pods in any system namespace (those with
names that start with <code>kube-</code>) nor in any namespace where that access grant allows the possibility
of privilege escalation.</p>
<h3 id=encrypt-secrets-at-rest>Encrypt secrets at rest</h3>
<p>In general, the etcd database will contain any information accessible via the Kubernetes API
and may grant an attacker significant visibility into the state of your cluster. Always encrypt
your backups using a well reviewed backup and encryption solution, and consider using full disk
encryption where possible.</p>
<p>Kubernetes supports <a href=/docs/tasks/administer-cluster/encrypt-data/>encryption at rest</a>, a feature
introduced in 1.7, and beta since 1.13. This will encrypt <code>Secret</code> resources in etcd, preventing
parties that gain access to your etcd backups from viewing the content of those secrets. While
this feature is currently beta, it offers an additional level of defense when backups
are not encrypted or an attacker gains read access to etcd.</p>
<h3 id=receiving-alerts-for-security-updates-and-reporting-vulnerabilities>Receiving alerts for security updates and reporting vulnerabilities</h3>
<p>Join the <a href=https://groups.google.com/forum/#!forum/kubernetes-announce>kubernetes-announce</a>
group for emails about security announcements. See the
<a href=/docs/reference/issues-security/security/>security reporting</a>
page for more on how to report vulnerabilities.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-f58763cc9447491b6c40f939a02d441d>33 - Set Kubelet parameters via a config file</h1>
<p>A subset of the Kubelet's configuration parameters may be
set via an on-disk config file, as a substitute for command-line flags.</p>
<p>Providing parameters via a config file is the recommended approach because
it simplifies node deployment and configuration management.</p>
<h2 id=create-the-config-file>Create the config file</h2>
<p>The subset of the Kubelet's configuration that can be configured via a file
is defined by the
<a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>
struct.</p>
<p>The configuration file must be a JSON or YAML representation of the parameters
in this struct. Make sure the Kubelet has read permissions on the file.</p>
<p>Here is an example of what this file might look like:</p>
<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: &quot;192.168.0.8&quot;,
port: 20250,
serializeImagePulls: false,
evictionHard:
    memory.available:  &quot;200Mi&quot;
</code></pre><p>In the example, the Kubelet is configured to serve on IP address 192.168.0.8 and port 20250, pull images in parallel,
and evict Pods when available memory drops below 200Mi.
All other Kubelet configuration values are left at their built-in defaults, unless overridden
by flags. Command line flags which target the same value as a config file will override that value.</p>
<h2 id=start-a-kubelet-process-configured-via-the-config-file>Start a Kubelet process configured via the config file</h2>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If you use kubeadm to initialize your cluster, use the kubelet-config while creating your cluster with <code>kubeadmin init</code>.
See <a href=/docs/setup/production-environment/tools/kubeadm/kubelet-integration/>configuring kubelet using kubeadm</a> for details.
</div>
<p>Start the Kubelet with the <code>--config</code> flag set to the path of the Kubelet's config file.
The Kubelet will then load its config from this file.</p>
<p>Note that command line flags which target the same value as a config file will override that value.
This helps ensure backwards compatibility with the command-line API.</p>
<p>Note that relative file paths in the Kubelet config file are resolved relative to the
location of the Kubelet config file, whereas relative paths in command line flags are resolved
relative to the Kubelet's current working directory.</p>
<p>Note that some default values differ between command-line flags and the Kubelet config file.
If <code>--config</code> is provided and the values are not specified via the command line, the
defaults for the <code>KubeletConfiguration</code> version apply.
In the above example, this version is <code>kubelet.config.k8s.io/v1beta1</code>.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Learn more about kubelet configuration by checking the
<a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>
reference.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1e966f5d0540bbee0876f9d0d08d54dc>34 - Share a Cluster with Namespaces</h1>
<p>This page shows how to view, work in, and delete <a class=glossary-tooltip title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespaces>namespaces</a>. The page also shows how to use Kubernetes namespaces to subdivide your cluster.</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>Have an <a href=/docs/setup/>existing Kubernetes cluster</a>.</li>
<li>You have a basic understanding of Kubernetes <a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>, <a class=glossary-tooltip title="A way to expose an application running on a set of Pods as a network service." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Services>Services</a>, and <a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployments>Deployments</a>.</li>
</ul>
<h2 id=viewing-namespaces>Viewing namespaces</h2>
<ol>
<li>List the current namespaces in a cluster using:</li>
</ol>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get namespaces
</code></pre></div><pre><code>NAME          STATUS    AGE
default       Active    11d
kube-system   Active    11d
kube-public   Active    11d
</code></pre><p>Kubernetes starts with three initial namespaces:</p>
<ul>
<li><code>default</code> The default namespace for objects with no other namespace</li>
<li><code>kube-system</code> The namespace for objects created by the Kubernetes system</li>
<li><code>kube-public</code> This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.</li>
</ul>
<p>You can also get the summary of a specific namespace using:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get namespaces &lt;name&gt;
</code></pre></div><p>Or you can get detailed information with:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe namespaces &lt;name&gt;
</code></pre></div><pre><code>Name:           default
Labels:         &lt;none&gt;
Annotations:    &lt;none&gt;
Status:         Active

No resource quota.

Resource Limits
 Type       Resource    Min Max Default
 ----               --------    --- --- ---
 Container          cpu         -   -   100m
</code></pre><p>Note that these details show both resource quota (if present) as well as resource limit ranges.</p>
<p>Resource quota tracks aggregate usage of resources in the <em>Namespace</em> and allows cluster operators
to define <em>Hard</em> resource usage limits that a <em>Namespace</em> may consume.</p>
<p>A limit range defines min/max constraints on the amount of resources a single entity can consume in
a <em>Namespace</em>.</p>
<p>See <a href=https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md>Admission control: Limit Range</a></p>
<p>A namespace can be in one of two phases:</p>
<ul>
<li><code>Active</code> the namespace is in use</li>
<li><code>Terminating</code> the namespace is being deleted, and can not be used for new objects</li>
</ul>
<p>For more details, see <a href=/docs/reference/kubernetes-api/cluster-resources/namespace-v1/>Namespace</a>
in the API reference.</p>
<h2 id=creating-a-new-namespace>Creating a new namespace</h2>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Avoid creating namespace with prefix <code>kube-</code>, since it is reserved for Kubernetes system namespaces.
</div>
<ol>
<li>
<p>Create a new YAML file called <code>my-namespace.yaml</code> with the contents:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Namespace<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>&lt;insert-namespace-name-here&gt;<span style=color:#bbb>
</span></code></pre></div><p>Then run:</p>
<pre><code>kubectl create -f ./my-namespace.yaml
</code></pre></li>
<li>
<p>Alternatively, you can create namespace using below command:</p>
<pre><code>kubectl create namespace &lt;insert-namespace-name-here&gt;
</code></pre></li>
</ol>
<p>The name of your namespace must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-label-names>DNS label</a>.</p>
<p>There's an optional field <code>finalizers</code>, which allows observables to purge resources whenever the namespace is deleted. Keep in mind that if you specify a nonexistent finalizer, the namespace will be created but will get stuck in the <code>Terminating</code> state if the user tries to delete it.</p>
<p>More information on <code>finalizers</code> can be found in the namespace <a href=https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#finalizers>design doc</a>.</p>
<h2 id=deleting-a-namespace>Deleting a namespace</h2>
<p>Delete a namespace with</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete namespaces &lt;insert-some-namespace-name&gt;
</code></pre></div><div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> This deletes <em>everything</em> under the namespace!
</div>
<p>This delete is asynchronous, so for a time you will see the namespace in the <code>Terminating</code> state.</p>
<h2 id=subdividing-your-cluster-using-kubernetes-namespaces>Subdividing your cluster using Kubernetes namespaces</h2>
<ol>
<li>
<p>Understand the default namespace</p>
<p>By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods,
Services, and Deployments used by the cluster.</p>
<p>Assuming you have a fresh cluster, you can introspect the available namespaces by doing the following:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get namespaces
</code></pre></div><pre><code>NAME      STATUS    AGE
default   Active    13m
</code></pre></li>
<li>
<p>Create new namespaces</p>
<p>For this exercise, we will create two additional Kubernetes namespaces to hold our content.</p>
<p>In a scenario where an organization is using a shared Kubernetes cluster for development and production use cases:</p>
<p>The development team would like to maintain a space in the cluster where they can get a view on the list of Pods, Services, and Deployments
they use to build and run their application. In this space, Kubernetes resources come and go, and the restrictions on who can or cannot modify resources
are relaxed to enable agile development.</p>
<p>The operations team would like to maintain a space in the cluster where they can enforce strict procedures on who can or cannot manipulate the set of
Pods, Services, and Deployments that run the production site.</p>
<p>One pattern this organization could follow is to partition the Kubernetes cluster into two namespaces: <code>development</code> and <code>production</code>.</p>
<p>Let's create two new namespaces to hold our work.</p>
<p>Create the <code>development</code> namespace using kubectl:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</code></pre></div><p>And then let's create the <code>production</code> namespace using kubectl:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</code></pre></div><p>To be sure things are right, list all of the namespaces in our cluster.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get namespaces --show-labels
</code></pre></div><pre><code>NAME          STATUS    AGE       LABELS
default       Active    32m       &lt;none&gt;
development   Active    29s       name=development
production    Active    23s       name=production
</code></pre></li>
<li>
<p>Create pods in each namespace</p>
<p>A Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster.</p>
<p>Users interacting with one namespace do not see the content in another namespace.</p>
<p>To demonstrate this, let's spin up a simple Deployment and Pods in the <code>development</code> namespace.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create deployment snowflake --image<span style=color:#666>=</span>k8s.gcr.io/serve_hostname  -n<span style=color:#666>=</span>development --replicas<span style=color:#666>=</span><span style=color:#666>2</span>
</code></pre></div><p>We have created a deployment whose replica size is 2 that is running the pod called <code>snowflake</code> with a basic container that serves the hostname.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment -n<span style=color:#666>=</span>development
</code></pre></div><pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
snowflake    2/2     2            2           2m
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>snowflake -n<span style=color:#666>=</span>development
</code></pre></div><pre><code>NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
</code></pre><p>And this is great, developers are able to do what they want, and they do not have to worry about affecting content in the <code>production</code> namespace.</p>
<p>Let's switch to the <code>production</code> namespace and show how resources in one namespace are hidden from the other.</p>
<p>The <code>production</code> namespace should be empty, and the following commands should return nothing.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment -n<span style=color:#666>=</span>production
kubectl get pods -n<span style=color:#666>=</span>production
</code></pre></div><p>Production likes to run cattle, so let's create some cattle pods.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create deployment cattle --image<span style=color:#666>=</span>k8s.gcr.io/serve_hostname -n<span style=color:#666>=</span>production
kubectl scale deployment cattle --replicas<span style=color:#666>=</span><span style=color:#666>5</span> -n<span style=color:#666>=</span>production

kubectl get deployment -n<span style=color:#666>=</span>production
</code></pre></div><pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
cattle       5/5     5            5           10s
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>cattle -n<span style=color:#666>=</span>production
</code></pre></div><pre><code>NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
</code></pre></li>
</ol>
<p>At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.</p>
<p>As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different
authorization rules for each namespace.</p>
<h2 id=understanding-the-motivation-for-using-namespaces>Understanding the motivation for using namespaces</h2>
<p>A single cluster should be able to satisfy the needs of multiple users or groups of users (henceforth a 'user community').</p>
<p>Kubernetes <em>namespaces</em> help different projects, teams, or customers to share a Kubernetes cluster.</p>
<p>It does this by providing the following:</p>
<ol>
<li>A scope for <a href=/docs/concepts/overview/working-with-objects/names/>Names</a>.</li>
<li>A mechanism to attach authorization and policy to a subsection of the cluster.</li>
</ol>
<p>Use of multiple namespaces is optional.</p>
<p>Each user community wants to be able to work in isolation from other communities.</p>
<p>Each user community has its own:</p>
<ol>
<li>resources (pods, services, replication controllers, etc.)</li>
<li>policies (who can or cannot perform actions in their community)</li>
<li>constraints (this community is allowed this much quota, etc.)</li>
</ol>
<p>A cluster operator may create a Namespace for each unique user community.</p>
<p>The Namespace provides a unique scope for:</p>
<ol>
<li>named resources (to avoid basic naming collisions)</li>
<li>delegated management authority to trusted users</li>
<li>ability to limit community resource consumption</li>
</ol>
<p>Use cases include:</p>
<ol>
<li>As a cluster operator, I want to support multiple user communities on a single cluster.</li>
<li>As a cluster operator, I want to delegate authority to partitions of the cluster to trusted users
in those communities.</li>
<li>As a cluster operator, I want to limit the amount of resources each community can consume in order
to limit the impact to other communities using the cluster.</li>
<li>As a cluster user, I want to interact with resources that are pertinent to my user community in
isolation of what other user communities are doing on the cluster.</li>
</ol>
<h2 id=understanding-namespaces-and-dns>Understanding namespaces and DNS</h2>
<p>When you create a <a href=/docs/concepts/services-networking/service/>Service</a>, it creates a corresponding <a href=/docs/concepts/services-networking/dns-pod-service/>DNS entry</a>.
This entry is of the form <code>&lt;service-name>.&lt;namespace-name>.svc.cluster.local</code>, which means
that if a container uses <code>&lt;service-name></code> it will resolve to the service which
is local to a namespace. This is useful for using the same configuration across
multiple namespaces such as Development, Staging and Production. If you want to reach
across namespaces, you need to use the fully qualified domain name (FQDN).</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Learn more about <a href=/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference>setting the namespace preference</a>.</li>
<li>Learn more about <a href=/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-for-a-request>setting the namespace for a request</a></li>
<li>See <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/namespaces.md>namespaces design</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-fe6b50655c29ab0b7c1ee549ff64c138>35 - Upgrade A Cluster</h1>
<p>This page provides an overview of the steps you should follow to upgrade a
Kubernetes cluster.</p>
<p>The way that you upgrade a cluster depends on how you initially deployed it
and on any subsequent changes.</p>
<p>At a high level, the steps you perform are:</p>
<ul>
<li>Upgrade the <a class=glossary-tooltip title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a></li>
<li>Upgrade the nodes in your cluster</li>
<li>Upgrade clients such as <a class=glossary-tooltip title="A command line tool for communicating with a Kubernetes cluster." data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a></li>
<li>Adjust manifests and other resources based on the API changes that accompany the
new Kubernetes version</li>
</ul>
<h2 id=before-you-begin>Before you begin</h2>
<p>You must have an existing cluster. This page is about upgrading from Kubernetes
1.22 to Kubernetes 1.23. If your cluster
is not currently running Kubernetes 1.22 then please check
the documentation for the version of Kubernetes that you plan to upgrade to.</p>
<h2 id=upgrade-approaches>Upgrade approaches</h2>
<h3 id=upgrade-kubeadm>kubeadm</h3>
<p>If your cluster was deployed using the <code>kubeadm</code> tool, refer to
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading kubeadm clusters</a>
for detailed information on how to upgrade the cluster.</p>
<p>Once you have upgraded the cluster, remember to
<a href=/docs/tasks/tools/>install the latest version of <code>kubectl</code></a>.</p>
<h3 id=manual-deployments>Manual deployments</h3>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> These steps do not account for third-party extensions such as network and storage
plugins.
</div>
<p>You should manually update the control plane following this sequence:</p>
<ul>
<li>etcd (all instances)</li>
<li>kube-apiserver (all control plane hosts)</li>
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
<li>cloud controller manager, if you use one</li>
</ul>
<p>At this point you should
<a href=/docs/tasks/tools/>install the latest version of <code>kubectl</code></a>.</p>
<p>For each node in your cluster, <a href=/docs/tasks/administer-cluster/safely-drain-node/>drain</a>
that node and then either replace it with a new node that uses the 1.23
kubelet, or upgrade the kubelet on that node and bring the node back into service.</p>
<h3 id=upgrade-other>Other deployments</h3>
<p>Refer to the documentation for your cluster deployment tool to learn the recommended set
up steps for maintenance.</p>
<h2 id=post-upgrade-tasks>Post-upgrade tasks</h2>
<h3 id=switch-your-cluster-s-storage-api-version>Switch your cluster's storage API version</h3>
<p>The objects that are serialized into etcd for a cluster's internal
representation of the Kubernetes resources active in the cluster are
written using a particular version of the API.</p>
<p>When the supported API changes, these objects may need to be rewritten
in the newer API. Failure to do this will eventually result in resources
that are no longer decodable or usable by the Kubernetes API server.</p>
<p>For each affected object, fetch it using the latest supported API and then
write it back also using the latest supported API.</p>
<h3 id=update-manifests>Update manifests</h3>
<p>Upgrading to a new Kubernetes version can provide new APIs.</p>
<p>You can use <code>kubectl convert</code> command to convert manifests between different API versions.
For example:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl convert -f pod.yaml --output-version v1
</code></pre></div><p>The <code>kubectl</code> tool replaces the contents of <code>pod.yaml</code> with a manifest that sets <code>kind</code> to
Pod (unchanged), but with a revised <code>apiVersion</code>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-4e9de5bc3973e5d2bb8f09ff940c3319>36 - Use Cascading Deletion in a Cluster</h1>
<p>This page shows you how to specify the type of
<a href=/docs/concepts/architecture/garbage-collection/#cascading-deletion>cascading deletion</a>
to use in your cluster during <a class=glossary-tooltip title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/garbage-collection/ target=_blank aria-label="garbage collection">garbage collection</a>.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>You also need to <a href=/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment>create a sample Deployment</a>
to experiment with the different types of cascading deletion. You will need to
recreate the Deployment for each type.</p>
<h2 id=check-owner-references-on-your-pods>Check owner references on your pods</h2>
<p>Check that the <code>ownerReferences</code> field is present on your pods:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx --output<span style=color:#666>=</span>yaml
</code></pre></div><p>The output has an <code>ownerReferences</code> field similar to this:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>    </span>...<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ownerReferences</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>blockOwnerDeletion</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>controller</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-deployment-6b474476c4<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>uid</span>:<span style=color:#bbb> </span>4fdcd81c-bd5d-41f7-97af-3a3b759af9a7<span style=color:#bbb>
</span><span style=color:#bbb>    </span>...<span style=color:#bbb>
</span></code></pre></div><h2 id=use-foreground-cascading-deletion>Use foreground cascading deletion</h2>
<p>By default, Kubernetes uses <a href=/docs/concepts/architecture/garbage-collection/#background-deletion>background cascading deletion</a>
to delete dependents of an object. You can switch to foreground cascading deletion
using either <code>kubectl</code> or the Kubernetes API, depending on the Kubernetes
version your cluster runs.
To check the version, enter <code>kubectl version</code>.
</p>
<ul class="nav nav-tabs" id=foreground-deletion role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#foreground-deletion-0 role=tab aria-controls=foreground-deletion-0 aria-selected=true>Kubernetes 1.20.x and later</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#foreground-deletion-1 role=tab aria-controls=foreground-deletion-1>Versions prior to Kubernetes 1.20.x</a></li></ul>
<div class=tab-content id=foreground-deletion><div id=foreground-deletion-0 class="tab-pane show active" role=tabpanel aria-labelledby=foreground-deletion-0>
<p><p>You can delete objects using foreground cascading deletion using <code>kubectl</code> or the
Kubernetes API.</p>
<p><strong>Using kubectl</strong></p>
<p>Run the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployment nginx-deployment --cascade<span style=color:#666>=</span>foreground
</code></pre></div><p><strong>Using the Kubernetes API</strong></p>
<ol>
<li>
<p>Start a local proxy session:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</code></pre></div></li>
<li>
<p>Use <code>curl</code> to trigger deletion:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</code></pre></div><p>The output contains a <code>foregroundDeletion</code> <a class=glossary-tooltip title="A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/finalizers/ target=_blank aria-label=finalizer>finalizer</a>
like this:</p>
<pre><code>&quot;kind&quot;: &quot;Deployment&quot;,
&quot;apiVersion&quot;: &quot;apps/v1&quot;,
&quot;metadata&quot;: {
    &quot;name&quot;: &quot;nginx-deployment&quot;,
    &quot;namespace&quot;: &quot;default&quot;,
    &quot;uid&quot;: &quot;d1ce1b02-cae8-4288-8a53-30e84d8fa505&quot;,
    &quot;resourceVersion&quot;: &quot;1363097&quot;,
    &quot;creationTimestamp&quot;: &quot;2021-07-08T20:24:37Z&quot;,
    &quot;deletionTimestamp&quot;: &quot;2021-07-08T20:27:39Z&quot;,
    &quot;finalizers&quot;: [
      &quot;foregroundDeletion&quot;
    ]
    ...
</code></pre></li>
</ol>
</div>
<div id=foreground-deletion-1 class=tab-pane role=tabpanel aria-labelledby=foreground-deletion-1>
<p><p>You can delete objects using foreground cascading deletion by calling the
Kubernetes API.</p>
<p>For details, read the <a href=/docs/home/supported-doc-versions/>documentation for your Kubernetes version</a>.</p>
<ol>
<li>
<p>Start a local proxy session:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</code></pre></div></li>
<li>
<p>Use <code>curl</code> to trigger deletion:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</code></pre></div><p>The output contains a <code>foregroundDeletion</code> <a class=glossary-tooltip title="A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/finalizers/ target=_blank aria-label=finalizer>finalizer</a>
like this:</p>
<pre><code>&quot;kind&quot;: &quot;Deployment&quot;,
&quot;apiVersion&quot;: &quot;apps/v1&quot;,
&quot;metadata&quot;: {
    &quot;name&quot;: &quot;nginx-deployment&quot;,
    &quot;namespace&quot;: &quot;default&quot;,
    &quot;uid&quot;: &quot;d1ce1b02-cae8-4288-8a53-30e84d8fa505&quot;,
    &quot;resourceVersion&quot;: &quot;1363097&quot;,
    &quot;creationTimestamp&quot;: &quot;2021-07-08T20:24:37Z&quot;,
    &quot;deletionTimestamp&quot;: &quot;2021-07-08T20:27:39Z&quot;,
    &quot;finalizers&quot;: [
      &quot;foregroundDeletion&quot;
    ]
    ...
</code></pre></li>
</ol>
</div></div>
<h2 id=use-background-cascading-deletion>Use background cascading deletion</h2>
<ol>
<li><a href=/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment>Create a sample Deployment</a>.</li>
<li>Use either <code>kubectl</code> or the Kubernetes API to delete the Deployment,
depending on the Kubernetes version your cluster runs.
To check the version, enter <code>kubectl version</code>.
</li>
</ol>
<ul class="nav nav-tabs" id=background-deletion role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#background-deletion-0 role=tab aria-controls=background-deletion-0 aria-selected=true>Kubernetes version 1.20.x and later</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#background-deletion-1 role=tab aria-controls=background-deletion-1>Versions prior to Kubernetes 1.20.x</a></li></ul>
<div class=tab-content id=background-deletion><div id=background-deletion-0 class="tab-pane show active" role=tabpanel aria-labelledby=background-deletion-0>
<p><p>You can delete objects using background cascading deletion using <code>kubectl</code>
or the Kubernetes API.</p>
<p>Kubernetes uses background cascading deletion by default, and does so
even if you run the following commands without the <code>--cascade</code> flag or the
<code>propagationPolicy</code> argument.</p>
<p><strong>Using kubectl</strong></p>
<p>Run the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployment nginx-deployment --cascade<span style=color:#666>=</span>background
</code></pre></div><p><strong>Using the Kubernetes API</strong></p>
<ol>
<li>
<p>Start a local proxy session:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</code></pre></div></li>
<li>
<p>Use <code>curl</code> to trigger deletion:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Background&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</code></pre></div><p>The output is similar to this:</p>
<pre><code>&quot;kind&quot;: &quot;Status&quot;,
&quot;apiVersion&quot;: &quot;v1&quot;,
...
&quot;status&quot;: &quot;Success&quot;,
&quot;details&quot;: {
    &quot;name&quot;: &quot;nginx-deployment&quot;,
    &quot;group&quot;: &quot;apps&quot;,
    &quot;kind&quot;: &quot;deployments&quot;,
    &quot;uid&quot;: &quot;cc9eefb9-2d49-4445-b1c1-d261c9396456&quot;
}
</code></pre></li>
</ol>
</div>
<div id=background-deletion-1 class=tab-pane role=tabpanel aria-labelledby=background-deletion-1>
<p><p>Kubernetes uses background cascading deletion by default, and does so
even if you run the following commands without the <code>--cascade</code> flag or the
<code>propagationPolicy: Background</code> argument.</p>
<p>For details, read the <a href=/docs/home/supported-doc-versions/>documentation for your Kubernetes version</a>.</p>
<p><strong>Using kubectl</strong></p>
<p>Run the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployment nginx-deployment --cascade<span style=color:#666>=</span><span style=color:#a2f>true</span>
</code></pre></div><p><strong>Using the Kubernetes API</strong></p>
<ol>
<li>
<p>Start a local proxy session:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</code></pre></div></li>
<li>
<p>Use <code>curl</code> to trigger deletion:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Background&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</code></pre></div><p>The output is similar to this:</p>
<pre><code>&quot;kind&quot;: &quot;Status&quot;,
&quot;apiVersion&quot;: &quot;v1&quot;,
...
&quot;status&quot;: &quot;Success&quot;,
&quot;details&quot;: {
    &quot;name&quot;: &quot;nginx-deployment&quot;,
    &quot;group&quot;: &quot;apps&quot;,
    &quot;kind&quot;: &quot;deployments&quot;,
    &quot;uid&quot;: &quot;cc9eefb9-2d49-4445-b1c1-d261c9396456&quot;
}
</code></pre></li>
</ol>
</div></div>
<h2 id=set-orphan-deletion-policy>Delete owner objects and orphan dependents</h2>
<p>By default, when you tell Kubernetes to delete an object, the
<a class=glossary-tooltip title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> also deletes
dependent objects. You can make Kubernetes <em>orphan</em> these dependents using
<code>kubectl</code> or the Kubernetes API, depending on the Kubernetes version your
cluster runs.
To check the version, enter <code>kubectl version</code>.
</p>
<ul class="nav nav-tabs" id=orphan-objects role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#orphan-objects-0 role=tab aria-controls=orphan-objects-0 aria-selected=true>Kubernetes version 1.20.x and later</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#orphan-objects-1 role=tab aria-controls=orphan-objects-1>Versions prior to Kubernetes 1.20.x</a></li></ul>
<div class=tab-content id=orphan-objects><div id=orphan-objects-0 class="tab-pane show active" role=tabpanel aria-labelledby=orphan-objects-0>
<p><p><strong>Using kubectl</strong></p>
<p>Run the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployment nginx-deployment --cascade<span style=color:#666>=</span>orphan
</code></pre></div><p><strong>Using the Kubernetes API</strong></p>
<ol>
<li>
<p>Start a local proxy session:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</code></pre></div></li>
<li>
<p>Use <code>curl</code> to trigger deletion:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</code></pre></div><p>The output contains <code>orphan</code> in the <code>finalizers</code> field, similar to this:</p>
<pre><code>&quot;kind&quot;: &quot;Deployment&quot;,
&quot;apiVersion&quot;: &quot;apps/v1&quot;,
&quot;namespace&quot;: &quot;default&quot;,
&quot;uid&quot;: &quot;6f577034-42a0-479d-be21-78018c466f1f&quot;,
&quot;creationTimestamp&quot;: &quot;2021-07-09T16:46:37Z&quot;,
&quot;deletionTimestamp&quot;: &quot;2021-07-09T16:47:08Z&quot;,
&quot;deletionGracePeriodSeconds&quot;: 0,
&quot;finalizers&quot;: [
  &quot;orphan&quot;
],
...
</code></pre></li>
</ol>
</div>
<div id=orphan-objects-1 class=tab-pane role=tabpanel aria-labelledby=orphan-objects-1>
<p><p>For details, read the <a href=/docs/home/supported-doc-versions/>documentation for your Kubernetes version</a>.</p>
<p><strong>Using kubectl</strong></p>
<p>Run the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployment nginx-deployment --cascade<span style=color:#666>=</span>orphan
</code></pre></div><p><strong>Using the Kubernetes API</strong></p>
<ol>
<li>
<p>Start a local proxy session:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</code></pre></div></li>
<li>
<p>Use <code>curl</code> to trigger deletion:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</code></pre></div><p>The output contains <code>orphan</code> in the <code>finalizers</code> field, similar to this:</p>
<pre><code>&quot;kind&quot;: &quot;Deployment&quot;,
&quot;apiVersion&quot;: &quot;apps/v1&quot;,
&quot;namespace&quot;: &quot;default&quot;,
&quot;uid&quot;: &quot;6f577034-42a0-479d-be21-78018c466f1f&quot;,
&quot;creationTimestamp&quot;: &quot;2021-07-09T16:46:37Z&quot;,
&quot;deletionTimestamp&quot;: &quot;2021-07-09T16:47:08Z&quot;,
&quot;deletionGracePeriodSeconds&quot;: 0,
&quot;finalizers&quot;: [
  &quot;orphan&quot;
],
...
</code></pre></li>
</ol>
</div></div>
<p>You can check that the Pods managed by the Deployment are still running:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</code></pre></div><h2 id=what-s-next>What's next</h2>
<ul>
<li>Learn about <a href=/docs/concepts/overview/working-with-objects/owners-dependents/>owners and dependents</a> in Kubernetes.</li>
<li>Learn about Kubernetes <a href=/docs/concepts/overview/working-with-objects/finalizers/>finalizers</a>.</li>
<li>Learn about <a href=/docs/concepts/architecture/garbage-collection/>garbage collection</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-669c88964b4a9eb2b040057266e4b60d>37 - Using a KMS provider for data encryption</h1>
<p>This page shows how to configure a Key Management Service (KMS) provider and plugin to enable secret data encryption.</p>
<h2 id=before-you-begin>Before you begin</h2>
<ul>
<li>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
</li>
<li>
<p>Kubernetes version 1.10.0 or later is required</p>
</li>
<li>
<p>etcd v3 or later is required</p>
</li>
</ul>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.12 [beta]</code>
</div>
<p>The KMS encryption provider uses an envelope encryption scheme to encrypt data in etcd. The data is encrypted using a data encryption key (DEK); a new DEK is generated for each encryption. The DEKs are encrypted with a key encryption key (KEK) that is stored and managed in a remote KMS. The KMS provider uses gRPC to communicate with a specific KMS
plugin. The KMS plugin, which is implemented as a gRPC server and deployed on the same host(s) as the Kubernetes master(s), is responsible for all communication with the remote KMS.</p>
<h2 id=configuring-the-kms-provider>Configuring the KMS provider</h2>
<p>To configure a KMS provider on the API server, include a provider of type <code>kms</code> in the providers array in the encryption configuration file and set the following properties:</p>
<ul>
<li><code>name</code>: Display name of the KMS plugin.</li>
<li><code>endpoint</code>: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.</li>
<li><code>cachesize</code>: Number of data encryption keys (DEKs) to be cached in the clear.
When cached, DEKs can be used without another call to the KMS;
whereas DEKs that are not cached require a call to the KMS to unwrap.</li>
<li><code>timeout</code>: How long should kube-apiserver wait for kms-plugin to respond before returning an error (default is 3 seconds).</li>
</ul>
<p>See <a href=/docs/tasks/administer-cluster/encrypt-data>Understanding the encryption at rest configuration.</a></p>
<h2 id=implementing-a-kms-plugin>Implementing a KMS plugin</h2>
<p>To implement a KMS plugin, you can develop a new plugin gRPC server or enable a KMS plugin already provided by your cloud provider. You then integrate the plugin with the remote KMS and deploy it on the Kubernetes master.</p>
<h3 id=enabling-the-kms-supported-by-your-cloud-provider>Enabling the KMS supported by your cloud provider</h3>
<p>Refer to your cloud provider for instructions on enabling the cloud provider-specific KMS plugin.</p>
<h3 id=developing-a-kms-plugin-grpc-server>Developing a KMS plugin gRPC server</h3>
<p>You can develop a KMS plugin gRPC server using a stub file available for Go. For other languages, you use a proto file to create a stub file that you can use to develop the gRPC server code.</p>
<ul>
<li>
<p>Using Go: Use the functions and data structures in the stub file: <a href=https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.pb.go>service.pb.go</a> to develop the gRPC server code</p>
</li>
<li>
<p>Using languages other than Go: Use the protoc compiler with the proto file: <a href=https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.proto>service.proto</a> to generate a stub file for the specific language</p>
</li>
</ul>
<p>Then use the functions and data structures in the stub file to develop the server code.</p>
<p><strong>Notes:</strong></p>
<ul>
<li>
<p>kms plugin version: <code>v1beta1</code></p>
<p>In response to procedure call Version, a compatible KMS plugin should return v1beta1 as VersionResponse.version.</p>
</li>
<li>
<p>message version: <code>v1beta1</code></p>
<p>All messages from KMS provider have the version field set to current version v1beta1.</p>
</li>
<li>
<p>protocol: UNIX domain socket (<code>unix</code>)</p>
<p>The gRPC server should listen at UNIX domain socket.</p>
</li>
</ul>
<h3 id=integrating-a-kms-plugin-with-the-remote-kms>Integrating a KMS plugin with the remote KMS</h3>
<p>The KMS plugin can communicate with the remote KMS using any protocol supported by the KMS.
All configuration data, including authentication credentials the KMS plugin uses to communicate with the remote KMS,
are stored and managed by the KMS plugin independently. The KMS plugin can encode the ciphertext with additional metadata that may be required before sending it to the KMS for decryption.</p>
<h3 id=deploying-the-kms-plugin>Deploying the KMS plugin</h3>
<p>Ensure that the KMS plugin runs on the same host(s) as the Kubernetes master(s).</p>
<h2 id=encrypting-your-data-with-the-kms-provider>Encrypting your data with the KMS provider</h2>
<p>To encrypt the data:</p>
<ol>
<li>
<p>Create a new encryption configuration file using the appropriate properties for the <code>kms</code> provider:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EncryptionConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- secrets<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>providers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>kms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myKmsPlugin<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>endpoint</span>:<span style=color:#bbb> </span>unix:///tmp/socketfile.sock<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>cachesize</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>timeout</span>:<span style=color:#bbb> </span>3s<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>identity</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></code></pre></div></li>
<li>
<p>Set the <code>--encryption-provider-config</code> flag on the kube-apiserver to point to the location of the configuration file.</p>
</li>
<li>
<p>Restart your API server.</p>
</li>
</ol>
<h2 id=verifying-that-the-data-is-encrypted>Verifying that the data is encrypted</h2>
<p>Data is encrypted when written to etcd. After restarting your <code>kube-apiserver</code>,
any newly created or updated secret should be encrypted when stored. To verify,
you can use the <code>etcdctl</code> command line program to retrieve the contents of your secret.</p>
<ol>
<li>
<p>Create a new secret called secret1 in the default namespace:</p>
<pre><code>kubectl create secret generic secret1 -n default --from-literal=mykey=mydata
</code></pre></li>
<li>
<p>Using the etcdctl command line, read that secret out of etcd:</p>
<pre><code>ETCDCTL_API=3 etcdctl get /kubernetes.io/secrets/default/secret1 [...] | hexdump -C
</code></pre><p>where <code>[...]</code> must be the additional arguments for connecting to the etcd server.</p>
</li>
<li>
<p>Verify the stored secret is prefixed with <code>k8s:enc:kms:v1:</code>, which indicates that the <code>kms</code> provider has encrypted the resulting data.</p>
</li>
<li>
<p>Verify that the secret is correctly decrypted when retrieved via the API:</p>
<pre><code>kubectl describe secret secret1 -n default
</code></pre><p>should match <code>mykey: mydata</code></p>
</li>
</ol>
<h2 id=ensuring-all-secrets-are-encrypted>Ensuring all secrets are encrypted</h2>
<p>Because secrets are encrypted on write, performing an update on a secret encrypts that content.</p>
<p>The following command reads all secrets and then updates them to apply server side encryption.
If an error occurs due to a conflicting write, retry the command.
For larger clusters, you may wish to subdivide the secrets by namespace or script an update.</p>
<pre><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre><h2 id=switching-from-a-local-encryption-provider-to-the-kms-provider>Switching from a local encryption provider to the KMS provider</h2>
<p>To switch from a local encryption provider to the <code>kms</code> provider and re-encrypt all of the secrets:</p>
<ol>
<li>
<p>Add the <code>kms</code> provider as the first entry in the configuration file as shown in the following example.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EncryptionConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- secrets<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>providers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>kms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name </span>:<span style=color:#bbb> </span>myKmsPlugin<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>endpoint</span>:<span style=color:#bbb> </span>unix:///tmp/socketfile.sock<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>cachesize</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>aescbc</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>keys</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>key1<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style=color:#bbb>
</span></code></pre></div></li>
<li>
<p>Restart all kube-apiserver processes.</p>
</li>
<li>
<p>Run the following command to force all secrets to be re-encrypted using the <code>kms</code> provider.</p>
<pre><code>kubectl get secrets --all-namespaces -o json| kubectl replace -f -
</code></pre></li>
</ol>
<h2 id=disabling-encryption-at-rest>Disabling encryption at rest</h2>
<p>To disable encryption at rest:</p>
<ol>
<li>
<p>Place the <code>identity</code> provider as the first entry in the configuration file:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EncryptionConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- secrets<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>providers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>identity</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>kms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name </span>:<span style=color:#bbb> </span>myKmsPlugin<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>endpoint</span>:<span style=color:#bbb> </span>unix:///tmp/socketfile.sock<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>cachesize</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></code></pre></div></li>
<li>
<p>Restart all kube-apiserver processes.</p>
</li>
<li>
<p>Run the following command to force all secrets to be decrypted.</p>
<pre><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre></li>
</ol>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-e1afcdac8d5e8458274b3c481c5ebcda>38 - Using CoreDNS for Service Discovery</h1>
<p>This page describes the CoreDNS upgrade process and how to install CoreDNS instead of kube-dns.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
Your Kubernetes server must be at or later than version v1.9.
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=about-coredns>About CoreDNS</h2>
<p><a href=https://coredns.io>CoreDNS</a> is a flexible, extensible DNS server
that can serve as the Kubernetes cluster DNS.
Like Kubernetes, the CoreDNS project is hosted by the
<a class=glossary-tooltip title="Cloud Native Computing Foundation" data-toggle=tooltip data-placement=top href=https://cncf.io/ target=_blank aria-label=CNCF>CNCF</a>.</p>
<p>You can use CoreDNS instead of kube-dns in your cluster by replacing
kube-dns in an existing deployment, or by using tools like kubeadm
that will deploy and upgrade the cluster for you.</p>
<h2 id=installing-coredns>Installing CoreDNS</h2>
<p>For manual deployment or replacement of kube-dns, see the documentation at the
<a href=https://github.com/coredns/deployment/tree/master/kubernetes>CoreDNS GitHub project.</a></p>
<h2 id=migrating-to-coredns>Migrating to CoreDNS</h2>
<h3 id=upgrading-an-existing-cluster-with-kubeadm>Upgrading an existing cluster with kubeadm</h3>
<p>In Kubernetes version 1.21, kubeadm removed its support for <code>kube-dns</code> as a DNS application.
For <code>kubeadm</code> v1.23, the only supported cluster DNS application
is CoreDNS.</p>
<p>You can move to CoreDNS when you use <code>kubeadm</code> to upgrade a cluster that is
using <code>kube-dns</code>. In this case, <code>kubeadm</code> generates the CoreDNS configuration
("Corefile") based upon the <code>kube-dns</code> ConfigMap, preserving configurations for
stub domains, and upstream name server.</p>
<h2 id=upgrading-coredns>Upgrading CoreDNS</h2>
<p>You can check the version of CoreDNS that kubeadm installs for each version of
Kubernetes in the page
<a href=https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md>CoreDNS version in Kubernetes</a>.</p>
<p>CoreDNS can be upgraded manually in case you want to only upgrade CoreDNS
or use your own custom image.
There is a helpful <a href=https://github.com/coredns/deployment/blob/master/kubernetes/Upgrading_CoreDNS.md>guideline and walkthrough</a>
available to ensure a smooth upgrade.
Make sure the existing CoreDNS configuration ("Corefile") is retained when
upgrading your cluster.</p>
<p>If you are upgrading your cluster using the <code>kubeadm</code> tool, <code>kubeadm</code>
can take care of retaining the existing CoreDNS configuration automatically.</p>
<h2 id=tuning-coredns>Tuning CoreDNS</h2>
<p>When resource utilisation is a concern, it may be useful to tune the
configuration of CoreDNS. For more details, check out the
<a href=https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md>documentation on scaling CoreDNS</a>.</p>
<h2 id=what-s-next>What's next</h2>
<p>You can configure <a href=https://coredns.io>CoreDNS</a> to support many more use cases than
kube-dns does by modifying the CoreDNS configuration ("Corefile").
For more information, see the <a href=https://coredns.io/plugins/kubernetes/>documentation</a>
for the <code>kubernetes</code> CoreDNS plugin, or read the
<a href=https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/>Custom DNS Entries for Kubernetes</a>.
in the CoreDNS blog.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-9ceed97f912df7289ed8872e290cfbad>39 - Using NodeLocal DNSCache in Kubernetes clusters</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>
<p>This page provides an overview of NodeLocal DNSCache feature in Kubernetes.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
To check the version, enter <code>kubectl version</code>.
</p>
<h2 id=introduction>Introduction</h2>
<p>NodeLocal DNSCache improves Cluster DNS performance by running a DNS caching agent
on cluster nodes as a DaemonSet. In today's architecture, Pods in 'ClusterFirst' DNS mode
reach out to a kube-dns <code>serviceIP</code> for DNS queries. This is translated to a
kube-dns/CoreDNS endpoint via iptables rules added by kube-proxy.
With this new architecture, Pods will reach out to the DNS caching agent
running on the same node, thereby avoiding iptables DNAT rules and connection tracking.
The local caching agent will query kube-dns service for cache misses of cluster
hostnames ("<code>cluster.local</code>" suffix by default).</p>
<h2 id=motivation>Motivation</h2>
<ul>
<li>
<p>With the current DNS architecture, it is possible that Pods with the highest DNS QPS
have to reach out to a different node, if there is no local kube-dns/CoreDNS instance.
Having a local cache will help improve the latency in such scenarios.</p>
</li>
<li>
<p>Skipping iptables DNAT and connection tracking will help reduce
<a href=https://github.com/kubernetes/kubernetes/issues/56903>conntrack races</a>
and avoid UDP DNS entries filling up conntrack table.</p>
</li>
<li>
<p>Connections from local caching agent to kube-dns service can be upgraded to TCP.
TCP conntrack entries will be removed on connection close in contrast with
UDP entries that have to timeout
(<a href=https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt>default</a>
<code>nf_conntrack_udp_timeout</code> is 30 seconds)</p>
</li>
<li>
<p>Upgrading DNS queries from UDP to TCP would reduce tail latency attributed to
dropped UDP packets and DNS timeouts usually up to 30s (3 retries + 10s timeout).
Since the nodelocal cache listens for UDP DNS queries, applications don't need to be changed.</p>
</li>
<li>
<p>Metrics & visibility into DNS requests at a node level.</p>
</li>
<li>
<p>Negative caching can be re-enabled, thereby reducing number of queries to kube-dns service.</p>
</li>
</ul>
<h2 id=architecture-diagram>Architecture Diagram</h2>
<p>This is the path followed by DNS Queries after NodeLocal DNSCache is enabled:</p>
<figure class=diagram-medium>
<img src=/images/docs/nodelocaldns.svg alt="NodeLocal DNSCache flow"> <figcaption>
<h4>Nodelocal DNSCache flow</h4><p>This image shows how NodeLocal DNSCache handles DNS queries.</p>
</figcaption>
</figure>
<h2 id=configuration>Configuration</h2>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The local listen IP address for NodeLocal DNSCache can be any address that
can be guaranteed to not collide with any existing IP in your cluster.
It's recommended to use an address with a local scope, per example,
from the 'link-local' range '169.254.0.0/16' for IPv4 or from the
'Unique Local Address' range in IPv6 'fd00::/8'.
</div>
<p>This feature can be enabled using the following steps:</p>
<ul>
<li>
<p>Prepare a manifest similar to the sample
<a href=https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml><code>nodelocaldns.yaml</code></a>
and save it as <code>nodelocaldns.yaml.</code></p>
</li>
<li>
<p>If using IPv6, the CoreDNS configuration file need to enclose all the IPv6 addresses
into square brackets if used in 'IP:Port' format.
If you are using the sample manifest from the previous point, this will require to modify
<a href=https://github.com/kubernetes/kubernetes/blob/b2ecd1b3a3192fbbe2b9e348e095326f51dc43dd/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml#L70>the configuration line L70</a>
like this: "<code>health [__PILLAR__LOCAL__DNS__]:8080</code>"</p>
</li>
<li>
<p>Substitute the variables in the manifest with the right values:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>kubedns</span><span style=color:#666>=</span><span style=color:#b44>`</span>kubectl get svc kube-dns -n kube-system -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>={</span>.spec.clusterIP<span style=color:#666>}</span><span style=color:#b44>`</span>
<span style=color:#b8860b>domain</span><span style=color:#666>=</span>&lt;cluster-domain&gt;
<span style=color:#b8860b>localdns</span><span style=color:#666>=</span>&lt;node-local-address&gt;
</code></pre></div><p><code>&lt;cluster-domain></code> is "<code>cluster.local</code>" by default. <code>&lt;node-local-address></code> is the
local listen IP address chosen for NodeLocal DNSCache.</p>
<ul>
<li>
<p>If kube-proxy is running in IPTABLES mode:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sed -i <span style=color:#b44>&#34;s/__PILLAR__LOCAL__DNS__/</span><span style=color:#b8860b>$localdns</span><span style=color:#b44>/g; s/__PILLAR__DNS__DOMAIN__/</span><span style=color:#b8860b>$domain</span><span style=color:#b44>/g; s/__PILLAR__DNS__SERVER__/</span><span style=color:#b8860b>$kubedns</span><span style=color:#b44>/g&#34;</span> nodelocaldns.yaml
</code></pre></div><p><code>__PILLAR__CLUSTER__DNS__</code> and <code>__PILLAR__UPSTREAM__SERVERS__</code> will be populated by
the <code>node-local-dns</code> pods.
In this mode, the <code>node-local-dns</code> pods listen on both the kube-dns service IP
as well as <code>&lt;node-local-address></code>, so pods can lookup DNS records using either IP address.</p>
</li>
<li>
<p>If kube-proxy is running in IPVS mode:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sed -i <span style=color:#b44>&#34;s/__PILLAR__LOCAL__DNS__/</span><span style=color:#b8860b>$localdns</span><span style=color:#b44>/g; s/__PILLAR__DNS__DOMAIN__/</span><span style=color:#b8860b>$domain</span><span style=color:#b44>/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/</span><span style=color:#b8860b>$kubedns</span><span style=color:#b44>/g&#34;</span> nodelocaldns.yaml
</code></pre></div><p>In this mode, the <code>node-local-dns</code> pods listen only on <code>&lt;node-local-address></code>.
The <code>node-local-dns</code> interface cannot bind the kube-dns cluster IP since the
interface used for IPVS loadbalancing already uses this address.
<code>__PILLAR__UPSTREAM__SERVERS__</code> will be populated by the node-local-dns pods.</p>
</li>
</ul>
</li>
<li>
<p>Run <code>kubectl create -f nodelocaldns.yaml</code></p>
</li>
<li>
<p>If using kube-proxy in IPVS mode, <code>--cluster-dns</code> flag to kubelet needs to be modified
to use <code>&lt;node-local-address></code> that NodeLocal DNSCache is listening on.
Otherwise, there is no need to modify the value of the <code>--cluster-dns</code> flag,
since NodeLocal DNSCache listens on both the kube-dns service IP as well as
<code>&lt;node-local-address></code>.</p>
</li>
</ul>
<p>Once enabled, the <code>node-local-dns</code> Pods will run in the <code>kube-system</code> namespace
on each of the cluster nodes. This Pod runs <a href=https://github.com/coredns/coredns>CoreDNS</a>
in cache mode, so all CoreDNS metrics exposed by the different plugins will
be available on a per-node basis.</p>
<p>You can disable this feature by removing the DaemonSet, using <code>kubectl delete -f &lt;manifest></code>.
You should also revert any changes you made to the kubelet configuration.</p>
<h2 id=stubdomains-and-upstream-server-configuration>StubDomains and Upstream server Configuration</h2>
<p>StubDomains and upstream servers specified in the <code>kube-dns</code> ConfigMap in the <code>kube-system</code> namespace
are automatically picked up by <code>node-local-dns</code> pods. The ConfigMap contents need to follow the format
shown in <a href=/docs/tasks/administer-cluster/dns-custom-nameservers/#example-1>the example</a>.
The <code>node-local-dns</code> ConfigMap can also be modified directly with the stubDomain configuration
in the Corefile format. Some cloud providers might not allow modifying <code>node-local-dns</code> ConfigMap directly.
In those cases, the <code>kube-dns</code> ConfigMap can be updated.</p>
<h2 id=setting-memory-limits>Setting memory limits</h2>
<p>The <code>node-local-dns</code> Pods use memory for storing cache entries and processing queries.
Since they do not watch Kubernetes objects, the cluster size or the number of Services/Endpoints
do not directly affect memory usage. Memory usage is influenced by the DNS query pattern.
From <a href=https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md>CoreDNS docs</a>,</p>
<blockquote>
<p>The default cache size is 10000 entries, which uses about 30 MB when completely filled.</p>
</blockquote>
<p>This would be the memory usage for each server block (if the cache gets completely filled).
Memory usage can be reduced by specifying smaller cache sizes.</p>
<p>The number of concurrent queries is linked to the memory demand, because each extra
goroutine used for handling a query requires an amount of memory. You can set an upper limit
using the <code>max_concurrent</code> option in the forward plugin.</p>
<p>If a <code>node-local-dns</code> Pod attempts to use more memory than is available (because of total system
resources, or because of a configured
<a href=/docs/concepts/configuration/manage-resources-containers/>resource limit</a>), the operating system
may shut down that pod's container.
If this happens, the container that is terminated (“OOMKilled”) does not clean up the custom
packet filtering rules that it previously added during startup.
The <code>node-local-dns</code> container should get restarted (since managed as part of a DaemonSet), but this
will lead to a brief DNS downtime each time that the container fails: the packet filtering rules direct
DNS queries to a local Pod that is unhealthy.</p>
<p>You can determine a suitable memory limit by running node-local-dns pods without a limit and
measuring the peak usage. You can also set up and use a
<a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>VerticalPodAutoscaler</a>
in <em>recommender mode</em>, and then check its recommendations.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-fe5ad73163d38596340536ec03a205f0>40 - Using sysctls in a Kubernetes Cluster</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>
<p>This document describes how to configure and use kernel parameters within a
Kubernetes cluster using the <a class=glossary-tooltip title="An interface for getting and setting Unix kernel parameters" data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/sysctl-cluster/ target=_blank aria-label=sysctl>sysctl</a>
interface.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Starting from Kubernetes version 1.23, the kubelet supports the use of either <code>/</code> or <code>.</code>
as separators for sysctl names.
For example, you can represent the same sysctl name as <code>kernel.shm_rmid_forced</code> using a
period as the separator, or as <code>kernel/shm_rmid_forced</code> using a slash as a separator.
For more sysctl parameter conversion method details, please refer to
the page <a href=https://man7.org/linux/man-pages/man5/sysctl.d.5.html>sysctl.d(5)</a> from
the Linux man-pages project.
Setting Sysctls for a Pod and PodSecurityPolicy features do not yet support
setting sysctls with slashes.
</div>
<h2 id=before-you-begin>Before you begin</h2>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
<p>For some steps, you also need to be able to reconfigure the command line
options for the kubelets running on your cluster.</p>
<h2 id=listing-all-sysctl-parameters>Listing all Sysctl Parameters</h2>
<p>In Linux, the sysctl interface allows an administrator to modify kernel
parameters at runtime. Parameters are available via the <code>/proc/sys/</code> virtual
process file system. The parameters cover various subsystems such as:</p>
<ul>
<li>kernel (common prefix: <code>kernel.</code>)</li>
<li>networking (common prefix: <code>net.</code>)</li>
<li>virtual memory (common prefix: <code>vm.</code>)</li>
<li>MDADM (common prefix: <code>dev.</code>)</li>
<li>More subsystems are described in <a href=https://www.kernel.org/doc/Documentation/sysctl/README>Kernel docs</a>.</li>
</ul>
<p>To get a list of all parameters, you can run</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo sysctl -a
</code></pre></div><h2 id=enabling-unsafe-sysctls>Enabling Unsafe Sysctls</h2>
<p>Sysctls are grouped into <em>safe</em> and <em>unsafe</em> sysctls. In addition to proper
namespacing, a <em>safe</em> sysctl must be properly <em>isolated</em> between pods on the
same node. This means that setting a <em>safe</em> sysctl for one pod</p>
<ul>
<li>must not have any influence on any other pod on the node</li>
<li>must not allow to harm the node's health</li>
<li>must not allow to gain CPU or memory resources outside of the resource limits
of a pod.</li>
</ul>
<p>By far, most of the <em>namespaced</em> sysctls are not necessarily considered <em>safe</em>.
The following sysctls are supported in the <em>safe</em> set:</p>
<ul>
<li><code>kernel.shm_rmid_forced</code>,</li>
<li><code>net.ipv4.ip_local_port_range</code>,</li>
<li><code>net.ipv4.tcp_syncookies</code>,</li>
<li><code>net.ipv4.ping_group_range</code> (since Kubernetes 1.18),</li>
<li><code>net.ipv4.ip_unprivileged_port_start</code> (since Kubernetes 1.22).</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The example <code>net.ipv4.tcp_syncookies</code> is not namespaced on Linux kernel version 4.4 or lower.
</div>
<p>This list will be extended in future Kubernetes versions when the kubelet
supports better isolation mechanisms.</p>
<p>All <em>safe</em> sysctls are enabled by default.</p>
<p>All <em>unsafe</em> sysctls are disabled by default and must be allowed manually by the
cluster admin on a per-node basis. Pods with disabled unsafe sysctls will be
scheduled, but will fail to launch.</p>
<p>With the warning above in mind, the cluster admin can allow certain <em>unsafe</em>
sysctls for very special situations such as high-performance or real-time
application tuning. <em>Unsafe</em> sysctls are enabled on a node-by-node basis with a
flag of the kubelet; for example:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubelet --allowed-unsafe-sysctls <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  <span style=color:#b44>&#39;kernel.msg*,net.core.somaxconn&#39;</span> ...
</code></pre></div><p>For <a class=glossary-tooltip title="A tool for running Kubernetes locally." data-toggle=tooltip data-placement=top href=/docs/setup/learning-environment/minikube/ target=_blank aria-label=Minikube>Minikube</a>, this can be done via the <code>extra-config</code> flag:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>minikube start --extra-config<span style=color:#666>=</span><span style=color:#b44>&#34;kubelet.allowed-unsafe-sysctls=kernel.msg*,net.core.somaxconn&#34;</span>...
</code></pre></div><p>Only <em>namespaced</em> sysctls can be enabled this way.</p>
<h2 id=setting-sysctls-for-a-pod>Setting Sysctls for a Pod</h2>
<p>A number of sysctls are <em>namespaced</em> in today's Linux kernels. This means that
they can be set independently for each pod on a node. Only namespaced sysctls
are configurable via the pod securityContext within Kubernetes.</p>
<p>The following sysctls are known to be namespaced. This list could change
in future versions of the Linux kernel.</p>
<ul>
<li><code>kernel.shm*</code>,</li>
<li><code>kernel.msg*</code>,</li>
<li><code>kernel.sem</code>,</li>
<li><code>fs.mqueue.*</code>,</li>
<li>The parameters under <code>net.*</code> that can be set in container networking
namespace. However, there are exceptions (e.g.,
<code>net.netfilter.nf_conntrack_max</code> and <code>net.netfilter.nf_conntrack_expect_max</code>
can be set in container networking namespace but they are unnamespaced).</li>
</ul>
<p>Sysctls with no namespace are called <em>node-level</em> sysctls. If you need to set
them, you must manually configure them on each node's operating system, or by
using a DaemonSet with privileged containers.</p>
<p>Use the pod securityContext to configure namespaced sysctls. The securityContext
applies to all containers in the same pod.</p>
<p>This example uses the pod securityContext to set a safe sysctl
<code>kernel.shm_rmid_forced</code> and two unsafe sysctls <code>net.core.somaxconn</code> and
<code>kernel.msgmax</code>. There is no distinction between <em>safe</em> and <em>unsafe</em> sysctls in
the specification.</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> Only modify sysctl parameters after you understand their effects, to avoid
destabilizing your operating system.
</div>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>sysctl-example<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>securityContext</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>sysctls</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kernel.shm_rmid_forced<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>net.core.somaxconn<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1024&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kernel.msgmax<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;65536&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></code></pre></div>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> Due to their nature of being <em>unsafe</em>, the use of <em>unsafe</em> sysctls
is at-your-own-risk and can lead to severe problems like wrong behavior of
containers, resource shortage or complete breakage of a node.
</div>
<p>It is good practice to consider nodes with special sysctl settings as
<em>tainted</em> within a cluster, and only schedule pods onto them which need those
sysctl settings. It is suggested to use the Kubernetes <a href=/docs/reference/generated/kubectl/kubectl-commands/#taint><em>taints and toleration</em>
feature</a> to implement this.</p>
<p>A pod with the <em>unsafe</em> sysctls will fail to launch on any node which has not
enabled those two <em>unsafe</em> sysctls explicitly. As with <em>node-level</em> sysctls it
is recommended to use
<a href=/docs/reference/generated/kubectl/kubectl-commands/#taint><em>taints and toleration</em> feature</a> or
<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taints on nodes</a>
to schedule those pods onto the right nodes.</p>
<h2 id=podsecuritypolicy>PodSecurityPolicy</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>
<p>You can further control which sysctls can be set in pods by specifying lists of
sysctls or sysctl patterns in the <code>forbiddenSysctls</code> and/or
<code>allowedUnsafeSysctls</code> fields of the PodSecurityPolicy. A sysctl pattern ends
with a <code>*</code> character, such as <code>kernel.*</code>. A <code>*</code> character on its own matches
all sysctls.</p>
<p>By default, all safe sysctls are allowed.</p>
<p>Both <code>forbiddenSysctls</code> and <code>allowedUnsafeSysctls</code> are lists of plain sysctl names
or sysctl patterns (which end with <code>*</code>). The string <code>*</code> matches all sysctls.</p>
<p>The <code>forbiddenSysctls</code> field excludes specific sysctls. You can forbid a
combination of safe and unsafe sysctls in the list. To forbid setting any
sysctls, use <code>*</code> on its own.</p>
<p>If you specify any unsafe sysctl in the <code>allowedUnsafeSysctls</code> field and it is
not present in the <code>forbiddenSysctls</code> field, that sysctl can be used in Pods
using this PodSecurityPolicy. To allow all unsafe sysctls in the
PodSecurityPolicy to be set, use <code>*</code> on its own.</p>
<p>Do not configure these two fields such that there is overlap, meaning that a
given sysctl is both allowed and forbidden.</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong> If you allow unsafe sysctls via the <code>allowedUnsafeSysctls</code> field
in a PodSecurityPolicy, any pod using such a sysctl will fail to start
if the sysctl is not allowed via the <code>--allowed-unsafe-sysctls</code> kubelet
flag as well on that node.
</div>
<p>This example allows unsafe sysctls prefixed with <code>kernel.msg</code> to be set and
disallows setting of the <code>kernel.shm_rmid_forced</code> sysctl.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>policy/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PodSecurityPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>sysctl-psp<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>allowedUnsafeSysctls</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- kernel.msg*<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>forbiddenSysctls</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- kernel.shm_rmid_forced<span style=color:#bbb>
</span><span style=color:#bbb> </span>...<span style=color:#bbb>
</span></code></pre></div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-778055e4a4415ca195169b42cd42ddf9>41 - Utilizing the NUMA-aware Memory Manager</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>
<p>The Kubernetes <em>Memory Manager</em> enables the feature of guaranteed memory (and hugepages)
allocation for pods in the <code>Guaranteed</code> <a class=glossary-tooltip title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-qos-class" target=_blank aria-label="QoS class">QoS class</a>.</p>
<p>The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod.
The Memory Manager feeds the central manager (<em>Topology Manager</em>) with these affinity hints.
Based on both the hints and Topology Manager policy, the pod is rejected or admitted to the node.</p>
<p>Moreover, the Memory Manager ensures that the memory which a pod requests
is allocated from a minimum number of NUMA nodes.</p>
<p>The Memory Manager is only pertinent to Linux based hosts.</p>
<h2 id=before-you-begin>Before you begin</h2>
<p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>minikube</a>
or you can use one of these Kubernetes playgrounds:</p>
<ul>
<li><a href=https://www.katacoda.com/courses/kubernetes/playground>Katacoda</a></li>
<li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li>
</ul>
Your Kubernetes server must be at or later than version v1.21.
To check the version, enter <code>kubectl version</code>.
</p>
<p>To align memory resources with other requested resources in a Pod spec:</p>
<ul>
<li>the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node.
See <a href=/docs/tasks/administer-cluster/cpu-management-policies/>control CPU Management Policies</a>;</li>
<li>the Topology Manager should be enabled and proper Topology Manager policy should be configured on a Node.
See <a href=/docs/tasks/administer-cluster/topology-manager/>control Topology Management Policies</a>.</li>
</ul>
<p>Starting from v1.22, the Memory Manager is enabled by default through <code>MemoryManager</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>.</p>
<p>Preceding v1.22, the <code>kubelet</code> must be started with the following flag:</p>
<p><code>--feature-gates=MemoryManager=true</code></p>
<p>in order to enable the Memory Manager feature.</p>
<h2 id=how-memory-manager-operates>How Memory Manager Operates?</h2>
<p>The Memory Manager currently offers the guaranteed memory (and hugepages) allocation
for Pods in Guaranteed QoS class.
To immediately put the Memory Manager into operation follow the guidelines in the section
<a href=#memory-manager-configuration>Memory Manager configuration</a>, and subsequently,
prepare and deploy a <code>Guaranteed</code> pod as illustrated in the section
<a href=#placing-a-pod-in-the-guaranteed-qos-class>Placing a Pod in the Guaranteed QoS class</a>.</p>
<p>The Memory Manager is a Hint Provider, and it provides topology hints for
the Topology Manager which then aligns the requested resources according to these topology hints.
It also enforces <code>cgroups</code> (i.e. <code>cpuset.mems</code>) for pods.
The complete flow diagram concerning pod admission and deployment process is illustrated in
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#design-overview>Memory Manager KEP: Design Overview</a> and below:</p>
<p><img src=/images/docs/memory-manager-diagram.svg alt="Memory Manager in the pod admission and deployment process"></p>
<p>During this process, the Memory Manager updates its internal counters stored in
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps>Node Map and Memory Maps</a> to manage guaranteed memory allocation.</p>
<p>The Memory Manager updates the Node Map during the startup and runtime as follows.</p>
<h3 id=startup>Startup</h3>
<p>This occurs once a node administrator employs <code>--reserved-memory</code> (section
<a href=#reserved-memory-flag>Reserved memory flag</a>).
In this case, the Node Map becomes updated to reflect this reservation as illustrated in
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-start-up-with-examples>Memory Manager KEP: Memory Maps at start-up (with examples)</a>.</p>
<p>The administrator must provide <code>--reserved-memory</code> flag when <code>Static</code> policy is configured.</p>
<h3 id=runtime>Runtime</h3>
<p>Reference <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-runtime-with-examples>Memory Manager KEP: Memory Maps at runtime (with examples)</a> illustrates
how a successful pod deployment affects the Node Map, and it also relates to
how potential Out-of-Memory (OOM) situations are handled further by Kubernetes or operating system.</p>
<p>Important topic in the context of Memory Manager operation is the management of NUMA groups.
Each time pod's memory request is in excess of single NUMA node capacity, the Memory Manager
attempts to create a group that comprises several NUMA nodes and features extend memory capacity.
The problem has been solved as elaborated in
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes>Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?</a>.
Also, reference <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples>Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)</a>
illustrates how the management of groups occurs.</p>
<h2 id=memory-manager-configuration>Memory Manager configuration</h2>
<p>Other Managers should be first pre-configured. Next, the Memory Manger feature should be enabled
and be run with <code>Static</code> policy (section <a href=#policy-static>Static policy</a>).
Optionally, some amount of memory can be reserved for system or kubelet processes to increase
node stability (section <a href=#reserved-memory-flag>Reserved memory flag</a>).</p>
<h3 id=policies>Policies</h3>
<p>Memory Manager supports two policies. You can select a policy via a <code>kubelet</code> flag <code>--memory-manager-policy</code>:</p>
<ul>
<li><code>None</code> (default)</li>
<li><code>Static</code></li>
</ul>
<h4 id=policy-none>None policy</h4>
<p>This is the default policy and does not affect the memory allocation in any way.
It acts the same as if the Memory Manager is not present at all.</p>
<p>The <code>None</code> policy returns default topology hint. This special hint denotes that Hint Provider
(Memory Manger in this case) has no preference for NUMA affinity with any resource.</p>
<h4 id=policy-static>Static policy</h4>
<p>In the case of the <code>Guaranteed</code> pod, the <code>Static</code> Memory Manger policy returns topology hints
relating to the set of NUMA nodes where the memory can be guaranteed,
and reserves the memory through updating the internal <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps>NodeMap</a> object.</p>
<p>In the case of the <code>BestEffort</code> or <code>Burstable</code> pod, the <code>Static</code> Memory Manager policy sends back
the default topology hint as there is no request for the guaranteed memory,
and does not reserve the memory in the internal <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps>NodeMap</a> object.</p>
<h3 id=reserved-memory-flag>Reserved memory flag</h3>
<p>The <a href=/docs/tasks/administer-cluster/reserve-compute-resources/>Node Allocatable</a> mechanism
is commonly used by node administrators to reserve K8S node system resources for the kubelet
or operating system processes in order to enhance the node stability.
A dedicated set of flags can be used for this purpose to set the total amount of reserved memory
for a node. This pre-configured value is subsequently utilized to calculate
the real amount of node's "allocatable" memory available to pods.</p>
<p>The Kubernetes scheduler incorporates "allocatable" to optimise pod scheduling process.
The foregoing flags include <code>--kube-reserved</code>, <code>--system-reserved</code> and <code>--eviction-threshold</code>.
The sum of their values will account for the total amount of reserved memory.</p>
<p>A new <code>--reserved-memory</code> flag was added to Memory Manager to allow for this total reserved memory
to be split (by a node administrator) and accordingly reserved across many NUMA nodes.</p>
<p>The flag specifies a comma-separated list of memory reservations of different memory types per NUMA node.
Memory reservations across multiple NUMA nodes can be specified using semicolon as separator.
This parameter is only useful in the context of the Memory Manager feature.
The Memory Manager will not use this reserved memory for the allocation of container workloads.</p>
<p>For example, if you have a NUMA node "NUMA0" with <code>10Gi</code> of memory available, and
the <code>--reserved-memory</code> was specified to reserve <code>1Gi</code> of memory at "NUMA0",
the Memory Manager assumes that only <code>9Gi</code> is available for containers.</p>
<p>You can omit this parameter, however, you should be aware that the quantity of reserved memory
from all NUMA nodes should be equal to the quantity of memory specified by the
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/>Node Allocatable feature</a>.
If at least one node allocatable parameter is non-zero, you will need to specify
<code>--reserved-memory</code> for at least one NUMA node.
In fact, <code>eviction-hard</code> threshold value is equal to <code>100Mi</code> by default, so
if <code>Static</code> policy is used, <code>--reserved-memory</code> is obligatory.</p>
<p>Also, avoid the following configurations:</p>
<ol>
<li>duplicates, i.e. the same NUMA node or memory type, but with a different value;</li>
<li>setting zero limit for any of memory types;</li>
<li>NUMA node IDs that do not exist in the machine hardware;</li>
<li>memory type names different than <code>memory</code> or <code>hugepages-&lt;size></code>
(hugepages of particular <code>&lt;size></code> should also exist).</li>
</ol>
<p>Syntax:</p>
<p><code>--reserved-memory N:memory-type1=value1,memory-type2=value2,...</code></p>
<ul>
<li><code>N</code> (integer) - NUMA node index, e.g. <code>0</code></li>
<li><code>memory-type</code> (string) - represents memory type:
<ul>
<li><code>memory</code> - conventional memory</li>
<li><code>hugepages-2Mi</code> or <code>hugepages-1Gi</code> - hugepages</li>
</ul>
</li>
<li><code>value</code> (string) - the quantity of reserved memory, e.g. <code>1Gi</code></li>
</ul>
<p>Example usage:</p>
<p><code>--reserved-memory 0:memory=1Gi,hugepages-1Gi=2Gi</code></p>
<p>or</p>
<p><code>--reserved-memory 0:memory=1Gi --reserved-memory 1:memory=2Gi</code></p>
<p>or</p>
<p><code>--reserved-memory '0:memory=1Gi;1:memory=2Gi'</code></p>
<p>When you specify values for <code>--reserved-memory</code> flag, you must comply with the setting that
you prior provided via Node Allocatable Feature flags.
That is, the following rule must be obeyed for each memory type:</p>
<p><code>sum(reserved-memory(i)) = kube-reserved + system-reserved + eviction-threshold</code>,</p>
<p>where <code>i</code> is an index of a NUMA node.</p>
<p>If you do not follow the formula above, the Memory Manager will show an error on startup.</p>
<p>In other words, the example above illustrates that for the conventional memory (<code>type=memory</code>),
we reserve <code>3Gi</code> in total, i.e.:</p>
<p><code>sum(reserved-memory(i)) = reserved-memory(0) + reserved-memory(1) = 1Gi + 2Gi = 3Gi</code></p>
<p>An example of kubelet command-line arguments relevant to the node Allocatable configuration:</p>
<ul>
<li><code>--kube-reserved=cpu=500m,memory=50Mi</code></li>
<li><code>--system-reserved=cpu=123m,memory=333Mi</code></li>
<li><code>--eviction-hard=memory.available&lt;500Mi</code></li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The default hard eviction threshold is 100MiB, and <strong>not</strong> zero.
Remember to increase the quantity of memory that you reserve by setting <code>--reserved-memory</code>
by that hard eviction threshold. Otherwise, the kubelet will not start Memory Manager and
display an error.
</div>
<p>Here is an example of a correct configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>--feature-gates<span style=color:#666>=</span><span style=color:#b8860b>MemoryManager</span><span style=color:#666>=</span><span style=color:#a2f>true</span>
--kube-reserved<span style=color:#666>=</span><span style=color:#b8860b>cpu</span><span style=color:#666>=</span>4,memory<span style=color:#666>=</span>4Gi
--system-reserved<span style=color:#666>=</span><span style=color:#b8860b>cpu</span><span style=color:#666>=</span>1,memory<span style=color:#666>=</span>1Gi
--memory-manager-policy<span style=color:#666>=</span>Static
--reserved-memory <span style=color:#b44>&#39;0:memory=3Gi;1:memory=2148Mi&#39;</span>
</code></pre></div><p>Let us validate the configuration above:</p>
<ol>
<li><code>kube-reserved + system-reserved + eviction-hard(default) = reserved-memory(0) + reserved-memory(1)</code></li>
<li><code>4GiB + 1GiB + 100MiB = 3GiB + 2148MiB</code></li>
<li><code>5120MiB + 100MiB = 3072MiB + 2148MiB</code></li>
<li><code>5220MiB = 5220MiB</code> (which is correct)</li>
</ol>
<h2 id=placing-a-pod-in-the-guaranteed-qos-class>Placing a Pod in the Guaranteed QoS class</h2>
<p>If the selected policy is anything other than <code>None</code>, the Memory Manager identifies pods
that are in the <code>Guaranteed</code> QoS class.
The Memory Manager provides specific topology hints to the Topology Manager for each <code>Guaranteed</code> pod.
For pods in a QoS class other than <code>Guaranteed</code>, the Memory Manager provides default topology hints
to the Topology Manager.</p>
<p>The following excerpts from pod manifests assign a pod to the <code>Guaranteed</code> QoS class.</p>
<p>Pod with integer CPU(s) runs in the <code>Guaranteed</code> QoS class, when <code>requests</code> are equal to <code>limits</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/device</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/device</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Also, a pod sharing CPU(s) runs in the <code>Guaranteed</code> QoS class, when <code>requests</code> are equal to <code>limits</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;300m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/device</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;300m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/device</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Notice that both CPU and memory requests must be specified for a Pod to lend it to Guaranteed QoS class.</p>
<h2 id=troubleshooting>Troubleshooting</h2>
<p>The following means can be used to troubleshoot the reason why a pod could not be deployed or
became rejected at a node:</p>
<ul>
<li>pod status - indicates topology affinity errors</li>
<li>system logs - include valuable information for debugging, e.g., about generated hints</li>
<li>state file - the dump of internal state of the Memory Manager
(includes <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps>Node Map and Memory Maps</a>)</li>
<li>starting from v1.22, the <a href=#device-plugin-resource-api>device plugin resource API</a> can be used
to retrieve information about the memory reserved for containers</li>
</ul>
<h3 id=TopologyAffinityError>Pod status (TopologyAffinityError)</h3>
<p>This error typically occurs in the following situations:</p>
<ul>
<li>a node has not enough resources available to satisfy the pod's request</li>
<li>the pod's request is rejected due to particular Topology Manager policy constraints</li>
</ul>
<p>The error appears in the status of a pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods
</code></pre></div><pre><code class=language-none data-lang=none>NAME         READY   STATUS                  RESTARTS   AGE
guaranteed   0/1     TopologyAffinityError   0          113s
</code></pre><p>Use <code>kubectl describe pod &lt;id></code> or <code>kubectl get events</code> to obtain detailed error message:</p>
<pre><code class=language-none data-lang=none>Warning  TopologyAffinityError  10m   kubelet, dell8  Resources cannot be allocated with Topology locality
</code></pre><h3 id=system-logs>System logs</h3>
<p>Search system logs with respect to a particular pod.</p>
<p>The set of hints that Memory Manager generated for the pod can be found in the logs.
Also, the set of hints generated by CPU Manager should be present in the logs.</p>
<p>Topology Manager merges these hints to calculate a single best hint.
The best hint should be also present in the logs.</p>
<p>The best hint indicates where to allocate all the resources.
Topology Manager tests this hint against its current policy, and based on the verdict,
it either admits the pod to the node or rejects it.</p>
<p>Also, search the logs for occurrences associated with the Memory Manager,
e.g. to find out information about <code>cgroups</code> and <code>cpuset.mems</code> updates.</p>
<h3 id=examine-the-memory-manager-state-on-a-node>Examine the memory manager state on a node</h3>
<p>Let us first deploy a sample <code>Guaranteed</code> pod whose specification is as follows:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>guaranteed<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>guaranteed<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>consumer<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>150Gi<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>150Gi<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;sleep&#34;</span>,<span style=color:#b44>&#34;infinity&#34;</span>]<span style=color:#bbb>
</span></code></pre></div><p>Next, let us log into the node where it was deployed and examine the state file in
<code>/var/lib/kubelet/memory_manager_state</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
   <span style=color:green;font-weight:700>&#34;policyName&#34;</span>:<span style=color:#b44>&#34;Static&#34;</span>,
   <span style=color:green;font-weight:700>&#34;machineState&#34;</span>:{
      <span style=color:green;font-weight:700>&#34;0&#34;</span>:{
         <span style=color:green;font-weight:700>&#34;numberOfAssignments&#34;</span>:<span style=color:#666>1</span>,
         <span style=color:green;font-weight:700>&#34;memoryMap&#34;</span>:{
            <span style=color:green;font-weight:700>&#34;hugepages-1Gi&#34;</span>:{
               <span style=color:green;font-weight:700>&#34;total&#34;</span>:<span style=color:#666>0</span>,
               <span style=color:green;font-weight:700>&#34;systemReserved&#34;</span>:<span style=color:#666>0</span>,
               <span style=color:green;font-weight:700>&#34;allocatable&#34;</span>:<span style=color:#666>0</span>,
               <span style=color:green;font-weight:700>&#34;reserved&#34;</span>:<span style=color:#666>0</span>,
               <span style=color:green;font-weight:700>&#34;free&#34;</span>:<span style=color:#666>0</span>
            },
            <span style=color:green;font-weight:700>&#34;memory&#34;</span>:{
               <span style=color:green;font-weight:700>&#34;total&#34;</span>:<span style=color:#666>134987354112</span>,
               <span style=color:green;font-weight:700>&#34;systemReserved&#34;</span>:<span style=color:#666>3221225472</span>,
               <span style=color:green;font-weight:700>&#34;allocatable&#34;</span>:<span style=color:#666>131766128640</span>,
               <span style=color:green;font-weight:700>&#34;reserved&#34;</span>:<span style=color:#666>131766128640</span>,
               <span style=color:green;font-weight:700>&#34;free&#34;</span>:<span style=color:#666>0</span>
            }
         },
         <span style=color:green;font-weight:700>&#34;nodes&#34;</span>:[
            <span style=color:#666>0</span>,
            <span style=color:#666>1</span>
         ]
      },
      <span style=color:green;font-weight:700>&#34;1&#34;</span>:{
         <span style=color:green;font-weight:700>&#34;numberOfAssignments&#34;</span>:<span style=color:#666>1</span>,
         <span style=color:green;font-weight:700>&#34;memoryMap&#34;</span>:{
            <span style=color:green;font-weight:700>&#34;hugepages-1Gi&#34;</span>:{
               <span style=color:green;font-weight:700>&#34;total&#34;</span>:<span style=color:#666>0</span>,
               <span style=color:green;font-weight:700>&#34;systemReserved&#34;</span>:<span style=color:#666>0</span>,
               <span style=color:green;font-weight:700>&#34;allocatable&#34;</span>:<span style=color:#666>0</span>,
               <span style=color:green;font-weight:700>&#34;reserved&#34;</span>:<span style=color:#666>0</span>,
               <span style=color:green;font-weight:700>&#34;free&#34;</span>:<span style=color:#666>0</span>
            },
            <span style=color:green;font-weight:700>&#34;memory&#34;</span>:{
               <span style=color:green;font-weight:700>&#34;total&#34;</span>:<span style=color:#666>135286722560</span>,
               <span style=color:green;font-weight:700>&#34;systemReserved&#34;</span>:<span style=color:#666>2252341248</span>,
               <span style=color:green;font-weight:700>&#34;allocatable&#34;</span>:<span style=color:#666>133034381312</span>,
               <span style=color:green;font-weight:700>&#34;reserved&#34;</span>:<span style=color:#666>29295144960</span>,
               <span style=color:green;font-weight:700>&#34;free&#34;</span>:<span style=color:#666>103739236352</span>
            }
         },
         <span style=color:green;font-weight:700>&#34;nodes&#34;</span>:[
            <span style=color:#666>0</span>,
            <span style=color:#666>1</span>
         ]
      }
   },
   <span style=color:green;font-weight:700>&#34;entries&#34;</span>:{
      <span style=color:green;font-weight:700>&#34;fa9bdd38-6df9-4cf9-aa67-8c4814da37a8&#34;</span>:{
         <span style=color:green;font-weight:700>&#34;guaranteed&#34;</span>:[
            {
               <span style=color:green;font-weight:700>&#34;numaAffinity&#34;</span>:[
                  <span style=color:#666>0</span>,
                  <span style=color:#666>1</span>
               ],
               <span style=color:green;font-weight:700>&#34;type&#34;</span>:<span style=color:#b44>&#34;memory&#34;</span>,
               <span style=color:green;font-weight:700>&#34;size&#34;</span>:<span style=color:#666>161061273600</span>
            }
         ]
      }
   },
   <span style=color:green;font-weight:700>&#34;checksum&#34;</span>:<span style=color:#666>4142013182</span>
}
</code></pre></div><p>It can be deduced from the state file that the pod was pinned to both NUMA nodes, i.e.:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=color:#b44>&#34;numaAffinity&#34;</span><span>:</span>[
   <span style=color:#666>0</span>,
   <span style=color:#666>1</span>
]<span>,</span>
</code></pre></div><p>Pinned term means that pod's memory consumption is constrained (through <code>cgroups</code> configuration)
to these NUMA nodes.</p>
<p>This automatically implies that Memory Manager instantiated a new group that
comprises these two NUMA nodes, i.e. <code>0</code> and <code>1</code> indexed NUMA nodes.</p>
<p>Notice that the management of groups is handled in a relatively complex manner, and
further elaboration is provided in Memory Manager KEP in <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples>this</a> and <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes>this</a> sections.</p>
<p>In order to analyse memory resources available in a group,the corresponding entries from
NUMA nodes belonging to the group must be added up.</p>
<p>For example, the total amount of free "conventional" memory in the group can be computed
by adding up the free memory available at every NUMA node in the group,
i.e., in the <code>"memory"</code> section of NUMA node <code>0</code> (<code>"free":0</code>) and NUMA node <code>1</code> (<code>"free":103739236352</code>).
So, the total amount of free "conventional" memory in this group is equal to <code>0 + 103739236352</code> bytes.</p>
<p>The line <code>"systemReserved":3221225472</code> indicates that the administrator of this node reserved
<code>3221225472</code> bytes (i.e. <code>3Gi</code>) to serve kubelet and system processes at NUMA node <code>0</code>,
by using <code>--reserved-memory</code> flag.</p>
<h3 id=device-plugin-resource-api>Device plugin resource API</h3>
<p>By employing the <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>API</a>,
the information about reserved memory for each container can be retrieved, which is contained
in protobuf <code>ContainerMemory</code> message.
This information can be retrieved solely for pods in Guaranteed QoS class.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#design-overview>Memory Manager KEP: Design Overview</a></li>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-start-up-with-examples>Memory Manager KEP: Memory Maps at start-up (with examples)</a></li>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-runtime-with-examples>Memory Manager KEP: Memory Maps at runtime (with examples)</a></li>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples>Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)</a></li>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps>Memory Manager KEP: The Concept of Node Map and Memory Maps</a></li>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes>Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?</a></li>
</ul>
</div>
</main>
</div>
</div>
<footer class=d-print-none>
<div class=footer__links>
<nav>
<a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a>
</nav>
</div>
<div class=container-fluid>
<div class=row>
<div class="col-6 col-sm-2 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list">
<a class=text-white target=_blank href=https://discuss.kubernetes.io>
<i class="fa fa-envelope"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter>
<a class=text-white target=_blank href=https://twitter.com/kubernetesio>
<i class="fab fa-twitter"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar>
<a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
<i class="fas fa-calendar-alt"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube>
<a class=text-white target=_blank href=https://youtube.com/kubernetescommunity>
<i class="fab fa-youtube"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank href=https://slack.k8s.io>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute>
<a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide>
<i class="fas fa-edit"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow">
<a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes>
<i class="fab fa-stack-overflow"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-8 text-center order-sm-2">
<small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small>
<br>
<small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small>
<br>
<small class=text-white>ICP license: 京ICP备17074266号-3</small>
</div>
</div>
</div>
</footer>
</div>
<script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script>
<script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script>
<script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script>
</body>
</html>