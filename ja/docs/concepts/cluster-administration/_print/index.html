<!doctype html><html lang=ja class=no-js>
<head>
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JPP6RFM2BP"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JPP6RFM2BP')</script>
<link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=zh href=https://kubernetes.io/zh/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/cluster-administration/>
<link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/cluster-administration/>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.87.0">
<link rel=canonical type=text/html href=https://kubernetes.io/ja/docs/concepts/cluster-administration/>
<link rel="shortcut icon" type=image/png href=/images/favicon.png>
<link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=manifest href=/manifest.webmanifest>
<link rel=apple-touch-icon href=/images/kubernetes-192x192.png>
<title>クラスターの管理 | Kubernetes</title><meta property="og:title" content="クラスターの管理">
<meta property="og:description" content="プロダクショングレードのコンテナ管理基盤">
<meta property="og:type" content="website">
<meta property="og:url" content="https://kubernetes.io/ja/docs/concepts/cluster-administration/"><meta property="og:site_name" content="Kubernetes">
<meta itemprop=name content="クラスターの管理">
<meta itemprop=description content="プロダクショングレードのコンテナ管理基盤"><meta name=twitter:card content="summary">
<meta name=twitter:title content="クラスターの管理">
<meta name=twitter:description content="プロダクショングレードのコンテナ管理基盤">
<link href=/scss/main.css rel=stylesheet>
<script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script>
<meta name=theme-color content="#326ce5">
<link rel=stylesheet href=/css/feature-states.css>
<meta name=description content>
<meta property="og:description" content>
<meta name=twitter:description content>
<meta property="og:url" content="https://kubernetes.io/ja/docs/concepts/cluster-administration/">
<meta property="og:title" content="クラスターの管理">
<meta name=twitter:title content="クラスターの管理">
<meta name=twitter:image content="https://kubernetes.io/images/favicon.png">
<meta name=twitter:image:alt content="Kubernetes">
<meta property="og:image" content="/images/kubernetes-horizontal-color.png">
<meta property="og:type" content="article">
<script src=/js/script.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary>
<a class=navbar-brand href=/ja/></a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-2 mb-lg-0">
<a class="nav-link active" href=/ja/docs/>ドキュメント</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/ja/blog/>Blogs</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/ja/training/>トレーニング</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/ja/partners/>パートナー</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/ja/community/>コミュニティ</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/ja/case-studies/>ケーススタディ</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
バージョン
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/ja/docs/concepts/cluster-administration/>v1.27</a>
<a class=dropdown-item href=https://v1-26.docs.kubernetes.io/ja/docs/concepts/cluster-administration/>v1.26</a>
<a class=dropdown-item href=https://v1-25.docs.kubernetes.io/ja/docs/concepts/cluster-administration/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/ja/docs/concepts/cluster-administration/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/ja/docs/concepts/cluster-administration/>v1.23</a>
</div>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
日本語 Japanese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/docs/concepts/cluster-administration/>English</a>
<a class=dropdown-item href=/zh/docs/concepts/cluster-administration/>中文 Chinese</a>
<a class=dropdown-item href=/ko/docs/concepts/cluster-administration/>한국어 Korean</a>
<a class=dropdown-item href=/fr/docs/concepts/cluster-administration/>Français</a>
<a class=dropdown-item href=/it/docs/concepts/cluster-administration/>Italiano</a>
<a class=dropdown-item href=/de/docs/concepts/cluster-administration/>Deutsch</a>
<a class=dropdown-item href=/es/docs/concepts/cluster-administration/>Español</a>
<a class=dropdown-item href=/pt-br/docs/concepts/cluster-administration/>Português</a>
<a class=dropdown-item href=/id/docs/concepts/cluster-administration/>Bahasa Indonesia</a>
<a class=dropdown-item href=/ru/docs/concepts/cluster-administration/>Русский</a>
</div>
</li>
</ul>
</div>
<button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
これは、このセクションの複数ページの印刷可能なビューです。
<a href=# onclick="return print(),!1">印刷するには、ここをクリックしてください</a>.
</p><p>
<a href=/ja/docs/concepts/cluster-administration/>このページの通常のビューに戻る</a>.
</p>
</div>
<h1 class=title>クラスターの管理</h1>
<ul>
<li>1: <a href=#pg-fb494ea3b1874bd753dcd11c3f35c2dc>クラスター管理の概要</a></li>
<li>2: <a href=#pg-2bf9a93ab5ba014fb6ff70b22c29d432>証明書</a></li>
<li>3: <a href=#pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>リソースの管理</a></li>
<li>4: <a href=#pg-d649067a69d8d5c7e71564b42b96909e>クラスターのネットワーク</a></li>
<li>5: <a href=#pg-5cc31ecfba86467f8884856412cfb6b2>システムログ</a></li>
<li>6: <a href=#pg-c4b1e87a84441f8a90699a345ce48d68>ロギングのアーキテクチャ</a></li>
<li>7: <a href=#pg-2e05a56491965ae320c2662590b2ca18>コンテナイメージのガベージコレクション</a></li>
<li>8: <a href=#pg-08e94e6a480e0d6b2de72d84a1b97617>Kubernetesのプロキシー</a></li>
<li>9: <a href=#pg-85d633ae590aa20ec024f1b7af1d74fc>アドオンのインストール</a></li>
</ul>
<div class=content>
</div>
</div>
<div class=td-content>
<h1 id=pg-fb494ea3b1874bd753dcd11c3f35c2dc>1 - クラスター管理の概要</h1>
<p>このページはKubernetesクラスターの作成や管理者向けの内容です。Kubernetesのコア<a href=/ja/docs/concepts/>コンセプト</a>についてある程度精通していることを前提とします。</p>
<h2 id=クラスターのプランニング>クラスターのプランニング</h2>
<p>Kubernetesクラスターの計画、セットアップ、設定の例を知るには<a href=/ja/docs/setup/>設定</a>のガイドを参照してください。この記事で列挙されているソリューションは<em>ディストリビューション</em> と呼ばれます。</p>
<p>ガイドを選択する前に、いくつかの考慮事項を挙げます。</p>
<ul>
<li>ユーザーのコンピューター上でKubernetesを試したいでしょうか、それとも高可用性のあるマルチノードクラスターを構築したいでしょうか？あなたのニーズにあったディストリビューションを選択してください。</li>
<li><strong>もしあなたが高可用性を求める場合</strong>、 <a href=/docs/concepts/cluster-administration/federation/>複数ゾーンにまたがるクラスター</a>の設定について学んでください。</li>
<li><a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>のような<strong>ホストされているKubernetesクラスター</strong>を使用するのか、それとも<strong>自分自身でクラスターをホストするのでしょうか</strong>？</li>
<li>使用するクラスターは<strong>オンプレミス</strong>なのか、それとも<strong>クラウド(IaaS)</strong> でしょうか？Kubernetesはハイブリッドクラスターを直接サポートしていません。その代わりユーザーは複数のクラスターをセットアップできます。</li>
<li>Kubernetesを <strong>「ベアメタル」なハードウェア</strong>上で稼働させますか？それとも<strong>仮想マシン(VMs)</strong> 上で稼働させますか？</li>
<li><strong>もしオンプレミスでKubernetesを構築する場合</strong>、どの<a href=/ja/docs/concepts/cluster-administration/networking/>ネットワークモデル</a>が最適か検討してください。</li>
<li><strong>ただクラスターを稼働させたいだけ</strong>でしょうか、それとも<strong>Kubernetesプロジェクトのコードの開発</strong>を行いたいでしょうか？もし後者の場合、開発が進行中のディストリビューションを選択してください。いくつかのディストリビューションはバイナリリリースのみ使用していますが、多くの選択肢があります。</li>
<li>クラスターを稼働させるのに必要な<a href=/ja/docs/concepts/overview/components/>コンポーネント</a>についてよく理解してください。</li>
</ul>
<p>注意: 全てのディストリビューションがアクティブにメンテナンスされている訳ではありません。最新バージョンのKubernetesでテストされたディストリビューションを選択してください。</p>
<h2 id=クラスターの管理>クラスターの管理</h2>
<ul>
<li>
<p><a href=/ja/docs/concepts/architecture/nodes/>ノードの管理</a>方法について学んでください。</p>
</li>
<li>
<p>共有クラスターにおける<a href=/ja/docs/concepts/policy/resource-quotas/>リソースクォータ</a>のセットアップと管理方法について学んでください。</p>
</li>
</ul>
<h2 id=クラスターをセキュアにする>クラスターをセキュアにする</h2>
<ul>
<li>
<p><a href=/ja/docs/concepts/cluster-administration/certificates/>Certificates</a>では、異なるツールチェインを使用して証明書を作成する方法を説明します。</p>
</li>
<li>
<p><a href=/ja/docs/concepts/containers/container-environment/>Kubernetes コンテナの環境</a>では、Kubernetesノード上でのKubeletが管理するコンテナの環境について説明します。</p>
</li>
<li>
<p><a href=/docs/concepts/security/controlling-access>Kubernetes APIへのアクセス制御</a>では、Kubernetesで自身のAPIに対するアクセスコントロールがどのように実装されているかを説明します。</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/authentication/>認証</a>では、様々な認証オプションを含むKubernetesでの認証について説明します。</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/authorization/>認可</a>では、認証とは別に、HTTPリクエストの処理方法を制御します。</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/admission-controllers/>アドミッションコントローラーの使用</a>では、認証と認可の後にKubernetes APIに対するリクエストをインターセプトするプラグインについて説明します。</p>
</li>
<li>
<p><a href=/docs/concepts/cluster-administration/sysctl-cluster/>Kubernetesクラスターでのsysctlの使用</a>では、管理者向けにカーネルパラメーターを設定するため<code>sysctl</code>コマンドラインツールの使用方法について解説します。</p>
</li>
<li>
<p><a href=/docs/tasks/debug-application-cluster/audit/>クラスターの監査</a>では、Kubernetesの監査ログの扱い方について解説します。</p>
</li>
</ul>
<h3 id=kubeletをセキュアにする>kubeletをセキュアにする</h3>
<ul>
<li><a href=/ja/docs/concepts/architecture/master-node-communication/>マスターとノードのコミュニケーション</a></li>
<li><a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>TLSのブートストラップ</a></li>
<li><a href=/docs/admin/kubelet-authentication-authorization/>Kubeletの認証/認可</a></li>
</ul>
<h2 id=オプションのクラスターサービス>オプションのクラスターサービス</h2>
<ul>
<li>
<p><a href=/ja/docs/concepts/services-networking/dns-pod-service/>DNSのインテグレーション</a>では、DNS名をKubernetes Serviceに直接名前解決する方法を解説します。</p>
</li>
<li>
<p><a href=/docs/concepts/cluster-administration/logging/>クラスターアクティビィのロギングと監視</a>では、Kubernetesにおけるロギングがどのように行われ、どう実装されているかについて解説します。</p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2bf9a93ab5ba014fb6ff70b22c29d432>2 - 証明書</h1>
<p>クライアント証明書認証を使用する場合、<code>easyrsa</code>や<code>openssl</code>、<code>cfssl</code>を用いて、手動で証明書を生成できます。</p>
<h3 id=easyrsa>easyrsa</h3>
<p><strong>easyrsa</strong>を用いると、クラスターの証明書を手動で生成できます。</p>
<ol>
<li>
<p>パッチを当てたバージョンのeasyrsa3をダウンロードして解凍し、初期化します。</p>
<pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz
tar xzf easy-rsa.tar.gz
cd easy-rsa-master/easyrsa3
./easyrsa init-pki
</code></pre>
</li>
<li>
<p>新しい認証局(CA)を生成します。<code>--batch</code>は自動モードを設定し、<code>--req-cn</code>はCAの新しいルート証明書の共通名(CN)を指定します。</p>
<pre><code>./easyrsa --batch &quot;--req-cn=${MASTER_IP}@`date +%s`&quot; build-ca nopass
</code></pre>
</li>
<li>
<p>サーバー証明書と鍵を生成します。
引数<code>--subject-alt-name</code>は、APIサーバーへのアクセスに使用できるIPおよびDNS名を設定します。
<code>MASTER_CLUSTER_IP</code>は通常、APIサーバーとコントローラーマネージャーコンポーネントの両方で引数<code>--service-cluster-ip-range</code>として指定されるサービスCIDRの最初のIPです。
引数<code>--days</code>は、証明書の有効期限が切れるまでの日数を設定するために使われます。
以下の例は、デフォルトのDNSドメイン名として<code>cluster.local</code>を使用していることを前提とします。</p>
<pre><code>./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;\
&quot;IP:${MASTER_CLUSTER_IP},&quot;\
&quot;DNS:kubernetes,&quot;\
&quot;DNS:kubernetes.default,&quot;\
&quot;DNS:kubernetes.default.svc,&quot;\
&quot;DNS:kubernetes.default.svc.cluster,&quot;\
&quot;DNS:kubernetes.default.svc.cluster.local&quot; \
--days=10000 \
build-server-full server nopass
</code></pre>
</li>
<li>
<p><code>pki/ca.crt</code>、<code>pki/issued/server.crt</code>、<code>pki/private/server.key</code>をディレクトリーにコピーします。</p>
</li>
<li>
<p>以下のパラメーターを、APIサーバーの開始パラメーターとして追加します。</p>
<pre><code>--client-ca-file=/yourdirectory/ca.crt
--tls-cert-file=/yourdirectory/server.crt
--tls-private-key-file=/yourdirectory/server.key
</code></pre>
</li>
</ol>
<h3 id=openssl>openssl</h3>
<p><strong>openssl</strong>はクラスターの証明書を手動で生成できます。</p>
<ol>
<li>
<p>2048ビットのca.keyを生成します。</p>
<pre><code>openssl genrsa -out ca.key 2048
</code></pre>
</li>
<li>
<p>ca.keyに応じて、ca.crtを生成します。証明書の有効期間を設定するには、-daysを使用します。</p>
<pre><code>openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt
</code></pre>
</li>
<li>
<p>2048ビットのserver.keyを生成します。</p>
<pre><code>openssl genrsa -out server.key 2048
</code></pre>
</li>
<li>
<p>証明書署名要求(CSR)を生成するための設定ファイルを生成します。
ファイル(例: <code>csr.conf</code>)に保存する前に、かぎ括弧で囲まれた値(例: <code>&lt;MASTER_IP></code>)を必ず実際の値に置き換えてください。
<code>MASTER_CLUSTER_IP</code>の値は、前節で説明したAPIサーバーのサービスクラスターIPであることに注意してください。
以下の例は、デフォルトのDNSドメイン名として<code>cluster.local</code>を使用していることを前提とします。</p>
<pre><code>[ req ]
default_bits = 2048
prompt = no
default_md = sha256
req_extensions = req_ext
distinguished_name = dn

[ dn ]
C = &lt;country&gt;
ST = &lt;state&gt;
L = &lt;city&gt;
O = &lt;organization&gt;
OU = &lt;organization unit&gt;
CN = &lt;MASTER_IP&gt;

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
IP.1 = &lt;MASTER_IP&gt;
IP.2 = &lt;MASTER_CLUSTER_IP&gt;

[ v3_ext ]
authorityKeyIdentifier=keyid,issuer:always
basicConstraints=CA:FALSE
keyUsage=keyEncipherment,dataEncipherment
extendedKeyUsage=serverAuth,clientAuth
subjectAltName=@alt_names
</code></pre>
</li>
<li>
<p>設定ファイルに基づいて、証明書署名要求を生成します。</p>
<pre><code>openssl req -new -key server.key -out server.csr -config csr.conf
</code></pre>
</li>
<li>
<p>ca.key、ca.crt、server.csrを使用してサーバー証明書を生成します。</p>
<pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out server.crt -days 10000 \
-extensions v3_ext -extfile csr.conf
</code></pre>
</li>
<li>
<p>証明書を表示します。</p>
<pre><code>openssl x509  -noout -text -in ./server.crt
</code></pre>
</li>
</ol>
<p>最後にAPIサーバーの起動パラメーターに、同様のパラメーターを追加します。</p>
<h3 id=cfssl>cfssl</h3>
<p><strong>cfssl</strong>も証明書を生成するためのツールです。</p>
<ol>
<li>
<p>以下のように、ダウンロードして解凍し、コマンドラインツールを用意します。
使用しているハードウェアアーキテクチャやcfsslのバージョンに応じて、サンプルコマンドの調整が必要な場合があります。</p>
<pre><code>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o cfssl
chmod +x cfssl
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o cfssljson
chmod +x cfssljson
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -o cfssl-certinfo
chmod +x cfssl-certinfo
</code></pre>
</li>
<li>
<p>アーティファクトを保持するディレクトリーを生成し、cfsslを初期化します。</p>
<pre><code>mkdir cert
cd cert
../cfssl print-defaults config &gt; config.json
../cfssl print-defaults csr &gt; csr.json
</code></pre>
</li>
<li>
<p>CAファイルを生成するためのJSON設定ファイル(例: <code>ca-config.json</code>)を生成します。</p>
<pre><code>{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre>
</li>
<li>
<p>CA証明書署名要求(CSR)用のJSON設定ファイル(例: <code>ca-csr.json</code>)を生成します。
かぎ括弧で囲まれた値は、必ず使用したい実際の値に置き換えてください。</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;:[{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
</li>
<li>
<p>CA鍵(<code>ca-key.pem</code>)と証明書(<code>ca.pem</code>)を生成します。</p>
<pre><code>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</code></pre>
</li>
<li>
<p>APIサーバーの鍵と証明書を生成するためのJSON設定ファイル(例: <code>server-csr.json</code>)を生成します。
かぎ括弧で囲まれた値は、必ず使用したい実際の値に置き換えてください。
<code>MASTER_CLUSTER_IP</code>の値は、前節で説明したAPIサーバーのサービスクラスターIPです。
以下の例は、デフォルトのDNSドメイン名として<code>cluster.local</code>を使用していることを前提とします。</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;&lt;MASTER_IP&gt;&quot;,
    &quot;&lt;MASTER_CLUSTER_IP&gt;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
</li>
<li>
<p>APIサーバーの鍵と証明書を生成します。デフォルトでは、それぞれ<code>server-key.pem</code>と<code>server.pem</code>というファイルに保存されます。</p>
<pre><code>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
--config=ca-config.json -profile=kubernetes \
server-csr.json | ../cfssljson -bare server
</code></pre>
</li>
</ol>
<h2 id=自己署名ca証明書の配布>自己署名CA証明書の配布</h2>
<p>クライアントノードは、自己署名CA証明書を有効だと認識しないことがあります。
プロダクション用でない場合や、会社のファイアウォールの背後で実行する場合は、自己署名CA証明書をすべてのクライアントに配布し、有効な証明書のローカルリストを更新できます。</p>
<p>各クライアントで、以下の操作を実行します。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
sudo update-ca-certificates
</code></pre></div><pre><code>Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><h2 id=証明書api>証明書API</h2>
<p><code>certificates.k8s.io</code>APIを用いることで、<a href=/ja/docs/tasks/tls/managing-tls-in-a-cluster>こちら</a>のドキュメントにあるように、認証に使用するx509証明書をプロビジョニングすることができます。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>3 - リソースの管理</h1>
<p>アプリケーションをデプロイし、Serviceを介して外部に公開できました。さて、どうしますか？Kubernetesは、スケーリングや更新など、アプリケーションのデプロイを管理するための多くのツールを提供します。
我々が取り上げる機能についての詳細は<a href=/ja/docs/concepts/configuration/overview/>設定ファイル</a>と<a href=/ja/docs/concepts/overview/working-with-objects/labels/>ラベル</a>について詳細に説明します。</p>
<h2 id=リソースの設定を管理する>リソースの設定を管理する</h2>
<p>多くのアプリケーションではDeploymentやServiceなど複数のリソースの作成を要求します。複数のリソースの管理は、同一のファイルにひとまとめにしてグループ化すると簡単になります(YAMLファイル内で<code>---</code>で区切る)。
例えば:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/ja/examples/application/nginx-app.yaml download=application/nginx-app.yaml><code>application/nginx-app.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('application-nginx-app-yaml')" title="Copy application/nginx-app.yaml to clipboard">
</img>
</div>
<div class=includecode id=application-nginx-app-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx-svc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>複数のリソースは単一のリソースと同様の方法で作成できます。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>service/my-nginx-svc created
deployment.apps/my-nginx created
</code></pre></div><p>リソースは、ファイル内に記述されている順番通りに作成されます。そのため、Serviceを最初に指定するのが理想です。スケジューラーがServiceに関連するPodを、Deploymentなどのコントローラーによって作成されるときに確実に拡散できるようにするためです。</p>
<p><code>kubectl apply</code>もまた、複数の<code>-f</code>による引数指定を許可しています。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</code></pre></div><p>個別のファイルに加えて、-fの引数としてディレクトリ名も指定できます:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/application/nginx/
</code></pre></div><p><code>kubectl</code>は<code>.yaml</code>、<code>.yml</code>、<code>.json</code>といったサフィックスの付くファイルを読み込みます。</p>
<p>同じマイクロサービス、アプリケーションティアーのリソースは同一のファイルにまとめ、アプリケーションに関するファイルをグループ化するために、それらのファイルを同一のディレクトリに配備するのを推奨します。アプリケーションのティアーがDNSを通じて互いにバインドされると、アプリケーションスタックの全てのコンポーネントをひとまとめにして簡単にデプロイできます。</p>
<p>リソースの設定ソースとして、URLも指定できます。githubから取得した設定ファイルから直接手軽にデプロイができます:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx/nginx-deployment.yaml
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps/my-nginx created
</code></pre></div><h2 id=kubectlによる一括操作>kubectlによる一括操作</h2>
<p><code>kubectl</code>が一括して実行できる操作はリソースの作成のみではありません。作成済みのリソースの削除などの他の操作を実行するために、設定ファイルからリソース名を取得することができます。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</code></pre></div><p>2つのリソースだけを削除する場合には、コマンドラインでリソース/名前というシンタックスを使うことで簡単に指定できます。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployments/my-nginx services/my-nginx-svc
</code></pre></div><p>さらに多くのリソースに対する操作では、リソースをラベルでフィルターするために<code>-l</code>や<code>--selector</code>を使ってセレクター(ラベルクエリ)を指定するのが簡単です:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployment,services -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</code></pre></div><p><code>kubectl</code>は同様のシンタックスでリソース名を出力するので、<code>$()</code>や<code>xargs</code>を使ってパイプで操作するのが容易です:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get <span style=color:#a2f;font-weight:700>$(</span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service<span style=color:#a2f;font-weight:700>)</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT<span style=color:#666>(</span>S<span style=color:#666>)</span>      AGE
my-nginx-svc   LoadBalancer   10.0.0.208   &lt;pending&gt;     80/TCP       0s
</code></pre></div><p>上記のコマンドで、最初に<code>examples/application/nginx/</code>配下でリソースを作成し、<code>-o name</code>という出力フォーマットにより、作成されたリソースの名前を表示します(各リソースをresource/nameという形式で表示)。そして"service"のみ<code>grep</code>し、<code>kubectl get</code>を使って表示させます。</p>
<p>あるディレクトリ内の複数のサブディレクトリをまたいでリソースを管理するような場合、<code>--filename,-f</code>フラグと合わせて<code>--recursive</code>や<code>-R</code>を指定することでサブディレクトリに対しても再帰的に操作が可能です。</p>
<p>例えば、開発環境用に必要な全ての<a class=glossary-tooltip title="A serialized specification of one or more Kubernetes API objects." data-toggle=tooltip data-placement=top href="/ja/docs/reference/glossary/?all=true#term-manifest" target=_blank aria-label=マニフェスト>マニフェスト</a>をリソースタイプによって整理している<code>project/k8s/development</code>というディレクトリがあると仮定します。</p>
<pre><code>project/k8s/development
├── configmap
│   └── my-configmap.yaml
├── deployment
│   └── my-deployment.yaml
└── pvc
    └── my-pvc.yaml
</code></pre><p>デフォルトでは、<code>project/k8s/development</code>における一括操作は、どのサブディレクトリも処理せず、ディレクトリの第1階層で処理が止まります。下記のコマンドによってこのディレクトリ配下でリソースを作成しようとすると、エラーが発生します。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f project/k8s/development
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>error: you must provide one or more resources by argument or filename <span style=color:#666>(</span>.json|.yaml|.yml|stdin<span style=color:#666>)</span>
</code></pre></div><p>代わりに、下記のように<code>--filename,-f</code>フラグと合わせて<code>--recursive</code>や<code>-R</code>を指定してください:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f project/k8s/development --recursive
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre></div><p><code>--recursive</code>フラグは<code>kubectl {create,get,delete,describe,rollout}</code>などのような<code>--filename,-f</code>フラグを扱うどの操作でも有効です。</p>
<p>また、<code>--recursive</code>フラグは複数の<code>-f</code>フラグの引数を指定しても有効です。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>namespace/development created
namespace/staging created
configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre></div><p><code>kubectl</code>についてさらに知りたい場合は、<a href=/ja/docs/reference/kubectl/overview/>kubectlの概要</a>を参照してください。</p>
<h2 id=ラベルを有効に使う>ラベルを有効に使う</h2>
<p>これまで取り上げた例では、リソースに対して最大1つのラベルを適用してきました。リソースのセットを他のセットと区別するために、複数のラベルが必要な状況があります。</p>
<p>例えば、異なるアプリケーション間では、異なる<code>app</code>ラベルを使用したり、<a href=https://github.com/kubernetes/examples/tree/master/guestbook/>ゲストブックの例</a>のようなマルチティアーのアプリケーションでは、各ティアーを区別する必要があります。frontendというティアーでは下記のラベルを持ちます。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></code></pre></div><p>Redisマスターやスレーブでは異なる<code>tier</code>ラベルを持ち、加えて<code>role</code>ラベルも持つことでしょう。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>master<span style=color:#bbb>
</span></code></pre></div><p>そして</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>slave<span style=color:#bbb>
</span></code></pre></div><p>ラベルを使用すると、ラベルで指定された任意の次元に沿ってリソースを分割できます。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml
kubectl get pods -Lapp -Ltier -Lrole
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME                           READY     STATUS    RESTARTS   AGE       APP         TIER       ROLE
guestbook-fe-4nlpb             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
guestbook-fe-ght6d             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
guestbook-fe-jpy62             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
guestbook-redis-master-5pg3b   1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    master
guestbook-redis-slave-2q2yf    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
guestbook-redis-slave-qgazl    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
my-nginx-divi2                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
my-nginx-o0ef1                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -lapp<span style=color:#666>=</span>guestbook,role<span style=color:#666>=</span>slave
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME                          READY     STATUS    RESTARTS   AGE
guestbook-redis-slave-2q2yf   1/1       Running   <span style=color:#666>0</span>          3m
guestbook-redis-slave-qgazl   1/1       Running   <span style=color:#666>0</span>          3m
</code></pre></div><h2 id=canary-deployments-カナリアデプロイ>Canary deployments カナリアデプロイ</h2>
<p>複数のラベルが必要な他の状況として、異なるリリース間でのDeploymentや、同一コンポーネントの設定を区別することが挙げられます。よく知られたプラクティスとして、本番環境の実際のトラフィックを受け付けるようにするために、新しいリリースを完全にロールアウトする前に、新しい<em>カナリア版</em>のアプリケーションを過去のリリースと合わせてデプロイする方法があります。</p>
<p>例えば、異なるリリースバージョンを分けるために<code>track</code>ラベルを使用できます。</p>
<p>主要な安定板のリリースでは<code>track</code>ラベルに<code>stable</code>という値をつけることがあるでしょう。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>stable<span style=color:#bbb>
</span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v3<span style=color:#bbb>
</span></code></pre></div><p>そして2つの異なるPodのセットを上書きしないようにするため、<code>track</code>ラベルに異なる値を持つ(例: <code>canary</code>)ようなguestbookフロントエンドの新しいリリースを作成できます。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-canary<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v4<span style=color:#bbb>
</span></code></pre></div><p>frontend Serviceは、トラフィックを両方のアプリケーションにリダイレクトさせるために、両方のアプリケーションに共通したラベルのサブセットを選択して両方のレプリカを扱えるようにします。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></code></pre></div><p>安定版とカナリア版リリースで本番環境の実際のトラフィックを転送する割合を決めるため、双方のレプリカ数を変更できます(このケースでは3対1)。
最新版のリリースをしても大丈夫な場合、安定版のトラックを新しいアプリケーションにして、カナリア版を削除します。</p>
<p>さらに具体的な例については、<a href=https://github.com/kelseyhightower/talks/tree/master/kubecon-eu-2016/demo#deploy-a-canary>tutorial of deploying Ghost</a>を参照してください。</p>
<h2 id=ラベルの更新>ラベルの更新</h2>
<p>新しいリソースを作成する前に、既存のPodと他のリソースのラベルの変更が必要な状況があります。これは<code>kubectl label</code>で実行できます。
例えば、全てのnginx Podを frontendティアーとしてラベル付けするには、下記のコマンドを実行するのみです。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl label pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx <span style=color:#b8860b>tier</span><span style=color:#666>=</span>fe
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>pod/my-nginx-2035384211-j5fhi labeled
pod/my-nginx-2035384211-u2c7e labeled
pod/my-nginx-2035384211-u3t6x labeled
</code></pre></div><p>これは最初に"app=nginx"というラベルのついたPodをフィルターし、そのPodに対して"tier=fe"というラベルを追加します。
ラベル付けしたPodを確認するには、下記のコマンドを実行してください。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx -L tier
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME                        READY     STATUS    RESTARTS   AGE       TIER
my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          23m       fe
my-nginx-2035384211-u2c7e   1/1       Running   <span style=color:#666>0</span>          23m       fe
my-nginx-2035384211-u3t6x   1/1       Running   <span style=color:#666>0</span>          23m       fe
</code></pre></div><p>このコマンドでは"app=nginx"というラベルのついた全てのPodを出力し、Podのtierという項目も表示します(<code>-L</code>または<code>--label-columns</code>で指定)。</p>
<p>さらなる情報は、<a href=/ja/docs/concepts/overview/working-with-objects/labels/>ラベル</a>や<a href=/docs/reference/generated/kubectl/kubectl-commands/#label>kubectl label</a>を参照してください。</p>
<h2 id=アノテーションの更新>アノテーションの更新</h2>
<p>リソースに対してアノテーションを割り当てたい状況があります。アノテーションは、ツール、ライブラリなどのAPIクライアントによって取得するための任意の非識別メタデータです。アノテーションの割り当ては<code>kubectl annotate</code>で可能です。例:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl annotate pods my-nginx-v4-9gw19 <span style=color:#b8860b>description</span><span style=color:#666>=</span><span style=color:#b44>&#39;my frontend running nginx&#39;</span>
kubectl get pods my-nginx-v4-9gw19 -o yaml
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>apiVersion: v1
kind: pod
metadata:
  annotations:
    description: my frontend running nginx
...
</code></pre></div><p>さらなる情報は、<a href=/ja/docs/concepts/overview/working-with-objects/annotations/>アノテーション</a> や、<a href=/docs/reference/generated/kubectl/kubectl-commands/#annotate>kubectl annotate</a>を参照してください。</p>
<h2 id=アプリケーションのスケール>アプリケーションのスケール</h2>
<p>アプリケーションの負荷が増減するとき、<code>kubectl</code>を使って簡単にスケールできます。例えば、nginxのレプリカを3から1に減らす場合、下記を実行します:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl scale deployment/my-nginx --replicas<span style=color:#666>=</span><span style=color:#666>1</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps/my-nginx scaled
</code></pre></div><p>実行すると、Deploymentによって管理されるPod数が1となります。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME                        READY     STATUS    RESTARTS   AGE
my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          30m
</code></pre></div><p>システムに対してnginxのレプリカ数を自動で選択させるには、下記のように1から3の範囲で指定します。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl autoscale deployment/my-nginx --min<span style=color:#666>=</span><span style=color:#666>1</span> --max<span style=color:#666>=</span><span style=color:#666>3</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>horizontalpodautoscaler.autoscaling/my-nginx autoscaled
</code></pre></div><p>実行すると、nginxのレプリカは必要に応じて自動でスケールアップ、スケールダウンします。</p>
<p>さらなる情報は、<a href=/docs/reference/generated/kubectl/kubectl-commands/#scale>kubectl scale</a>、<a href=/docs/reference/generated/kubectl/kubectl-commands/#autoscale>kubectl autoscale</a> and <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>horizontal pod autoscaler</a>を参照してください。</p>
<h2 id=リソースの直接的アップデート>リソースの直接的アップデート</h2>
<p>場合によっては、作成したリソースに対して処理を中断させずに更新を行う必要があります。</p>
<h3 id=kubectl-apply>kubectl apply</h3>
<p>開発者が設定するリソースをコードとして管理しバージョニングも行えるように、設定ファイルのセットをソースによって管理する方法が推奨されています。
この場合、クラスターに対して設定の変更をプッシュするために<a href=/docs/reference/generated/kubectl/kubectl-commands/#apply><code>kubectl apply</code></a>を使用できます。</p>
<p>このコマンドは、リソース設定の過去のバージョンと、今適用した変更を比較し、差分に現れないプロパティーに対して上書き変更することなくクラスターに適用させます。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
deployment.apps/my-nginx configured
</code></pre></div><p>注意として、前回の変更適用時からの設定の変更内容を決めるため、<code>kubectl apply</code>はリソースに対してアノテーションを割り当てます。変更が実施されると<code>kubectl apply</code>は、1つ前の設定内容と、今回変更しようとする入力内容と、現在のリソースの設定との3つの間で変更内容の差分をとります。</p>
<p>現在、リソースはこのアノテーションなしで作成されました。そのため、最初の<code>kubectl paply</code>の実行においては、与えられたにゅうチョクト、現在のリソースの設定の2つの間の差分が取られ、フォールバックします。この最初の実行の間、リソースが作成された時にプロパティーセットの削除を検知できません。この理由により、プロパティーの削除はされません。</p>
<p><code>kubectl apply</code>の実行後の全ての呼び出しや、<code>kubectl replace</code>や<code>kubectl edit</code>などの設定を変更する他のコマンドではアノテーションを更新します。<code>kubectl apply</code>した後の全ての呼び出しにおいて3-wayの差分取得によってプロパティの検知と削除を実施します。</p>
<h3 id=kubectl-edit>kubectl edit</h3>
<p>その他に、<code>kubectl edit</code>によってリソースの更新もできます。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit deployment/my-nginx
</code></pre></div><p>このコマンドは、最初にリソースを<code>get</code>し、テキストエディタでリソースを編集し、更新されたバージョンでリソースを<code>apply</code>します。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml
vi /tmp/nginx.yaml
<span style=color:#080;font-style:italic># yamlファイルを編集し、ファイルを保存します。</span>

kubectl apply -f /tmp/nginx.yaml
deployment.apps/my-nginx configured

rm /tmp/nginx.yaml
</code></pre></div><p>このコマンドによってより重大な変更を簡単に行えます。注意として、あなたの<code>EDITOR</code>や<code>KUBE_EDITOR</code>といった環境変数も指定できます。</p>
<p>さらなる情報は、<a href=/docs/reference/generated/kubectl/kubectl-commands/#edit>kubectl edit</a>を参照してください。</p>
<h3 id=kubectl-patch>kubectl patch</h3>
<p>APIオブジェクトの更新には<code>kubectl patch</code>を使うことができます。このコマンドはJSON patch、JSON merge patch、戦略的merge patchをサポートしています。
<a href=/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/>kubectl patchを使ったAPIオブジェクトの更新</a>や<a href=/docs/reference/generated/kubectl/kubectl-commands/#patch>kubectl patch</a>を参照してください。</p>
<h2 id=破壊的なアップデート>破壊的なアップデート</h2>
<p>一度初期化された後、更新できないようなリソースフィールドの更新が必要な場合や、Deploymentによって作成され、壊れている状態のPodを修正するなど、再帰的な変更を即座に行いたい場合があります。このようなフィールドを変更するため、リソースの削除と再作成を行う<code>replace --force</code>を使用してください。このケースでは、シンプルに元の設定ファイルを修正するのみです。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps/my-nginx deleted
deployment.apps/my-nginx replaced
</code></pre></div><h2 id=サービス停止なしでアプリケーションを更新する>サービス停止なしでアプリケーションを更新する</h2>
<p>ある時点で、前述したカナリアデプロイのシナリオにおいて、新しいイメージやイメージタグを指定することによって、デプロイされたアプリケーションを更新が必要な場合があります。<code>kubectl</code>ではいくつかの更新操作をサポートしており、それぞれの操作が異なるシナリオに対して適用可能です。</p>
<p>ここでは、Deploymentを使ってアプリケーションの作成と更新についてガイドします。</p>
<p>まずnginxのバージョン1.14.2を稼働させていると仮定します。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create deployment my-nginx --image<span style=color:#666>=</span>nginx:1.14.2
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps/my-nginx created
</code></pre></div><p>レプリカ数を3にします(新旧のリビジョンは混在します)。:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl scale deployment my-nginx --current-replicas<span style=color:#666>=</span><span style=color:#666>1</span> --replicas<span style=color:#666>=</span><span style=color:#666>3</span>
</code></pre></div><pre><code>deployment.apps/my-nginx scaled
</code></pre><p>バージョン1.16.1に更新するには、上述したkubectlコマンドを使って<code>.spec.template.spec.containers[0].image</code>の値を<code>nginx:1.14.2</code>から<code>nginx:1.16.1</code>に変更するだけでできます。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit deployment/my-nginx
</code></pre></div><p>できました!Deploymentはデプロイされたnginxのアプリケーションを宣言的にプログレッシブに更新します。更新途中では、決まった数の古いレプリカのみダウンし、一定数の新しいレプリカが希望するPod数以上作成されても良いことを保証します。詳細について学ぶには<a href=/ja/docs/concepts/workloads/controllers/deployment/>Deployment page</a>を参照してください。</p>
<h2 id=次の項目>次の項目</h2>
<ul>
<li><a href=/docs/tasks/debug-application-cluster/debug-application-introspection/>アプリケーションの調査とデバッグのための<code>kubectl</code>の使用方法</a>について学んでください。</li>
<li><a href=/ja/docs/concepts/configuration/overview/>設定のベストプラクティスとTIPS</a>を参照してください。</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-d649067a69d8d5c7e71564b42b96909e>4 - クラスターのネットワーク</h1>
<p>ネットワークはKubernetesにおける中心的な部分ですが、どのように動作するかを正確に理解することは難解な場合もあります。
Kubernetesには、4つの異なる対応すべきネットワークの問題があります:</p>
<ol>
<li>高度に結合されたコンテナ間の通信: これは、<a class=glossary-tooltip title="一番小さく一番シンプルな Kubernetes のオブジェクト。Pod とはクラスターで動作しているいくつかのコンテナのまとまりです。" data-toggle=tooltip data-placement=top href=/ja/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>および<code>localhost</code>通信によって解決されます。</li>
<li>Pod間の通信: 本ドキュメントの主な焦点です。</li>
<li>Podからサービスへの通信：これは<a href=/ja/docs/concepts/services-networking/service/>Service</a>でカバーされています。</li>
<li>外部からサービスへの通信：これは<a href=/ja/docs/concepts/services-networking/service/>Service</a>でカバーされています。</li>
</ol>
<p>Kubernetesは、言ってしまえばアプリケーション間でマシンを共有するためのものです。通常、マシンを共有するには、2つのアプリケーションが同じポートを使用しないようにする必要があります。
複数の開発者間でポートを調整することは、大規模に行うことは非常に難しく、ユーザーが制御できないクラスターレベルの問題に見合うことがあります。</p>
<p>動的ポート割り当てはシステムに多くの複雑さをもたらします。すべてのアプリケーションはパラメータとしてポートを管理する必要があり、APIサーバーにて動的なポート番号を設定値として注入する方法が必要となり、各サービスはお互いにお互いを見つける方法が必要です。Kubernetesはこれに対処するのではなく、別のアプローチを取ります。</p>
<h2 id=the-kubernetes-network-model>Kubernetesのネットワークモデル</h2>
<p>すべての<code>Pod</code>は独自のIPアドレスを持ちます。これは、<code>Pod</code>間のリンクを明示的に作成する必要がなく、コンテナポートをホストポートにマッピングする必要がほとんどないことを意味します。こうすることで、ポート割り当て、名前解決、サービスディスカバリー、負荷分散、アプリケーション設定、および移行の観点から、<code>Pod</code>をVMまたは物理ホストと同様に扱うことができる、クリーンで後方互換性のあるモデルを生み出しています。</p>
<p>Kubernetesは、ネットワークの実装に次の基本的な要件を課しています(意図的なネットワークセグメンテーションポリシーを除きます):</p>
<ul>
<li>ノード上のPodが、NATなしですべてのノード上のすべてのPodと通信できること</li>
<li>systemdやkubeletなどノード上にあるエージェントが、そのノード上のすべてのPodと通信できること</li>
</ul>
<p>注: ホストネットワークで実行される<code>Pod</code>をサポートするプラットフォームの場合(Linuxなど):</p>
<ul>
<li>ノードのホストネットワーク内のPodは、NATなしですべてのノード上のすべてのPodと通信できます</li>
</ul>
<p>このモデルは全体としてそれほど複雑ではないことに加え、KubernetesがVMからコンテナへのアプリへの移植を簡単にするという要望と基本的に互換性があります。ジョブがVMで実行されていた頃も、VMにはIPがあってプロジェクト内の他のVMと通信できました。これは同じ基本モデルです。</p>
<p>KubernetesのIPアドレスは<code>Pod</code>スコープに存在します。<code>Pod</code>内のコンテナは、IPアドレスとMACアドレスを含むネットワーク名前空間を共有します。これは、<code>Pod</code>内のコンテナがすべて<code>localhost</code>上の互いのポートに到達できることを意味します。また、<code>Pod</code>内のコンテナがポートの使用を調整する必要があることも意味しますが、これもVM内のプロセスと同じです。これのことを「IP-per-pod(Pod毎のIP)」モデルと呼びます。</p>
<p>この実装方法は実際に使われているコンテナランタイムの詳細部分です。</p>
<p><code>Pod</code>に転送する<code>ノード</code>自体のポート(ホストポートと呼ばれる)を要求することは可能ですが、これは非常にニッチな操作です。このポート転送の実装方法も、コンテナランタイムの詳細部分です。<code>Pod</code>自体は、ホストポートの有無を認識しません。</p>
<h2 id=how-to-implement-the-kubernetes-networking-model>Kubernetesネットワークモデルの実装方法</h2>
<p>このネットワークモデルを実装する方法はいくつかあります。このドキュメントは、こうした方法を網羅的にはカバーしませんが、いくつかの技術の紹介として、また出発点として役立つことを願っています。</p>
<p>この一覧はアルファベット順にソートされており、順序は優先ステータスを意味するものではありません。</p>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>備考:</strong>
このセクションでは、Kubernetesが必要とする機能を提供するサードパーティープロジェクトにリンクしています。これらのプロジェクトはアルファベット順に記載されていて、Kubernetesプロジェクトの作者は責任を持ちません。このリストにプロジェクトを追加するには、変更を提出する前に<a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a>をお読みください。<a href=#third-party-content-disclaimer>詳細はこちら。</a></div>
<h3 id=aci>ACI</h3>
<p><a href=https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/index.html>Cisco Application Centric Infrastructure</a> offers an integrated overlay and underlay SDN solution that supports containers, virtual machines, and bare metal servers.
<a href=https://www.github.com/noironetworks/aci-containers>ACI</a> provides container networking integration for ACI.
An overview of the integration is provided <a href=https://www.cisco.com/c/dam/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/solution-overview-c22-739493.pdf>here</a>.</p>
<h3 id=antrea>Antrea</h3>
<p>Project <a href=https://github.com/vmware-tanzu/antrea>Antrea</a> is an opensource Kubernetes networking solution intended to be Kubernetes native.
It leverages Open vSwitch as the networking data plane.
Open vSwitch is a high-performance programmable virtual switch that supports both Linux and Windows.
Open vSwitch enables Antrea to implement Kubernetes Network Policies in a high-performance and efficient manner.
Thanks to the "programmable" characteristic of Open vSwitch, Antrea is able to implement an extensive set of networking and security features and services on top of Open vSwitch.</p>
<h3 id=aos-from-apstra>AOS from Apstra</h3>
<p><a href=https://www.apstra.com/products/aos/>AOS</a> is an Intent-Based Networking system that creates and manages complex datacenter environments from a simple integrated platform. AOS leverages a highly scalable distributed design to eliminate network outages while minimizing costs.</p>
<p>The AOS Reference Design currently supports Layer-3 connected hosts that eliminate legacy Layer-2 switching problems. These Layer-3 hosts can be Linux servers (Debian, Ubuntu, CentOS) that create BGP neighbor relationships directly with the top of rack switches (TORs). AOS automates the routing adjacencies and then provides fine grained control over the route health injections (RHI) that are common in a Kubernetes deployment.</p>
<p>AOS has a rich set of REST API endpoints that enable Kubernetes to quickly change the network policy based on application requirements. Further enhancements will integrate the AOS Graph model used for the network design with the workload provisioning, enabling an end to end management system for both private and public clouds.</p>
<p>AOS supports the use of common vendor equipment from manufacturers including Cisco, Arista, Dell, Mellanox, HPE, and a large number of white-box systems and open network operating systems like Microsoft SONiC, Dell OPX, and Cumulus Linux.</p>
<p>Details on how the AOS system works can be accessed here: <a href=https://www.apstra.com/products/how-it-works/>https://www.apstra.com/products/how-it-works/</a></p>
<h3 id=aws-vpc-cni-for-kubernetes>AWS VPC CNI for Kubernetes</h3>
<p><a href=https://github.com/aws/amazon-vpc-cni-k8s>AWS VPC CNI</a>は、Kubernetesクラスター向けの統合されたAWS Virtual Private Cloud(VPC)ネットワーキングを提供します。このCNIプラグインは、高いスループットと可用性、低遅延、および最小のネットワークジッタを提供します。さらに、ユーザーは、Kubernetesクラスターを構築するための既存のAWS VPCネットワーキングとセキュリティのベストプラクティスを適用できます。これには、ネットワークトラフィックの分離にVPCフローログ、VPCルーティングポリシー、およびセキュリティグループを使用する機能が含まれます。</p>
<p>このCNIプラグインを使用すると、Kubernetes PodはVPCネットワーク上と同じIPアドレスをPod内に持つことができます。CNIはAWS Elastic Networking Interface(ENI)を各Kubernetesノードに割り当て、ノード上のPodに各ENIのセカンダリIP範囲を使用します。このCNIには、Podの起動時間を短縮するためのENIとIPアドレスの事前割り当ての制御が含まれており、最大2,000ノードの大規模クラスターが可能です。</p>
<p>さらに、このCNIは<a href=https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/calico.html>ネットワークポリシーの適用のためにCalico</a>と一緒に実行できます。AWS VPC CNIプロジェクトは、<a href=https://github.com/aws/amazon-vpc-cni-k8s>GitHubのドキュメント</a>とともにオープンソースで公開されています。</p>
<h3 id=azure-cni-for-kubernetes>Azure CNI for Kubernetes</h3>
<p><a href=https://docs.microsoft.com/en-us/azure/virtual-network/container-networking-overview>Azure CNI</a> is an <a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>open source</a> plugin that integrates Kubernetes Pods with an Azure Virtual Network (also known as VNet) providing network performance at par with VMs. Pods can connect to peered VNet and to on-premises over Express Route or site-to-site VPN and are also directly reachable from these networks. Pods can access Azure services, such as storage and SQL, that are protected by Service Endpoints or Private Link. You can use VNet security policies and routing to filter Pod traffic. The plugin assigns VNet IPs to Pods by utilizing a pool of secondary IPs pre-configured on the Network Interface of a Kubernetes node.</p>
<p>Azure CNI is available natively in the [Azure Kubernetes Service (AKS)] (<a href=https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni)>https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni)</a>.</p>
<h3 id=big-cloud-fabric-from-big-switch-networks>Big Cloud Fabric from Big Switch Networks</h3>
<p><a href=https://www.bigswitch.com/container-network-automation>Big Cloud Fabric</a> is a cloud native networking architecture, designed to run Kubernetes in private cloud/on-premises environments. Using unified physical & virtual SDN, Big Cloud Fabric tackles inherent container networking problems such as load balancing, visibility, troubleshooting, security policies & container traffic monitoring.</p>
<p>With the help of the Big Cloud Fabric's virtual pod multi-tenant architecture, container orchestration systems such as Kubernetes, RedHat OpenShift, Mesosphere DC/OS & Docker Swarm will be natively integrated alongside with VM orchestration systems such as VMware, OpenStack & Nutanix. Customers will be able to securely inter-connect any number of these clusters and enable inter-tenant communication between them if needed.</p>
<p>BCF was recognized by Gartner as a visionary in the latest <a href=https://go.bigswitch.com/17GatedDocuments-MagicQuadrantforDataCenterNetworking_Reg.html>Magic Quadrant</a>. One of the BCF Kubernetes on-premises deployments (which includes Kubernetes, DC/OS & VMware running on multiple DCs across different geographic regions) is also referenced <a href=https://portworx.com/architects-corner-kubernetes-satya-komala-nio/>here</a>.</p>
<h3 id=calico>Calico</h3>
<p><a href=https://docs.projectcalico.org/>Calico</a>は、コンテナ、仮想マシン、ホストベースのワークロードのためのオープンソースのネットワーク及びネットワークセキュリティのソリューションです。Calicoは、純粋なLinuxのeBPFデータプレーンや、Linuxの標準的なネットワークデータプレーン、WindowsのHNSデータプレーンを含む、複数のデータプレーンをサポートしています。Calicoは完全なネットワークスタックを提供していますが、<a href=https://docs.projectcalico.org/networking/determine-best-networking#calico-compatible-cni-plugins-and-cloud-provider-integrations>クラウドプロバイダーのCNI</a>と組み合わせてネットワークポリシーを提供することもできます。</p>
<h3 id=cilium>Cilium</h3>
<p><a href=https://github.com/cilium/cilium>Cilium</a> is open source software for
providing and transparently securing network connectivity between application
containers. Cilium is L7/HTTP aware and can enforce network policies on L3-L7
using an identity based security model that is decoupled from network
addressing, and it can be used in combination with other CNI plugins.</p>
<h3 id=cni-genie-from-huawei>CNI-Genie from Huawei</h3>
<p><a href=https://github.com/Huawei-PaaS/CNI-Genie>CNI-Genie</a> is a CNI plugin that enables Kubernetes to <a href=https://github.com/Huawei-PaaS/CNI-Genie/blob/master/docs/multiple-cni-plugins/README.md#what-cni-genie-feature-1-multiple-cni-plugins-enables>simultaneously have access to different implementations</a> of the <a href=/ja/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>Kubernetes network model</a> in runtime. This includes any implementation that runs as a <a href=https://github.com/containernetworking/cni#3rd-party-plugins>CNI plugin</a>, such as <a href=https://github.com/coreos/flannel#flannel>Flannel</a>, <a href=http://docs.projectcalico.org/>Calico</a>, <a href=https://github.com/romana/romana>Romana</a>, <a href=https://www.weave.works/products/weave-net/>Weave-net</a>.</p>
<p>CNI-Genie also supports <a href=https://github.com/Huawei-PaaS/CNI-Genie/blob/master/docs/multiple-ips/README.md#feature-2-extension-cni-genie-multiple-ip-addresses-per-pod>assigning multiple IP addresses to a pod</a>, each from a different CNI plugin.</p>
<h3 id=cni-ipvlan-vpc-k8s>cni-ipvlan-vpc-k8s</h3>
<p><a href=https://github.com/lyft/cni-ipvlan-vpc-k8s>cni-ipvlan-vpc-k8s</a> contains a set
of CNI and IPAM plugins to provide a simple, host-local, low latency, high
throughput, and compliant networking stack for Kubernetes within Amazon Virtual
Private Cloud (VPC) environments by making use of Amazon Elastic Network
Interfaces (ENI) and binding AWS-managed IPs into Pods using the Linux kernel's
IPvlan driver in L2 mode.</p>
<p>The plugins are designed to be straightforward to configure and deploy within a
VPC. Kubelets boot and then self-configure and scale their IP usage as needed
without requiring the often recommended complexities of administering overlay
networks, BGP, disabling source/destination checks, or adjusting VPC route
tables to provide per-instance subnets to each host (which is limited to 50-100
entries per VPC). In short, cni-ipvlan-vpc-k8s significantly reduces the
network complexity required to deploy Kubernetes at scale within AWS.</p>
<h3 id=coil>Coil</h3>
<p><a href=https://github.com/cybozu-go/coil>Coil</a>は、容易に連携できるよう設計されていて、フレキシブルなEgressネットワークを提供することができるCNIプラグインです。
Coilはベアメタルと比較して低いオーバーヘッドで操作することができ、また外部のネットワークへの任意のEgress NATゲートウェイを定義することができます。</p>
<h3 id=contiv>Contiv</h3>
<p><a href=https://github.com/contiv/netplugin>Contiv</a> provides configurable networking (native l3 using BGP, overlay using vxlan, classic l2, or Cisco-SDN/ACI) for various use cases.</p>
<h3 id=contrail-tungsten-fabric>Contrail / Tungsten Fabric</h3>
<p><a href=https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a>, based on <a href=https://tungsten.io>Tungsten Fabric</a>, is a truly open, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with various orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide different isolation modes for virtual machines, containers/pods and bare metal workloads.</p>
<h3 id=danm>DANM</h3>
<p><a href=https://github.com/nokia/danm>DANM</a> is a networking solution for telco workloads running in a Kubernetes cluster. It's built up from the following components:</p>
<ul>
<li>A CNI plugin capable of provisioning IPVLAN interfaces with advanced features</li>
<li>An in-built IPAM module with the capability of managing multiple, cluster-wide, discontinuous L3 networks and provide a dynamic, static, or no IP allocation scheme on-demand</li>
<li>A CNI metaplugin capable of attaching multiple network interfaces to a container, either through its own CNI, or through delegating the job to any of the popular CNI solution like SRI-OV, or Flannel in parallel</li>
<li>A Kubernetes controller capable of centrally managing both VxLAN and VLAN interfaces of all Kubernetes hosts</li>
<li>Another Kubernetes controller extending Kubernetes' Service-based service discovery concept to work over all network interfaces of a Pod</li>
</ul>
<p>With this toolset DANM is able to provide multiple separated network interfaces, the possibility to use different networking back ends and advanced IPAM features for the pods.</p>
<h3 id=flannel>Flannel</h3>
<p><a href=https://github.com/coreos/flannel#flannel>Flannel</a> is a very simple overlay
network that satisfies the Kubernetes requirements. Many
people have reported success with Flannel and Kubernetes.</p>
<h3 id=google-compute-engine-gce>Google Compute Engine (GCE)</h3>
<p>For the Google Compute Engine cluster configuration scripts, <a href=https://cloud.google.com/vpc/docs/routes>advanced
routing</a> is used to
assign each VM a subnet (default is <code>/24</code> - 254 IPs). Any traffic bound for that
subnet will be routed directly to the VM by the GCE network fabric. This is in
addition to the "main" IP address assigned to the VM, which is NAT'ed for
outbound internet access. A linux bridge (called <code>cbr0</code>) is configured to exist
on that subnet, and is passed to docker's <code>--bridge</code> flag.</p>
<p>Docker is started with:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>DOCKER_OPTS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--bridge=cbr0 --iptables=false --ip-masq=false&#34;</span>
</code></pre></div><p>This bridge is created by Kubelet (controlled by the <code>--network-plugin=kubenet</code>
flag) according to the <code>Node</code>'s <code>.spec.podCIDR</code>.</p>
<p>Docker will now allocate IPs from the <code>cbr-cidr</code> block. Containers can reach
each other and <code>Nodes</code> over the <code>cbr0</code> bridge. Those IPs are all routable
within the GCE project network.</p>
<p>GCE itself does not know anything about these IPs, though, so it will not NAT
them for outbound internet traffic. To achieve that an iptables rule is used
to masquerade (aka SNAT - to make it seem as if packets came from the <code>Node</code>
itself) traffic that is bound for IPs outside the GCE project network
(10.0.0.0/8).</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>iptables -t nat -A POSTROUTING ! -d 10.0.0.0/8 -o eth0 -j MASQUERADE
</code></pre></div><p>Lastly IP forwarding is enabled in the kernel (so the kernel will process
packets for bridged containers):</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sysctl net.ipv4.ip_forward<span style=color:#666>=</span><span style=color:#666>1</span>
</code></pre></div><p>The result of all this is that all <code>Pods</code> can reach each other and can egress
traffic to the internet.</p>
<h3 id=jaguar>Jaguar</h3>
<p><a href=https://gitlab.com/sdnlab/jaguar>Jaguar</a> is an open source solution for Kubernetes's network based on OpenDaylight. Jaguar provides overlay network using vxlan and Jaguar CNIPlugin provides one IP address per pod.</p>
<h3 id=k-vswitch>k-vswitch</h3>
<p><a href=https://github.com/k-vswitch/k-vswitch>k-vswitch</a> is a simple Kubernetes networking plugin based on <a href=https://www.openvswitch.org/>Open vSwitch</a>. It leverages existing functionality in Open vSwitch to provide a robust networking plugin that is easy-to-operate, performant and secure.</p>
<h3 id=knitter>Knitter</h3>
<p><a href=https://github.com/ZTE/Knitter/>Knitter</a> is a network solution which supports multiple networking in Kubernetes. It provides the ability of tenant management and network management. Knitter includes a set of end-to-end NFV container networking solutions besides multiple network planes, such as keeping IP address for applications, IP address migration, etc.</p>
<h3 id=kube-ovn>Kube-OVN</h3>
<p><a href=https://github.com/alauda/kube-ovn>Kube-OVN</a> is an OVN-based kubernetes network fabric for enterprises. With the help of OVN/OVS, it provides some advanced overlay network features like subnet, QoS, static IP allocation, traffic mirroring, gateway, openflow-based network policy and service proxy.</p>
<h3 id=kube-router>Kube-router</h3>
<p><a href=https://github.com/cloudnativelabs/kube-router>Kube-router</a> is a purpose-built networking solution for Kubernetes that aims to provide high performance and operational simplicity. Kube-router provides a Linux <a href=https://www.linuxvirtualserver.org/software/ipvs.html>LVS/IPVS</a>-based service proxy, a Linux kernel forwarding-based pod-to-pod networking solution with no overlays, and iptables/ipset-based network policy enforcer.</p>
<h3 id=l2-networks-and-linux-bridging>L2 networks and linux bridging</h3>
<p>If you have a "dumb" L2 network, such as a simple switch in a "bare-metal"
environment, you should be able to do something similar to the above GCE setup.
Note that these instructions have only been tried very casually - it seems to
work, but has not been thoroughly tested. If you use this technique and
perfect the process, please let us know.</p>
<p>Follow the "With Linux Bridge devices" section of
<a href=http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/>this very nice tutorial</a> from
Lars Kellogg-Stedman.</p>
<h3 id=multus-a-multi-network-plugin>Multus (a Multi Network plugin)</h3>
<p>Multus is a Multi CNI plugin to support the Multi Networking feature in Kubernetes using CRD based network objects in Kubernetes.</p>
<p>Multus supports all <a href=https://github.com/containernetworking/plugins>reference plugins</a> (eg. <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>Flannel</a>, <a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp>DHCP</a>, <a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan>Macvlan</a>) that implement the CNI specification and 3rd party plugins (eg. <a href=https://github.com/projectcalico/cni-plugin>Calico</a>, <a href=https://github.com/weaveworks/weave>Weave</a>, <a href=https://github.com/cilium/cilium>Cilium</a>, <a href=https://github.com/contiv/netplugin>Contiv</a>). In addition to it, Multus supports <a href=https://github.com/hustcat/sriov-cni>SRIOV</a>, <a href=https://github.com/Intel-Corp/sriov-cni>DPDK</a>, <a href=https://github.com/intel/vhost-user-net-plugin>OVS-DPDK & VPP</a> workloads in Kubernetes with both cloud native and NFV based applications in Kubernetes.</p>
<h3 id=ovn4nfv-k8s-plugin-ovn-based-cni-controller-plugin>OVN4NFV-K8s-Plugin (OVN based CNI controller & plugin)</h3>
<p><a href=https://github.com/opnfv/ovn4nfv-k8s-plugin>OVN4NFV-K8S-Plugin</a> is OVN based CNI controller plugin to provide cloud native based Service function chaining(SFC), Multiple OVN overlay networking, dynamic subnet creation, dynamic creation of virtual networks, VLAN Provider network, Direct provider network and pluggable with other Multi-network plugins, ideal for edge based cloud native workloads in Multi-cluster networking</p>
<h3 id=nsx-t>NSX-T</h3>
<p><a href=https://docs.vmware.com/en/VMware-NSX-T/index.html>VMware NSX-T</a> is a network virtualization and security platform. NSX-T can provide network virtualization for a multi-cloud and multi-hypervisor environment and is focused on emerging application frameworks and architectures that have heterogeneous endpoints and technology stacks. In addition to vSphere hypervisors, these environments include other hypervisors such as KVM, containers, and bare metal.</p>
<p><a href=https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf>NSX-T Container Plug-in (NCP)</a> provides integration between NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.</p>
<h3 id=nuage-networks-vcs-virtualized-cloud-services>Nuage Networks VCS (Virtualized Cloud Services)</h3>
<p><a href=https://www.nuagenetworks.net>Nuage</a> provides a highly scalable policy-based Software-Defined Networking (SDN) platform. Nuage uses the open source Open vSwitch for the data plane along with a feature rich SDN Controller built on open standards.</p>
<p>The Nuage platform uses overlays to provide seamless policy-based networking between Kubernetes Pods and non-Kubernetes environments (VMs and bare metal servers). Nuage's policy abstraction model is designed with applications in mind and makes it easy to declare fine-grained policies for applications.The platform's real-time analytics engine enables visibility and security monitoring for Kubernetes applications.</p>
<h3 id=ovn-open-virtual-networking>OVN (Open Virtual Networking)</h3>
<p>OVN is an opensource network virtualization solution developed by the
Open vSwitch community. It lets one create logical switches, logical routers,
stateful ACLs, load-balancers etc to build different virtual networking
topologies. The project has a specific Kubernetes plugin and documentation
at <a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>.</p>
<h3 id=romana>Romana</h3>
<p><a href=https://github.com/romana/romana>Romana</a> is an open source network and security automation solution that lets you deploy Kubernetes without an overlay network. Romana supports Kubernetes <a href=/docs/concepts/services-networking/network-policies/>Network Policy</a> to provide isolation across network namespaces.</p>
<h3 id=weave-net-from-weaveworks>Weave Net from Weaveworks</h3>
<p><a href=https://www.weave.works/products/weave-net/>Weave Net</a> is a
resilient and simple to use network for Kubernetes and its hosted applications.
Weave Net runs as a <a href=https://www.weave.works/docs/net/latest/cni-plugin/>CNI plug-in</a>
or stand-alone. In either version, it doesn't require any configuration or extra code
to run, and in both cases, the network provides one IP address per pod - as is standard for Kubernetes.</p>
<h2 id=次の項目>次の項目</h2>
<p>ネットワークモデルの初期設計とその根拠、および将来の計画については、<a href=https://git.k8s.io/community/contributors/design-proposals/network/networking.md>ネットワーク設計ドキュメント</a>で詳細に説明されています。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-5cc31ecfba86467f8884856412cfb6b2>5 - システムログ</h1>
<p>システムコンポーネントのログは、クラスター内で起こったイベントを記録します。このログはデバッグのために非常に役立ちます。ログのverbosityを設定すると、ログをどの程度詳細に見るのかを変更できます。ログはコンポーネント内のエラーを表示する程度の荒い粒度にすることも、イベントのステップバイステップのトレース(HTTPのアクセスログ、Podの状態の変更、コントローラーの動作、スケジューラーの決定など)を表示するような細かい粒度に設定することもできます。</p>
<h2 id=klog>klog</h2>
<p>klogは、Kubernetesのログライブラリです。<a href=https://github.com/kubernetes/klog>klog</a>は、Kubernetesのシステムコンポーネント向けのログメッセージを生成します。</p>
<p>klogの設定に関する詳しい情報については、<a href=/docs/reference/command-line-tools-reference/>コマンドラインツールのリファレンス</a>を参照してください。</p>
<p>klogネイティブ形式の例:</p>
<pre><code>I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
</code></pre><h3 id=構造化ログ>構造化ログ</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code>
</div>
<div class="alert alert-danger warning callout" role=alert>
<strong>警告:</strong> <p>構造化ログへのマイグレーションは現在進行中の作業です。このバージョンでは、すべてのログメッセージが構造化されているわけではありません。ログファイルをパースする場合、JSONではないログの行にも対処しなければなりません。</p>
<p>ログの形式と値のシリアライズは変更される可能性があります。</p>
</div>
<p>構造化ログは、ログメッセージに単一の構造を導入し、プログラムで情報の抽出ができるようにするものです。構造化ログは、僅かな労力とコストで保存・処理できます。新しいメッセージ形式は後方互換性があり、デフォルトで有効化されます。</p>
<p>構造化ログの形式:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=color:#b44>&lt;klog header&gt; &#34;&lt;message&gt;&#34; &lt;key1&gt;</span><span style=color:#666>=</span><span style=color:#b44>&#34;&lt;value1&gt;&#34; &lt;key2&gt;=&#34;&lt;value2&gt;&#34; ...</span>
</code></pre></div><p>例:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=color:#b44>I1025 00:15:15.525108       1 controller_utils.go:116] &#34;Pod status updated&#34; pod</span><span style=color:#666>=</span><span style=color:#b44>&#34;kube-system/kubedns&#34; status=&#34;ready&#34;</span>
</code></pre></div><h3 id=jsonログ形式>JSONログ形式</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code>
</div>
<div class="alert alert-danger warning callout" role=alert>
<strong>警告:</strong> <p>JSONの出力は多数の標準のklogフラグをサポートしていません。非対応のklogフラグの一覧については、<a href=/docs/reference/command-line-tools-reference/>コマンドラインツールリファレンス</a>を参照してください。</p>
<p>すべてのログがJSON形式で書き込むことに対応しているわけではありません(たとえば、プロセスの開始時など)。ログのパースを行おうとしている場合、JSONではないログの行に対処できるようにしてください。</p>
<p>フィールド名とJSONのシリアライズは変更される可能性があります。</p>
</div>
<p><code>--logging-format=json</code>フラグは、ログの形式をネイティブ形式klogからJSON形式に変更します。以下は、JSONログ形式の例(pretty printしたもの)です。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
   <span style=color:green;font-weight:700>&#34;ts&#34;</span>: <span style=color:#666>1580306777.04728</span>,
   <span style=color:green;font-weight:700>&#34;v&#34;</span>: <span style=color:#666>4</span>,
   <span style=color:green;font-weight:700>&#34;msg&#34;</span>: <span style=color:#b44>&#34;Pod status updated&#34;</span>,
   <span style=color:green;font-weight:700>&#34;pod&#34;</span>:{
      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;nginx-1&#34;</span>,
      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
   },
   <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;ready&#34;</span>
}
</code></pre></div><p>特別な意味を持つキー:</p>
<ul>
<li><code>ts</code> - Unix時間のタイムスタンプ(必須、float)</li>
<li><code>v</code> - verbosity (必須、int、デフォルトは0)</li>
<li><code>err</code> - エラー文字列 (オプション、string)</li>
<li><code>msg</code> - メッセージ (必須、string)</li>
</ul>
<p>現在サポートされているJSONフォーマットの一覧:</p>
<ul>
<li><a class=glossary-tooltip title=コントロールプレーン上で動作するコンポーネントで、複数のコントローラープロセスを実行します。 data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li>
<li><a class=glossary-tooltip title="Kubernetes APIを提供するコントロールプレーンのコンポーネントです。" data-toggle=tooltip data-placement=top href=/ja/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li>
<li><a class=glossary-tooltip title=コントロールプレーン上で動作するコンポーネントで、新しく作られたPodにノードが割り当てられているか監視し、割り当てられていなかった場合にそのPodを実行するノードを選択します。 data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li>
<li><a class=glossary-tooltip title=クラスター内の各ノードで実行されるエージェントです。各コンテナがPodで実行されていることを保証します。 data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a></li>
</ul>
<h3 id=ログのサニタイズ>ログのサニタイズ</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>
<div class="alert alert-danger warning callout" role=alert>
<strong>警告:</strong> ログのサニタイズ大きな計算のオーバーヘッドを引き起こす可能性があるため、本番環境では有効にするべきではありません。
</div>
<p><code>--experimental-logging-sanitization</code>フラグはklogのサニタイズフィルタを有効にします。有効にすると、すべてのログの引数が機密データ(パスワード、キー、トークンなど)としてタグ付けされたフィールドについて検査され、これらのフィールドのログの記録は防止されます。</p>
<p>現在ログのサニタイズをサポートしているコンポーネント一覧:</p>
<ul>
<li>kube-controller-manager</li>
<li>kube-apiserver</li>
<li>kube-scheduler</li>
<li>kubelet</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>備考:</strong> ログのサニタイズフィルターは、ユーザーのワークロードのログが機密データを漏洩するのを防げるわけではありません。
</div>
<h3 id=ログのverbosityレベル>ログのverbosityレベル</h3>
<p><code>-v</code>フラグはログのverbosityを制御します。値を増やすとログに記録されるイベントの数が増えます。値を減らすとログに記録されるイベントの数が減ります。verbosityの設定を増やすと、ますます多くの深刻度の低いイベントをログに記録するようになります。verbosityの設定を0にすると、クリティカルなイベントだけをログに記録します。</p>
<h3 id=ログの場所>ログの場所</h3>
<p>システムコンポーネントには2種類あります。コンテナ内で実行されるコンポーネントと、コンテナ内で実行されないコンポーネントです。たとえば、次のようなコンポーネントがあります。</p>
<ul>
<li>Kubernetesのスケジューラーやkube-proxyはコンテナ内で実行されます。</li>
<li>kubeletやDockerのようなコンテナランタイムはコンテナ内で実行されません。</li>
</ul>
<p>systemdを使用しているマシンでは、kubeletとコンテナランタイムはjournaldに書き込みを行います。それ以外のマシンでは、<code>/var/log</code>ディレクトリ内の<code>.log</code>ファイルに書き込みます。コンテナ内部のシステムコンポーネントは、デフォルトのログ機構をバイパスするため、常に<code>/var/log</code>ディレクトリ内の<code>.log</code>ファイルに書き込みます。コンテナのログと同様に、<code>/var/log</code>ディレクトリ内のシステムコンポーネントのログはローテートする必要があります。<code>kube-up.sh</code>スクリプトによって作成されたKubernetesクラスターでは、ログローテーションは<code>logrotate</code>ツールで設定されます。<code>logrotate</code>ツールはログを1日ごとまたはログのサイズが100MBを超えたときにローテートします。</p>
<h2 id=次の項目>次の項目</h2>
<ul>
<li><a href=/docs/concepts/cluster-administration/logging/>Kubernetesのログのアーキテクチャ</a>について読む。</li>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging>構造化ログ</a>について読む。</li>
<li><a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>ログの深刻度の慣習</a>について読む。</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c4b1e87a84441f8a90699a345ce48d68>6 - ロギングのアーキテクチャ</h1>
<p>アプリケーションログは、アプリケーション内で何が起こっているかを理解するのに役立ちます。ログは、問題のデバッグとクラスターアクティビティの監視に特に役立ちます。最近のほとんどのアプリケーションには、何らかのロギングメカニズムがあります。同様に、コンテナエンジンはロギングをサポートするように設計されています。コンテナ化されたアプリケーションで、最も簡単で最も採用されているロギング方法は、標準出力と標準エラーストリームへの書き込みです。</p>
<p>ただし、コンテナエンジンまたはランタイムによって提供されるネイティブ機能は、たいていの場合、完全なロギングソリューションには十分ではありません。</p>
<p>たとえば、コンテナがクラッシュした場合やPodが削除された場合、またはノードが停止した場合に、アプリケーションのログにアクセスしたい場合があります。</p>
<p>クラスターでは、ノードやPod、またはコンテナに関係なく、ノードに個別のストレージとライフサイクルが必要です。この概念は、<em>クラスターレベルロギング</em> と呼ばれます。</p>
<p>クラスターレベルロギングのアーキテクチャでは、ログを保存、分析、およびクエリするための個別のバックエンドが必要です。Kubernetesは、ログデータ用のネイティブストレージソリューションを提供していません。代わりに、Kubernetesに統合される多くのロギングソリューションがあります。次のセクションでは、ノードでログを処理および保存する方法について説明します。</p>
<h2 id=basic-logging-in-kubernetes>Kubernetesでの基本的なロギング</h2>
<p>この例では、1秒に1回標準出力ストリームにテキストを書き込むコンテナを利用する、<code>Pod</code> specificationを使います。</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/ja/examples/debug/counter-pod.yaml download=debug/counter-pod.yaml><code>debug/counter-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('debug-counter-pod-yaml')" title="Copy debug/counter-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=debug-counter-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c,<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:#b44>&#39;i=0; while true; do echo &#34;$i: $(date)&#34;; i=$((i+1)); sleep 1; done&#39;</span>]<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>このPodを実行するには、次のコマンドを使用します:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</code></pre></div><p>出力は次のようになります:</p>
<pre><code class=language-console data-lang=console>pod/counter created
</code></pre><p>ログを取得するには、以下のように<code>kubectl logs</code>コマンドを使用します:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs counter
</code></pre></div><p>出力は次のようになります:</p>
<pre><code class=language-console data-lang=console>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><p>コンテナの以前のインスタンスからログを取得するために、<code>kubectl logs --previous</code>を使用できます。Podに複数のコンテナがある場合は、次のように-cフラグでコマンドにコンテナ名を追加することで、アクセスするコンテナのログを指定します。</p>
<pre><code class=language-console data-lang=console>kubectl logs counter -c count
</code></pre><p>詳細については、<a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code>ドキュメント</a>を参照してください。</p>
<h2 id=logging-at-the-node-level>ノードレベルでのロギング</h2>
<p><img src=/images/docs/user-guide/logging/logging-node-level.png alt="Node level logging"></p>
<p>コンテナエンジンは、生成された出力を処理して、コンテナ化されたアプリケーションの<code>stdout</code>と<code>stderr</code>ストリームにリダイレクトします。たとえば、Dockerコンテナエンジンは、これら2つのストリームを<a href=https://docs.docker.com/engine/admin/logging/overview>ロギングドライバー</a>にリダイレクトします。ロギングドライバーは、JSON形式でファイルに書き込むようにKubernetesで設定されています。</p>
<div class="alert alert-info note callout" role=alert>
<strong>備考:</strong> Docker JSONロギングドライバーは、各行を個別のメッセージとして扱います。Dockerロギングドライバーを使用する場合、複数行メッセージを直接サポートすることはできません。ロギングエージェントレベルあるいはそれ以上のレベルで、複数行のメッセージを処理する必要があります。
</div>
<p>デフォルトでは、コンテナが再起動すると、kubeletは1つの終了したコンテナをログとともに保持します。Podがノードから削除されると、対応する全てのコンテナが、ログとともに削除されます。</p>
<p>ノードレベルロギングでの重要な考慮事項は、ノードで使用可能な全てのストレージをログが消費しないように、ログローテーションを実装することです。Kubernetesはログのローテーションを担当しませんが、デプロイツールでそれに対処するソリューションを構築する必要があります。たとえば、<code>kube-up.sh</code>スクリプトによってデプロイされたKubernetesクラスターには、1時間ごとに実行するように構成された<a href=https://linux.die.net/man/8/logrotate><code>logrotate</code></a>ツールがあります。アプリケーションのログを自動的にローテーションするようにコンテナランタイムを構築することもできます。</p>
<p>例として、<a href=https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh><code>configure-helper</code> script</a>に対応するスクリプトである<code>kube-up.sh</code>が、どのようにGCPでCOSイメージのロギングを構築しているかについて、詳細な情報を見つけることができます。</p>
<p><strong>CRIコンテナランタイム</strong>を使用する場合、kubeletはログのローテーションとログディレクトリ構造の管理を担当します。kubeletはこの情報をCRIコンテナランタイムに送信し、ランタイムはコンテナログを指定された場所に書き込みます。2つのkubeletパラメーター、<a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration><code>container-log-max-size</code>と<code>container-log-max-files</code></a>を<a href=/docs/tasks/administer-cluster/kubelet-config-file/>kubelet設定ファイル</a>で使うことで、各ログファイルの最大サイズと各コンテナで許可されるファイルの最大数をそれぞれ設定できます。</p>
<p>基本的なロギングの例のように、<a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code></a>を実行すると、ノード上のkubeletがリクエストを処理し、ログファイルから直接読み取ります。kubeletはログファイルの内容を返します。</p>
<div class="alert alert-info note callout" role=alert>
<strong>備考:</strong> 外部システムがローテーションを実行した場合、またはCRIコンテナランタイムが使用されている場合は、最新のログファイルの内容のみが<code>kubectl logs</code>で利用可能になります。例えば、10MBのファイルがある場合、<code>logrotate</code>によるローテーションが実行されると、2つのファイルが存在することになります: 1つはサイズが10MBのファイルで、もう1つは空のファイルです。この例では、<code>kubectl logs</code>は最新のログファイルの内容、つまり空のレスポンスを返します。
</div>
<h3 id=system-component-logs>システムコンポーネントログ</h3>
<p>システムコンポーネントには、コンテナ内で実行されるものとコンテナ内で実行されないものの2種類があります。例えば以下のとおりです。</p>
<ul>
<li>Kubernetesスケジューラーとkube-proxyはコンテナ内で実行されます。</li>
<li>kubeletとコンテナランタイムはコンテナ内で実行されません。</li>
</ul>
<p>systemdを搭載したマシンでは、kubeletとコンテナランタイムがjournaldに書き込みます。systemdが存在しない場合、kubeletとコンテナランタイムは<code>var/log</code>ディレクトリ内の<code>.log</code>ファイルに書き込みます。コンテナ内のシステムコンポーネントは、デフォルトのロギングメカニズムを迂回して、常に<code>/var/log</code>ディレクトリに書き込みます。それらは<a href=https://github.com/kubernetes/klog><code>klog</code></a>というロギングライブラリを使用します。これらのコンポーネントのロギングの重大性に関する規則は、<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>development docs on logging</a>に記載されています。</p>
<p>コンテナログと同様に、<code>/var/log</code>ディレクトリ内のシステムコンポーネントログはローテーションする必要があります。<code>kube-up.sh</code>スクリプトによって生成されたKubernetesクラスターでは、これらのログは、<code>logrotate</code>ツールによって毎日、またはサイズが100MBを超えた時にローテーションされるように設定されています。</p>
<h2 id=cluster-level-logging-architectures>クラスターレベルロギングのアーキテクチャ</h2>
<p>Kubernetesはクラスターレベルロギングのネイティブソリューションを提供していませんが、検討可能な一般的なアプローチがいくつかあります。ここにいくつかのオプションを示します:</p>
<ul>
<li>全てのノードで実行されるノードレベルのロギングエージェントを使用します。</li>
<li>アプリケーションのPodにログインするための専用のサイドカーコンテナを含めます。</li>
<li>アプリケーション内からバックエンドに直接ログを送信します。</li>
</ul>
<h3 id=using-a-node-logging-agent>ノードロギングエージェントの使用</h3>
<p><img src=/images/docs/user-guide/logging/logging-with-node-agent.png alt="Using a node level logging agent"></p>
<p>各ノードに <em>ノードレベルのロギングエージェント</em> を含めることで、クラスターレベルロギングを実装できます。ロギングエージェントは、ログを公開したり、ログをバックエンドに送信したりする専用のツールです。通常、ロギングエージェントは、そのノード上の全てのアプリケーションコンテナからのログファイルを含むディレクトリにアクセスできるコンテナです。</p>
<p>ロギングエージェントは全てのノードで実行する必要があるため、エージェントを<code>DaemonSet</code>として実行することをおすすめします。</p>
<p>ノードレベルのロギングは、ノードごとに1つのエージェントのみを作成し、ノードで実行されているアプリケーションに変更を加える必要はありません。</p>
<p>コンテナはstdoutとstderrに書き込みますが、合意された形式はありません。ノードレベルのエージェントはこれらのログを収集し、集約のために転送します。</p>
<h3 id=sidecar-container-with-logging-agent>ロギングエージェントでサイドカーコンテナを使用する</h3>
<p>サイドカーコンテナは、次のいずれかの方法で使用できます:</p>
<ul>
<li>サイドカーコンテナは、アプリケーションログを自身の<code>stdout</code>にストリーミングします。</li>
<li>サイドカーコンテナは、アプリケーションコンテナからログを取得するように設定されたロギングエージェントを実行します。</li>
</ul>
<h4 id=streaming-sidecar-container>ストリーミングサイドカーコンテナ</h4>
<p><img src=/images/docs/user-guide/logging/logging-with-streaming-sidecar.png alt="Sidecar container with a streaming container"></p>
<p>サイドカーコンテナに自身の<code>stdout</code>や<code>stderr</code>ストリームへの書き込みを行わせることで、各ノードですでに実行されているkubeletとロギングエージェントを利用できます。サイドカーコンテナは、ファイル、ソケット、またはjournaldからログを読み取ります。各サイドカーコンテナは、ログを自身の<code>stdout</code>または<code>stderr</code>ストリームに出力します。</p>
<p>このアプローチにより、<code>stdout</code>または<code>stderr</code>への書き込みのサポートが不足している場合も含め、アプリケーションのさまざまな部分からいくつかのログストリームを分離できます。ログのリダイレクトの背後にあるロジックは最小限であるため、大きなオーバーヘッドにはなりません。さらに、<code>stdout</code>と<code>stderr</code>はkubeletによって処理されるため、<code>kubectl logs</code>のような組み込みツールを使用できます。</p>
<p>たとえば、Podは単一のコンテナを実行し、コンテナは2つの異なる形式を使用して2つの異なるログファイルに書き込みます。Podの構成ファイルは次のとおりです:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/ja/examples/admin/logging/two-files-counter-pod.yaml download=admin/logging/two-files-counter-pod.yaml><code>admin/logging/two-files-counter-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-logging-two-files-counter-pod-yaml')" title="Copy admin/logging/two-files-counter-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-logging-two-files-counter-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>      i=0;
</span><span style=color:#b44;font-style:italic>      while true;
</span><span style=color:#b44;font-style:italic>      do
</span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span><span style=color:#b44;font-style:italic>        sleep 1;
</span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>両方のコンポーネントをコンテナの<code>stdout</code>ストリームにリダイレクトできたとしても、異なる形式のログエントリを同じログストリームに書き込むことはおすすめしません。代わりに、2つのサイドカーコンテナを作成できます。各サイドカーコンテナは、共有ボリュームから特定のログファイルを追跡し、ログを自身の<code>stdout</code>ストリームにリダイレクトできます。</p>
<p>2つのサイドカーコンテナを持つPodの構成ファイルは次のとおりです:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/ja/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml download=admin/logging/two-files-counter-pod-streaming-sidecar.yaml><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-logging-two-files-counter-pod-streaming-sidecar-yaml')" title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-logging-two-files-counter-pod-streaming-sidecar-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>      i=0;
</span><span style=color:#b44;font-style:italic>      while true;
</span><span style=color:#b44;font-style:italic>      do
</span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span><span style=color:#b44;font-style:italic>        sleep 1;
</span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-1<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -f /var/log/1.log&#39;]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-2<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -f /var/log/2.log&#39;]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>これで、このPodを実行するときに、次のコマンドを実行して、各ログストリームに個別にアクセスできます:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs counter count-log-1
</code></pre></div><p>出力は次のようになります:</p>
<pre><code class=language-console data-lang=console>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs counter count-log-2
</code></pre></div><p>出力は次のようになります:</p>
<pre><code class=language-console data-lang=console>Mon Jan  1 00:00:00 UTC 2001 INFO 0
Mon Jan  1 00:00:01 UTC 2001 INFO 1
Mon Jan  1 00:00:02 UTC 2001 INFO 2
...
</code></pre><p>クラスターにインストールされているノードレベルのエージェントは、それ以上の設定を行わなくても、これらのログストリームを自動的に取得します。必要があれば、ソースコンテナに応じてログをパースするようにエージェントを構成できます。</p>
<p>CPUとメモリーの使用量が少ない(CPUの場合は数ミリコアのオーダー、メモリーの場合は数メガバイトのオーダー)にも関わらず、ログをファイルに書き込んでから<code>stdout</code>にストリーミングすると、ディスクの使用量が2倍になる可能性があることに注意してください。単一のファイルに書き込むアプリケーションがある場合は、ストリーミングサイドカーコンテナアプローチを実装するのではなく、<code>/dev/stdout</code>を宛先として設定することをおすすめします。</p>
<p>サイドカーコンテナを使用して、アプリケーション自体ではローテーションできないログファイルをローテーションすることもできます。このアプローチの例は、<code>logrotate</code>を定期的に実行する小さなコンテナです。しかし、<code>stdout</code>と<code>stderr</code>を直接使い、ローテーションと保持のポリシーをkubeletに任せることをおすすめします。</p>
<h4 id=sidecar-container-with-a-logging-agent>ロギングエージェントを使用したサイドカーコンテナ</h4>
<p><img src=/images/docs/user-guide/logging/logging-with-sidecar-agent.png alt="Sidecar container with a logging agent"></p>
<p>ノードレベルロギングのエージェントが、あなたの状況に必要なだけの柔軟性を備えていない場合は、アプリケーションで実行するように特別に構成した別のロギングエージェントを使用してサイドカーコンテナを作成できます。</p>
<div class="alert alert-info note callout" role=alert>
<strong>備考:</strong> サイドカーコンテナでロギングエージェントを使用すると、大量のリソースが消費される可能性があります。さらに、これらのログはkubeletによって制御されていないため、<code>kubectl logs</code>を使用してこれらのログにアクセスすることができません。
</div>
<p>ロギングエージェントを使用したサイドカーコンテナを実装するために使用できる、2つの構成ファイルを次に示します。最初のファイルには、fluentdを設定するための<a href=/ja/docs/tasks/configure-pod-container/configure-pod-configmap/><code>ConfigMap</code></a>が含まれています。</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/ja/examples/admin/logging/fluentd-sidecar-config.yaml download=admin/logging/fluentd-sidecar-config.yaml><code>admin/logging/fluentd-sidecar-config.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-logging-fluentd-sidecar-config-yaml')" title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-logging-fluentd-sidecar-config-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fluentd.conf</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span><span style=color:#b44;font-style:italic>      type tail
</span><span style=color:#b44;font-style:italic>      format none
</span><span style=color:#b44;font-style:italic>      path /var/log/1.log
</span><span style=color:#b44;font-style:italic>      pos_file /var/log/1.log.pos
</span><span style=color:#b44;font-style:italic>      tag count.format1
</span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span><span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span><span style=color:#b44;font-style:italic>      type tail
</span><span style=color:#b44;font-style:italic>      format none
</span><span style=color:#b44;font-style:italic>      path /var/log/2.log
</span><span style=color:#b44;font-style:italic>      pos_file /var/log/2.log.pos
</span><span style=color:#b44;font-style:italic>      tag count.format2
</span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span><span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    &lt;match **&gt;
</span><span style=color:#b44;font-style:italic>      type google_cloud
</span><span style=color:#b44;font-style:italic>    &lt;/match&gt;</span><span style=color:#bbb>    
</span></code></pre></div>
</div>
</div>
<div class="alert alert-info note callout" role=alert>
<strong>備考:</strong> fluentdの構成については、<a href=https://docs.fluentd.org/>fluentd documentation</a>を参照してください。
</div>
<p>2番目のファイルは、fluentdを実行しているサイドカーコンテナを持つPodを示しています。Podは、fluentdが構成データを取得できるボリュームをマウントします。</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/ja/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml download=admin/logging/two-files-counter-pod-agent-sidecar.yaml><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-logging-two-files-counter-pod-agent-sidecar-yaml')" title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-logging-two-files-counter-pod-agent-sidecar-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>      i=0;
</span><span style=color:#b44;font-style:italic>      while true;
</span><span style=color:#b44;font-style:italic>      do
</span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span><span style=color:#b44;font-style:italic>        sleep 1;
</span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-agent<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/fluentd-gcp:1.30<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>FLUENTD_ARGS<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>-c /etc/fluentd-config/fluentd.conf<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/fluentd-config<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>サンプル構成では、fluentdを任意のロギングエージェントに置き換えて、アプリケーションコンテナ内の任意のソースから読み取ることができます。</p>
<h3 id=exposing-logs-directly-from-the-application>アプリケーションから直接ログを公開する</h3>
<p><img src=/images/docs/user-guide/logging/logging-from-application.png alt="Exposing logs directly from the application"></p>
<p>すべてのアプリケーションから直接ログを公開または送信するクラスターロギングは、Kubernetesのスコープ外です。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2e05a56491965ae320c2662590b2ca18>7 - コンテナイメージのガベージコレクション</h1>
<p>ガベージコレクションは、未使用の<a href=/ja/docs/concepts/containers/#container-images>イメージ</a>と未使用の<a href=/ja/docs/concepts/containers/>コンテナ</a>をクリーンアップするkubeletの便利な機能です。kubeletコンテナのガベージコレクションを1分ごとに行い、イメージのガベージコレクションは5分ごとに行います。</p>
<p>存在することが期待されているコンテナを削除してkubeletの動作を壊す可能性があるため、外部のガベージコレクションのツールは推奨されません。</p>
<h2 id=イメージのガベージコレクション>イメージのガベージコレクション</h2>
<p>Kubernetesでは、すべてのイメージのライフサイクルの管理はcadvisorと協調してimageManager経由で行います。</p>
<p>イメージのガベージコレクションのポリシーについて考えるときは、<code>HighThresholdPercent</code>および<code>LowThresholdPercent</code>という2つの要因について考慮する必要があります。ディスク使用量がhigh thresholdを超えると、ガベージコレクションがトリガされます。ガベージコレクションは、low
thresholdが満たされるまで、最後に使われてから最も時間が経った(least recently used)イメージを削除します。</p>
<h2 id=コンテナのガベージコレクション>コンテナのガベージコレクション</h2>
<p>コンテナのガベージコレクションのポリシーは、3つのユーザー定義の変数を考慮に入れます。<code>MinAge</code>は、ガベージコレクションできるコンテナの最小の年齢です。<code>MaxPerPodContainer</code>は、すべての単一のPod(UID、コンテナ名)が保持することを許されているdead状態のコンテナの最大値です。<code>MaxContainers</code>はdead状態のコンテナの合計の最大値です。これらの変数は、<code>MinAge</code>は0に、<code>MaxPerPodContainer</code>と<code>MaxContainers</code>は0未満にそれぞれ設定することで個別に無効にできます。</p>
<p>kubeletは、未指定のコンテナ、削除されたコンテナ、前述のフラグにより設定された境界の外にあるコンテナに対して動作します。一般に、最も古いコンテナが最初に削除されます。<code>MaxPerPodContainer</code>と<code>MaxContainer</code>は、Podごとの保持するコンテナの最大値(<code>MaxPerPodContainer</code>)がグローバルのdead状態のコンテナの許容範囲(<code>MaxContainers</code>)外である場合には、互いに競合する可能性があります。このような状況では、<code>MaxPerPodContainer</code>が調整されます。最悪のケースのシナリオでは、<code>MaxPerPodContainer</code>が1にダウングレードされ、最も古いコンテナが強制退去されます。さらに、<code>MinAge</code>より古くなると、削除済みのPodが所有するコンテナが削除されます。</p>
<p>kubeletによって管理されないコンテナは、コンテナのガベージコレクションの対象にはなりません。</p>
<h2 id=ユーザー設定>ユーザー設定</h2>
<p>イメージのガベージコレクションを調整するために、以下のkubeletのフラグを使用して次のようなしきい値を調整できます。</p>
<ol>
<li><code>image-gc-high-threshold</code>: イメージのガベージコレクションをトリガするディスク使用量の割合(%)。デフォルトは85%。</li>
<li><code>image-gc-low-threshold</code>: イメージのガベージコレクションが解放を試みるディスク使用量の割合(%)。デフォルトは80%。</li>
</ol>
<p>ガベージコレクションのポリシーは、以下のkubeletのフラグを使用してカスタマイズできます。</p>
<ol>
<li><code>minimum-container-ttl-duration</code>: 完了したコンテナがガベージコレクションされる前に経過するべき最小期間。デフォルトは0分です。つまり、すべての完了したコンテナはガベージコレクションされます。</li>
<li><code>maximum-dead-containers-per-container</code>: コンテナごとに保持される古いインスタンスの最大値です。デフォルトは1です。</li>
<li><code>maximum-dead-containers</code>: グローバルに保持するべき古いコンテナのインスタンスの最大値です。デフォルトは-1です。つまり、グローバルなリミットは存在しません。</li>
</ol>
<p>コンテナは役に立たなくなる前にガベージコレクションされる可能性があります。こうしたコンテナには、トラブルシューティングに役立つログや他のデータが含まれるかもしれません。そのため、期待されるコンテナごとに最低でも1つのdead状態のコンテナが許容されるようにするために、<code>maximum-dead-containers-per-container</code>には十分大きな値を設定することが強く推奨されます。同様の理由で、<code>maximum-dead-containers</code>にも、より大きな値を設定することが推奨されます。詳しくは、<a href=https://github.com/kubernetes/kubernetes/issues/13287>こちらのissue</a>を読んでください。</p>
<h2 id=廃止>廃止</h2>
<p>このドキュメントにあるkubeletの一部のガベージコレクションの機能は、将来kubelet evictionで置換される予定です。</p>
<p>これには以下のものが含まれます。</p>
<table>
<thead>
<tr>
<th>既存のフラグ</th>
<th>新しいフラグ</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--image-gc-high-threshold</code></td>
<td><code>--eviction-hard</code>または<code>--eviction-soft</code></td>
<td>既存のevictionのシグナルがイメージのガベージコレクションをトリガする可能性がある</td>
</tr>
<tr>
<td><code>--image-gc-low-threshold</code></td>
<td><code>--eviction-minimum-reclaim</code></td>
<td>eviction reclaimが同等の動作を実現する</td>
</tr>
<tr>
<td><code>--maximum-dead-containers</code></td>
<td></td>
<td>古いログがコンテナのコンテキストの外部に保存されるようになったら廃止</td>
</tr>
<tr>
<td><code>--maximum-dead-containers-per-container</code></td>
<td></td>
<td>古いログがコンテナのコンテキストの外部に保存されるようになったら廃止</td>
</tr>
<tr>
<td><code>--minimum-container-ttl-duration</code></td>
<td></td>
<td>古いログがコンテナのコンテキストの外部に保存されるようになったら廃止</td>
</tr>
<tr>
<td><code>--low-diskspace-threshold-mb</code></td>
<td><code>--eviction-hard</code> or <code>eviction-soft</code></td>
<td>evictionはディスクのしきい値を他のリソースに一般化している</td>
</tr>
<tr>
<td><code>--outofdisk-transition-frequency</code></td>
<td><code>--eviction-pressure-transition-period</code></td>
<td>evictionはディスクのpressure transitionを他のリソースに一般化している</td>
</tr>
</tbody>
</table>
<h2 id=次の項目>次の項目</h2>
<p>詳細については、<a href=/docs/tasks/administer-cluster/out-of-resource/>リソース不足のハンドリング方法を設定する</a>を参照してください。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-08e94e6a480e0d6b2de72d84a1b97617>8 - Kubernetesのプロキシー</h1>
<p>このページではKubernetesと併用されるプロキシーについて説明します。</p>
<h2 id=プロキシー>プロキシー</h2>
<p>Kubernetesを使用する際に、いくつかのプロキシーを使用する場面があります。</p>
<ol>
<li>
<p><a href=/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api>kubectlのプロキシー</a>:</p>
<ul>
<li>ユーザーのデスクトップ上かPod内で稼働します</li>
<li>ローカルホストのアドレスからKubernetes apiserverへのプロキシーを行います</li>
<li>クライアントからプロキシー間ではHTTPを使用します</li>
<li>プロキシーからapiserverへはHTTPSを使用します</li>
<li>apiserverの場所を示します</li>
<li>認証用のヘッダーを追加します</li>
</ul>
</li>
<li>
<p><a href=/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>apiserverのプロキシー</a>:</p>
<ul>
<li>apiserver内で動作する踏み台となります</li>
<li>これがなければ到達不可能であるクラスターIPへ、クラスターの外部からのユーザーを接続します</li>
<li>apiserverのプロセス内で稼働します</li>
<li>クライアントからプロキシー間ではHTTPSを使用します(apiserverの設定により、HTTPを使用します)</li>
<li>プロキシーからターゲット間では利用可能な情報を使用して、プロキシーによって選択されたHTTPかHTTPSのいずれかを使用します</li>
<li>Node、Pod、Serviceへ到達するのに使えます</li>
<li>Serviceへ到達するときは負荷分散を行います</li>
</ul>
</li>
<li>
<p><a href=/ja/docs/concepts/services-networking/service/#ips-and-vips>kube proxy</a>:</p>
<ul>
<li>各ノード上で稼働します</li>
<li>UDP、TCP、SCTPをプロキシーします</li>
<li>HTTPを解釈しません</li>
<li>負荷分散機能を提供します</li>
<li>Serviceへ到達させるためのみに使用されます</li>
</ul>
</li>
<li>
<p>apiserverの前段にあるプロキシー/ロードバランサー:</p>
<ul>
<li>実際に存在するかどうかと実装はクラスターごとに異なります(例: nginx)</li>
<li>全てのクライアントと、1つ以上のapiserverの間に位置します</li>
<li>複数のapiserverがあるときロードバランサーとして稼働します</li>
</ul>
</li>
<li>
<p>外部サービス上で稼働するクラウドロードバランサー:</p>
<ul>
<li>いくつかのクラウドプロバイダーによって提供されます(例: AWS ELB、Google Cloud Load Balancer)</li>
<li><code>LoadBalancer</code>というtypeのKubernetes Serviceが作成されたときに自動で作成されます</li>
<li>たいていのクラウドロードバランサーはUDP/TCPのみサポートしています</li>
<li>SCTPのサポートはクラウドプロバイダーのロードバランサーの実装によって異なります</li>
<li>ロードバランサーの実装はクラウドプロバイダーによって異なります</li>
</ul>
</li>
</ol>
<p>Kubernetesユーザーのほとんどは、最初の2つのタイプ以外に心配する必要はありません。クラスター管理者はそれ以外のタイプのロードバランサーを正しくセットアップすることを保証します。</p>
<h2 id=リダイレクトの要求>リダイレクトの要求</h2>
<p>プロキシーはリダイレクトの機能を置き換えました。リダイレクトの使用は非推奨となります。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-85d633ae590aa20ec024f1b7af1d74fc>9 - アドオンのインストール</h1>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>備考:</strong>
このセクションでは、Kubernetesが必要とする機能を提供するサードパーティープロジェクトにリンクしています。これらのプロジェクトはアルファベット順に記載されていて、Kubernetesプロジェクトの作者は責任を持ちません。このリストにプロジェクトを追加するには、変更を提出する前に<a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a>をお読みください。<a href=#third-party-content-disclaimer>詳細はこちら。</a></div>
<p>アドオンはKubernetesの機能を拡張するものです。</p>
<p>このページでは、利用可能なアドオンの一部の一覧と、それぞれのアドオンのインストール方法へのリンクを提供します。</p>
<h2 id=ネットワークとネットワークポリシー>ネットワークとネットワークポリシー</h2>
<ul>
<li><a href=https://www.github.com/noironetworks/aci-containers>ACI</a>は、統合されたコンテナネットワークとネットワークセキュリティをCisco ACIを使用して提供します。</li>
<li><a href=https://antrea.io/>Antrea</a>は、L3またはL4で動作して、Open vSwitchをネットワークデータプレーンとして活用する、Kubernetes向けのネットワークとセキュリティサービスを提供します。</li>
<li><a href=https://docs.projectcalico.org/latest/introduction/>Calico</a>はネットワークとネットワークプリシーのプロバイダーです。Calicoは、BGPを使用または未使用の非オーバーレイおよびオーバーレイネットワークを含む、フレキシブルなさまざまなネットワークオプションをサポートします。Calicoはホスト、Pod、そして(IstioとEnvoyを使用している場合には)サービスメッシュ上のアプリケーションに対してネットワークポリシーを強制するために、同一のエンジンを使用します。</li>
<li><a href=https://github.com/tigera/canal/tree/master/k8s-install>Canal</a>はFlannelとCalicoをあわせたもので、ネットワークとネットワークポリシーを提供します。</li>
<li><a href=https://github.com/cilium/cilium>Cilium</a>は、L3のネットワークとネットワークポリシーのプラグインで、HTTP/API/L7のポリシーを透過的に強制できます。ルーティングとoverlay/encapsulationモードの両方をサポートしており、他のCNIプラグイン上で機能できます。</li>
<li><a href=https://github.com/Huawei-PaaS/CNI-Genie>CNI-Genie</a>は、KubernetesをCalico、Canal、Flannel、Romana、Weaveなど選択したCNIプラグインをシームレスに接続できるようにするプラグインです。</li>
<li><a href=https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a>は、<a href=https://tungsten.io>Tungsten Fabric</a>をベースにしている、オープンソースでマルチクラウドに対応したネットワーク仮想化およびポリシー管理プラットフォームです。ContrailおよびTungsten Fabricは、Kubernetes、OpenShift、OpenStack、Mesosなどのオーケストレーションシステムと統合されており、仮想マシン、コンテナ/Pod、ベアメタルのワークロードに隔離モードを提供します。</li>
<li><a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>Flannel</a>は、Kubernetesで使用できるオーバーレイネットワークプロバイダーです。</li>
<li><a href=https://github.com/ZTE/Knitter/>Knitter</a>は、1つのKubernetes Podで複数のネットワークインターフェイスをサポートするためのプラグインです。</li>
<li>Multus は、すべてのCNIプラグイン(たとえば、Calico、Cilium、Contiv、Flannel)に加えて、SRIOV、DPDK、OVS-DPDK、VPPをベースとするKubernetes上のワークロードをサポートする、複数のネットワークサポートのためのマルチプラグインです。</li>
<li><a href=https://github.com/ovn-org/ovn-kubernetes/>OVN-Kubernetes</a>は、Open vSwitch(OVS)プロジェクトから生まれた仮想ネットワーク実装である<a href=https://github.com/ovn-org/ovn/>OVN(Open Virtual Network)</a>をベースとする、Kubernetesのためのネットワークプロバイダです。OVN-Kubernetesは、OVSベースのロードバランサーおよびネットワークポリシーの実装を含む、Kubernetes向けのオーバーレイベースのネットワーク実装を提供します。</li>
<li><a href=https://github.com/opnfv/ovn4nfv-k8s-plugin>OVN4NFV-K8S-Plugin</a>は、クラウドネイティブベースのService function chaining(SFC)、Multiple OVNオーバーレイネットワーク、動的なサブネットの作成、動的な仮想ネットワークの作成、VLANプロバイダーネットワーク、Directプロバイダーネットワークを提供し、他のMulti-networkプラグインと付け替え可能なOVNベースのCNIコントローラープラグインです。</li>
<li><a href=https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf>NSX-T</a> Container Plug-in(NCP)は、VMware NSX-TとKubernetesなどのコンテナオーケストレーター間のインテグレーションを提供します。また、NSX-Tと、Pivotal Container Service(PKS)とOpenShiftなどのコンテナベースのCaaS/PaaSプラットフォームとのインテグレーションも提供します。</li>
<li><a href=https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst>Nuage</a>は、Kubernetes Podと非Kubernetes環境間で可視化とセキュリティモニタリングを使用してポリシーベースのネットワークを提供するSDNプラットフォームです。</li>
<li><a href=https://github.com/romana/romana>Romana</a>は、<a href=/ja/docs/concepts/services-networking/network-policies/>NetworkPolicy API</a>もサポートするPodネットワーク向けのL3のネットワークソリューションです。Kubeadmアドオンのインストールの詳細は<a href=https://github.com/romana/romana/tree/master/containerize>こちら</a>で確認できます。</li>
<li><a href=https://www.weave.works/docs/net/latest/kubernetes/kube-addon/>Weave Net</a>は、ネットワークパーティションの両面で機能し、外部データベースを必要とせずに、ネットワークとネットワークポリシーを提供します。</li>
</ul>
<h2 id=サービスディスカバリ>サービスディスカバリ</h2>
<ul>
<li><a href=https://coredns.io>CoreDNS</a>は、フレキシブルで拡張可能なDNSサーバーです。Pod向けのクラスター内DNSとして<a href=https://github.com/coredns/deployment/tree/master/kubernetes>インストール</a>できます。</li>
</ul>
<h2 id=可視化と制御>可視化と制御</h2>
<ul>
<li><a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>Dashboard</a>はKubernetes向けのダッシュボードを提供するウェブインターフェイスです。</li>
<li><a href=https://www.weave.works/documentation/scope-latest-installing/#k8s>Weave Scope</a>は、コンテナ、Pod、Serviceなどをグラフィカルに可視化するツールです。<a href=https://cloud.weave.works/>Weave Cloud account</a>と組み合わせて使うか、UIを自分でホストして使います。</li>
</ul>
<h2 id=インフラストラクチャ>インフラストラクチャ</h2>
<ul>
<li><a href=https://kubevirt.io/user-guide/#/installation/installation>KubeVirt</a>は仮想マシンをKubernetes上で実行するためのアドオンです。通常、ベアメタルのクラスタで実行します。</li>
</ul>
<h2 id=レガシーなアドオン>レガシーなアドオン</h2>
<p>いくつかのアドオンは、廃止された<a href=https://git.k8s.io/kubernetes/cluster/addons>cluster/addons</a>ディレクトリに掲載されています。</p>
<p>よくメンテナンスされたアドオンはここにリンクしてください。PRを歓迎しています。</p>
</div>
</main>
</div>
</div>
<footer class=d-print-none>
<div class=footer__links>
<nav>
<a class=text-white href=/ja/docs/home/>ホーム</a>
<a class=text-white href=/ja/blog/>Blogs</a>
<a class=text-white href=/ja/training/>トレーニング</a>
<a class=text-white href=/ja/partners/>パートナー</a>
<a class=text-white href=/ja/community/>コミュニティ</a>
<a class=text-white href=/ja/case-studies/>ケーススタディ</a>
</nav>
</div>
<div class=container-fluid>
<div class=row>
<div class="col-6 col-sm-2 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list">
<a class=text-white target=_blank href=https://discuss.kubernetes.io>
<i class="fa fa-envelope"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter>
<a class=text-white target=_blank href=https://twitter.com/kubernetesio>
<i class="fab fa-twitter"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar>
<a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
<i class="fas fa-calendar-alt"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube>
<a class=text-white target=_blank href=https://youtube.com/kubernetescommunity>
<i class="fab fa-youtube"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank href=https://slack.k8s.io>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute>
<a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide>
<i class="fas fa-edit"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow">
<a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes>
<i class="fab fa-stack-overflow"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-8 text-center order-sm-2">
<small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small>
<br>
<small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small>
<br>
<small class=text-white>ICP license: 京ICP备17074266号-3</small>
</div>
</div>
</div>
</footer>
</div>
<script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script>
<script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script>
<script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script>
</body>
</html>