<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Kubernetes 1.24: Volume Populators Graduate to Beta</title><link>https://kubernetes.io/blog/2022/05/16/volume-populators-beta/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/16/volume-populators-beta/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong>
Ben Swartzlander (NetApp)&lt;/p>
&lt;p>The volume populators feature is now two releases old and entering beta! The &lt;code>AnyVolumeDataSouce&lt;/code> feature
gate defaults to enabled in Kubernetes v1.24, which means that users can specify any custom resource
as the data source of a PVC.&lt;/p>
&lt;p>An &lt;a href="https://kubernetes.io/blog/2021/08/30-volume-populators-redesigned/">earlier blog article&lt;/a> detailed how the
volume populators feature works. In short, a cluster administrator can install a CRD and
associated populator controller in the cluster, and any user who can create instances of
the CR can create pre-populated volumes by taking advantage of the populator.&lt;/p>
&lt;p>Multiple populators can be installed side by side for different purposes. The SIG storage
community is already seeing some implementations in public, and more prototypes should
appear soon.&lt;/p>
&lt;p>Cluster administrations are &lt;strong>strongly encouraged&lt;/strong> to install the
volume-data-source-validator controller and associated &lt;code>VolumePopulator&lt;/code> CRD before installing
any populators so that users can get feedback about invalid PVC data sources.&lt;/p>
&lt;h2 id="new-features">New Features&lt;/h2>
&lt;p>The &lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a> library
on which populators are built now includes metrics to help operators monitor and detect
problems. This library is now beta and latest release is v1.0.1.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-csi/volume-data-source-validator">volume data source validator&lt;/a>
controller also has metrics support added, and is in beta. The &lt;code>VolumePopulator&lt;/code> CRD is
beta and the latest release is v1.0.1.&lt;/p>
&lt;h2 id="trying-it-out">Trying it out&lt;/h2>
&lt;p>To see how this works, you can install the sample &amp;quot;hello&amp;quot; populator and try it
out.&lt;/p>
&lt;p>First install the volume-data-source-validator controller.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/client/config/crd/populator.storage.k8s.io_volumepopulators.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/rbac-data-source-validator.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/setup-data-source-validator.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next install the example populator.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/v1.0.1/example/hello-populator/crd.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/87a47467b86052819e9ad13d15036d65b9a32fbb/example/hello-populator/deploy.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Your cluster now has a new CustomResourceDefinition that provides a test API named Hello.
Create an instance of the &lt;code>Hello&lt;/code> custom resource, with some text:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.example.com/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileContents&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello, world!&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create a PVC that refers to that CR as its data source.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10Mi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSourceRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, run a Job that reads the file in the PVC.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-container&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>busybox:latest&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- cat&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /mnt/example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/mnt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeClaim&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Wait for the job to complete (including all of its dependencies).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl &lt;span style="color:#a2f">wait&lt;/span> --for&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b8860b">condition&lt;/span>&lt;span style="color:#666">=&lt;/span>Complete job/example-job
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And last examine the log from the job.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl logs job/example-job
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output should be:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">Hello, world!
&lt;/code>&lt;/pre>&lt;p>Note that the volume already contained a text file with the string contents from
the CR. This is only the simplest example. Actual populators can set up the volume
to contain arbitrary contents.&lt;/p>
&lt;h2 id="how-to-write-your-own-volume-populator">How to write your own volume populator&lt;/h2>
&lt;p>Developers interested in writing new poplators are encouraged to use the
&lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a> library
and to only supply a small controller wrapper around the library, and a pod image
capable of attaching to volumes and writing the appropriate data to the volume.&lt;/p>
&lt;p>Individual populators can be extremely generic such that they work with every type
of PVC, or they can do vendor specific things to rapidly fill a volume with data
if the volume was provisioned by a specific CSI driver from the same vendor, for
example, by communicating directly with the storage for that volume.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>The enhancement proposal,
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators">Volume Populators&lt;/a>, includes lots of detail about the history and technical implementation
of this feature.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources">Volume populators and data sources&lt;/a>, within the documentation topic about persistent volumes,
explains how to use this feature in your cluster.&lt;/p>
&lt;p>Please get involved by joining the Kubernetes storage SIG to help us enhance this
feature. There are a lot of good ideas already and we'd be thrilled to have more!&lt;/p></description></item><item><title>Blog: Storage Capacity Tracking reaches GA in Kubernetes 1.24</title><link>https://kubernetes.io/blog/2022/05/06/storage-capacity-ga/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/06/storage-capacity-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>The v1.24 release of Kubernetes brings &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">storage capacity&lt;/a>
tracking as a generally available feature.&lt;/p>
&lt;h2 id="problems-we-have-solved">Problems we have solved&lt;/h2>
&lt;p>As explained in more detail in the &lt;a href="https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/">previous blog post about this
feature&lt;/a>, storage capacity
tracking allows a CSI driver to publish information about remaining
capacity. The kube-scheduler then uses that information to pick suitable nodes
for a Pod when that Pod has volumes that still need to be provisioned.&lt;/p>
&lt;p>Without this information, a Pod may get stuck without ever being scheduled onto
a suitable node because kube-scheduler has to choose blindly and always ends up
picking a node for which the volume cannot be provisioned because the
underlying storage system managed by the CSI driver does not have sufficient
capacity left.&lt;/p>
&lt;p>Because CSI drivers publish storage capacity information that gets used at a
later time when it might not be up-to-date anymore, it can still happen that a
node is picked that doesn't work out after all. Volume provisioning recovers
from that by informing the scheduler that it needs to try again with a
different node.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/master/docs/storage-capacity-tracking.md">Load
tests&lt;/a>
that were done again for promotion to GA confirmed that all storage in a
cluster can be consumed by Pods with storage capacity tracking whereas Pods got
stuck without it.&lt;/p>
&lt;h2 id="problems-we-have-not-solved">Problems we have &lt;em>not&lt;/em> solved&lt;/h2>
&lt;p>Recovery from a failed volume provisioning attempt has one known limitation: if a Pod
uses two volumes and only one of them could be provisioned, then all future
scheduling decisions are limited by the already provisioned volume. If that
volume is local to a node and the other volume cannot be provisioned there, the
Pod is stuck. This problem pre-dates storage capacity tracking and while the
additional information makes it less likely to occur, it cannot be avoided in
all cases, except of course by only using one volume per Pod.&lt;/p>
&lt;p>An idea for solving this was proposed in a &lt;a href="https://github.com/kubernetes/enhancements/pull/1703">KEP
draft&lt;/a>: volumes that were
provisioned and haven't been used yet cannot have any valuable data and
therefore could be freed and provisioned again elsewhere. SIG Storage is
looking for interested developers who want to continue working on this.&lt;/p>
&lt;p>Also not solved is support in Cluster Autoscaler for Pods with volumes. For CSI
drivers with storage capacity tracking, a prototype was developed and discussed
in &lt;a href="https://github.com/kubernetes/autoscaler/pull/3887">a PR&lt;/a>. It was meant to
work with arbitrary CSI drivers, but that flexibility made it hard to configure
and slowed down scale up operations: because autoscaler was unable to simulate
volume provisioning, it only scaled the cluster by one node at a time, which
was seen as insufficient.&lt;/p>
&lt;p>Therefore that PR was not merged and a different approach with tighter coupling
between autoscaler and CSI driver will be needed. For this a better
understanding is needed about which local storage CSI drivers are used in
combination with cluster autoscaling. Should this lead to a new KEP, then users
will have to try out an implementation in practice before it can move to beta
or GA. So please reach out to SIG Storage if you have an interest in this
topic.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Thanks a lot to the members of the community who have contributed to this
feature or given feedback including members of &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scheduling">SIG
Scheduling&lt;/a>,
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">SIG
Autoscaling&lt;/a>,
and of course &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG
Storage&lt;/a>!&lt;/p></description></item><item><title>Blog: Dockershim: The Historical Context</title><link>https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kat Cosgrove&lt;/p>
&lt;p>Dockershim has been removed as of Kubernetes v1.24, and this is a positive move for the project. However, context is important for fully understanding something, be it socially or in software development, and this deserves a more in-depth review. Alongside the dockershim removal in Kubernetes v1.24, we’ve seen some confusion (sometimes at a panic level) and dissatisfaction with this decision in the community, largely due to a lack of context around this removal. The decision to deprecate and eventually remove dockershim from Kubernetes was not made quickly or lightly. Still, it’s been in the works for so long that many of today’s users are newer than that decision, and certainly newer than the choices that led to the dockershim being necessary in the first place.&lt;/p>
&lt;p>So what is the dockershim, and why is it going away?&lt;/p>
&lt;p>In the early days of Kubernetes, we only supported one container runtime. That runtime was Docker Engine. Back then, there weren’t really a lot of other options out there and Docker was the dominant tool for working with containers, so this was not a controversial choice. Eventually, we started adding more container runtimes, like rkt and hypernetes, and it became clear that Kubernetes users want a choice of runtimes working best for them. So Kubernetes needed a way to allow cluster operators the flexibility to use whatever runtime they choose.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface&lt;/a> (CRI) was released to allow that flexibility. The introduction of CRI was great for the project and users alike, but it did introduce a problem: Docker Engine’s use as a container runtime predates CRI, and Docker Engine is not CRI-compatible. To solve this issue, a small software shim (dockershim) was introduced as part of the kubelet component specifically to fill in the gaps between Docker Engine and CRI, allowing cluster operators to continue using Docker Engine as their container runtime largely uninterrupted.&lt;/p>
&lt;p>However, this little software shim was never intended to be a permanent solution. Over the course of years, its existence has introduced a lot of unnecessary complexity to the kubelet itself. Some integrations are inconsistently implemented for Docker because of this shim, resulting in an increased burden on maintainers, and maintaining vendor-specific code is not in line with our open source philosophy. To reduce this maintenance burden and move towards a more collaborative community in support of open standards, &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">KEP-2221 was introduced&lt;/a>, proposing the removal of the dockershim. With the release of Kubernetes v1.20, the deprecation was official.&lt;/p>
&lt;p>We didn’t do a great job communicating this, and unfortunately, the deprecation announcement led to some panic within the community. Confusion around what this meant for Docker as a company, if container images built by Docker would still run, and what Docker Engine actually is led to a conflagration on social media. This was our fault; we should have more clearly communicated what was happening and why at the time. To combat this, we released &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">a blog&lt;/a> and &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">accompanying FAQ&lt;/a> to allay the community’s fears and correct some misconceptions about what Docker is and how containers work within Kubernetes. As a result of the community’s concerns, Docker and Mirantis jointly agreed to continue supporting the dockershim code in the form of &lt;a href="https://www.mirantis.com/blog/the-future-of-dockershim-is-cri-dockerd/">cri-dockerd&lt;/a>, allowing you to continue using Docker Engine as your container runtime if need be. For the interest of users who want to try other runtimes, like containerd or cri-o, &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">migration documentation was written&lt;/a>.&lt;/p>
&lt;p>We later &lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">surveyed the community&lt;/a> and &lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim">discovered that there are still many users with questions and concerns&lt;/a>. In response, Kubernetes maintainers and the CNCF committed to addressing these concerns by extending documentation and other programs. In fact, this blog post is a part of this program. With so many end users successfully migrated to other runtimes, and improved documentation, we believe that everyone has a paved way to migration now.&lt;/p>
&lt;p>Docker is not going away, either as a tool or as a company. It’s an important part of the cloud native community and the history of the Kubernetes project. We wouldn’t be where we are without them. That said, removing dockershim from kubelet is ultimately good for the community, the ecosystem, the project, and open source at large. This is an opportunity for all of us to come together to support open standards, and we’re glad to be doing so with the help of Docker and the community.&lt;/p></description></item><item><title>Blog: Frontiers, fsGroups and frogs: the Kubernetes 1.23 release interview</title><link>https://kubernetes.io/blog/2022/04/29/frontiers-fsgroups-and-frogs-the-kubernetes-1.23-release-interview/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/29/frontiers-fsgroups-and-frogs-the-kubernetes-1.23-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>One of the highlights of hosting the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> is talking to the release managers for each new Kubernetes version. The release team is constantly refreshing. Many working their way from small documentation fixes, step up to shadow roles, and then eventually lead a release.&lt;/p>
&lt;p>As we prepare for the 1.24 release next week, &lt;a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">in accordance with long-standing tradition&lt;/a>, I'm pleased to bring you a look back at the story of 1.23. The release was led by &lt;a href="https://twitter.com/reylejano">Rey Lejano&lt;/a>, a Field Engineer at SUSE. &lt;a href="https://kubernetespodcast.com/episode/167-kubernetes-1.23/">I spoke to Rey&lt;/a> in December, as he was awaiting the birth of his first child.&lt;/p>
&lt;p>Make sure you &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe, wherever you get your podcasts&lt;/a>, so you hear all our stories from the Cloud Native community, including the story of 1.24 next week.&lt;/p>
&lt;p>&lt;em>This transcript has been lightly edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: I'd like to start with what is, of course, on top of everyone's mind at the moment. Let's talk African clawed frogs!&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] Oh, you mean &lt;a href="https://en.wikipedia.org/wiki/African_clawed_frog">Xenopus lavis&lt;/a>, the scientific name for the African clawed frog?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Of course.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Not many people know, but my background and my degree is actually in microbiology, from the University of California Davis. I did some research for about four years in biochemistry, in a biochemistry lab, and I &lt;a href="https://www.sciencedirect.com/science/article/pii/">do have a research paper published&lt;/a>. It's actually on glycoproteins, particularly something called &amp;quot;cortical granule lectin&amp;quot;. We used frogs, because they generate lots and lots of eggs, from which we can extract the protein. That protein prevents polyspermy. When the sperm goes into the egg, the egg releases a glycoprotein, cortical granule lectin, to the membrane, and prevents any other sperm from going inside the egg.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Were you able to take anything from the testing that we did on frogs and generalize that to higher-order mammals, perhaps?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes. Since mammals also have cortical granule lectin, we were able to analyze both the convergence and the evolutionary pattern, not just from multiple species of frogs, but also into mammals as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, there's a couple of different threads to unravel here. When you were young, what led you into the fields of biology, and perhaps more the technical side of it?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I think it was mostly from family, since I do have a family history in the medical field that goes back generations. So I kind of felt like that was the natural path going into college.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, of course, you're working in a more abstract tech field. What led you out of microbiology?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] Well, I've always been interested in tech. Taught myself a little programming when I was younger, before high school, did some web dev stuff. Just kind of got burnt out being in a lab. I was literally in the basement. I had a great opportunity to join a consultancy that specialized in &lt;a href="https://www.axelos.com/certifications/itil-service-management/what-is-itil">ITIL&lt;/a>. I actually started off with application performance management, went into monitoring, went into operation management and also ITIL, which is aligning your IT asset management and service managements with business services. Did that for a good number of years, actually.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's very interesting, as people describe the things that they went through and perhaps the technologies that they worked on, you can pretty much pinpoint how old they might be. There's a lot of people who come into tech these days that have never heard of ITIL. They have no idea what it is. It's basically just SRE with more process.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes, absolutely. It's not very cloud native. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Not at all.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: You don't really hear about it in the cloud native landscape. Definitely, you can tell someone's been in the field for a little bit, if they specialize or have worked with ITIL before.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned that you wanted to get out of the basement. That is quite often where people put the programmers. Did they just give you a bit of light in the new basement?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [LAUGHS] They did give us much better lighting. Able to get some vitamin D sometimes, as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: To wrap up the discussion about your previous career — over the course of the last year, with all of the things that have happened in the world, I could imagine that microbiology skills may be more in demand than perhaps they were when you studied them?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Oh, absolutely. I could definitely see a big increase of numbers of people going into the field. Also, reading what's going on with the world currently kind of brings back all the education I've learned in the past, as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you keep in touch with people you went through school with?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Just some close friends, but not in the microbiology field.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One thing that I think will probably happen as a result of the pandemic is a renewed interest in some of these STEM fields. It will be interesting to see what impact that has on society at large.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. I think that'll be great.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned working at a consultancy doing IT management, application performance monitoring, and so on. When did Kubernetes come into your professional life?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: One of my good friends at the company I worked at, left in mid-2015. He went on to a company that was pretty heavily into Docker. He taught me a little bit. I did my first &amp;quot;docker run&amp;quot; around 2015, maybe 2016. Then, one of the applications we were using for the ITIL framework was containerized around 2018 or so, also in Kubernetes. At that time, it was pretty buggy. That was my initial introduction to Kubernetes and containerised applications.&lt;/p>
&lt;p>Then I left that company, and I actually joined my friend over at &lt;a href="https://rx-m.com/">RX-M&lt;/a>, which is a cloud native consultancy and training firm. They specialize in Docker and Kubernetes. I was able to get my feet wet. I got my CKD, got my CKA as well. And they were really, really great at encouraging us to learn more about Kubernetes and also to be involved in the community.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You will have seen, then, the life cycle of people adopting Kubernetes and containerization at large, through your own initial journey and then through helping customers. How would you characterize how that journey has changed from the early days to perhaps today?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I think the early days, there was a lot of questions of, why do I have to containerize? Why can't I just stay with virtual machines?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's a line item on your CV.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] It is. And nowadays, I think people know the value of using containers, of orchestrating containers with Kubernetes. I don't want to say &amp;quot;jumping on the bandwagon&amp;quot;, but it's become the de-facto standard to orchestrate containers.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's not something that a consultancy needs to go out and pitch to customers that they should be doing. They're just taking it as, that will happen, and starting a bit further down the path, perhaps.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Absolutely.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Working at a consultancy like that, how much time do you get to work on improving process, perhaps for multiple customers, and then looking at how you can upstream that work, versus paid work that you do for just an individual customer at a time?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Back then, it would vary. They helped me introduce myself, and I learned a lot about the cloud native landscape and Kubernetes itself. They helped educate me as to how the cloud native landscape, and the tools around it, can be used together. My boss at that company, Randy, he actually encouraged us to start contributing upstream, and encouraged me to join the release team. He just said, this is a great opportunity. Definitely helped me with starting with the contributions early on.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Was the release team the way that you got involved with upstream Kubernetes contribution?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Actually, no. My first contribution was with SIG Docs. I met Taylor Dolezal — he was the release team lead for 1.19, but he is involved with SIG Docs as well. I met him at KubeCon 2019, I sat at his table during a luncheon. I remember Paris Pittman was hosting this luncheon at the Marriott. Taylor says he was involved with SIG Docs. He encouraged me to join. I started joining into meetings, started doing a few drive-by PRs. That's what we call them — drive-by — little typo fixes. Then did a little bit more, started to send better or higher quality pull requests, and also reviewing PRs.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When did you first formally take your release team role?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: That was in &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">1.18&lt;/a>, in December. My boss at the time encouraged me to apply. I did, was lucky enough to get accepted for the release notes shadow. Then from there, stayed in with release notes for a few cycles, then went into Docs, naturally then led Docs, then went to Enhancements, and now I'm the release lead for 1.23.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I don't know that a lot of people think about what goes into a good release note. What would you say does?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] You have to tell the end user what has changed or what effect that they might see in the release notes. It doesn't have to be highly technical. It could just be a few lines, and just saying what has changed, what they have to do if they have to do anything as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: As you moved through the process of shadowing, how did you learn from the people who were leading those roles?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I said this a few times when I was the release lead for this cycle. You get out of the release team as much as you put in, or it directly aligns to how much you put in. I learned a lot. I went into the release team having that mindset of learning from the role leads, learning from the other shadows, as well. That's actually a saying that my first role lead told me. I still carry it to heart, and that was back in 1.18. That was Eddie, in the very first meeting we had, and I still carry it to heart.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You, of course, were &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.23">the release lead for 1.23&lt;/a>. First of all, congratulations on the release.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Thank you very much.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The theme for this release is &lt;a href="https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/">The Next Frontier&lt;/a>. Tell me the story of how we came to the theme and then the logo.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: The Next Frontier represents a few things. It not only represents the next enhancements in this release, but Kubernetes itself also has a history of Star Trek references. The original codename for Kubernetes was Project Seven, a reference to Seven of Nine, originally from Star Trek Voyager. Also the seven spokes in the helm in the logo of Kubernetes as well. And, of course, Borg, the predecessor to Kubernetes.&lt;/p>
&lt;p>The Next Frontier continues that Star Trek reference. It's a fusion of two titles in the Star Trek universe. One is &lt;a href="https://en.wikipedia.org/wiki/Star_Trek_V:_The_Final_Frontier">Star Trek V, the Final Frontier&lt;/a>, and the Star Trek: The Next Generation.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you have any opinion on the fact that Star Trek V was an odd-numbered movie, and they are &lt;a href="https://screenrant.com/star-trek-movies-odd-number-curse-explained/">canonically referred to as being lesser than the even-numbered ones&lt;/a>?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I can't say, because I am such a sci-fi nerd that I love all of them even though they're bad. Even the post-Next Generation movies, after the series, I still liked all of them, even though I know some weren't that great.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Am I right in remembering that Star Trek V was the one directed by William Shatner?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes, that is correct.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think that says it all.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] Yes.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, I understand that the theme comes from a part of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-release/charter.md">SIG Release charter&lt;/a>?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes. There's a line in the SIG Release charter, &amp;quot;ensure there is a consistent group of community members in place to support the release process across time.&amp;quot; With the release team, we have new shadows that join every single release cycle. With this, we're growing with this community. We're growing the release team members. We're growing SIG Release. We're growing the Kubernetes community itself. For a lot of people, this is their first time contributing to open source, so that's why I say it's their new open source frontier.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And the logo is obviously very Star Trek-inspired. It sort of surprised me that it took that long for someone to go this route.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I was very surprised as well. I had to relearn Adobe Illustrator to create the logo.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This your own work, is it?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: This is my own work.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's very nice.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Thank you very much. Funny, the galaxy actually took me the longest time versus the ship. Took me a few days to get that correct. I'm always fine-tuning it, so there might be a final change when this is actually released.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: No frontier is ever truly final.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: True, very true.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Moving now from the theme of the release to the substance, perhaps, what is new in 1.23?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: We have 47 enhancements. I'm going to run through most of the stable ones, if not all of them, some of the key Beta ones, and a few of the Alpha enhancements for 1.23.&lt;/p>
&lt;p>One of the key enhancements is &lt;a href="https://github.com/kubernetes/enhancements/issues/563">dual-stack IPv4/IPv6&lt;/a>, which went GA in 1.23.&lt;/p>
&lt;p>Some background info: dual-stack was introduced as Alpha in 1.15. You probably saw a keynote at KubeCon 2019. Back then, the way dual-stack worked was that you needed two services — you needed a service per IP family. You would need a service for IPv4 and a service for IPv6. It was refactored in 1.20. In 1.21, it was in Beta; clusters were enabled to be dual-stack by default.&lt;/p>
&lt;p>And then in 1.23 we did remove the IPv6 dual-stack feature flag. It's not mandatory to use dual-stack. It's actually not &amp;quot;default&amp;quot; still. The pods, the services still default to single-stack. There are some requirements to be able to use dual-stack. The nodes have to be routable on IPv4 and IPv6 network interfaces. You need a CNI plugin that supports dual-stack. The pods themselves have to be configured to be dual-stack. And the services need the ipFamilyPolicy field to specify prefer dual-stack, or require dual-stack.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This sounds like there's an implication in this that v4 is still required. Do you see a world where we can actually move to v6-only clusters?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I think we'll be talking about IPv4 and IPv6 for many, many years to come. I remember a long time ago, they kept saying &amp;quot;it's going to be all IPv6&amp;quot;, and that was decades ago.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think I may have mentioned on the show before, but there was &lt;a href="https://www.youtube.com/watch?v=AEaJtZVimqs">a meeting in London that Vint Cerf attended&lt;/a>, and he gave a public presentation at the time to say, now is the time of v6. And that was 10 years ago at least. It's still not the time of v6, and my desktop still doesn't have Linux on it. One day.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [LAUGHS] In my opinion, that's one of the big key features that went stable for 1.23.&lt;/p>
&lt;p>One of the other highlights of 1.23 is &lt;a href="https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/">pod security admission going to Beta&lt;/a>. I know this feature is going to Beta, but I highlight this because as some people might know, PodSecurityPolicy, which was deprecated in 1.21, is targeted to be removed in 1.25. Pod security admission replaces pod security policy. It's an admission controller. It evaluates the pods against a predefined set of pod security standards to either admit or deny the pod for running.&lt;/p>
&lt;p>There's three levels of pod security standards. Privileged, that's totally open. Baseline, known privileges escalations are minimized. Or Restricted, which is hardened. And you could set pod security standards either to run in three modes, which is enforce: reject any pods that are in violation; to audit: pods are allowed to be created, but the violations are recorded; or warn: it will send a warning message to the user, and the pod is allowed.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned there that PodSecurityPolicy is due to be deprecated in two releases' time. Are we lining up these features so that pod security admission will be GA at that time?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes. Absolutely. I'll talk about that for another feature in a little bit as well. There's also another feature that went to GA. It was an API that went to GA, and therefore the Beta API is now deprecated. I'll talk about that a little bit.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: All right. Let's talk about what's next on the list.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Let's move on to more stable enhancements. One is the &lt;a href="https://github.com/kubernetes/enhancements/issues/592">TTL controller&lt;/a>. This cleans up jobs and pods after the jobs are finished. There is a TTL timer that starts when the job or pod is finished. This TTL controller watches all the jobs, and ttlSecondsAfterFinished needs to be set. The controller will see if the ttlSecondsAfterFinished, combined with the last transition time, if it's greater than now. If it is, then it will delete the job and the pods of that job.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Loosely, it could be called a garbage collector?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes. Garbage collector for pods and jobs, or jobs and pods.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: If Kubernetes is truly becoming a programming language, it of course has to have a garbage collector implemented.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. There's another one, too, coming in Alpha. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Tell me about that.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: That one is coming in in Alpha. It's actually one of my favorite features, because there's only a few that I'm going to highlight today. &lt;a href="https://github.com/kubernetes/enhancements/issues/1847">PVCs for StafeulSet will be cleaned up&lt;/a>. It will auto-delete PVCs created by StatefulSets, when you delete that StatefulSet.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What's next on our tour of stable features?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Next one is, &lt;a href="https://github.com/kubernetes/enhancements/issues/695">skip volume ownership change goes to stable&lt;/a>. This is from SIG Storage. There are times when you're running a stateful application, like many databases, they're sensitive to permission bits changing underneath. Currently, when a volume is bind mounted inside the container, the permissions of that volume will change recursively. It might take a really long time.&lt;/p>
&lt;p>Now, there's a field, the fsGroupChangePolicy, which allows you, as a user, to tell Kubernetes how you want the permission and ownership change for that volume to happen. You can set it to always, to always change permissions, or just on mismatch, to only do it when the permission ownership changes at the top level is different from what is expected.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It does feel like a lot of these enhancements came from a very particular use case where someone said, &amp;quot;hey, this didn't work for me and I've plumbed in a feature that works with exactly the thing I need to have&amp;quot;.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Absolutely. People create issues for these, then create Kubernetes enhancement proposals, and then get targeted for releases.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Another GA feature in this release — ephemeral volumes.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: We've always been able to use empty dir for ephemeral volumes, but now we could actually have &lt;a href="https://github.com/kubernetes/enhancements/issues/1698">ephemeral inline volumes&lt;/a>, meaning that you could take your standard CSI driver and be able to use ephemeral volumes with it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And, a long time coming, &lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: CronJobs is a funny one, because it was stable before 1.23. For 1.23, it was still tracked,but it was just cleaning up some of the old controller. With CronJobs, there's a v2 controller. What was cleaned up in 1.23 is just the old v1 controller.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Were there any other duplications or major cleanups of note in this release?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. There were a few you might see in the major themes. One's a little tricky, around FlexVolumes. This is one of the efforts from SIG Storage. They have an effort to migrate in-tree plugins to CSI drivers. This is a little tricky, because FlexVolumes were actually deprecated in November 2020. We're &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors">formally announcing it in 1.23&lt;/a>.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: FlexVolumes, in my mind, predate CSI as a concept. So it's about time to get rid of them.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes, it is. There's another deprecation, just some &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog">klog specific flags&lt;/a>, but other than that, there are no other big deprecations in 1.23.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The buzzword of the last KubeCon, and in some ways the theme of the last 12 months, has been secure software supply chain. What work is Kubernetes doing to improve in this area?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: For 1.23, Kubernetes is now SLSA compliant at Level 1, which means that provenance attestation files that describe the staging and release phases of the release process are satisfactory for the SLSA framework.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What needs to happen to step up to further levels?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Level 1 means a few things — that the build is scripted; that the provenance is available, meaning that the artifacts are verified and they're handed over from one phase to the next; and describes how the artifact is produced. Level 2 means that the source is version-controlled, which it is, provenance is authenticated, provenance is service-generated, and there is a build service. There are four levels of SLSA compliance.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It does seem like the levels were largely influenced by what it takes to build a big, secure project like this. It doesn't seem like it will take a lot of extra work to move up to verifiable provenance, for example. There's probably just a few lines of script required to meet many of those requirements.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Absolutely. I feel like we're almost there; we'll see what will come out of 1.24. And I do want to give a big shout-out to SIG Release and Release Engineering, primarily to Adolfo García Veytia, who is aka Puerco on GitHub and on Slack. He's been driving this forward.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You've mentioned some APIs that are being graduated in time to replace their deprecated version. Tell me about the new HPA API.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: The &lt;a href="https://github.com/kubernetes/enhancements/issues/2702">horizontal pod autoscaler v2 API&lt;/a>, is now stable, which means that the v2beta2 API is deprecated. Just for everyone's knowledge, the v1 API is not being deprecated. The difference is that v2 adds support for multiple and custom metrics to be used for HPA.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's also now a facility to validate my CRDs with an expression language.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. You can use the &lt;a href="https://github.com/google/cel-spec">Common Expression Language, or CEL&lt;/a>, to validate your CRDs, so you no longer need to use webhooks. This also makes the CRDs more self-contained and declarative, because the rules are now kept within the CRD object definition.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What new features, perhaps coming in Alpha or Beta, have taken your interest?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Aside from pod security policies, I really love &lt;a href="https://github.com/kubernetes/enhancements/issues/277">ephemeral containers&lt;/a> supporting kubectl debug. It launches an ephemeral container and a running pod, shares those pod namespaces, and you can do all your troubleshooting with just running kubectl debug.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's also been some interesting changes in the way that events are handled with kubectl.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. kubectl events has always had some issues, like how things weren't sorted. &lt;a href="https://github.com/kubernetes/enhancements/issues/1440">kubectl events improved&lt;/a> that so now you can do &lt;code>--watch&lt;/code>, and it will also sort with the &lt;code>--watch&lt;/code> option as well. That is something new. You can actually combine fields and custom columns. And also, you can list events in the timeline with doing the last N number of minutes. And you can also sort events using other criteria as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You are a field engineer at SUSE. Are there any things that are coming in that your individual customers that you deal with are looking out for?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: More of what I look out for to help the customers.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I really love kubectl events. Really love the PVCs being cleaned up with StatefulSets. Most of it's for selfish reasons that it will improve troubleshooting efforts. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I have always hoped that a release team lead would say to me, &amp;quot;yes, I have selfish reasons. And I finally got something I wanted in.&amp;quot;&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [LAUGHS]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Perhaps I should run to be release team lead, just so I can finally get init containers fixed once and for all.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Oh, init containers, I've been looking for that for a while. I've actually created animated GIFs on how init containers will be run with that Kubernetes enhancement proposal, but it's halted currently.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One day.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: One day. Maybe I shouldn't stay halted.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned there are obviously the things you look out for. Are there any things that are coming down the line, perhaps Alpha features or maybe even just proposals you've seen lately, that you're personally really looking forward to seeing which way they go?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. Oone is a very interesting one, it affects the whole community, so it's not just for personal reasons. As you may have known, Dockershim is deprecated. And we did release a blog that it will be removed in 1.24.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Scared a bunch of people.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Scared a bunch of people. From a survey, we saw that a lot of people are still using Docker and Dockershim. One of the enhancements for 1.23 is, &lt;a href="https://github.com/kubernetes/enhancements/issues/2040">kubelet CRI goes to Beta&lt;/a>. This promotes the CRI API, which is required. This had to be in Beta for Dockershim to be removed in 1.24.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, in the last release team lead interview, &lt;a href="https://kubernetespodcast.com/episode/157-kubernetes-1.22/">we spoke with Savitha Raghunathan&lt;/a>, and she talked about what she would advise you as her successor. It was to look out for the mental health of the team members. How were you able to take that advice on board?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: That was great advice from Savitha. A few things I've made note of with each release team meeting. After each release team meeting, I stop the recording, because we do record all the meetings and post them on YouTube. And I open up the floor to anyone who wants to say anything that's not recorded, that's not going to be on the agenda. Also, I tell people not to work on weekends. I broke this rule once, but other than that, I told people it could wait. Just be mindful of your mental health.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's just been announced that &lt;a href="https://twitter.com/JamesLaverack/status/1466834312993644551">James Laverack from Jetstack&lt;/a> will be the release team lead for 1.24. James and I shared an interesting Mexican dinner at the last KubeCon in San Diego.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Oh, nice. I didn't know you knew James.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The British tech scene. We're a very small world. What will your advice to James be?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: What I would tell James for 1.24 is use teachable moments in the release team meetings. When you're a shadow for the first time, it's very daunting. It's very difficult, because you don't know the repos. You don't know the release process. Everyone around you seems like they know the release process, and very familiar with what the release process is. But as a first-time shadow, you don't know all the vernacular for the community. I just advise to use teachable moments. Take a few minutes in the release team meetings to make it a little easier for new shadows to ramp up and to be familiar with the release process.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Has there been major evolution in the process in the time that you've been involved? Or do you think that it's effectively doing what it needs to do?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: It's always evolving. I remember my first time in release notes, 1.18, we said that our goal was to automate and program our way out so that we don't have a release notes team anymore. That's changed [CHUCKLES] quite a bit. Although there's been significant advancements in the release notes process by Adolfo and also James, they've created a subcommand in krel to generate release notes.&lt;/p>
&lt;p>But nowadays, all their release notes are richer. Still not there at the automation process yet. Every release cycle, there is something a little bit different. For this release cycle, we had a production readiness review deadline. It was a soft deadline. A production readiness review is a review by several people in the community. It's actually been required since 1.21, and it ensures that the enhancements are observable, scalable, supportable, and it's safe to operate in production, and could also be disabled or rolled back. In 1.23, we had a deadline to have the production readiness review completed by a specific date.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How have you found the change of schedule to three releases per year rather than four?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Moving to three releases a year from four, in my opinion, has been an improvement, because we support the last three releases, and now we can actually support the last releases in a calendar year instead of having 9 months out of 12 months of the year.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The next event on the calendar is a &lt;a href="https://www.kubernetes.dev/events/kcc2021/">Kubernetes contributor celebration&lt;/a> starting next Monday. What can we expect from that event?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: This is our second time running this virtual event. It's a virtual celebration to recognize the whole community and all of our accomplishments of the year, and also contributors. There's a number of events during this week of celebration. It starts the week of December 13.&lt;/p>
&lt;p>There's events like the Kubernetes Contributor Awards, where SIGs honor and recognize the hard work of the community and contributors. There's also a DevOps party game as well. There is a cloud native bake-off. I do highly suggest people to go to &lt;a href="https://www.kubernetes.dev/events/past-events/2021/kcc2021/">kubernetes.dev/celebration&lt;/a> to learn more.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How exactly does one judge a virtual bake-off?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: That I don't know. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I tasted my scones. I think they're the best. I rate them 10 out of 10.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. That is very difficult to do virtually. I would have to say, probably what the dish is, how closely it is tied with Kubernetes or open source or to CNCF. There's a few judges. I know Josh Berkus and Rin Oliver are a few of the judges running the bake-off.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes. We spoke with Josh about his love of the kitchen, and so he seems like a perfect fit for that role.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: He is.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Finally, your wife and yourself are expecting your first child in January. Have you had a production readiness review for that?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I think we failed that review. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's still time.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: We are working on refactoring. We're going to refactor a little bit in December, and &lt;code>--apply&lt;/code> again.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/reylejano">Rey Lejano&lt;/a> is a field engineer at SUSE, by way of Rancher Labs, and was the release team lead for Kubernetes 1.23. He is now also a co-chair for SIG Docs. His son Liam is now 3 and a half months old.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Increasing the security bar in Ingress-NGINX v1.2.0</title><link>https://kubernetes.io/blog/2022/04/28/ingress-nginx-1-2-0/</link><pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/28/ingress-nginx-1-2-0/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Ricardo Katz (VMware), James Strong (Chainguard)&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a> may be one of the most targeted components
of Kubernetes. An Ingress typically defines an HTTP reverse proxy, exposed to the Internet, containing
multiple websites, and with some privileged access to Kubernetes API (such as to read Secrets relating to
TLS certificates and their private keys).&lt;/p>
&lt;p>While it is a risky component in your architecture, it is still the most popular way to properly expose your services.&lt;/p>
&lt;p>Ingress-NGINX has been part of security assessments that figured out we have a big problem: we don't
do all proper sanitization before turning the configuration into an &lt;code>nginx.conf&lt;/code> file, which may lead to information
disclosure risks.&lt;/p>
&lt;p>While we understand this risk and the real need to fix this, it's not an easy process to do, so we took another approach to reduce (but not remove!) this risk in the current (v1.2.0) release.&lt;/p>
&lt;h2 id="meet-ingress-nginx-v1-2-0-and-the-chrooted-nginx-process">Meet Ingress NGINX v1.2.0 and the chrooted NGINX process&lt;/h2>
&lt;p>One of the main challenges is that Ingress-NGINX runs the web proxy server (NGINX) alongside the Ingress
controller (the component that has access to Kubernetes API that and that creates the &lt;code>nginx.conf&lt;/code> file).&lt;/p>
&lt;p>So, NGINX does have the same access to the filesystem of the controller (and Kubernetes service account token, and other configurations from the container). While splitting those components is our end goal, the project needed a fast response; that lead us to the idea of using &lt;code>chroot()&lt;/code>.&lt;/p>
&lt;p>Let's take a look into what an Ingress-NGINX container looked like before this change:&lt;/p>
&lt;p>&lt;img src="ingress-pre-chroot.png" alt="Ingress NGINX pre chroot">&lt;/p>
&lt;p>As we can see, the same container (not the Pod, the container!) that provides HTTP Proxy is the one that watches Ingress objects and writes the Container Volume&lt;/p>
&lt;p>Now, meet the new architecture:&lt;/p>
&lt;p>&lt;img src="ingress-post-chroot.png" alt="Ingress NGINX post chroot">&lt;/p>
&lt;p>What does all of this mean? A basic summary is: that we are isolating the NGINX service as a container inside the
controller container.&lt;/p>
&lt;p>While this is not strictly true, to understand what was done here, it's good to understand how
Linux containers (and underlying mechanisms such as kernel namespaces) work.
You can read about cgroups in the Kubernetes glossary: &lt;a href="https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-cgroup">&lt;code>cgroup&lt;/code>&lt;/a> and learn more about cgroups interact with namespaces in the NGINX project article
&lt;a href="https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/">What Are Namespaces and cgroups, and How Do They Work?&lt;/a>.
(As you read that, bear in mind that Linux kernel namespaces are a different thing from
&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">Kubernetes namespaces&lt;/a>).&lt;/p>
&lt;h2 id="skip-the-talk-what-do-i-need-to-use-this-new-approach">Skip the talk, what do I need to use this new approach?&lt;/h2>
&lt;p>While this increases the security, we made this feature an opt-in in this release so you can have
time to make the right adjustments in your environment(s). This new feature is only available from
release v1.2.0 of the Ingress-NGINX controller.&lt;/p>
&lt;p>There are two required changes in your deployments to use this feature:&lt;/p>
&lt;ul>
&lt;li>Append the suffix &amp;quot;-chroot&amp;quot; to the container image name. For example: &lt;code>gcr.io/k8s-staging-ingress-nginx/controller-chroot:v1.2.0&lt;/code>&lt;/li>
&lt;li>In your Pod template for the Ingress controller, find where you add the capability &lt;code>NET_BIND_SERVICE&lt;/code> and add the capability &lt;code>SYS_CHROOT&lt;/code>. After you edit the manifest, you'll see a snippet like:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">capabilities&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">drop&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ALL&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">add&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- NET_BIND_SERVICE&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- SYS_CHROOT&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you deploy the controller using the official Helm chart then change the following setting in
&lt;code>values.yaml&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">controller&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">chroot&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ingress controllers are normally set up cluster-wide (the IngressClass API is cluster scoped). If you manage the
Ingress-NGINX controller but you're not the overall cluster operator, then check with your cluster admin about
whether you can use the &lt;code>SYS_CHROOT&lt;/code> capability, &lt;strong>before&lt;/strong> you enable it in your deployment.&lt;/p>
&lt;h2 id="ok-but-how-does-this-increase-the-security-of-my-ingress-controller">OK, but how does this increase the security of my Ingress controller?&lt;/h2>
&lt;p>Take the following configuration snippet and imagine, for some reason it was added to your &lt;code>nginx.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code>location /randomthing/ {
alias /;
autoindex on;
}
&lt;/code>&lt;/pre>&lt;p>If you deploy this configuration, someone can call &lt;code>http://website.example/randomthing&lt;/code> and get some listing (and access) to the whole filesystem of the Ingress controller.&lt;/p>
&lt;p>Now, can you spot the difference between chrooted and non chrooted Nginx on the listings below?&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Without extra &lt;code>chroot()&lt;/code>&lt;/th>
&lt;th>With extra &lt;code>chroot()&lt;/code>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>bin&lt;/code>&lt;/td>
&lt;td>&lt;code>bin&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>dev&lt;/code>&lt;/td>
&lt;td>&lt;code>dev&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>etc&lt;/code>&lt;/td>
&lt;td>&lt;code>etc&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>home&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>lib&lt;/code>&lt;/td>
&lt;td>&lt;code>lib&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>media&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>mnt&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>opt&lt;/code>&lt;/td>
&lt;td>&lt;code>opt&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>proc&lt;/code>&lt;/td>
&lt;td>&lt;code>proc&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>root&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>run&lt;/code>&lt;/td>
&lt;td>&lt;code>run&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>sbin&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>srv&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>sys&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>tmp&lt;/code>&lt;/td>
&lt;td>&lt;code>tmp&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>usr&lt;/code>&lt;/td>
&lt;td>&lt;code>usr&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>var&lt;/code>&lt;/td>
&lt;td>&lt;code>var&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>dbg&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>nginx-ingress-controller&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>wait-shutdown&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The one in left side is not chrooted. So NGINX has full access to the filesystem. The one in right side is chrooted, so a new filesystem with only the required files to make NGINX work is created.&lt;/p>
&lt;h2 id="what-about-other-security-improvements-in-this-release">What about other security improvements in this release?&lt;/h2>
&lt;p>We know that the new &lt;code>chroot()&lt;/code> mechanism helps address some portion of the risk, but still, someone
can try to inject commands to read, for example, the &lt;code>nginx.conf&lt;/code> file and extract sensitive information.&lt;/p>
&lt;p>So, another change in this release (this is opt-out!) is the &lt;em>deep inspector&lt;/em>.
We know that some directives or regular expressions may be dangerous to NGINX, so the deep inspector
checks all fields from an Ingress object (during its reconciliation, and also with a
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">validating admission webhook&lt;/a>)
to verify if any fields contains these dangerous directives.&lt;/p>
&lt;p>The ingress controller already does this for annotations, and our goal is to move this existing validation to happen inside
deep inspection as part of a future release.&lt;/p>
&lt;p>You can take a look into the existing rules in &lt;a href="https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go">https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go&lt;/a>.&lt;/p>
&lt;p>Due to the nature of inspecting and matching all strings within relevant Ingress objects, this new feature may consume a bit more CPU. You can disable it by running the ingress controller with the command line argument &lt;code>--deep-inspect=false&lt;/code>.&lt;/p>
&lt;h2 id="what-s-next">What's next?&lt;/h2>
&lt;p>This is not our final goal. Our final goal is to split the control plane and the data plane processes.
In fact, doing so will help us also achieve a &lt;a href="https://gateway-api.sigs.k8s.io/">Gateway&lt;/a> API implementation,
as we may have a different controller as soon as it &amp;quot;knows&amp;quot; what to provide to the data plane
(we need some help here!!)&lt;/p>
&lt;p>Some other projects in Kubernetes already take this approach
(like &lt;a href="%E2%80%8B%E2%80%8Bhttps://github.com/kubernetes-sigs/kpng">KPNG&lt;/a>, the proposed replacement for &lt;code>kube-proxy&lt;/code>),
and we plan to align with them and get the same experience for Ingress-NGINX.&lt;/p>
&lt;h2 id="further-reading">Further reading&lt;/h2>
&lt;p>If you want to take a look into how chrooting was done in Ingress NGINX, take a look
into &lt;a href="https://github.com/kubernetes/ingress-nginx/pull/8337">https://github.com/kubernetes/ingress-nginx/pull/8337&lt;/a>
The release v1.2.0 containing all the changes can be found at
&lt;a href="https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0">https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes Removals and Deprecations In 1.24</title><link>https://kubernetes.io/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Mickey Boxell (Oracle)&lt;/p>
&lt;p>As Kubernetes evolves, features and APIs are regularly revisited and removed. New features may offer
an alternative or improved approach to solving existing problems, motivating the team to remove the
old approach.&lt;/p>
&lt;p>We want to make sure you are aware of the changes coming in the Kubernetes 1.24 release. The release will
&lt;strong>deprecate&lt;/strong> several (beta) APIs in favor of stable versions of the same APIs. The major change coming
in the Kubernetes 1.24 release is the
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">removal of Dockershim&lt;/a>.
This is discussed below and will be explored in more depth at release time. For an early look at the
changes coming in Kubernetes 1.24, take a look at the in-progress
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">CHANGELOG&lt;/a>.&lt;/p>
&lt;h2 id="a-note-about-dockershim">A note about Dockershim&lt;/h2>
&lt;p>It's safe to say that the removal receiving the most attention with the release of Kubernetes 1.24
is Dockershim. Dockershim was deprecated in v1.20. As noted in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">Kubernetes 1.20 changelog&lt;/a>:
&amp;quot;Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet
uses a module called &amp;quot;dockershim&amp;quot; which implements CRI support for Docker and it has seen maintenance
issues in the Kubernetes community.&amp;quot; With the upcoming release of Kubernetes 1.24, the Dockershim will
finally be removed.&lt;/p>
&lt;p>In the article &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker&lt;/a>,
the authors succinctly captured the change's impact and encouraged users to remain calm:&lt;/p>
&lt;blockquote>
&lt;p>Docker as an underlying runtime is being deprecated in favor of runtimes that use the
Container Runtime Interface (CRI) created for Kubernetes. Docker-produced images
will continue to work in your cluster with all runtimes, as they always have.&lt;/p>
&lt;/blockquote>
&lt;p>Several guides have been created with helpful information about migrating from dockershim
to container runtimes that are directly compatible with Kubernetes. You can find them on the
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim&lt;/a>
page in the Kubernetes documentation.&lt;/p>
&lt;p>For more information about why Kubernetes is moving away from dockershim, check out the aptly
named: &lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">Kubernetes is Moving on From Dockershim&lt;/a>
and the &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">updated dockershim removal FAQ&lt;/a>.&lt;/p>
&lt;p>Take a look at the &lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">Is Your Cluster Ready for v1.24?&lt;/a> post to learn about how to ensure your cluster continues to work after upgrading from v1.23 to v1.24.&lt;/p>
&lt;h2 id="the-kubernetes-api-removal-and-deprecation-process">The Kubernetes API removal and deprecation process&lt;/h2>
&lt;p>Kubernetes contains a large number of components that evolve over time. In some cases, this
evolution results in APIs, flags, or entire features, being removed. To prevent users from facing
breaking changes, Kubernetes contributors adopted a feature &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a>.
This policy ensures that stable APIs may only be deprecated when a newer stable version of that
same API is available and that APIs have a minimum lifetime as indicated by the following stability levels:&lt;/p>
&lt;ul>
&lt;li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.&lt;/li>
&lt;li>Beta or pre-release API versions must be supported for 3 releases after deprecation.&lt;/li>
&lt;li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.&lt;/li>
&lt;/ul>
&lt;p>Removals follow the same deprecation policy regardless of whether an API is removed due to a beta feature
graduating to stable or because that API was not proven to be successful. Kubernetes will continue to make
sure migration options are documented whenever APIs are removed.&lt;/p>
&lt;p>&lt;strong>Deprecated&lt;/strong> APIs are those that have been marked for removal in a future Kubernetes release. &lt;strong>Removed&lt;/strong>
APIs are those that are no longer available for use in current, supported Kubernetes versions after having
been deprecated. These removals have been superseded by newer, stable/generally available (GA) APIs.&lt;/p>
&lt;h2 id="api-removals-deprecations-and-other-changes-for-kubernetes-1-24">API removals, deprecations, and other changes for Kubernetes 1.24&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/281">Dynamic kubelet configuration&lt;/a>: &lt;code>DynamicKubeletConfig&lt;/code> is used to enable the dynamic configuration of the kubelet. The &lt;code>DynamicKubeletConfig&lt;/code> flag was deprecated in Kubernetes 1.22. In v1.24, this feature gate will be removed from the kubelet. See &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">Reconfigure kubelet&lt;/a>. Refer to the &lt;a href="https://github.com/kubernetes/enhancements/issues/281">&amp;quot;Dynamic kubelet config is removed&amp;quot; KEP&lt;/a> for more information.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107207">Dynamic log sanitization&lt;/a>: The experimental dynamic log sanitization feature is deprecated and will be removed in v1.24. This feature introduced a logging filter that could be applied to all Kubernetes system components logs to prevent various types of sensitive information from leaking via logs. Refer to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation">KEP-1753: Kubernetes system components logs sanitization&lt;/a> for more information and an &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#alternatives=">alternative approach&lt;/a>.&lt;/li>
&lt;li>In-tree provisioner to CSI driver migration: This applies to a number of in-tree plugins, including &lt;a href="https://github.com/kubernetes/enhancements/issues/2589">Portworx&lt;/a>. Refer to the &lt;a href="https://github.com/kubernetes/design-proposals-archive/blob/main/storage/csi-migration.md#background-and-motivations">In-tree Storage Plugin to CSI Migration Design Doc&lt;/a> for more information.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2221">Removing Dockershim from kubelet&lt;/a>: the Container Runtime Interface (CRI) for Docker (i.e. Dockershim) is currently a built-in container runtime in the kubelet code base. It was deprecated in v1.20. As of v1.24, the kubelet will no longer have dockershim. Check out this blog on &lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">what you need to do be ready for v1.24&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">Storage capacity tracking for pod scheduling&lt;/a>: The CSIStorageCapacity API supports exposing currently available storage capacity via CSIStorageCapacity objects and enhances scheduling of pods that use CSI volumes with late binding. In v1.24, the CSIStorageCapacity API will be stable. The API graduating to stable initates the deprecation of the v1beta1 CSIStorageCapacity API. Refer to the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking">Storage Capacity Constraints for Pod Scheduling KEP&lt;/a> for more information.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107533">The &lt;code>master&lt;/code> label is no longer present on kubeadm control plane nodes&lt;/a>. For new clusters, the label 'node-role.kubernetes.io/master' will no longer be added to control plane nodes, only the label 'node-role.kubernetes.io/control-plane' will be added. For more information, refer to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint">KEP-2067: Rename the kubeadm &amp;quot;master&amp;quot; label and taint&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD will be removed&lt;/a>. Volume snapshot and restore functionality for Kubernetes and the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface&lt;/a> (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, entered beta in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.21 and is now unsupported. Refer to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177: CSI Snapshot&lt;/a> and &lt;a href="https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v4.1.0">kubernetes-csi/external-snapshotter&lt;/a> for more information.&lt;/li>
&lt;/ul>
&lt;h2 id="what-to-do">What to do&lt;/h2>
&lt;h3 id="dockershim-removal">Dockershim removal&lt;/h3>
&lt;p>As stated earlier, there are several guides about
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim&lt;/a>.
You can start with &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">Finding what container runtime are on your nodes&lt;/a>.
If your nodes are using dockershim, there are other possible Docker Engine dependencies such as
Pods or third-party tools executing Docker commands or private registries in the Docker configuration file. You can follow the
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">Check whether Dockershim deprecation affects you&lt;/a> guide to review possible
Docker Engine dependencies. Before upgrading to v1.24, you decide to either remain using Docker Engine and
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">Migrate Docker Engine nodes from dockershim to cri-dockerd&lt;/a> or migrate to a CRI-compatible runtime. Here's a guide to
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">change the container runtime on a node from Docker Engine to containerd&lt;/a>.&lt;/p>
&lt;h3 id="kubectl-convert">&lt;code>kubectl convert&lt;/code>&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.io/docs/tasks/tools/included/kubectl-convert-overview/">&lt;code>kubectl convert&lt;/code>&lt;/a> plugin for &lt;code>kubectl&lt;/code>
can be helpful to address migrating off deprecated APIs. The plugin facilitates the conversion of
manifests between different API versions, for example, from a deprecated to a non-deprecated API
version. More general information about the API migration process can be found in the &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/">Deprecated API Migration Guide&lt;/a>.
Follow the &lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin">install &lt;code>kubectl convert&lt;/code> plugin&lt;/a>
documentation to download and install the &lt;code>kubectl-convert&lt;/code> binary.&lt;/p>
&lt;h3 id="looking-ahead">Looking ahead&lt;/h3>
&lt;p>The Kubernetes 1.25 and 1.26 releases planned for later this year will stop serving beta versions
of several currently stable Kubernetes APIs. The v1.25 release will also remove PodSecurityPolicy,
which was deprecated with Kubernetes 1.21 and will not graduate to stable. See &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy
Deprecation: Past, Present, and Future&lt;/a> for more information.&lt;/p>
&lt;p>The official &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25">list of API removals planned for Kubernetes 1.25&lt;/a> is:&lt;/p>
&lt;ul>
&lt;li>The beta CronJob API (batch/v1beta1)&lt;/li>
&lt;li>The beta EndpointSlice API (discovery.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta Event API (events.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta1)&lt;/li>
&lt;li>The beta PodDisruptionBudget API (policy/v1beta1)&lt;/li>
&lt;li>The beta PodSecurityPolicy API (policy/v1beta1)&lt;/li>
&lt;li>The beta RuntimeClass API (node.k8s.io/v1beta1)&lt;/li>
&lt;/ul>
&lt;p>The official &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26">list of API removals planned for Kubernetes 1.26&lt;/a> is:&lt;/p>
&lt;ul>
&lt;li>The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)&lt;/li>
&lt;/ul>
&lt;h3 id="want-to-know-more">Want to know more?&lt;/h3>
&lt;p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>We will formally announce the deprecations that come with &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a> as part of the CHANGELOG for that release.&lt;/li>
&lt;/ul>
&lt;p>For information on the process of deprecation and removal, check out the official Kubernetes &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">deprecation policy&lt;/a> document.&lt;/p></description></item><item><title>Blog: Is Your Cluster Ready for v1.24?</title><link>https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/</link><pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kat Cosgrove&lt;/p>
&lt;p>Way back in December of 2020, Kubernetes announced the &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">deprecation of Dockershim&lt;/a>. In Kubernetes, dockershim is a software shim that allows you to use the entire Docker engine as your container runtime within Kubernetes. In the upcoming v1.24 release, we are removing Dockershim - the delay between deprecation and removal in line with the &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">project’s policy&lt;/a> of supporting features for at least one year after deprecation. If you are a cluster operator, this guide includes the practical realities of what you need to know going into this release. Also, what do you need to do to ensure your cluster doesn’t fall over!&lt;/p>
&lt;h2 id="first-does-this-even-affect-you">First, does this even affect you?&lt;/h2>
&lt;p>If you are rolling your own cluster or are otherwise unsure whether or not this removal affects you, stay on the safe side and &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">check to see if you have any dependencies on Docker Engine&lt;/a>. Please note that using Docker Desktop to build your application containers is not a Docker dependency for your cluster. Container images created by Docker are compliant with the &lt;a href="https://opencontainers.org/">Open Container Initiative (OCI)&lt;/a>, a Linux Foundation governance structure that defines industry standards around container formats and runtimes. They will work just fine on any container runtime supported by Kubernetes.&lt;/p>
&lt;p>If you are using a managed Kubernetes service from a cloud provider, and you haven’t explicitly changed the container runtime, there may be nothing else for you to do. Amazon EKS, Azure AKS, and Google GKE all default to containerd now, though you should make sure they do not need updating if you have any node customizations. To check the runtime of your nodes, follow &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">Find Out What Container Runtime is Used on a Node&lt;/a>.&lt;/p>
&lt;p>Regardless of whether you are rolling your own cluster or using a managed Kubernetes service from a cloud provider, you may need to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">migrate telemetry or security agents that rely on Docker Engine&lt;/a>.&lt;/p>
&lt;h2 id="i-have-a-docker-dependency-what-now">I have a Docker dependency. What now?&lt;/h2>
&lt;p>If your Kubernetes cluster depends on Docker Engine and you intend to upgrade to Kubernetes v1.24 (which you should eventually do for security and similar reasons), you will need to change your container runtime from Docker Engine to something else or use &lt;a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd&lt;/a>. Since &lt;a href="https://containerd.io/">containerd&lt;/a> is a graduated CNCF project and the runtime within Docker itself, it’s a safe bet as an alternative container runtime. Fortunately, the Kubernetes project has already documented the process of &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">changing a node’s container runtime&lt;/a>, using containerd as an example. Instructions are similar for switching to one of the other supported runtimes.&lt;/p>
&lt;h2 id="i-want-to-upgrade-kubernetes-and-i-need-to-maintain-compatibility-with-docker-as-a-runtime-what-are-my-options">I want to upgrade Kubernetes, and I need to maintain compatibility with Docker as a runtime. What are my options?&lt;/h2>
&lt;p>Fear not, you aren’t being left out in the cold and you don’t have to take the security risk of staying on an old version of Kubernetes. Mirantis and Docker have jointly released, and are maintaining, a replacement for dockershim. That replacement is called &lt;a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd&lt;/a>. If you do need to maintain compatibility with Docker as a runtime, install cri-dockerd following the instructions in the project’s documentation.&lt;/p>
&lt;h2 id="is-that-it">Is that it?&lt;/h2>
&lt;p>Yes. As long as you go into this release aware of the changes being made and the details of your own clusters, and you make sure to communicate clearly with your development teams, it will be minimally dramatic. You may have some changes to make to your cluster, application code, or scripts, but all of these requirements are documented. Switching from using Docker Engine as your runtime to using &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">one of the other supported container runtimes&lt;/a> effectively means removing the middleman, since the purpose of dockershim is to access the container runtime used by Docker itself. From a practical perspective, this removal is better both for you and for Kubernetes maintainers in the long-run.&lt;/p>
&lt;p>If you still have questions, please first check the &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">Dockershim Removal FAQ&lt;/a>.&lt;/p></description></item><item><title>Blog: Meet Our Contributors - APAC (Aus-NZ region)</title><link>https://kubernetes.io/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</link><pubDate>Wed, 16 Mar 2022 12:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</guid><description>
&lt;p>&lt;strong>Authors &amp;amp; Interviewers:&lt;/strong> &lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a>, &lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a>, &lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>, &lt;a href="https://github.com/bradmccoydev">Brad McCoy&lt;/a>, &lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>, &lt;a href="https://github.com/jayesh-srivastava">Jayesh Srivastava&lt;/a>, &lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a>, &lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a>, &lt;a href="github.com/Priyankasaggu11929/">Priyanka Saggu&lt;/a>, &lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>, &lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Good day, everyone 👋&lt;/p>
&lt;p>Welcome back to the second episode of the &amp;quot;Meet Our Contributors&amp;quot; blog post series for APAC.&lt;/p>
&lt;p>This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project.&lt;/p>
&lt;p>So, without further ado, let's get straight to the blog.&lt;/p>
&lt;h2 id="caleb-woodbine-https-github-com-bobymcbobs">&lt;a href="https://github.com/BobyMCbobs">Caleb Woodbine&lt;/a>&lt;/h2>
&lt;p>Caleb Woodbine is currently a member of the ii.nz organisation.&lt;/p>
&lt;p>He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from &lt;a href="https://github.com/hh">Hippie Hacker&lt;/a>, a fellow contributor from New Zealand.&lt;/p>
&lt;p>He has made major contributions to Kubernetes project since then through &lt;code>SIG k8s-infra&lt;/code> and &lt;code>k8s-conformance&lt;/code> working group.&lt;/p>
&lt;p>Caleb is also a co-organizer of the &lt;a href="https://www.meetup.com/cloudnative-nz/">CloudNative NZ&lt;/a> community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="dylan-graham-https-github-com-dylangraham">&lt;a href="https://github.com/DylanGraham">Dylan Graham&lt;/a>&lt;/h2>
&lt;p>Dylan Graham is a cloud engineer from Adeliade, Australia. He has been contributing to the upstream Kubernetes project since 2018.&lt;/p>
&lt;p>He stated that being a part of such a large-scale project was initially overwhelming, but that the community's friendliness and openness assisted him in getting through it.&lt;/p>
&lt;p>He began by contributing to the project documentation and is now mostly focused on the community support for the APAC region.&lt;/p>
&lt;p>He believes that consistent attendance at community/project meetings, taking on project tasks, and seeking community guidance as needed can help new aspiring developers become effective contributors.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>The feeling of being a part of a large community is really special. I've met some amazing people, even some before the pandemic in real life :)&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="hippie-hacker-https-github-com-hh">&lt;a href="https://github.com/hh">Hippie Hacker&lt;/a>&lt;/h2>
&lt;p>Hippie has worked for the CNCF.io as a Strategic Initiatives contractor from New Zealand for almost 5+ years. He is an active contributor to k8s-infra, API conformance testing, Cloud provider conformance submissions, and apisnoop.cncf.io domains of the upstream Kubernetes &amp;amp; CNCF projects.&lt;/p>
&lt;p>He recounts their early involvement with the Kubernetes project, which began roughly 5 years ago when their firm, ii.nz, demonstrated &lt;a href="https://ii.nz/post/bringing-the-cloud-to-your-community/">network booting from a Raspberry Pi using PXE and running Gitlab in-cluster to install Kubernetes on servers&lt;/a>.&lt;/p>
&lt;p>He describes their own contributing experience as someone who, at first, tried to do all of the hard lifting on their own, but eventually saw the benefit of group contributions which reduced burnout and task division which allowed folks to keep moving forward on their own momentum.&lt;/p>
&lt;p>He recommends that new contributors use pair programming.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>The cross pollination of approaches and two pairs of eyes on the same work can often yield a much more amplified effect than a PR comment / approval alone can afford.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="nick-young-https-github-com-youngnick">&lt;a href="https://github.com/youngnick">Nick Young&lt;/a>&lt;/h2>
&lt;p>Nick Young works at VMware as a technical lead for Contour, a CNCF ingress controller. He was active with the upstream Kubernetes project from the beginning, and eventually became the chair of the LTS working group, where he advocated user concerns. He is currently the SIG Network Gateway API subproject's maintainer.&lt;/p>
&lt;p>His contribution path was notable in that he began working on major areas of the Kubernetes project early on, skewing his trajectory.&lt;/p>
&lt;p>He asserts the best thing a new contributor can do is to &amp;quot;start contributing&amp;quot;. Naturally, if it is relevant to their employment, that is excellent; however, investing non-work time in contributing can pay off in the long run in terms of work. He believes that new contributors, particularly those who are currently Kubernetes users, should be encouraged to participate in higher-level project discussions.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Just being active and contributing will get you a long way. Once you've been active for a while, you'll find that you're able to answer questions, which will mean you're asked questions, and before you know it you are an expert.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.&lt;/p>
&lt;p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋&lt;/p></description></item><item><title>Blog: Updated: Dockershim Removal FAQ</title><link>https://kubernetes.io/blog/2022/02/17/dockershim-faq/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/02/17/dockershim-faq/</guid><description>
&lt;p>&lt;strong>This is an update to the original &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Dockershim Deprecation FAQ&lt;/a> article,
published in late 2020.&lt;/strong>&lt;/p>
&lt;p>This document goes over some frequently asked questions regarding the
deprecation and removal of &lt;em>dockershim&lt;/em>, that was
&lt;a href="https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/">announced&lt;/a>
as a part of the Kubernetes v1.20 release. For more detail
on what that means, check out the blog post
&lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker&lt;/a>.&lt;/p>
&lt;p>Also, you can read &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">check whether dockershim removal affects you&lt;/a>
to determine how much impact the removal of dockershim would have for you
or for your organization.&lt;/p>
&lt;p>As the Kubernetes 1.24 release has become imminent, we've been working hard to try to make this a smooth transition.&lt;/p>
&lt;ul>
&lt;li>We've written a blog post detailing our &lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">commitment and next steps&lt;/a>.&lt;/li>
&lt;li>We believe there are no major blockers to migration to &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#container-runtimes">other container runtimes&lt;/a>.&lt;/li>
&lt;li>There is also a &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim&lt;/a> guide available.&lt;/li>
&lt;li>We've also created a page to list
&lt;a href="https://kubernetes.io/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/">articles on dockershim removal and on using CRI-compatible runtimes&lt;/a>.
That list includes some of the already mentioned docs, and also covers selected external sources
(including vendor guides).&lt;/li>
&lt;/ul>
&lt;h3 id="why-is-the-dockershim-being-removed-from-kubernetes">Why is the dockershim being removed from Kubernetes?&lt;/h3>
&lt;p>Early versions of Kubernetes only worked with a specific container runtime:
Docker Engine. Later, Kubernetes added support for working with other container runtimes.
The CRI standard was &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">created&lt;/a> to
enable interoperability between orchestrators (like Kubernetes) and many different container
runtimes.
Docker Engine doesn't implement that interface (CRI), so the Kubernetes project created
special code to help with the transition, and made that &lt;em>dockershim&lt;/em> code part of Kubernetes
itself.&lt;/p>
&lt;p>The dockershim code was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Dockershim Removal Kubernetes Enhancement Proposal&lt;/a>.
In fact, maintaining dockershim had become a heavy burden on the Kubernetes maintainers.&lt;/p>
&lt;p>Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.&lt;/p>
&lt;h3 id="can-i-still-use-docker-engine-in-kubernetes-1-23">Can I still use Docker Engine in Kubernetes 1.23?&lt;/h3>
&lt;p>Yes, the only thing changed in 1.20 is a single warning log printed at &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
startup if using Docker Engine as the runtime. You'll see this warning in all versions up to 1.23. The dockershim removal occurs in Kubernetes 1.24.&lt;/p>
&lt;h3 id="when-will-dockershim-be-removed">When will dockershim be removed?&lt;/h3>
&lt;p>Given the impact of this change, we are using an extended deprecation timeline.
Removal of dockershim is scheduled for Kubernetes v1.24, see &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Dockershim Removal Kubernetes Enhancement Proposal&lt;/a>.
The Kubernetes project will be working closely with vendors and other ecosystem groups to ensure
a smooth transition and will evaluate things as the situation evolves.&lt;/p>
&lt;h3 id="can-i-still-use-docker-engine-as-my-container-runtime">Can I still use Docker Engine as my container runtime?&lt;/h3>
&lt;p>First off, if you use Docker on your own PC to develop or test containers: nothing changes.
You can still use Docker locally no matter what container runtime(s) you use for your
Kubernetes clusters. Containers make this kind of interoperability possible.&lt;/p>
&lt;p>Mirantis and Docker have &lt;a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/">committed&lt;/a> to maintaining a replacement adapter for
Docker Engine, and to maintain that adapter even after the in-tree dockershim is removed
from Kubernetes. The replacement adapter is named &lt;a href="https://github.com/Mirantis/cri-dockerd">&lt;code>cri-dockerd&lt;/code>&lt;/a>.&lt;/p>
&lt;h3 id="will-my-existing-container-images-still-work">Will my existing container images still work?&lt;/h3>
&lt;p>Yes, the images produced from &lt;code>docker build&lt;/code> will work with all CRI implementations.
All your existing images will still work exactly the same.&lt;/p>
&lt;h4 id="what-about-private-images">What about private images?&lt;/h4>
&lt;p>Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.&lt;/p>
&lt;h3 id="are-docker-and-containers-the-same-thing">Are Docker and containers the same thing?&lt;/h3>
&lt;p>Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.&lt;/p>
&lt;h3 id="are-there-examples-of-folks-using-other-runtimes-in-production-today">Are there examples of folks using other runtimes in production today?&lt;/h3>
&lt;p>All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.&lt;/p>
&lt;p>Additionally, the &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the &lt;a href="https://cri-o.io/">CRI-O&lt;/a> runtime in production since June 2019.&lt;/p>
&lt;p>For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation (&lt;a href="https://cncf.io">CNCF&lt;/a>).&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="people-keep-referencing-oci-what-is-that">People keep referencing OCI, what is that?&lt;/h3>
&lt;p>OCI stands for the &lt;a href="https://opencontainers.org/about/overview/">Open Container Initiative&lt;/a>, which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>, which is the underlying default runtime for both
&lt;a href="https://containerd.io/">containerd&lt;/a> and &lt;a href="https://cri-o.io/">CRI-O&lt;/a>. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.&lt;/p>
&lt;h3 id="which-cri-implementation-should-i-use">Which CRI implementation should I use?&lt;/h3>
&lt;p>That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape&lt;/a> in case another would be an
even better fit for your environment.&lt;/p>
&lt;h3 id="what-should-i-look-out-for-when-changing-cri-implementations">What should I look out for when changing CRI implementations?&lt;/h3>
&lt;p>While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:&lt;/p>
&lt;ul>
&lt;li>Logging configuration&lt;/li>
&lt;li>Runtime resource limitations&lt;/li>
&lt;li>Node provisioning scripts that call docker or use docker via it's control socket&lt;/li>
&lt;li>Kubectl plugins that require docker CLI or the control socket&lt;/li>
&lt;li>Tools from the Kubernetes project that require direct access to Docker Engine
(for example: the deprecated &lt;code>kube-imagepuller&lt;/code> tool)&lt;/li>
&lt;li>Configuration of functionality like &lt;code>registry-mirrors&lt;/code> and insecure registries&lt;/li>
&lt;li>Other support scripts or daemons that expect Docker Engine to be available and are run
outside of Kubernetes (for example, monitoring or security agents)&lt;/li>
&lt;li>GPUs or special hardware and how they integrate with your runtime and Kubernetes&lt;/li>
&lt;/ul>
&lt;p>If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your &lt;code>dockerd&lt;/code> configuration, you’ll need to adapt that for your new container
runtime where possible.&lt;/p>
&lt;p>Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> tool as a drop-in replacement (see &lt;a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl">mapping from docker cli to crictl&lt;/a>) and for the
latter you can use newer container build options like &lt;a href="https://github.com/genuinetools/img">img&lt;/a>, &lt;a href="https://github.com/containers/buildah">buildah&lt;/a>,
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>, or &lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a> that don’t require Docker.&lt;/p>
&lt;p>For containerd, you can start with their &lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">documentation&lt;/a> to see what configuration
options are available as you migrate things over.&lt;/p>
&lt;p>For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">Container Runtimes&lt;/a>.&lt;/p>
&lt;h3 id="what-if-i-have-more-questions">What if I have more questions?&lt;/h3>
&lt;p>If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: &lt;a href="https://discuss.kubernetes.io/">https://discuss.kubernetes.io/&lt;/a>.&lt;/p>
&lt;p>You can discuss the decision to remove dockershim via a dedicated
&lt;a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub issue&lt;/a>.&lt;/p>
&lt;p>You can also check out the excellent blog post
&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">Wait, Docker is deprecated in Kubernetes now?&lt;/a> a more in-depth technical
discussion of the changes.&lt;/p>
&lt;h3 id="is-there-any-tooling-that-can-help-me-find-dockershim-in-use">Is there any tooling that can help me find dockershim in use&lt;/h3>
&lt;p>Yes! The &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">Detector for Docker Socket (DDS)&lt;/a> is a kubectl plugin that you can
install and then use to check your cluster. DDS can detect if active Kubernetes workloads
are mounting the Docker Engine socket (&lt;code>docker.sock&lt;/code>) as a volume.
Find more details and usage patterns in the DDS project's &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">README&lt;/a>.&lt;/p>
&lt;h3 id="can-i-have-a-hug">Can I have a hug?&lt;/h3>
&lt;p>Yes, we're still giving hugs as requested. 🤗🤗🤗&lt;/p></description></item><item><title>Blog: SIG Node CI Subproject Celebrates Two Years of Test Improvements</title><link>https://kubernetes.io/blog/2022/02/16/sig-node-ci-subproject-celebrates/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/02/16/sig-node-ci-subproject-celebrates/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sergey Kanzhelev (Google), Elana Hashman (Red Hat)&lt;/p>
&lt;p>Ensuring the reliability of SIG Node upstream code is a continuous effort
that takes a lot of behind-the-scenes effort from many contributors.
There are frequent releases of Kubernetes, base operating systems,
container runtimes, and test infrastructure that result in a complex matrix that
requires attention and steady investment to &amp;quot;keep the lights on.&amp;quot;
In May 2020, the Kubernetes node special interest group (&amp;quot;SIG Node&amp;quot;) organized a new
subproject for continuous integration (CI) for node-related code and tests. Since its
inauguration, the SIG Node CI subproject has run a weekly meeting, and even the full hour
is often not enough to complete triage of all bugs, test-related PRs and issues, and discuss all
related ongoing work within the subgroup.&lt;/p>
&lt;p>Over the past two years, we've fixed merge-blocking and release-blocking tests, reducing time to merge Kubernetes contributors' pull requests thanks to reduced test flakes. When we started, Node test jobs only passed 42% of the time, and through our efforts, we now ensure a consistent &amp;gt;90% job pass rate. We've closed 144 test failure issues and merged 176 pull requests just in kubernetes/kubernetes. And we've helped subproject participants ascend the Kubernetes contributor ladder, with 3 new org members, 6 new reviewers, and 2 new approvers.&lt;/p>
&lt;p>The Node CI subproject is an approachable first stop to help new contributors
get started with SIG Node. There is a low barrier to entry for new contributors
to address high-impact bugs and test fixes, although there is a long
road before contributors can climb the entire contributor ladder:
it took over a year to establish two new approvers for the group.
The complexity of all the different components that power Kubernetes nodes
and its test infrastructure requires a sustained investment over a long period
for developers to deeply understand the entire system,
both at high and low levels of detail.&lt;/p>
&lt;p>We have several regular contributors at our meetings, however; our reviewers
and approvers pool is still small. It is our goal to continue to grow
contributors to ensure a sustainable distribution of work
that does not just fall to a few key approvers.&lt;/p>
&lt;p>It's not always obvious how subprojects within SIGs are formed, operate,
and work. Each is unique to its sponsoring SIG and tailored to the projects
that the group is intended to support. As a group that has welcomed many
first-time SIG Node contributors, we'd like to share some of the details and
accomplishments over the past two years,
helping to demystify our inner workings and celebrate the hard work
of all our dedicated contributors!&lt;/p>
&lt;h2 id="timeline">Timeline&lt;/h2>
&lt;p>&lt;em>&lt;strong>May 2020.&lt;/strong>&lt;/em> SIG Node CI group was formed on May 11, 2020, with more than
&lt;a href="https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib">30 volunteers&lt;/a>
signed up, to improve SIG Node CI signal and overall observability.
Victor Pickard focused on getting
&lt;a href="https://testgrid.k8s.io/sig-node">testgrid jobs&lt;/a> passing
when Ning Liao suggested forming a group around this effort and came up with
the &lt;a href="https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf">original group charter document&lt;/a>.
The SIG Node chairs sponsored group creation with Victor as a subproject lead.
Sergey Kanzhelev joined Victor shortly after as a co-lead.&lt;/p>
&lt;p>At the kick-off meeting, we discussed which tests to concentrate on fixing first
and discussed merge-blocking and release-blocking tests, many of which were failing due
to infrastructure issues or buggy test code.&lt;/p>
&lt;p>The subproject launched weekly hour-long meetings to discuss ongoing work
discussion and triage.&lt;/p>
&lt;p>&lt;em>&lt;strong>June 2020.&lt;/strong>&lt;/em> Morgan Bauer, Karan Goel, and Jorge Alarcon Ochoa were
recognized as reviewers for the SIG Node CI group for their contributions,
helping significantly with the early stages of the subproject.
David Porter and Roy Yang also joined the SIG test failures GitHub team.&lt;/p>
&lt;p>&lt;em>&lt;strong>August 2020.&lt;/strong>&lt;/em> All merge-blocking and release-blocking tests were passing,
with some flakes. However, only 42% of all SIG Node test jobs were green, as there
were many flakes and failing tests.&lt;/p>
&lt;p>&lt;em>&lt;strong>October 2020.&lt;/strong>&lt;/em> Amim Knabben becomes a Kubernetes org member for his
contributions to the subproject.&lt;/p>
&lt;p>&lt;em>&lt;strong>January 2021.&lt;/strong>&lt;/em> With healthy presubmit and critical periodic jobs passing,
the subproject discussed its goal for cleaning up the rest of periodic tests
and ensuring they passed without flakes.&lt;/p>
&lt;p>Elana Hashman joined the subproject, stepping up to help lead it after
Victor's departure.&lt;/p>
&lt;p>&lt;em>&lt;strong>February 2021.&lt;/strong>&lt;/em> Artyom Lukianov becomes a Kubernetes org member for his
contributions to the subproject.&lt;/p>
&lt;p>&lt;em>&lt;strong>August 2021.&lt;/strong>&lt;/em> After SIG Node successfully ran a &lt;a href="https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ">bug scrub&lt;/a>
to clean up its bug backlog, the scope of the meeting was extended to
include bug triage to increase overall reliability, anticipating issues
before they affect the CI signal.&lt;/p>
&lt;p>Subproject leads Elana Hashman and Sergey Kanzhelev are both recognized as
approvers on all node test code, supported by SIG Node and SIG Testing.&lt;/p>
&lt;p>&lt;em>&lt;strong>September 2021.&lt;/strong>&lt;/em> After significant deflaking progress with serial tests in
the 1.22 release spearheaded by Francesco Romani, the subproject set a goal
for getting the serial job fully passing by the 1.23 release date.&lt;/p>
&lt;p>Mike Miranda becomes a Kubernetes org member for his contributions
to the subproject.&lt;/p>
&lt;p>&lt;em>&lt;strong>November 2021.&lt;/strong>&lt;/em> Throughout 2021, SIG Node had no merge or
release-blocking test failures. Many flaky tests from past releases are removed
from release-blocking dashboards as they had been fully cleaned up.&lt;/p>
&lt;p>Danielle Lancashire was recognized as a reviewer for SIG Node's subgroup, test code.&lt;/p>
&lt;p>The final node serial tests were completely fixed. The serial tests consist of
many disruptive and slow tests which tend to be flakey and are hard
to troubleshoot. By the 1.23 release freeze, the last serial tests were
fixed and the job was passing without flakes.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900">&lt;img src="serial-tests-green.png" alt="Slack announcement that Serial tests are green">&lt;/a>&lt;/p>
&lt;p>The 1.23 release got a special shout out for the tests quality and CI signal.
The SIG Node CI subproject was proud to have helped contribute to such
a high-quality release, in part due to our efforts in identifying
and fixing flakes in Node and beyond.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200">&lt;img src="release-mostly-green.png" alt="Slack shoutout that release was mostly green">&lt;/a>&lt;/p>
&lt;p>&lt;em>&lt;strong>December 2021.&lt;/strong>&lt;/em> An estimated 90% of test jobs were passing at the time of
the 1.23 release (up from 42% in August 2020).&lt;/p>
&lt;p>Dockershim code was removed from Kubernetes. This affected nearly half of SIG Node's
test jobs and the SIG Node CI subproject reacted quickly and retargeted all the
tests. SIG Node was the first SIG to complete test migrations off dockershim,
providing examples for other affected SIGs. The vast majority of new jobs passed
at the time of introduction without further fixes required. The &lt;a href="https://k8s.io/dockershim">effort of
removing dockershim&lt;/a>) from Kubernetes is ongoing.
There are still some wrinkles from the dockershim removal as we uncover more
dependencies on dockershim, but we plan to stabilize all test jobs
by the 1.24 release.&lt;/p>
&lt;h2 id="statistics">Statistics&lt;/h2>
&lt;p>Our regular meeting attendees and subproject participants for the past few months:&lt;/p>
&lt;ul>
&lt;li>Aditi Sharma&lt;/li>
&lt;li>Artyom Lukianov&lt;/li>
&lt;li>Arnaud Meukam&lt;/li>
&lt;li>Danielle Lancashire&lt;/li>
&lt;li>David Porter&lt;/li>
&lt;li>Davanum Srinivas&lt;/li>
&lt;li>Elana Hashman&lt;/li>
&lt;li>Francesco Romani&lt;/li>
&lt;li>Matthias Bertschy&lt;/li>
&lt;li>Mike Miranda&lt;/li>
&lt;li>Paco Xu&lt;/li>
&lt;li>Peter Hunt&lt;/li>
&lt;li>Ruiwen Zhao&lt;/li>
&lt;li>Ryan Phillips&lt;/li>
&lt;li>Sergey Kanzhelev&lt;/li>
&lt;li>Skyler Clark&lt;/li>
&lt;li>Swati Sehgal&lt;/li>
&lt;li>Wenjun Wu&lt;/li>
&lt;/ul>
&lt;p>The &lt;a href="https://github.com/kubernetes/test-infra/">kubernetes/test-infra&lt;/a> source code repository contains test definitions. The number of
Node PRs just in that repository:&lt;/p>
&lt;ul>
&lt;li>2020 PRs (since May): &lt;a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+">183&lt;/a>&lt;/li>
&lt;li>2021 PRs: &lt;a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+">264&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Triaged issues and PRs on CI board (including triaging away from the subgroup scope):&lt;/p>
&lt;ul>
&lt;li>2020 (since May): &lt;a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31">132&lt;/a>&lt;/li>
&lt;li>2021: &lt;a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+">532&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="future">Future&lt;/h2>
&lt;p>Just &amp;quot;keeping the lights on&amp;quot; is a bold task and we are committed to improving this experience.
We are working to simplify the triage and review processes for SIG Node.&lt;/p>
&lt;p>Specifically, we are working on better test organization, naming,
and tracking:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/pull/3042">https://github.com/kubernetes/enhancements/pull/3042&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/issues/24641">https://github.com/kubernetes/test-infra/issues/24641&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0">Kubernetes SIG-Node CI Testgrid Tracker&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>We are also constantly making progress on improved tests debuggability and de-flaking.&lt;/p>
&lt;p>If any of this interests you, we'd love for you to join us!
There's plenty to learn in debugging test failures, and it will help you gain
familiarity with the code that SIG Node maintains.&lt;/p>
&lt;p>You can always find information about the group on the
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> page.
We give group updates at our maintainer track sessions, such as
&lt;a href="https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google">KubeCon + CloudNativeCon Europe 2021&lt;/a> and
&lt;a href="https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;amp;w=100%25&amp;amp;sidebar=yes&amp;amp;bg=no">KubeCon + CloudNative North America 2021&lt;/a>.
Join us in our mission to keep the kubelet and other SIG Node components reliable and ensure smooth and uneventful releases!&lt;/p></description></item><item><title>Blog: Spotlight on SIG Multicluster</title><link>https://kubernetes.io/blog/2022/02/07/sig-multicluster-spotlight-2022/</link><pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/02/07/sig-multicluster-spotlight-2022/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Dewan Ahmed (Aiven) and Chris Short (AWS)&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-multicluster">SIG Multicluster&lt;/a> is the SIG focused on how Kubernetes concepts are expanded and used beyond the cluster boundary. Historically, Kubernetes resources only interacted within that boundary - KRU or Kubernetes Resource Universe (not an actual Kubernetes concept). Kubernetes clusters, even now, don't really know anything about themselves or, about other clusters. Absence of cluster identifiers is a case in point. With the growing adoption of multicloud and multicluster deployments, the work SIG Multicluster doing is gaining a lot of attention. In this blog, &lt;a href="https://twitter.com/jeremyot">Jeremy Olmsted-Thompson, Google&lt;/a> and &lt;a href="https://twitter.com/ChrisShort">Chris Short, AWS&lt;/a> discuss the interesting problems SIG Multicluster is solving and how you can get involved. Their initials &lt;strong>JOT&lt;/strong> and &lt;strong>CS&lt;/strong> will be used for brevity.&lt;/p>
&lt;h2 id="a-summary-of-their-conversation">A summary of their conversation&lt;/h2>
&lt;p>&lt;strong>CS&lt;/strong>: How long has the SIG Multicluster existed and how was the SIG in its infancy? How long have you been with this SIG?&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>: I've been around for almost two years in the SIG Multicluster. All I know about the infancy years is from the lore but even in the early days, it was always about solving this same problem. Early efforts have been things like &lt;a href="https://github.com/kubernetes-sigs/kubefed">KubeFed&lt;/a>. I think there are still folks using KubeFed but it's a smaller slice. Back then, I think people out there deploying large numbers of Kubernetes clusters were really not at a point where we had a ton of real concrete use cases. Projects like KubeFed and &lt;a href="https://github.com/kubernetes-retired/cluster-registry">Cluster Registry&lt;/a> were developed around that time and the need back then can be associated to these projects. The motivation for these projects were how do we solve the problems that we think people are &lt;strong>going to have&lt;/strong>, when they start expanding to multiple clusters. Honestly, in some ways, it was trying to do too much at that time.&lt;/p>
&lt;p>&lt;strong>CS&lt;/strong>: How does KubeFed differ from the current state of SIG Multicluster? How does the &lt;strong>lore&lt;/strong> differ from the &lt;strong>now&lt;/strong>?&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>: Yeah, it was like trying to get ahead of potential problems instead of addressing specific problems. I think towards the end of 2019, there was a slow down in SIG multicluster work and we kind of picked it back up with one of the most active recent projects that is the &lt;a href="https://github.com/kubernetes-sigs/mcs-api">SIG Multicluster services (MCS)&lt;/a>.&lt;/p>
&lt;p>Now this is the shift to solving real specific problems. For example,&lt;/p>
&lt;blockquote>
&lt;p>I've got workloads that are spread across multiple clusters and I need them to talk to each other.&lt;/p>
&lt;/blockquote>
&lt;p>Okay, that's very straightforward and we know that we need to solve that. To get started, let's make sure that these projects can work together on a common API so you get the same kind of portability that you get with Kubernetes.&lt;/p>
&lt;p>There's a few implementations of the MCS API out there and more are being developed. But, we didn't build an implementation because depending on how you're deploying things there could be hundreds of implementations. As long as you only need the basic Multicluster service functionality, it'll just work on whatever background you want, whether it's Submariner, GKE, or a service mesh.&lt;/p>
&lt;p>My favorite example of &amp;quot;then vs. now&amp;quot; is cluster ID. A few years ago, there was an effort to define a cluster ID. A lot of really good thought went into this concept, for example, how do we make a cluster ID is unique across multiple clusters. How do we make this ID globally unique so it'll work in every contact? Let's say, there's an acquisition or merger of teams - does the cluster IDs still remain unique for those teams?&lt;/p>
&lt;p>With Multicluster services, we found the need for an actual cluster ID, and it has a very specific need. To address this specific need, we're no longer considering every single Kubernetes cluster out there rather the ClusterSets - a grouping of clusters that work together in some kind of bounds. That's a much narrower scope than considering clusters everywhere in time and space. It also leaves flexibility for an implementer to define the boundary (a ClusterSet) beyond which this cluster ID will no longer be unique.&lt;/p>
&lt;p>&lt;strong>CS&lt;/strong>: How do you feel about the current state of SIG Multicluster versus where you're hoping to be in future?&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>: There's a few projects that are kind of getting started, for example, Work API. In the future, I think that some common practices around how do we deploy things across clusters are going to develop.&lt;/p>
&lt;blockquote>
&lt;p>If I have clusters deployed in a bunch of different regions; what's the best way to actually do that?&lt;/p>
&lt;/blockquote>
&lt;p>The answer is, almost always, &amp;quot;it depends&amp;quot;. Why are you doing this? Is it because there's some kind of compliance that makes you care about locality? Is it performance? Is it availability?&lt;/p>
&lt;p>I think revisiting registry patterns will probably be a natural step after we have cluster IDs, that is, how do you actually associate these clusters together? Maybe you've got a distributed deployment that you run in your own data centers all over the world. I imagine that expanding the API in that space is going to be important as more multi cluster features develop. It really depends on what the community starts doing with these tools.&lt;/p>
&lt;p>&lt;strong>CS&lt;/strong>: In the early days of Kubernetes, we used to have a few large Kubernetes clusters and now we're dealing with many small Kubernetes clusters - even multiple clusters for our own dev environments. How has this shift from a few large clusters to many small clusters affected the SIG? Has it accelerated the work or make it challenging in any way?&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>: I think that it has created a lot of ambiguity that needs solving. Originally, you'd have a dev cluster, a staging cluster, and a prod cluster. When the multi region thing came in, we started needing dev/staging/prod clusters, per region. And then, sometimes clusters really need more isolation due to compliance or some regulations issues. Thus, we're ending up with a lot of clusters. I think figuring out the right balance on how many clusters should you actually have is important. The power of Kubernetes is being able to deploy a lot of things managed by a single control plane. So, it's not like every single workload that gets deployed should be in its own cluster. But I think it's pretty clear that we can't put every single workload in a single cluster.&lt;/p>
&lt;p>&lt;strong>CS&lt;/strong>: What are some of your favorite things about this SIG?&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>: The complexity of the problems, the people and the newness of the space. We don't have right answers and we have to figure this out. At the beginning, we couldn't even think about multi clusters because there was no way to connect services across clusters. Now there is and we're starting to go tackle those problems, I think that this is a really fun place to be in because I expect that the SIG is going to get a lot busier the next couple of years. It's a very collaborative group and we definitely would like more people to come join us, get involved, raise their problems and bring their ideas.&lt;/p>
&lt;p>&lt;strong>CS&lt;/strong>: What do you think keeps people in this group? How has the pandemic affected you?&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>: I think it definitely got a little bit quieter during the pandemic. But for the most part; it's a very distributed group so whether you're calling in to our weekly meetings from a conference room or from your home, it doesn't make that huge of a difference. During the pandemic, a lot of people had time to focus on what's next for their scale and growth. I think that's what keeps people in the group - we have real problems that need to be solved which are very new in this space. And it's fun :)&lt;/p>
&lt;h2 id="wrap-up">Wrap up&lt;/h2>
&lt;p>&lt;strong>CS&lt;/strong>: That's all we have for today. Thanks Jeremy for your time.&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>: Thanks Chris. Everybody is welcome at our &lt;a href="https://github.com/kubernetes/community/tree/master/sig-multicluster#meetings">bi-weekly meetings&lt;/a>. We love as many people to come as possible and welcome all questions and all ideas. It's a new space and it'd be great to grow the community.&lt;/p></description></item><item><title>Blog: Securing Admission Controllers</title><link>https://kubernetes.io/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</link><pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Rory McCune (Aqua Security)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Admission control&lt;/a> is a key part of Kubernetes security, alongside authentication and authorization. Webhook admission controllers are extensively used to help improve the security of Kubernetes clusters in a variety of ways including restricting the privileges of workloads and ensuring that images deployed to the cluster meet organization’s security requirements.&lt;/p>
&lt;p>However, as with any additional component added to a cluster, security risks can present themselves. A security risk example is if the deployment and management of the admission controller are not handled correctly. To help admission controller users and designers manage these risks appropriately, the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-security#security-docs">security documentation&lt;/a> subgroup of SIG Security has spent some time developing a &lt;a href="https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control">threat model for admission controllers&lt;/a>. This threat model looks at likely risks which may arise from the incorrect use of admission controllers, which could allow security policies to be bypassed, or even allow an attacker to get unauthorised access to the cluster.&lt;/p>
&lt;p>From the threat model, we developed a set of security best practices that should be adopted to ensure that cluster operators can get the security benefits of admission controllers whilst avoiding any risks from using them.&lt;/p>
&lt;h2 id="admission-controllers-and-good-practices-for-security">Admission controllers and good practices for security&lt;/h2>
&lt;p>From the threat model, a couple of themes emerged around how to ensure the security of admission controllers.&lt;/p>
&lt;h3 id="secure-webhook-configuration">Secure webhook configuration&lt;/h3>
&lt;p>It’s important to ensure that any security component in a cluster is well configured and admission controllers are no different here. There are a couple of security best practices to consider when using admission controllers&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Correctly configured TLS for all webhook traffic&lt;/strong>. Communications between the API server and the admission controller webhook should be authenticated and encrypted to ensure that attackers who may be in a network position to view or modify this traffic cannot do so. To achieve this access the API server and webhook must be using certificates from a trusted certificate authority so that they can validate their mutual identities&lt;/li>
&lt;li>&lt;strong>Only authenticated access allowed&lt;/strong>. If an attacker can send an admission controller large numbers of requests, they may be able to overwhelm the service causing it to fail. Ensuring all access requires strong authentication should mitigate that risk.&lt;/li>
&lt;li>&lt;strong>Admission controller fails closed&lt;/strong>. This is a security practice that has a tradeoff, so whether a cluster operator wants to configure it will depend on the cluster’s threat model. If an admission controller fails closed, when the API server can’t get a response from it, all deployments will fail. This stops attackers bypassing the admission controller by disabling it, but, can disrupt the cluster’s operation. As clusters can have multiple webhooks, one approach to hit a middle ground might be to have critical controls on a fail closed setups and less critical controls allowed to fail open.&lt;/li>
&lt;li>&lt;strong>Regular reviews of webhook configuration&lt;/strong>. Configuration mistakes can lead to security issues, so it’s important that the admission controller webhook configuration is checked to make sure the settings are correct. This kind of review could be done automatically by an Infrastructure As Code scanner or manually by an administrator.&lt;/li>
&lt;/ul>
&lt;h3 id="secure-cluster-configuration-for-admission-control">Secure cluster configuration for admission control&lt;/h3>
&lt;p>In most cases, the admission controller webhook used by a cluster will be installed as a workload in the cluster. As a result, it’s important to ensure that Kubernetes' security features that could impact its operation are well configured.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Restrict &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC&lt;/a> rights&lt;/strong>. Any user who has rights which would allow them to modify the configuration of the webhook objects or the workload that the admission controller uses could disrupt its operation. So it’s important to make sure that only cluster administrators have those rights.&lt;/li>
&lt;li>&lt;strong>Prevent privileged workloads&lt;/strong>. One of the realities of container systems is that if a workload is given certain privileges, it will be possible to break out to the underlying cluster node and impact other containers on that node. Where admission controller services run in the cluster they’re protecting, it’s important to ensure that any requirement for privileged workloads is carefully reviewed and restricted as much as possible.&lt;/li>
&lt;li>&lt;strong>Strictly control external system access&lt;/strong>. As a security service in a cluster admission controller systems will have access to sensitive information like credentials. To reduce the risk of this information being sent outside the cluster, &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies&lt;/a> should be used to restrict the admission controller services access to external networks.&lt;/li>
&lt;li>&lt;strong>Each cluster has a dedicated webhook&lt;/strong>. Whilst it may be possible to have admission controller webhooks that serve multiple clusters, there is a risk when using that model that an attack on the webhook service would have a larger impact where it’s shared. Also where multiple clusters use an admission controller there will be increased complexity and access requirements, making it harder to secure.&lt;/li>
&lt;/ul>
&lt;h3 id="admission-controller-rules">Admission controller rules&lt;/h3>
&lt;p>A key element of any admission controller used for Kubernetes security is the rulebase it uses. The rules need to be able to accurately meet their goals avoiding false positive and false negative results.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Regularly test and review rules&lt;/strong>. Admission controller rules need to be tested to ensure their accuracy. They also need to be regularly reviewed as the Kubernetes API will change with each new version, and rules need to be assessed with each Kubernetes release to understand any changes that may be required to keep them up to date.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Meet Our Contributors - APAC (India region)</title><link>https://kubernetes.io/blog/2022/01/10/meet-our-contributors-india-ep-01/</link><pubDate>Mon, 10 Jan 2022 12:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/01/10/meet-our-contributors-india-ep-01/</guid><description>
&lt;p>&lt;strong>Authors &amp;amp; Interviewers:&lt;/strong> &lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a>, &lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a>, &lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>, &lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>, &lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a>, &lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a>, &lt;a href="https://github.com/CIPHERTron">Pritish Samal&lt;/a>, &lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>, &lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;p>&lt;strong>Editor:&lt;/strong> &lt;a href="https://psaggu.com">Priyanka Saggu&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Good day, everyone 👋&lt;/p>
&lt;p>Welcome to the first episode of the APAC edition of the &amp;quot;Meet Our Contributors&amp;quot; blog post series.&lt;/p>
&lt;p>In this post, we'll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives.&lt;/p>
&lt;p>💫 &lt;em>Let's get started, so without further ado…&lt;/em>&lt;/p>
&lt;h2 id="arsh-sharma-https-github-com-rinkiyakedad">&lt;a href="https://github.com/RinkiyaKeDad">Arsh Sharma&lt;/a>&lt;/h2>
&lt;p>Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project.&lt;/p>
&lt;p>He is presently a CI Signal shadow on the Kubernetes 1.23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the &lt;a href="https://github.com/cert-manager/infrastructure">cert-manager&lt;/a> tools development work that is being done under the aegis of SIG Architecture.&lt;/p>
&lt;p>To the newcomers, Arsh helps plan their early contributions sustainably.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>I would encourage folks to contribute in a way that's sustainable. What I mean by that
is that it's easy to be very enthusiastic early on and take up more stuff than one can
actually handle. This can often lead to burnout in later stages. It's much more sustainable
to work on things iteratively.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="kunal-kushwaha-https-github-com-kunal-kushwaha">&lt;a href="https://github.com/kunal-kushwaha">Kunal Kushwaha&lt;/a>&lt;/h2>
&lt;p>Kunal Kushwaha is a core member of the Kubernetes marketing council. He is also a CNCF ambassador and one of the founders of the &lt;a href="https://community.cncf.io/cloud-native-students/">CNCF Students Program&lt;/a>.. He also served as a Communications role shadow during the 1.22 release cycle.&lt;/p>
&lt;p>At the end of his first year, Kunal began contributing to the &lt;a href="https://github.com/fabric8io/kubernetes-client">fabric8io kubernetes-client&lt;/a> project. He was then selected to work on the same project as part of Google Summer of Code. Kunal mentored people on the same project, first through Google Summer of Code then through Google Code-in.&lt;/p>
&lt;p>As an open-source enthusiast, he believes that diverse participation in the community is beneficial since it introduces new perspectives and opinions and respect for one's peers. He has worked on various open-source projects, and his participation in communities has considerably assisted his development as a developer.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>I believe if you find yourself in a place where you do not know much about the
project, that's a good thing because now you can learn while contributing and the
community is there to help you. It has helped me a lot in gaining skills, meeting
people from around the world and also helping them. You can learn on the go,
you don't have to be an expert. Make sure to also check out no code contributions
because being a beginner is a skill and you can bring new perspectives to the
organisation.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="madhav-jivarajani-https-github-com-madhavjivrajani">&lt;a href="https://github.com/MadhavJivrajani">Madhav Jivarajani&lt;/a>&lt;/h2>
&lt;p>Madhav Jivarajani works on the VMware Upstream Kubernetes stability team. He began contributing to the Kubernetes project in January 2021 and has since made significant contributions to several areas of work under SIG Architecture, SIG API Machinery, and SIG ContribEx (contributor experience).&lt;/p>
&lt;p>Among several significant contributions are his recent efforts toward the Archival of &lt;a href="https://github.com/kubernetes/community/issues/6055">design proposals&lt;/a>, refactoring the &lt;a href="https://github.com/kubernetes/k8s.io/pull/2713">&amp;quot;groups&amp;quot; codebase&lt;/a> under k8s-infra repository to make it mockable and testable, and improving the functionality of the &lt;a href="https://github.com/kubernetes/test-infra/issues/23129">GitHub k8s bot&lt;/a>.&lt;/p>
&lt;p>In addition to his technical efforts, Madhav oversees many projects aimed at assisting new contributors. He organises bi-weekly &amp;quot;KEP reading club&amp;quot; sessions to help newcomers understand the process of adding new features, deprecating old ones, and making other key changes to the upstream project. He has also worked on developing &lt;a href="https://github.com/kubernetes-sigs/contributor-katacoda">Katacoda scenarios&lt;/a> to assist new contributors to become acquainted with the process of contributing to k/k. In addition to his current efforts to meet with community members every week, he has organised several &lt;a href="https://www.youtube.com/watch?v=FgsXbHBRYIc">new contributors workshops (NCW)&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>I initially did not know much about Kubernetes. I joined because the community was
super friendly. But what made me stay was not just the people, but the project itself.
My solution to not feeling overwhelmed in the community was to gain as much context
and knowledge into the topics that I was interested in and were being discussed. And
as a result I continued to dig deeper into Kubernetes and the design of it.
I am a systems nut &amp;amp; thus Kubernetes was an absolute goldmine for me.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="rajas-kakodkar-https-github-com-rajaskakodkar">&lt;a href="https://github.com/rajaskakodkar">Rajas Kakodkar&lt;/a>&lt;/h2>
&lt;p>Rajas Kakodkar currently works at VMware as a Member of Technical Staff. He has been engaged in many aspects of the upstream Kubernetes project since 2019.&lt;/p>
&lt;p>He is now a key contributor to the Testing special interest group. He is also active in the SIG Network community. Lately, Rajas has contributed significantly to the &lt;a href="https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/">NetworkPolicy++&lt;/a> and &lt;a href="https://github.com/kubernetes-sigs/kpng">&lt;code>kpng&lt;/code>&lt;/a> sub-projects.&lt;/p>
&lt;p>One of the first challenges he ran across was that he was in a different time zone than the upstream project's regular meeting hours. However, async interactions on community forums progressively corrected that problem.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>I enjoy contributing to Kubernetes not just because I get to work on
cutting edge tech but more importantly because I get to work with
awesome people and help in solving real world problems.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="rajula-vineet-reddy-https-github-com-rajula96reddy">&lt;a href="https://github.com/rajula96reddy">Rajula Vineet Reddy&lt;/a>&lt;/h2>
&lt;p>Rajula Vineet Reddy, a Junior Engineer at CERN, is a member of the Marketing Council team under SIG ContribEx . He also served as a release shadow for SIG Release during the 1.22 and 1.23 Kubernetes release cycles.&lt;/p>
&lt;p>He started looking at the Kubernetes project as part of a university project with the help of one of his professors. Over time, he spent a significant amount of time reading the project's documentation, Slack discussions, GitHub issues, and blogs, which helped him better grasp the Kubernetes project and piqued his interest in contributing upstream. One of his key contributions was his assistance with automation in the SIG ContribEx Upstream Marketing subproject.&lt;/p>
&lt;p>According to Rajula, attending project meetings and shadowing various project roles are vital for learning about the community.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>I find the community very helpful and it's always&lt;/em>
“you get back as much as you contribute”.
&lt;em>The more involved you are, the more you will understand, get to learn and
contribute new things.&lt;/em>&lt;/p>
&lt;p>&lt;em>The first step to&lt;/em> “come forward and start” &lt;em>is hard. But it's all gonna be
smooth after that. Just take that jump.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. We're thrilled to have other folks assisting us in reaching out to even more wonderful individuals of the community. Your suggestions would be much appreciated.&lt;/p>
&lt;p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋&lt;/p></description></item><item><title>Blog: Kubernetes is Moving on From Dockershim: Commitments and Next Steps</title><link>https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</link><pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)&lt;/p>
&lt;p>Kubernetes is removing dockershim in the upcoming v1.24 release. We're excited
to reaffirm our community values by supporting open source container runtimes,
enabling a smaller kubelet, and increasing engineering velocity for teams using
Kubernetes. If you &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">use Docker Engine as a container runtime&lt;/a>
for your Kubernetes cluster, get ready to migrate in 1.24! To check if you're
affected, refer to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">Check whether dockershim deprecation affects you&lt;/a>.&lt;/p>
&lt;h2 id="why-we-re-moving-away-from-dockershim">Why we’re moving away from dockershim&lt;/h2>
&lt;p>Docker was the first container runtime used by Kubernetes. This is one of the
reasons why Docker is so familiar to many Kubernetes users and enthusiasts.
Docker support was hardcoded into Kubernetes – a component the project refers to
as dockershim.
As containerization became an industry standard, the Kubernetes project added support
for additional runtimes. This culminated in the implementation of the
container runtime interface (CRI), letting system components (like the kubelet)
talk to container runtimes in a standardized way. As a result, dockershim became
an anomaly in the Kubernetes project.
Dependencies on Docker and dockershim have crept into various tools
and projects in the CNCF ecosystem ecosystem, resulting in fragile code.&lt;/p>
&lt;p>By removing the
dockershim CRI, we're embracing the first value of CNCF: &amp;quot;&lt;a href="https://github.com/cncf/foundation/blob/master/charter.md#3-values">Fast is better than
slow&lt;/a>&amp;quot;.
Stay tuned for future communications on the topic!&lt;/p>
&lt;h2 id="deprecation-timeline">Deprecation timeline&lt;/h2>
&lt;p>We &lt;a href="https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/">formally announced&lt;/a> the dockershim deprecation in December 2020. Full removal is targeted
in Kubernetes 1.24, in April 2022. This timeline
aligns with our &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior">deprecation policy&lt;/a>,
which states that deprecated behaviors must function for at least 1 year
after their announced deprecation.&lt;/p>
&lt;p>We'll support Kubernetes version 1.23, which includes
dockershim, for another year in the Kubernetes project. For managed
Kubernetes providers, vendor support is likely to last even longer, but this is
dependent on the companies themselves. Regardless, we're confident all cluster operations will have
time to migrate. If you have more questions about the dockershim removal, refer
to the &lt;a href="https://kubernetes.io/dockershim">Dockershim Deprecation FAQ&lt;/a>.&lt;/p>
&lt;p>We asked you whether you feel prepared for the migration from dockershim in this
survey: &lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">Are you ready for Dockershim removal&lt;/a>.
We had over 600 responses. To everybody who took time filling out the survey,
thank you.&lt;/p>
&lt;p>The results show that we still have a lot of ground to cover to help you to
migrate smoothly. Other container runtimes exist, and have been promoted
extensively. However, many users told us they still rely on dockershim,
and sometimes have dependencies that need to be re-worked. Some of these
dependencies are outside of your control. Based on your feedback, here are some
of the steps we are taking to help.&lt;/p>
&lt;h2 id="our-next-steps">Our next steps&lt;/h2>
&lt;p>Based on the feedback you provided:&lt;/p>
&lt;ul>
&lt;li>CNCF and the 1.24 release team are committed to delivering documentation in
time for the 1.24 release. This includes more informative blog posts like this
one, updating existing code samples, tutorials, and tasks, and producing a
migration guide for cluster operators.&lt;/li>
&lt;li>We are reaching out to the rest of the CNCF community to help prepare them for
this change.&lt;/li>
&lt;/ul>
&lt;p>If you're part of a project with dependencies on dockershim, or if you're
interested in helping with the migration effort, please join us! There's always
room for more contributors, whether to our transition tools or to our
documentation. To get started, say hello in the
&lt;a href="https://kubernetes.slack.com/archives/C0BP8PW9G">#sig-node&lt;/a>
channel on &lt;a href="https://slack.kubernetes.io/">Kubernetes Slack&lt;/a>!&lt;/p>
&lt;h2 id="final-thoughts">Final thoughts&lt;/h2>
&lt;p>As a project, we've already seen cluster operators increasingly adopt other
container runtimes through 2021.
We believe there are no major blockers to migration. The steps we're taking to
improve the migration experience will light the path more clearly for you.&lt;/p>
&lt;p>We understand that migration from dockershim is yet another action you may need to
do to keep your Kubernetes infrastructure up to date. For most of you, this step
will be straightforward and transparent. In some cases, you will encounter
hiccups or issues. The community has discussed at length whether postponing the
dockershim removal would be helpful. For example, we recently talked about it in
the &lt;a href="https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid">SIG Node discussion on November 11th&lt;/a>
and in the &lt;a href="https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx">Kubernetes Steering committee meeting held on December 6th&lt;/a>.
We already &lt;a href="https://github.com/kubernetes/enhancements/pull/2481/">postponed&lt;/a> it
once in 2021 because the adoption rate of other
runtimes was lower than we wanted, which also gave us more time to identify
potential blocking issues.&lt;/p>
&lt;p>At this point, we believe that the value that you (and Kubernetes) gain from
dockershim removal makes up for the migration effort you'll have. Start planning
now to avoid surprises. We'll have more updates and guides before Kubernetes
1.24 is released.&lt;/p></description></item><item><title>Blog: Kubernetes-in-Kubernetes and the WEDOS PXE bootable server farm</title><link>https://kubernetes.io/blog/2021/12/22/kubernetes-in-kubernetes-and-pxe-bootable-server-farm/</link><pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/22/kubernetes-in-kubernetes-and-pxe-bootable-server-farm/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Andrei Kvapil (WEDOS)&lt;/p>
&lt;p>When you own two data centers, thousands of physical servers, virtual machines and hosting for hundreds of thousands sites, Kubernetes can actually simplify the management of all these things. As practice has shown, by using Kubernetes, you can declaratively describe and manage not only applications, but also the infrastructure itself. I work for the largest Czech hosting provider &lt;strong>WEDOS Internet a.s&lt;/strong> and today I'll show you two of my projects — &lt;a href="https://github.com/kvaps/kubernetes-in-kubernetes">Kubernetes-in-Kubernetes&lt;/a> and &lt;a href="https://github.com/kvaps/kubefarm">Kubefarm&lt;/a>.&lt;/p>
&lt;p>With their help you can deploy a fully working Kubernetes cluster inside another Kubernetes using Helm in just a couple of commands. How and why?&lt;/p>
&lt;p>Let me introduce you to how our infrastructure works. All our physical servers can be divided into two groups: &lt;strong>control-plane&lt;/strong> and &lt;strong>compute&lt;/strong> nodes. Control plane nodes are usually set up manually, have a stable OS installed, and designed to run all cluster services including Kubernetes control-plane. The main task of these nodes is to ensure the smooth operation of the cluster itself. Compute nodes do not have any operating system installed by default, instead they are booting the OS image over the network directly from the control plane nodes. Their work is to carry out the workload.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme01.svg"
alt="Kubernetes cluster layout"/>
&lt;/figure>
&lt;p>Once nodes have downloaded their image, they can continue to work without keeping connection to the PXE server. That is, a PXE server is just keeping rootfs image and does not hold any other complex logic. After our nodes have booted, we can safely restart the PXE server, nothing critical will happen to them.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme02.svg"
alt="Kubernetes cluster after bootstrapping"/>
&lt;/figure>
&lt;p>After booting, the first thing our nodes do is join to the existing Kubernetes cluster, namely, execute the &lt;strong>kubeadm join&lt;/strong> command so that kube-scheduler could schedule some pods on them and launch various workloads afterwards. From the beginning we used the scheme when nodes were joined into the same cluster used for the control-plane nodes.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme03.svg"
alt="Kubernetes scheduling containers to the compute nodes"/>
&lt;/figure>
&lt;p>This scheme worked stably for over two years. However later we decided to add containerized Kubernetes to it. And now we can spawn new Kubernetes-clusters very easily right on our control-plane nodes which are now member special admin-clusters. Now, compute nodes can be joined directly to their own clusters - depending on the configuration.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/scheme04.svg"
alt="Multiple clusters are running in single Kubernetes, compute nodes joined to them"/>
&lt;/figure>
&lt;h2 id="kubefarm">Kubefarm&lt;/h2>
&lt;p>This project came with the goal of enabling anyone to deploy such an infrastructure in just a couple of commands using Helm and get about the same in the end.&lt;/p>
&lt;p>At this time, we moved away from the idea of a monocluster. Because it turned out to be not very convenient for managing work of several development teams in the same cluster. The fact is that Kubernetes was never designed as a multi-tenant solution and at the moment it does not provide sufficient means of isolation between projects. Therefore, running separate clusters for each team turned out to be a good idea. However, there should not be too many clusters, to let them be convenient to manage. Nor is it too small to have sufficient independence between development teams.&lt;/p>
&lt;p>The scalability of our clusters became noticeably better after that change. The more clusters you have per number of nodes, the smaller the failure domain and the more stable they work. And as a bonus, we got a fully declaratively described infrastructure. Thus, now you can deploy a new Kubernetes cluster in the same way as deploying any other application in Kubernetes.&lt;/p>
&lt;p>It uses &lt;a href="http://github.com/kvaps/kubernetes-in-kubernetes">Kubernetes-in-Kubernetes&lt;/a> as a basis, &lt;a href="https://github.com/ltsp/ltsp/">LTSP&lt;/a> as PXE-server from which the nodes are booted, and automates the DHCP server configuration using &lt;a href="https://github.com/kvaps/dnsmasq-controller">dnsmasq-controller&lt;/a>:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/kubefarm.png"
alt="Kubefarm"/>
&lt;/figure>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>Now let's see how it works. In general, if you look at Kubernetes as from an application perspective, you can note that it follows all the principles of &lt;a href="https://12factor.net/">The Twelve-Factor App&lt;/a>, and is actually written very well. Thus, it means running Kubernetes as an app in a different Kubernetes shouldn't be a big deal.&lt;/p>
&lt;h3 id="running-kubernetes-in-kubernetes">Running Kubernetes in Kubernetes&lt;/h3>
&lt;p>Now let's take a look at the &lt;a href="https://github.com/kvaps/kubernetes-in-kubernetes">Kubernetes-in-Kubernetes&lt;/a> project, which provides a ready-made Helm chart for running Kubernetes in Kubernetes.&lt;/p>
&lt;p>Here is the parameters that you can pass to Helm in the values file:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kvaps/kubernetes-in-kubernetes/tree/v0.13.1/deploy/helm/kubernetes">&lt;strong>kubernetes/values.yaml&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;img alt="Kubernetes is just five binaries" style="float: right; max-height: 280px;" src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/5binaries.png">
&lt;p>Beside &lt;strong>persistence&lt;/strong> (storage parameters for the cluster), the Kubernetes control-plane components are described here: namely: &lt;strong>etcd cluster&lt;/strong>, &lt;strong>apiserver&lt;/strong>, &lt;strong>controller-manager&lt;/strong> and &lt;strong>scheduler&lt;/strong>. These are pretty much standard Kubernetes components. There is a light-hearted saying that “Kubernetes is just five binaries”. So here is where the configuration for these binaries is located.&lt;/p>
&lt;p>If you ever tried to bootstrap a cluster using kubeadm, then this config will remind you it's configuration. But in addition to Kubernetes entities, you also have an admin container. In fact, it is a container which holds two binaries inside: &lt;strong>kubectl&lt;/strong> and &lt;strong>kubeadm&lt;/strong>. They are used to generate kubeconfig for the above components and to perform the initial configuration for the cluster. Also, in an emergency, you can always exec into it to check and manage your cluster.&lt;/p>
&lt;p>After the release &lt;a href="https://asciinema.org/a/407280">has been deployed&lt;/a>, you can see a list of pods: &lt;strong>admin-container&lt;/strong>, &lt;strong>apiserver&lt;/strong> in two replicas, &lt;strong>controller-manager&lt;/strong>, &lt;strong>etcd-cluster&lt;/strong>, &lt;strong>scheduller&lt;/strong> and the initial job that initializes the cluster. In the end you have a command, which allows you to get shell into the admin container, you can use it to see what is happening inside:&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407280?autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot01.svg" alt="">&lt;/a>&lt;/p>
&lt;p>Also, let's take look at the certificates. If you've ever installed Kubernetes, then you know that it has a &lt;em>scary&lt;/em> directory &lt;code>/etc/kubernetes/pki&lt;/code> with a bunch of some certificates. In case of Kubernetes-in-Kubernetes, you have fully automated management of them with cert-manager. Thus, it is enough to pass all certificates parameters to Helm during installation, and all the certificates will automatically be generated for your cluster.&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407280?t=15&amp;amp;autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot02.svg" alt="">&lt;/a>&lt;/p>
&lt;p>Looking at one of the certificates, eg. apiserver, you can see that it has a list of DNS names and IP addresses. If you want to make this cluster accessible outside, then just describe the additional DNS names in the values file and update the release. This will update the certificate resource, and cert-manager will regenerate the certificate. You'll no longer need to think about this. If kubeadm certificates need to be renewed at least once a year, here the cert-manager will take care and automatically renew them.&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407280?t=25&amp;amp;autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot03.svg" alt="">&lt;/a>&lt;/p>
&lt;p>Now let's log into the admin container and look at the cluster and nodes. Of course, there are no nodes, yet, because at the moment you have deployed just the blank control-plane for Kubernetes. But in kube-system namespace you can see some coredns pods waiting for scheduling and configmaps already appeared. That is, you can conclude that the cluster is working:&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407280?t=30&amp;amp;autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot04.svg" alt="">&lt;/a>&lt;/p>
&lt;p>Here is the &lt;a href="https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_kink_network.html">diagram of the deployed cluster&lt;/a>. You can see services for all Kubernetes components: &lt;strong>apiserver&lt;/strong>, &lt;strong>controller-manager&lt;/strong>, &lt;strong>etcd-cluster&lt;/strong> and &lt;strong>scheduler&lt;/strong>. And the pods on right side to which they forward traffic.&lt;/p>
&lt;p>&lt;a href="https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_kink_network.html">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/argocd01.png" alt="">&lt;/a>&lt;/p>
&lt;p>&lt;em>By the way, the diagram, is drawn in &lt;a href="https://argoproj.github.io/argo-cd/">ArgoCD&lt;/a> — the GitOps tool we use to manage our clusters, and cool diagrams are one of its features.&lt;/em>&lt;/p>
&lt;h3 id="orchestrating-physical-servers">Orchestrating physical servers&lt;/h3>
&lt;p>OK, now you can see the way how is our Kubernetes control-plane deployed, but what about worker nodes, how are we adding them? As I already said, all our servers are bare metal. We do not use virtualization to run Kubernetes, but we orchestrate all physical servers by ourselves.&lt;/p>
&lt;p>Also, we do use Linux network boot feature very actively. Moreover, this is exactly the booting, not some kind of automation of the installation. When the nodes are booting, they just run a ready-made system image for them. That is, to update any node, we just need to reboot it - and it will download a new image. It is very easy, simple and convenient.&lt;/p>
&lt;p>For this, the &lt;a href="https://github.com/kvaps/kubefarm">Kubefarm&lt;/a> project was created, which allows you to automate this. The most commonly used examples can be found in the &lt;a href="https://github.com/kvaps/kubefarm/tree/v0.13.1/examples">examples&lt;/a> directory. The most standard of them named &lt;a href="https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/generic">generic&lt;/a>. Let's take a look at values.yaml:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kvaps/kubefarm/blob/v0.13.1/examples/generic/values.yaml">&lt;strong>generic/values.yaml&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Here you can specify the parameters which are passed into the upstream Kubernetes-in-Kubernetes chart. In order for you control-plane to be accessible from the outside, it is enough to specify the IP address here, but if you wish, you can specify some DNS name here.&lt;/p>
&lt;p>In the PXE server configuration you can specify a timezone. You can also add an SSH key for logging in without a password (but you can also specify a password), as well as kernel modules and parameters that should be applied during booting the system.&lt;/p>
&lt;p>Next comes the &lt;strong>nodePools&lt;/strong> configuration, i.e. the nodes themselves. If you've ever used a terraform module for gke, then this logic will remind you of it. Here you statically describe all nodes with a set of parameters:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Name&lt;/strong> (hostname);&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>MAC-addresses&lt;/strong> — we have nodes with two network cards, and each one can boot from any of the MAC addresses specified here.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>IP-address&lt;/strong>, which the DHCP server should issue to this node.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In this example, you have two pools: the first has five nodes, the second has only one, the second pool has also two tags assigned. Tags are the way to describe configuration for specific nodes. For example, you can add specific DHCP options for some pools, options for the PXE server for booting (e.g. here is debug option enabled) and set of &lt;strong>kubernetesLabels&lt;/strong> and &lt;strong>kubernetesTaints&lt;/strong> options. What does that mean?&lt;/p>
&lt;p>For example, in this configuration you have a second nodePool with one node. The pool has &lt;strong>debug&lt;/strong> and &lt;strong>foo&lt;/strong> tags assigned. Now see the options for &lt;strong>foo&lt;/strong> tag in &lt;strong>kubernetesLabels&lt;/strong>. This means that the m1c43 node will boot with these two labels and taint assigned. Everything seems to be simple. Now &lt;a href="https://asciinema.org/a/407282">let's try&lt;/a> this in practice.&lt;/p>
&lt;h3 id="demo">Demo&lt;/h3>
&lt;p>Go to &lt;a href="https://github.com/kvaps/kubefarm/tree/v0.13.1/examples">examples&lt;/a> and update previously deployed chart to Kubefarm. Just use the &lt;a href="https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/generic">generic&lt;/a> parameters and look at the pods. You can see that a PXE server and one more job were added. This job essentially goes to the deployed Kubernetes cluster and creates a new token. Now it will run repeatedly every 12 hours to generate a new token, so that the nodes can connect to your cluster.&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407282?autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot05.svg" alt="">&lt;/a>&lt;/p>
&lt;p>In a &lt;a href="https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_Applications_kubefarm-network.html">graphical representation&lt;/a>, it looks about the same, but now apiserver started to be exposed outside.&lt;/p>
&lt;p>&lt;a href="https://kvaps.github.io/images/posts/Kubernetes-in-Kubernetes-and-PXE-bootable-servers-farm/Argo_CD_Applications_kubefarm-network.html">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/argocd02.png" alt="">&lt;/a>&lt;/p>
&lt;p>In the diagram, the IP is highlighted in green, the PXE server can be reached through it. At the moment, Kubernetes does not allow creating a single LoadBalancer service for TCP and UDP protocols by default, so you have to create two different services with the same IP address. One is for TFTP, and the second for HTTP, through which the system image is downloaded.&lt;/p>
&lt;p>But this simple example is not always enough, sometimes you might need to modify the logic at boot. For example, here is a directory &lt;a href="https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/advanced_network">advanced_network&lt;/a>, inside which there is a &lt;a href="https://github.com/kvaps/kubefarm/tree/v0.13.1/examples/advanced_network">values file&lt;/a> with a simple shell script. Let's call it &lt;code>network.sh&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kvaps/kubefarm/blob/v0.13.1/examples/advanced_network/values.yaml#L14-L78">&lt;strong>network.sh&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>All this script does is take environment variables at boot time, and generates a network configuration based on them. It creates a directory and puts the netplan config inside. For example, a bonding interface is created here. Basically, this script can contain everything you need. It can hold the network configuration or generate the system services, add some hooks or describe any other logic. Anything that can be described in bash or shell languages will work here, and it will be executed at boot time.&lt;/p>
&lt;p>Let's see how it can be &lt;a href="https://asciinema.org/a/407284">deployed&lt;/a>. Let's pass the generic values file as the first parameter, and an additional values file as the second parameter. This is a standard Helm feature. This way you can also pass the secrets, but in this case, the configuration is just expanded by the second file:&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407284?autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot06.svg" alt="">&lt;/a>&lt;/p>
&lt;p>Let's look at the configmap &lt;strong>foo-kubernetes-ltsp&lt;/strong> for the netboot server and make sure that &lt;code>network.sh&lt;/code> script is really there. These commands used to configure the network at boot time:&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407284?t=15&amp;amp;autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot07.svg" alt="">&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407286">Here&lt;/a> you can see how it works in principle. The chassis interface (we use HPE Moonshots 1500) have the nodes, you can enter &lt;code>show node list&lt;/code> command to get a list of all the nodes. Now you can see the booting process.&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407286?autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot08.svg" alt="">&lt;/a>&lt;/p>
&lt;p>You can also get their MAC addresses by &lt;code>show node macaddr all&lt;/code> command. We have a clever operator that collects MAC-addresses from chassis automatically and passes them to the DHCP server. Actually, it's just creating custom configuration resources for dnsmasq-controller which is running in same admin Kubernetes cluster. Also, trough this interface you can control the nodes themselves, e.g. turn them on and off.&lt;/p>
&lt;p>If you have no such opportunity to enter the chassis through iLO and collect a list of MAC addresses for your nodes, you can consider using &lt;a href="https://asciinema.org/a/407287">catchall cluster&lt;/a> pattern. Purely speaking, it is just a cluster with a dynamic DHCP pool. Thus, all nodes that are not described in the configuration to other clusters will automatically join to this cluster.&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407287?autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot09.svg" alt="">&lt;/a>&lt;/p>
&lt;p>For example, you can see a special cluster with some nodes. They are joined to the cluster with an auto-generated name based on their MAC address. Starting from this point you can connect to them and see what happens there. Here you can somehow prepare them, for example, set up the file system and then rejoin them to another cluster.&lt;/p>
&lt;p>Now let's try connecting to the node terminal and see how it is booting. After the BIOS, the network card is configured, here it sends a request to the DHCP server from a specific MAC address, which redirects it to a specific PXE server. Later the kernel and initrd image are downloaded from the server using the standard HTTP protocol:&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407286?t=28&amp;amp;autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot10.svg" alt="">&lt;/a>&lt;/p>
&lt;p>After loading the kernel, the node downloads the rootfs image and transfers control to systemd. Then the booting proceeds as usual, and after that the node joins Kubernetes:&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407286?t=80&amp;amp;autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot11.svg" alt="">&lt;/a>&lt;/p>
&lt;p>If you take a look at &lt;strong>fstab&lt;/strong>, you can see only two entries there: &lt;strong>/var/lib/docker&lt;/strong> and &lt;strong>/var/lib/kubelet&lt;/strong>, they are mounted as &lt;strong>tmpfs&lt;/strong> (in fact, from RAM). At the same time, the root partition is mounted as &lt;strong>overlayfs&lt;/strong>, so all changes that you make here on the system will be lost on the next reboot.&lt;/p>
&lt;p>Looking into the block devices on the node, you can see some nvme disk, but it has not yet been mounted anywhere. There is also a loop device - this is the exact rootfs image downloaded from the server. At the moment it is located in RAM, occupies 653 MB and mounted with the &lt;strong>loop&lt;/strong> option.&lt;/p>
&lt;p>If you look in &lt;strong>/etc/ltsp&lt;/strong>, you find the &lt;code>network.sh&lt;/code> file that was executed at boot. From containers, you can see running &lt;code>kube-proxy&lt;/code> and &lt;code>pause&lt;/code> container for it.&lt;/p>
&lt;p>&lt;a href="https://asciinema.org/a/407286?t=100&amp;amp;autoplay=1">&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/screenshot12.svg" alt="">&lt;/a>&lt;/p>
&lt;h2 id="details">Details&lt;/h2>
&lt;h3 id="network-boot-image">Network Boot Image&lt;/h3>
&lt;p>But where does the main image come from? There is a little trick here. The image for the nodes is built through the &lt;a href="https://github.com/kvaps/kubefarm/tree/v0.13.1/build/ltsp">Dockerfile&lt;/a> along with the server. The &lt;a href="https://docs.docker.com/develop/develop-images/multistage-build/">Docker multi-stage build&lt;/a> feature allows you to easily add any packages and kernel modules exactly at the stage of the image build. It looks like this:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kvaps/kubefarm/blob/v0.13.1/build/ltsp/Dockerfile">&lt;strong>Dockerfile&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>What's going on here? First, we take a regular Ubuntu 20.04 and install all the packages we need. First of all we install the &lt;strong>kernel&lt;/strong>, &lt;strong>lvm&lt;/strong>, &lt;strong>systemd&lt;/strong>, &lt;strong>ssh&lt;/strong>. In general, everything that you want to see on the final node should be described here. Here we also install &lt;code>docker&lt;/code> with &lt;code>kubelet&lt;/code> and &lt;code>kubeadm&lt;/code>, which are used to join the node to the cluster.&lt;/p>
&lt;p>And then we perform an additional configuration. In the last stage, we simply install &lt;code>tftp&lt;/code> and &lt;code>nginx&lt;/code> (which serves our image to clients), &lt;strong>grub&lt;/strong> (bootloader). Then root of the previous stages copied into the final image and generate squashed image from it. That is, in fact, we get a docker image, which has both the server and the boot image for our nodes. At the same time, it can be easily updated by changing the Dockerfile.&lt;/p>
&lt;h3 id="webhooks-and-api-aggregation-layer">Webhooks and API aggregation layer&lt;/h3>
&lt;p>I want to pay special attention to the problem of webhooks and aggregation layer. In general, webhooks is a Kubernetes feature that allows you to respond to the creation or modification of any resources. Thus, you can add a handler so that when resources are applied, Kubernetes must send request to some pod and check if configuration of this resource is correct, or make additional changes to it.&lt;/p>
&lt;p>But the point is, in order for the webhooks to work, the apiserver must have direct access to the cluster for which it is running. And if it is started in a separate cluster, like our case, or even separately from any cluster, then Konnectivity service can help us here. Konnectivity is one of the optional but officially supported Kubernetes components.&lt;/p>
&lt;p>Let's take cluster of four nodes for example, each of them is running a &lt;code>kubelet&lt;/code> and we have other Kubernetes components running outside: &lt;code>kube-apiserver&lt;/code>, &lt;code>kube-scheduler&lt;/code> and &lt;code>kube-controller-manager&lt;/code>. By default, all these components interact with the apiserver directly - this is the most known part of the Kubernetes logic. But in fact, there is also a reverse connection. For example, when you want to view the logs or run a &lt;code>kubectl exec command&lt;/code>, the API server establishes a connection to the specific kubelet independently:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity01.svg"
alt="Kubernetes apiserver reaching kubelet"/>
&lt;/figure>
&lt;p>But the problem is that if we have a webhook, then it usually runs as a standard pod with a service in our cluster. And when apiserver tries to reach it, it will fail because it will try to access an in-cluster service named &lt;strong>webhook.namespace.svc&lt;/strong> being outside of the cluster where it is actually running:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity02.svg"
alt="Kubernetes apiserver can&amp;#39;t reach webhook"/>
&lt;/figure>
&lt;p>And here Konnectivity comes to our rescue. Konnectivity is a tricky proxy server developed especially for Kubernetes. It can be deployed as a server next to the apiserver. And Konnectivity-agent is deployed in several replicas directly in the cluster you want to access. The agent establishes a connection to the server and sets up a stable channel to make apiserver able to access all webhooks and all kubelets in the cluster. Thus, now all communication with the cluster will take place through the Konnectivity-server:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-12-22-kubernetes-in-kubernetes-and-pxe-bootable-server-farm/konnectivity03.svg"
alt="Kubernetes apiserver reaching webhook via konnectivity"/>
&lt;/figure>
&lt;h2 id="our-plans">Our plans&lt;/h2>
&lt;p>Of course, we are not going to stop at this stage. People interested in the project often write to me. And if there will be a sufficient number of interested people, I hope to move Kubernetes-in-Kubernetes project under &lt;a href="https://github.com/kubernetes-sigs">Kubernetes SIGs&lt;/a>, by representing it in form of the official Kubernetes Helm chart. Perhaps, by making this project independent we'll gather an even larger community.&lt;/p>
&lt;p>I am also thinking of integrating it with the Machine Controller Manager, which would allow creating worker nodes, not only of physical servers, but also, for example, for creating virtual machines using kubevirt and running them in the same Kubernetes cluster. By the way, it also allows to spawn virtual machines in the clouds, and have a control-plane deployed locally.&lt;/p>
&lt;p>I am also considering the option of integrating with the Cluster-API so that you can create physical Kubefarm clusters directly through the Kubernetes environment. But at the moment I'm not completely sure about this idea. If you have any thoughts on this matter, I'll be happy to listen to them.&lt;/p></description></item><item><title>Blog: Using Admission Controllers to Detect Container Drift at Runtime</title><link>https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/</link><pubDate>Tue, 21 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Saifuding Diliyaer (Box)
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/12/21/admission-controllers-for-container-drift/intro-illustration.png"
alt="Introductory illustration"/> &lt;figcaption>
&lt;p>Illustration by Munire Aireti&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>At Box, we use Kubernetes (K8s) to manage hundreds of micro-services that enable Box to stream data at a petabyte scale. When it comes to the deployment process, we run &lt;a href="https://github.com/box/kube-applier">kube-applier&lt;/a> as part of the GitOps workflows with declarative configuration and automated deployment. Developers declare their K8s apps manifest into a Git repository that requires code reviews and automatic checks to pass, before any changes can get merged and applied inside our K8s clusters. With &lt;code>kubectl exec&lt;/code> and other similar commands, however, developers are able to directly interact with running containers and alter them from their deployed state. This interaction could then subvert the change control and code review processes that are enforced in our CI/CD pipelines. Further, it allows such impacted containers to continue receiving traffic long-term in production.&lt;/p>
&lt;p>To solve this problem, we developed our own K8s component called &lt;a href="https://github.com/box/kube-exec-controller">kube-exec-controller&lt;/a> along with its corresponding &lt;a href="https://github.com/box/kube-exec-controller#kubectl-pi">kubectl plugin&lt;/a>. They function together in detecting and terminating potentially mutated containers (caused by interactive kubectl commands), as well as revealing the interaction events directly to the target Pods for better visibility.&lt;/p>
&lt;h2 id="admission-control-for-interactive-kubectl-commands">Admission control for interactive kubectl commands&lt;/h2>
&lt;p>Once a request is sent to K8s, it needs to be authenticated and authorized by the API server to proceed. Additionally, K8s has a separate layer of protection called &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">admission controllers&lt;/a>, which can intercept the request before an object is persisted in &lt;em>etcd&lt;/em>. There are various predefined admission controls compiled into the API server binary (e.g. ResourceQuota to enforce hard resource usage limits per namespace). Besides, there are two dynamic admission controls named &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">MutatingAdmissionWebhook&lt;/a> and &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">ValidatingAdmissionWebhook&lt;/a>, used for mutating or validating K8s requests respectively. The latter is what we adopted to detect container drift at runtime caused by interactive kubectl commands. This whole process can be divided into three steps as explained in detail below.&lt;/p>
&lt;h3 id="1-admit-interactive-kubectl-command-requests">1. Admit interactive kubectl command requests&lt;/h3>
&lt;p>First of all, we needed to enable a validating webhook that sends qualified requests to &lt;em>kube-exec-controller&lt;/em>. To add the new validation mechanism applying to interactive kubectl commands specifically, we configured the webhook’s rules with resources as &lt;code>[pods/exec, pods/attach]&lt;/code>, and operations as &lt;code>CONNECT&lt;/code>. These rules tell the cluster's API server that all &lt;code>exec&lt;/code> and &lt;code>attach&lt;/code> requests should be subject to our admission control webhook. In the ValidatingAdmissionWebhook that we configured, we specified a &lt;code>service&lt;/code> reference (could also be replaced with &lt;code>url&lt;/code> that gives the location of the webhook) and &lt;code>caBundle&lt;/code> to allow validating its X.509 certificate, both under the &lt;code>clientConfig&lt;/code> stanza.&lt;/p>
&lt;p>Here is a short example of what our ValidatingWebhookConfiguration object looks like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admissionregistration.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ValidatingWebhookConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-validating-webhook-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">webhooks&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>validate-pod-interaction.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">sideEffects&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>None&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operations&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;CONNECT&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;pods/exec&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;pods/attach&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">failurePolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Fail&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">clientConfig&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">service&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># reference to kube-exec-controller service deployed inside the K8s cluster&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-exec-controller&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/admit-pod-interaction&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">caBundle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;{{VALUE}}&amp;#34;&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># PEM encoded CA bundle to validate kube-exec-controller&amp;#39;s certificate&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">admissionReviewVersions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-label-the-target-pod-with-potentially-mutated-containers">2. Label the target Pod with potentially mutated containers&lt;/h3>
&lt;p>Once a request of &lt;code>kubectl exec&lt;/code> comes in, &lt;em>kube-exec-controller&lt;/em> makes an internal note to label the associated Pod. The added labels mean that we can not only query all the affected Pods, but also enable the security mechanism to retrieve previously identified Pods, in case the controller service itself gets restarted.&lt;/p>
&lt;p>The admission control process cannot directly modify the targeted in its admission response. This is because the &lt;code>pods/exec&lt;/code> request is against a subresource of the Pod API, and the API kind for that subresource is &lt;code>PodExecOptions&lt;/code>. As a result, there is a separate process in &lt;em>kube-exec-controller&lt;/em> that patches the labels asynchronously. The admission control always permits the &lt;code>exec&lt;/code> request, then acts as a client of the K8s API to label the target Pod and to log related events. Developers can check whether their Pods are affected or not using &lt;code>kubectl&lt;/code> or similar tools. For example:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pod --show-labels
NAME READY STATUS RESTARTS AGE LABELS
test-pod 1/1 Running 0 2s box.com/podInitialInteractionTimestamp=1632524400,box.com/podInteractorUsername=username-1,box.com/podTTLDuration=1h0m0s
$ kubectl describe pod test-pod
...
Events:
Type Reason Age    From                            Message
----       ------       ----   ----                            -------
Warning PodInteraction 5s admission-controller-service Pod was interacted with 'kubectl exec' command by user 'username-1' initially at time 2021-09-24 16:00:00 -0800 PST
Warning PodInteraction 5s admission-controller-service Pod will be evicted at time 2021-09-24 17:00:00 -0800 PST (in about 1h0m0s).
&lt;/code>&lt;/pre>&lt;h3 id="3-evict-the-target-pod-after-a-predefined-period">3. Evict the target Pod after a predefined period&lt;/h3>
&lt;p>As you can see in the above event messages, the affected Pod is not evicted immediately. At times, developers might have to get into their running containers necessarily for debugging some live issues. Therefore, we define a time to live (TTL) of affected Pods based on the environment of clusters they are running. In particular, we allow a longer time in our dev clusters as it is more common to run &lt;code>kubectl exec&lt;/code> or other interactive commands for active development.&lt;/p>
&lt;p>For our production clusters, we specify a lower time limit so as to avoid the impacted Pods serving traffic abidingly. The &lt;em>kube-exec-controller&lt;/em> internally sets and tracks a timer for each Pod that matches the associated TTL. Once the timer is up, the controller evicts that Pod using K8s API. The eviction (rather than deletion) is to ensure service availability, since the cluster respects any configured &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget&lt;/a> (PDB). Let's say if a user has defined &lt;em>x&lt;/em> number of Pods as critical in their PDB, the eviction (as requested by &lt;em>kube-exec-controller&lt;/em>) does not continue when the target workload has fewer than &lt;em>x&lt;/em> Pods running.&lt;/p>
&lt;p>Here comes a sequence diagram of the entire workflow mentioned above:&lt;/p>
&lt;!-- Mermaid Live Editor link - https://mermaid-js.github.io/mermaid-live-editor/edit/#pako:eNp9kjFPAzEMhf-KlalIbWd0QpUQdGJB3JrFTUyJmjhHzncFof53nGtpqYTYEuu958-Wv4zLnkxjenofiB09BtwWTJbRSS6QCLCHu01ZPdJIMXdUYNZTGYOjRd4zlRvLHRYJLnTIArvbtozV83TbAnZhUcVUrkXo04OU2I6uKu99Cn0fMsNDZik5Rm3SHntYTrRYrabUBl4GBmt2w4acRKAPcrBcLq0Bl1NC9pYnoRouHZopX9RX9aotddJeADaf4DDGwFuQN4IRY_Ao9bunzVvOO13COeYCcR9j3k-OCQDP9KfgC8TJsFbZIHSxnGljzp1lgKs2v9HXugMBwe2WPHTZ94CvottB6Ap5eg2s9cBaUnrLVEP_Yp5ynrOf3fxPV2V1lBOhmZtEJWHweiFfldQa1SWyptGnAuAQxRrLB5UOna6P1j7o4ZhGykBzg4Pk9pPdz_-oOR3ZsXj4BjrP5rU-->
&lt;p>&lt;img src="https://kubernetes.io/images/sequence_diagram.svg" alt="Sequence Diagram">&lt;/p>
&lt;h2 id="a-new-kubectl-plugin-for-better-user-experience">A new kubectl plugin for better user experience&lt;/h2>
&lt;p>Our admission controller component works great for solving the container drift issue we had on the platform. It is also able to submit all related Events to the target Pod that has been affected. However, K8s clusters don't retain Events very long (the default retention period is one hour). We need to provide other ways for developers to get their Pod interaction activity. A &lt;a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">kubectl plugin&lt;/a> is a perfect choice for us to expose this information. We named our plugin &lt;code>kubectl pi&lt;/code> (short for &lt;code>pod-interaction&lt;/code>) and provide two subcommands: &lt;code>get&lt;/code> and &lt;code>extend&lt;/code>.&lt;/p>
&lt;p>When the &lt;code>get&lt;/code> subcommand is called, the plugin checks the metadata attached by our admission controller and transfers it to human-readable information. Here is an example output from running &lt;code>kubectl pi get&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ kubectl pi get test-pod
POD-NAME INTERACTOR POD-TTL EXTENSION EXTENSION-REQUESTER EVICTION-TIME
test-pod  username-1  1h0m0s   /          /                    2021-09-24 17:00:00 -0800 PST
&lt;/code>&lt;/pre>&lt;p>The plugin can also be used to extend the TTL for a Pod that is marked for future eviction. This is useful in case developers need extra time to debug ongoing issues. To achieve this, a developer uses the &lt;code>kubectl pi extend&lt;/code> subcommand, where the plugin patches the relevant &lt;em>annotations&lt;/em> for the given Pod. These &lt;em>annotations&lt;/em> include the duration and username who made the extension request for transparency (displayed in the table returned from the &lt;code>kubectl pi get&lt;/code> command).&lt;/p>
&lt;p>Correspondingly, there is another webhook defined in &lt;em>kube-exec-controller&lt;/em> which admits valid annotation updates. Once admitted, those updates reset the eviction timer of the target Pod as requested. An example of requesting the extension from the developer side would be:&lt;/p>
&lt;pre>&lt;code>$ kubectl pi extend test-pod --duration=30m
Successfully extended the termination time of pod/test-pod with a duration=30m
 
$ kubectl pi get test-pod
POD-NAME  INTERACTOR  POD-TTL  EXTENSION  EXTENSION-REQUESTER  EVICTION-TIME
test-pod  username-1  1h0m0s   30m        username-2           2021-09-24 17:30:00 -0800 PST
&lt;/code>&lt;/pre>&lt;h2 id="future-improvement">Future improvement&lt;/h2>
&lt;p>Although our admission controller service works great in handling interactive requests to a Pod, it could as well evict the Pod while the actual commands are no-op in these requests. For instance, developers sometimes run &lt;code>kubectl exec&lt;/code> merely to check their service logs stored on hosts. Nevertheless, the target Pods would still get bounced despite the state of their containers not changing at all. One of the improvements here could be adding the ability to distinguish the commands that are passed to the interactive requests, so that no-op commands should not always force a Pod eviction. However, this becomes challenging when developers get a shell to a running container and execute commands inside the shell, since they will no longer be visible to our admission controller service.&lt;/p>
&lt;p>Another item worth pointing out here is the choice of using K8s &lt;em>labels&lt;/em> and &lt;em>annotations&lt;/em>. In our design, we decided to have all immutable metadata attached as &lt;em>labels&lt;/em> for better enforcing the immutability in our admission control. Yet some of these metadata could fit better as &lt;em>annotations&lt;/em>. For instance, we had a label with the key &lt;code>box.com/podInitialInteractionTimestamp&lt;/code> used to list all affected Pods in &lt;em>kube-exec-controller&lt;/em> code, although its value would be unlikely to query for. As a more ideal design in the K8s world, a single &lt;em>label&lt;/em> could be preferable in our case for identification with other metadata applied as &lt;em>annotations&lt;/em> instead.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>With the power of admission controllers, we are able to secure our K8s clusters by detecting potentially mutated containers at runtime, and evicting their Pods without affecting service availability. We also utilize kubectl plugins to provide flexibility of the eviction time and hence, bringing a better and more self-independent experience to service owners. We are proud to announce that we have open-sourced the whole project for the community to leverage in their own K8s clusters. Any contribution is more than welcomed and appreciated. You can find this project hosted on GitHub at &lt;a href="https://github.com/box/kube-exec-controller">https://github.com/box/kube-exec-controller&lt;/a>&lt;/p>
&lt;p>&lt;em>Special thanks to Ayush Sobti and Ethan Goldblum for their technical guidance on this project.&lt;/em>&lt;/p></description></item><item><title>Blog: What's new in Security Profiles Operator v0.4.0</title><link>https://kubernetes.io/blog/2021/12/17/security-profiles-operator/</link><pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/17/security-profiles-operator/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jakub Hrozek, Juan Antonio Osorio, Paulo Gomes, Sascha Grunert&lt;/p>
&lt;hr>
&lt;p>The &lt;a href="https://sigs.k8s.io/security-profiles-operator">Security Profiles Operator (SPO)&lt;/a>
is an out-of-tree Kubernetes enhancement to make the management of
&lt;a href="https://en.wikipedia.org/wiki/Seccomp">seccomp&lt;/a>,
&lt;a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux">SELinux&lt;/a> and
&lt;a href="https://en.wikipedia.org/wiki/AppArmor">AppArmor&lt;/a> profiles easier and more
convenient. We're happy to announce that we recently &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.4.0">released
v0.4.0&lt;/a>
of the operator, which contains a ton of new features, fixes and usability
improvements.&lt;/p>
&lt;h2 id="what-s-new">What's new&lt;/h2>
&lt;p>It has been a while since the last
&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.3.0">v0.3.0&lt;/a>
release of the operator. We added new features, fine-tuned existing ones and
reworked our documentation in 290 commits over the past half year.&lt;/p>
&lt;p>One of the highlights is that we're now able to record seccomp and SELinux
profiles using the operators &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#log-enricher-based-recording">log enricher&lt;/a>.
This allows us to reduce the dependencies required for profile recording to have
&lt;a href="https://linux.die.net/man/8/auditd">auditd&lt;/a> or
&lt;a href="https://en.wikipedia.org/wiki/Syslog">syslog&lt;/a> (as fallback) running on the
nodes. All profile recordings in the operator work in the same way by using the
&lt;code>ProfileRecording&lt;/code> CRD as well as their corresponding &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels">label
selectors&lt;/a>. The log
enricher itself can be also used to gather meaningful insights about seccomp and
SELinux messages of a node. Checkout the &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-the-log-enricher">official
documentation&lt;/a>
to learn more about it.&lt;/p>
&lt;h3 id="seccomp-related-improvements">seccomp related improvements&lt;/h3>
&lt;p>Beside the log enricher based recording we now offer an alternative to record
seccomp profiles by utilizing &lt;a href="https://ebpf.io">ebpf&lt;/a>. This optional feature can
be enabled by setting &lt;code>enableBpfRecorder&lt;/code> to &lt;code>true&lt;/code>. This results in running a
dedicated container, which ships a custom bpf module on every node to collect
the syscalls for containers. It even supports older Kernel versions which do not
expose the &lt;a href="https://www.kernel.org/doc/html/latest/bpf/btf.html">BPF Type Format (BTF)&lt;/a> per
default as well as the &lt;code>amd64&lt;/code> and &lt;code>arm64&lt;/code> architectures. Checkout
&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#ebpf-based-recording">our documentation&lt;/a>
to see it in action. By the way, we now add the seccomp profile architecture of
the recorder host to the recorded profile as well.&lt;/p>
&lt;p>We also graduated the seccomp profile API from &lt;code>v1alpha1&lt;/code> to &lt;code>v1beta1&lt;/code>. This
aligns with our overall goal to stabilize the CRD APIs over time. The only thing
which has changed is that the seccomp profile type &lt;code>Architectures&lt;/code> now points to
&lt;code>[]Arch&lt;/code> instead of &lt;code>[]*Arch&lt;/code>.&lt;/p>
&lt;h3 id="selinux-enhancements">SELinux enhancements&lt;/h3>
&lt;p>Managing SELinux policies (an equivalent to using &lt;code>semodule&lt;/code> that
you would normally call on a single server) is not done by SPO
itself, but by another container called selinuxd to provide better
isolation. This release switched to using selinuxd containers from
a personal repository to images located under &lt;a href="https://quay.io/organization/security-profiles-operator">our team's quay.io
repository&lt;/a>.
The selinuxd repository has moved as well to &lt;a href="https://github.com/containers/selinuxd">the containers GitHub
organization&lt;/a>.&lt;/p>
&lt;p>Please note that selinuxd links dynamically to &lt;code>libsemanage&lt;/code> and mounts the
SELinux directories from the nodes, which means that the selinuxd container
must be running the same distribution as the cluster nodes. SPO defaults
to using CentOS-8 based containers, but we also build Fedora based ones.
If you are using another distribution and would like us to add support for
it, please file &lt;a href="https://github.com/containers/selinuxd/issues">an issue against selinuxd&lt;/a>.&lt;/p>
&lt;h4 id="profile-recording">Profile Recording&lt;/h4>
&lt;p>This release adds support for recording of SELinux profiles.
The recording itself is managed via an instance of a &lt;code>ProfileRecording&lt;/code> Custom
Resource as seen in an
&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/profilerecording-selinux-logs.yaml">example&lt;/a>
in our repository. From the user's point of view it works pretty much the same
as recording of seccomp profiles.&lt;/p>
&lt;p>Under the hood, to know what the workload is doing SPO installs a special
permissive policy called &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/profiles/selinuxrecording.cil">selinuxrecording&lt;/a>
on startup which allows everything and logs all AVCs to &lt;code>audit.log&lt;/code>.
These AVC messages are scraped by the log enricher component and when
the recorded workload exits, the policy is created.&lt;/p>
&lt;h4 id="selinuxprofile-crd-graduation">&lt;code>SELinuxProfile&lt;/code> CRD graduation&lt;/h4>
&lt;p>An &lt;code>v1alpha2&lt;/code> version of the &lt;code>SelinuxProfile&lt;/code> object has been introduced. This
removes the raw Common Intermediate Language (CIL) from the object itself and
instead adds a simple policy language to ease the writing and parsing
experience.&lt;/p>
&lt;p>Alongside, a &lt;code>RawSelinuxProfile&lt;/code> object was also introduced. This contains a
wrapped and raw representation of the policy. This was intended for folks to be
able to take their existing policies into use as soon as possible. However, on
validations are done here.&lt;/p>
&lt;h3 id="apparmor-support">AppArmor support&lt;/h3>
&lt;p>This version introduces the initial support for AppArmor, allowing users to load and
unload AppArmor profiles into cluster nodes by using the new &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/crds/apparmorprofile.yaml">AppArmorProfile&lt;/a> CRD.&lt;/p>
&lt;p>To enable AppArmor support use the &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/config.yaml#L10">enableAppArmor feature gate&lt;/a> switch of your SPO configuration.
Then use our &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/apparmorprofile.yaml">apparmor example&lt;/a> to deploy your first profile across your cluster.&lt;/p>
&lt;h3 id="metrics">Metrics&lt;/h3>
&lt;p>The operator now exposes metrics, which are described in detail in
our new &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-metrics">metrics documentation&lt;/a>.
We decided to secure the metrics retrieval process by using
&lt;a href="https://github.com/brancz/kube-rbac-proxy">kube-rbac-proxy&lt;/a>, while we ship an
additional &lt;code>spo-metrics-client&lt;/code> cluster role (and binding) to retrieve the
metrics from within the cluster. If you're using
&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift">OpenShift&lt;/a>,
then we provide an out of the box working
&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#automatic-servicemonitor-deployment">&lt;code>ServiceMonitor&lt;/code>&lt;/a>
to access the metrics.&lt;/p>
&lt;h4 id="debuggability-and-robustness">Debuggability and robustness&lt;/h4>
&lt;p>Beside all those new features, we decided to restructure parts of the Security
Profiles Operator internally to make it better to debug and more robust. For
example, we now maintain an internal &lt;a href="https://grpc.io">gRPC&lt;/a> API to communicate
within the operator across different features. We also improved the performance
of the log enricher, which now caches results for faster retrieval of the log
data. The operator can be put into a more &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#set-logging-verbosity">verbose log mode&lt;/a>
by setting &lt;code>verbosity&lt;/code> from &lt;code>0&lt;/code> to &lt;code>1&lt;/code>.&lt;/p>
&lt;p>We also print the used &lt;code>libseccomp&lt;/code> and &lt;code>libbpf&lt;/code> versions on startup, as well as
expose CPU and memory profiling endpoints for each container via the
&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#enable-cpu-and-memory-profiling">&lt;code>enableProfiling&lt;/code> option&lt;/a>.
Dedicated liveness and startup probes inside of the operator daemon will now
additionally improve the life cycle of the operator.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Thank you for reading this update. We're looking forward to future enhancements
of the operator and would love to get your feedback about the latest release.
Feel free to reach out to us via the Kubernetes slack
&lt;a href="https://kubernetes.slack.com/messages/security-profiles-operator">#security-profiles-operator&lt;/a>
for any feedback or question.&lt;/p></description></item><item><title>Blog: Kubernetes 1.23: StatefulSet PVC Auto-Deletion (alpha)</title><link>https://kubernetes.io/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Matthew Cary (Google)&lt;/p>
&lt;p>Kubernetes v1.23 introduced a new, alpha-level policy for
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets&lt;/a> that controls the lifetime of
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaims&lt;/a> (PVCs) generated from the
StatefulSet spec template for cases when they should be deleted automatically when the StatefulSet
is deleted or pods in the StatefulSet are scaled down.&lt;/p>
&lt;h2 id="what-problem-does-this-solve">What problem does this solve?&lt;/h2>
&lt;p>A StatefulSet spec can include Pod and PVC templates. When a replica is first created, the
Kubernetes control plane creates a PVC for that replica if one does not already exist. The behavior
before Kubernetes v1.23 was that the control plane never cleaned up the PVCs created for
StatefulSets - this was left up to the cluster administrator, or to some add-on automation that
you’d have to find, check suitability, and deploy. The common pattern for managing PVCs, either
manually or through tools such as Helm, is that the PVCs are tracked by the tool that manages them,
with explicit lifecycle. Workflows that use StatefulSets must determine on their own what PVCs are
created by a StatefulSet and what their lifecycle should be.&lt;/p>
&lt;p>Before this new feature, when a StatefulSet-managed replica disappears, either because the
StatefulSet is reducing its replica count, or because its StatefulSet is deleted, the PVC and its
backing volume remains and must be manually deleted. While this behavior is appropriate when the
data is critical, in many cases the persistent data in these PVCs is either temporary, or can be
reconstructed from another source. In those cases, PVCs and their backing volumes remaining after
their StatefulSet or replicas have been deleted are not necessary, incur cost, and require manual
cleanup.&lt;/p>
&lt;h2 id="the-new-statefulset-pvc-retention-policy">The new StatefulSet PVC retention policy&lt;/h2>
&lt;p>If you enable the alpha feature, a StatefulSet spec includes a PersistentVolumeClaim retention
policy. This is used to control if and when PVCs created from a StatefulSet’s &lt;code>volumeClaimTemplate&lt;/code>
are deleted. This first iteration of the retention policy contains two situations where PVCs may be
deleted.&lt;/p>
&lt;p>The first situation is when the StatefulSet resource is deleted (which implies that all replicas are
also deleted). This is controlled by the &lt;code>whenDeleted&lt;/code> policy. The second situation, controlled by
&lt;code>whenScaled&lt;/code> is when the StatefulSet is scaled down, which removes some but not all of the replicas
in a StatefulSet. In both cases the policy can either be &lt;code>Retain&lt;/code>, where the corresponding PVCs are
not touched, or &lt;code>Delete&lt;/code>, which means that PVCs are deleted. The deletion is done with a normal
&lt;a href="https://kubernetes.io/docs/concepts/architecture/garbage-collection/">object deletion&lt;/a>, so that, for example, all
retention policies for the underlying PV are respected.&lt;/p>
&lt;p>This policy forms a matrix with four cases. I’ll walk through and give an example for each one.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>&lt;code>whenDeleted&lt;/code> and &lt;code>whenScaled&lt;/code> are both &lt;code>Retain&lt;/code>.&lt;/strong> This matches the existing behavior for
StatefulSets, where no PVCs are deleted. This is also the default retention policy. It’s
appropriate to use when data on StatefulSet volumes may be irreplaceable and should only be
deleted manually.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;code>whenDeleted&lt;/code> is &lt;code>Delete&lt;/code> and &lt;code>whenScaled&lt;/code> is &lt;code>Retain&lt;/code>.&lt;/strong> In this case, PVCs are deleted only when
the entire StatefulSet is deleted. If the StatefulSet is scaled down, PVCs are not touched,
meaning they are available to be reattached if a scale-up occurs with any data from the previous
replica. This might be used for a temporary StatefulSet, such as in a CI instance or ETL
pipeline, where the data on the StatefulSet is needed only during the lifetime of the
StatefulSet lifetime, but while the task is running the data is not easily reconstructible. Any
retained state is needed for any replicas that scale down and then up.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;code>whenDeleted&lt;/code> and &lt;code>whenScaled&lt;/code> are both &lt;code>Delete&lt;/code>.&lt;/strong> PVCs are deleted immediately when their
replica is no longer needed. Note this does not include when a Pod is deleted and a new version
rescheduled, for example when a node is drained and Pods need to migrate elsewhere. The PVC is
deleted only when the replica is no longer needed as signified by a scale-down or StatefulSet
deletion. This use case is for when data does not need to live beyond the life of its
replica. Perhaps the data is easily reconstructable and the cost savings of deleting unused PVCs
is more important than quick scale-up, or perhaps that when a new replica is created, any data
from a previous replica is not usable and must be reconstructed anyway.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;code>whenDeleted&lt;/code> is &lt;code>Retain&lt;/code> and &lt;code>whenScaled&lt;/code> is &lt;code>Delete&lt;/code>.&lt;/strong> This is similar to the previous case,
when there is little benefit to keeping PVCs for fast reuse during scale-up. An example of a
situation where you might use this is an Elasticsearch cluster. Typically you would scale that
workload up and down to match demand, whilst ensuring a minimum number of replicas (for example:
3). When scaling down, data is migrated away from removed replicas and there is no benefit to
retaining those PVCs. However, it can be useful to bring the entire Elasticsearch cluster down
temporarily for maintenance. If you need to take the Elasticsearch system offline, you can do
this by temporarily deleting the StatefulSet, and then bringing the Elasticsearch cluster back
by recreating the StatefulSet. The PVCs holding the Elasticsearch data will still exist and the
new replicas will automatically use them.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Visit the
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies">documentation&lt;/a> to
see all the details.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Enable the feature and try it out! Enable the &lt;code>StatefulSetAutoDeletePVC&lt;/code> feature gate on a cluster,
then create a StatefulSet using the new policy. Test it out and tell us what you think!&lt;/p>
&lt;p>I'm very curious to see if this owner reference mechanism works well in practice. For example, we
realized there is no mechanism in Kubernetes for knowing who set a reference, so it’s possible that
the StatefulSet controller may fight with custom controllers that set their own
references. Fortunately, maintaining the existing retention behavior does not involve any new owner
references, so default behavior will be compatible.&lt;/p>
&lt;p>Please tag any issues you report with the label &lt;code>sig/apps&lt;/code> and assign them to Matthew Cary
(&lt;a href="https://github.com/mattcary">@mattcary&lt;/a> at GitHub).&lt;/p>
&lt;p>Enjoy!&lt;/p></description></item><item><title>Blog: Kubernetes 1.23: Prevent PersistentVolume leaks when deleting out of order</title><link>https://kubernetes.io/blog/2021/12/15/kubernetes-1-23-prevent-persistentvolume-leaks-when-deleting-out-of-order/</link><pubDate>Wed, 15 Dec 2021 10:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2021/12/15/kubernetes-1-23-prevent-persistentvolume-leaks-when-deleting-out-of-order/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Deepak Kinni (VMware)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolume&lt;/a> (or PVs for short) are
associated with &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaim-policy">Reclaim Policy&lt;/a>.
The Reclaim Policy is used to determine the actions that need to be taken by the storage
backend on deletion of the PV.
Where the reclaim policy is &lt;code>Delete&lt;/code>, the expectation is that the storage backend
releases the storage resource that was allocated for the PV. In essence, the reclaim
policy needs to honored on PV deletion.&lt;/p>
&lt;p>With the recent Kubernetes v1.23 release, an alpha feature lets you configure your
cluster to behave that way and honor the configured reclaim policy.&lt;/p>
&lt;h2 id="how-did-reclaim-work-in-previous-kubernetes-releases">How did reclaim work in previous Kubernetes releases?&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#Introduction">PersistentVolumeClaim&lt;/a> (or PVC for short) is
a request for storage by a user. A PV and PVC are considered &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#Binding">Bound&lt;/a>
if there is a newly created PV or a matching PV is found. The PVs themselves are
backed by a volume allocated by the storage backend.&lt;/p>
&lt;p>Normally, if the volume is to be deleted, then the expectation is to delete the
PVC for a bound PV-PVC pair. However, there are no restrictions to delete a PV
prior to deleting a PVC.&lt;/p>
&lt;p>First, I'll demonstrate the behavior for clusters that are running an older version of Kubernetes.&lt;/p>
&lt;h4 id="retrieve-an-pvc-that-is-bound-to-a-pv">Retrieve an PVC that is bound to a PV&lt;/h4>
&lt;p>Retrieve an existing PVC &lt;code>example-vanilla-block-pvc&lt;/code>&lt;/p>
&lt;pre>&lt;code>kubectl get pvc example-vanilla-block-pvc
&lt;/code>&lt;/pre>&lt;p>The following output shows the PVC and it's &lt;code>Bound&lt;/code> PV, the PV is shown under the &lt;code>VOLUME&lt;/code> column:&lt;/p>
&lt;pre>&lt;code>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
example-vanilla-block-pvc Bound pvc-6791fdd4-5fad-438e-a7fb-16410363e3da 5Gi RWO example-vanilla-block-sc 19s
&lt;/code>&lt;/pre>&lt;h4 id="delete-pv">Delete PV&lt;/h4>
&lt;p>When I try to delete a bound PV, the cluster blocks and the &lt;code>kubectl&lt;/code> tool does
not return back control to the shell; for example:&lt;/p>
&lt;pre>&lt;code>kubectl delete pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da
&lt;/code>&lt;/pre>&lt;pre>&lt;code>persistentvolume &amp;quot;pvc-6791fdd4-5fad-438e-a7fb-16410363e3da&amp;quot; deleted
^C
&lt;/code>&lt;/pre>&lt;p>Retrieving the PV:&lt;/p>
&lt;pre>&lt;code>kubectl get pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da
&lt;/code>&lt;/pre>&lt;p>It can be observed that the PV is in &lt;code>Terminating&lt;/code> state&lt;/p>
&lt;pre>&lt;code>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE
pvc-6791fdd4-5fad-438e-a7fb-16410363e3da 5Gi RWO Delete Terminating default/example-vanilla-block-pvc example-vanilla-block-sc 2m23s
&lt;/code>&lt;/pre>&lt;h4 id="delete-pvc">Delete PVC&lt;/h4>
&lt;pre>&lt;code>kubectl delete pvc example-vanilla-block-pvc
&lt;/code>&lt;/pre>&lt;p>The following output is seen if the PVC gets successfully deleted:&lt;/p>
&lt;pre>&lt;code>persistentvolumeclaim &amp;quot;example-vanilla-block-pvc&amp;quot; deleted
&lt;/code>&lt;/pre>&lt;p>The PV object from the cluster also gets deleted. When attempting to retrieve the PV
it will be observed that the PV is no longer found:&lt;/p>
&lt;pre>&lt;code>kubectl get pv pvc-6791fdd4-5fad-438e-a7fb-16410363e3da
&lt;/code>&lt;/pre>&lt;pre>&lt;code>Error from server (NotFound): persistentvolumes &amp;quot;pvc-6791fdd4-5fad-438e-a7fb-16410363e3da&amp;quot; not found
&lt;/code>&lt;/pre>&lt;p>Although the PV is deleted the underlying storage resource is not deleted, and
needs to be removed manually.&lt;/p>
&lt;p>To sum it up, the reclaim policy associated with the Persistent Volume is currently
ignored under certain circumstance. For a &lt;code>Bound&lt;/code> PV-PVC pair the ordering of PV-PVC
deletion determines whether the PV reclaim policy is honored. The reclaim policy
is honored if the PVC is deleted first, however, if the PV is deleted prior to
deleting the PVC then the reclaim policy is not exercised. As a result of this behavior,
the associated storage asset in the external infrastructure is not removed.&lt;/p>
&lt;h2 id="pv-reclaim-policy-with-kubernetes-v1-23">PV reclaim policy with Kubernetes v1.23&lt;/h2>
&lt;p>The new behavior ensures that the underlying storage object is deleted from the backend when users attempt to delete a PV manually.&lt;/p>
&lt;h4 id="how-to-enable-new-behavior">How to enable new behavior?&lt;/h4>
&lt;p>To make use of the new behavior, you must have upgraded your cluster to the v1.23 release of Kubernetes.
You need to make sure that you are running the CSI &lt;a href="https://github.com/kubernetes-csi/external-provisioner">&lt;code>external-provisioner&lt;/code>&lt;/a> version &lt;code>4.0.0&lt;/code>, or later.
You must also enable the &lt;code>HonorPVReclaimPolicy&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a> for the
&lt;code>external-provisioner&lt;/code> and for the &lt;code>kube-controller-manager&lt;/code>.&lt;/p>
&lt;p>If you're not using a CSI driver to integrate with your storage backend, the fix isn't
available. The Kubernetes project doesn't have a current plan to fix the bug for in-tree
storage drivers: the future of those in-tree drivers is deprecation and migration to CSI.&lt;/p>
&lt;h4 id="how-does-it-work">How does it work?&lt;/h4>
&lt;p>The new behavior is achieved by adding a finalizer &lt;code>external-provisioner.volume.kubernetes.io/finalizer&lt;/code> on new and existing PVs, the finalizer is only removed after the storage from backend is deleted.&lt;/p>
&lt;p>An example of a PV with the finalizer, notice the new finalizer in the finalizers list&lt;/p>
&lt;pre>&lt;code>kubectl get pv pvc-a7b7e3ba-f837-45ba-b243-dec7d8aaed53 -o yaml
&lt;/code>&lt;/pre>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">pv.kubernetes.io/provisioned-by&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi.vsphere.vmware.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">creationTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2021-11-17T19:28:56Z&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">finalizers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- kubernetes.io/pv-protection&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- external-provisioner.volume.kubernetes.io/finalizer&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pvc-a7b7e3ba-f837-45ba-b243-dec7d8aaed53&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;194711&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">uid&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>087f14f2-4157-4e95-8a70-8294b039d30e&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">capacity&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claimRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-vanilla-block-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;194677&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">uid&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>a7b7e3ba-f837-45ba-b243-dec7d8aaed53&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi.vsphere.vmware.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsType&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ext4&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeAttributes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage.kubernetes.io/csiProvisionerIdentity&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1637110610497-8081&lt;/span>-csi.vsphere.vmware.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vSphere CNS Block Volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>2dacf297-803f-4ccc-afc7-3d3c3f02051e&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeReclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-vanilla-block-sc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">phase&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Bound&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The presence of the finalizer prevents the PV object from being removed from the
cluster. As stated previously, the finalizer is only removed from the PV object
after it is successfully deleted from the storage backend. To learn more about
finalizers, please refer to &lt;a href="https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/">Using Finalizers to Control Deletion&lt;/a>.&lt;/p>
&lt;h4 id="what-about-csi-migrated-volumes">What about CSI migrated volumes?&lt;/h4>
&lt;p>The fix is applicable to CSI migrated volumes as well. However, when the feature
&lt;code>HonorPVReclaimPolicy&lt;/code> is enabled on 1.23, and CSI Migration is disabled, the finalizer
is removed from the PV object if it exists.&lt;/p>
&lt;h3 id="some-caveats">Some caveats&lt;/h3>
&lt;ol>
&lt;li>The fix is applicable only to CSI volumes and migrated volumes. In-tree volumes will exhibit older behavior.&lt;/li>
&lt;li>The fix is introduced as an alpha feature in the &lt;a href="https://github.com/kubernetes-csi/external-provisioner">external-provisioner&lt;/a> under the feature gate &lt;code>HonorPVReclaimPolicy&lt;/code>. The feature is disabled by default, and needs to be enabled explicitly.&lt;/li>
&lt;/ol>
&lt;h3 id="references">References&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2644-honor-pv-reclaim-policy">KEP-2644&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-provisioner/issues/546">Volume leak issue&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="how-do-i-get-involved">How do I get involved?&lt;/h3>
&lt;p>The Kubernetes Slack channel &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and migration working group teams.&lt;/p>
&lt;p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution:&lt;/p>
&lt;ul>
&lt;li>Jan Šafránek (jsafrane)&lt;/li>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;li>Matthew Wong (wongma7)&lt;/li>
&lt;/ul>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Kubernetes 1.23: Kubernetes In-Tree to CSI Volume Migration Status Update</title><link>https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Jiawei Wang (Google)&lt;/p>
&lt;p>The Kubernetes in-tree storage plugin to &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI)&lt;/a> migration infrastructure has already been &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">beta&lt;/a> since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.&lt;/p>
&lt;p>Since then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for GA.
This article is intended to give a status update to the feature as well as changes between Kubernetes 1.17 and 1.23. In addition, I will also cover the future roadmap for the CSI migration feature GA for each storage plugin.&lt;/p>
&lt;h2 id="quick-recap-what-is-csi-migration-and-why-migrate">Quick recap: What is CSI Migration, and why migrate?&lt;/h2>
&lt;p>The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins.
Kubernetes support for the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#README">Container Storage Interface&lt;/a> has been
&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">generally available&lt;/a> since Kubernetes v1.13.
Support for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).&lt;/p>
&lt;p>As more CSI Drivers were created and became production ready, SIG Storage group wanted all Kubernetes users to benefit from the CSI model. However, we cannot break API compatibility with the existing storage API types. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.&lt;/p>
&lt;p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as &lt;code>kubernetes.io/gce-pd&lt;/code> or &lt;code>kubernetes.io/aws-ebs&lt;/code> with a corresponding &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI driver&lt;/a> from the storage backend.
If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. Existing &lt;code>StorageClass&lt;/code>, &lt;code>PersistentVolume&lt;/code> and &lt;code>PersistentVolumeClaim&lt;/code> objects should continue to work.
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have.
However, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.&lt;/p>
&lt;p>For example, suppose you are a &lt;code>kubernetes.io/gce-pd&lt;/code> user, after CSI migration, you can still use &lt;code>kubernetes.io/gce-pd&lt;/code> to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing API/Interface will still function correctly. However, the underlying function calls are all going through the &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI driver&lt;/a> instead of the in-tree Kubernetes function.&lt;/p>
&lt;p>This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.&lt;/p>
&lt;h2 id="what-has-been-changed-and-what-s-new">What has been changed, and what's new?&lt;/h2>
&lt;p>Building on the work done in Kubernetes v1.17 and earlier, the releases since then have
made a series of changes:&lt;/p>
&lt;h3 id="new-feature-gates">New feature gates&lt;/h3>
&lt;p>The Kubernetes v1.21 release deprecated the &lt;code>CSIMigration{provider}Complete&lt;/code> feature flags, and stopped honoring them. In their place came new feature flags named &lt;code>InTreePlugin{vendor}Unregister&lt;/code>, that replace the old feature flag and retain all the functionality that &lt;code>CSIMigration{provider}Complete&lt;/code> provided.&lt;/p>
&lt;p>&lt;code>CSIMigration{provider}Complete&lt;/code> was introduced before as a supplementary feature gate once CSI migration is enabled on all of the nodes. This flag unregisters the in-tree storage plugin you specify with the &lt;code>{provider}&lt;/code> part of the flag name.&lt;/p>
&lt;p>When you enable that feature gate, then instead of using the in-tree driver code, your cluster directly selects and uses the relevant CSI driver. This happens without any check for whether CSI migration is enabled on the node, or whether you have in fact deployed that CSI driver.&lt;/p>
&lt;p>While this feature gate is a great helper, SIG Storage (and, I'm sure, lots of cluster operators) also wanted a feature gate that lets you disable an in-tree storage plugin, even without also enabling CSI migration. For example, you might want to disable the EBS storage plugin on a GCE cluster, because EBS volumes are specific to a different vendor's cloud (AWS).&lt;/p>
&lt;p>To make this possible, Kubernetes v1.21 introduced a new feature flag set: &lt;code>InTreePlugin{vendor}Unregister&lt;/code>.&lt;/p>
&lt;p>&lt;code>InTreePlugin{vendor}Unregister&lt;/code> is a standalone feature gate that can be enabled and disabled independently from CSI Migration. When enabled, the component will not register the specific in-tree storage plugin to the supported list. If the cluster operator only enables this flag, end users will get an error from PVC saying it cannot find the plugin when the plugin is used. The cluster operator may want to enable this regardless of CSI Migration if they do not want to support the legacy in-tree APIs and only support CSI moving forward.&lt;/p>
&lt;h3 id="observability">Observability&lt;/h3>
&lt;p>Kubernetes v1.21 introduced &lt;a href="https://github.com/kubernetes/kubernetes/issues/98279">metrics&lt;/a> for tracking CSI migration.
You can use these metrics to observe how your cluster is using storage services and whether access to that storage is using the legacy in-tree driver or its CSI-based replacement.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Components&lt;/th>
&lt;th>Metrics&lt;/th>
&lt;th>Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kube-Controller-Manager&lt;/td>
&lt;td>storage_operation_duration_seconds&lt;/td>
&lt;td>A new label &lt;code>migrated&lt;/code> is added to the metric to indicate whether this storage operation is a CSI migration operation(string value &lt;code>true&lt;/code> for enabled and &lt;code>false&lt;/code> for not enabled).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kubelet&lt;/td>
&lt;td>csi_operations_seconds&lt;/td>
&lt;td>The new metric exposes labels including &lt;code>driver_name&lt;/code>, &lt;code>method_name&lt;/code>, &lt;code>grpc_status_code&lt;/code> and &lt;code>migrated&lt;/code>. The meaning of these labels is identical to &lt;code>csi_sidecar_operations_seconds&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSI Sidecars(provisioner, attacher, resizer)&lt;/td>
&lt;td>csi_sidecar_operations_seconds&lt;/td>
&lt;td>A new label &lt;code>migrated&lt;/code> is added to the metric to indicate whether this storage operation is a CSI migration operation(string value &lt;code>true&lt;/code> for enabled and &lt;code>false&lt;/code> for not enabled).&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="bug-fixes-and-feature-improvement">Bug fixes and feature improvement&lt;/h3>
&lt;p>We have fixed numerous bugs like dangling attachment, garbage collection, incorrect topology label through the help of our beta testers.&lt;/p>
&lt;h3 id="cloud-provider-cluster-lifecycle-collaboration">Cloud Provider &amp;amp;&amp;amp; Cluster Lifecycle Collaboration&lt;/h3>
&lt;p>SIG Storage has been working closely with SIG Cloud Provider and SIG Cluster Lifecycle on the rollout of CSI migration.&lt;/p>
&lt;p>If you are a user of a managed Kubernetes service, check with your provider if anything needs to be done. In many cases, the provider will manage the migration and no additional work is required.&lt;/p>
&lt;p>If you use a distribution of Kubernetes, check its official documentation for information about support for this feature. For the CSI Migration feature graduation to GA, SIG Storage and SIG Cluster Lifecycle are collaborating towards making the migration mechanisms available in tooling (such as kubeadm) as soon as they're available in Kubernetes itself.&lt;/p>
&lt;h2 id="timeline-and-status">What is the timeline / status?&lt;/h2>
&lt;p>The current and targeted releases for each individual driver is shown in the table below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Driver&lt;/th>
&lt;th>Alpha&lt;/th>
&lt;th>Beta (in-tree deprecated)&lt;/th>
&lt;th>Beta (on-by-default)&lt;/th>
&lt;th>GA&lt;/th>
&lt;th>Target &amp;quot;in-tree plugin&amp;quot; removal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AWS EBS&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GCE PD&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenStack Cinder&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure Disk&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure File&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vSphere&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ceph RBD&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Portworx&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The following storage drivers will not have CSI migration support. The ScaleIO driver was already removed; the others are deprecated and will be removed from core Kubernetes.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Driver&lt;/th>
&lt;th>Deprecated&lt;/th>
&lt;th>Code Removal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ScaleIO&lt;/td>
&lt;td>1.16&lt;/td>
&lt;td>1.22&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Flocker&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Quobyte&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>StorageOS&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="what-s-next">What's next?&lt;/h2>
&lt;p>With more CSI drivers graduating to GA, we hope to soon mark the overall CSI Migration feature as GA. We are expecting cloud provider in-tree storage plugins code removal to happen by Kubernetes v1.26 and v1.27.&lt;/p>
&lt;h2 id="what-should-i-do-as-a-user">What should I do as a user?&lt;/h2>
&lt;p>Note that all new features for the Kubernetes storage system (such as volume snapshotting) will only be added to the CSI interface. Therefore, if you are starting up a new cluster, creating stateful applications for the first time, or require these new features we recommend using CSI drivers natively (instead of the in-tree volume plugin API). Follow the &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">updated user guides for CSI drivers&lt;/a> and use the new CSI APIs.&lt;/p>
&lt;p>However, if you choose to roll a cluster forward or continue using specifications with the legacy volume APIs, CSI Migration will ensure we continue to support those deployments with the new CSI drivers. However, if you want to leverage new features like snapshot, it will require a manual migration to re-import an existing intree PV as a CSI PV.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>The Kubernetes Slack channel &lt;a href="https://kubernetes.slack.com/messages/csi-migration">#csi-migration&lt;/a> along with any of the standard &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and migration working group teams.&lt;/p>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:&lt;/p>
&lt;ul>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;li>Jan Šafránek (jsafrane)&lt;/li>
&lt;li>Hemant Kumar (gnufied)&lt;/li>
&lt;/ul>
&lt;p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:&lt;/p>
&lt;ul>
&lt;li>Andy Zhang (andyzhangz)&lt;/li>
&lt;li>Divyen Patel (divyenpatel)&lt;/li>
&lt;li>Deep Debroy (ddebroy)&lt;/li>
&lt;li>Humble Devassy Chirammal (humblec)&lt;/li>
&lt;li>Jing Xu (jingxu97)&lt;/li>
&lt;li>Jordan Liggitt (liggitt)&lt;/li>
&lt;li>Matthew Cary (mattcary)&lt;/li>
&lt;li>Matthew Wong (wongma7)&lt;/li>
&lt;li>Neha Arora (nearora-msft)&lt;/li>
&lt;li>Oksana Naumov (trierra)&lt;/li>
&lt;li>Saad Ali (saad-ali)&lt;/li>
&lt;li>Tim Bannister (sftim)&lt;/li>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;/ul>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Kubernetes 1.23: Pod Security Graduates to Beta</title><link>https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/</link><pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jim Angel (Google), Lachlan Evenson (Microsoft)&lt;/p>
&lt;p>With the release of Kubernetes v1.23, &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security admission&lt;/a> has now entered beta. Pod Security is a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">built-in&lt;/a> admission controller that evaluates pod specifications against a predefined set of &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> and determines whether to &lt;code>admit&lt;/code> or &lt;code>deny&lt;/code> the pod from running.&lt;/p>
&lt;p>Pod Security is the successor to &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/">PodSecurityPolicy&lt;/a> which was deprecated in the v1.21 release, and will be removed in Kubernetes v1.25. In this article, we cover the key concepts of Pod Security along with how to use it. We hope that cluster administrators and developers alike will use this new mechanism to enforce secure defaults for their workloads.&lt;/p>
&lt;h2 id="why-pod-security">Why Pod Security&lt;/h2>
&lt;p>The overall aim of Pod Security is to let you isolate workloads. You can run a cluster that runs different workloads and, without adding extra third-party tooling, implement controls that require Pods for a workload to restrict their own privileges to a defined bounding set.&lt;/p>
&lt;p>Pod Security overcomes key shortcomings of Kubernetes' existing, but deprecated, PodSecurityPolicy (PSP) mechanism:&lt;/p>
&lt;ul>
&lt;li>Policy authorization model — challenging to deploy with controllers.&lt;/li>
&lt;li>Risks around switching — a lack of dry-run/audit capabilities made it hard to enable PodSecurityPolicy.&lt;/li>
&lt;li>Inconsistent and Unbounded API — the large configuration surface and evolving constraints led to a complex and confusing API.&lt;/li>
&lt;/ul>
&lt;p>The shortcomings of PSP made it very difficult to use which led the community to reevaluate whether or not a better implementation could achieve the same goals. One of those goals was to provide an out-of-the-box solution to apply security best practices. Pod Security ships with predefined Pod Security levels that a cluster administrator can configure to meet the desired security posture.&lt;/p>
&lt;p>It's important to note that Pod Security doesn't have complete feature parity with the deprecated PodSecurityPolicy. Specifically, it doesn't have the ability to mutate or change Kubernetes resources to auto-remediate a policy violation on behalf of the user. Additionally, it doesn't provide fine-grained control over each allowed field and value within a pod specification or any other Kubernetes resource that you may wish to evaluate. If you need more fine-grained policy control then take a look at these &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#faq">other&lt;/a> projects which support such use cases.&lt;/p>
&lt;p>Pod Security also adheres to Kubernetes best practices of declarative object management by denying resources that violate the policy. This requires resources to be updated in source repositories, and tooling to be updated prior to being deployed to Kubernetes.&lt;/p>
&lt;h2 id="how-does-pod-security-work">How Does Pod Security Work?&lt;/h2>
&lt;p>Pod Security is a built-in &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">admission controller&lt;/a> starting with Kubernetes v1.22, but can also be run as a standalone &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/#webhook">webhook&lt;/a>. Admission controllers function by intercepting requests in the Kubernetes API server prior to persistence to storage. They can either &lt;code>admit&lt;/code> or &lt;code>deny&lt;/code> a request. In the case of Pod Security, pod specifications will be evaluated against a configured policy in the form of a Pod Security Standard. This means that security sensitive fields in a pod specification will only be allowed to have &lt;a href="h/docs/concepts/security/pod-security-standards/#profile-details">specific&lt;/a> values.&lt;/p>
&lt;h2 id="configuring-pod-security">Configuring Pod Security&lt;/h2>
&lt;h3 id="pod-security-standards">Pod Security Standards&lt;/h3>
&lt;p>In order to use Pod Security we first need to understand &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>. These standards define three different policy levels that range from permissive to restrictive. These levels are as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;code>privileged&lt;/code> — open and unrestricted&lt;/li>
&lt;li>&lt;code>baseline&lt;/code> — Covers known privilege escalations while minimizing restrictions&lt;/li>
&lt;li>&lt;code>restricted&lt;/code> — Highly restricted, hardening against known and unknown privilege escalations. May cause compatibility issues&lt;/li>
&lt;/ul>
&lt;p>Each of these policy levels define which fields are restricted within a pod specification and the allowed values. Some of the fields restricted by these policies include:&lt;/p>
&lt;ul>
&lt;li>&lt;code>spec.securityContext.sysctls&lt;/code>&lt;/li>
&lt;li>&lt;code>spec.hostNetwork&lt;/code>&lt;/li>
&lt;li>&lt;code>spec.volumes[*].hostPath&lt;/code>&lt;/li>
&lt;li>&lt;code>spec.containers[*].securityContext.privileged&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Policy levels are applied via labels on Namespace resources, which allows for granular per-namespace policy selection. The AdmissionConfiguration in the API server can also be configured to set cluster-wide default levels and exemptions.&lt;/p>
&lt;h3 id="policy-modes">Policy modes&lt;/h3>
&lt;p>Policies are applied in a specific mode. Multiple modes (with different policy levels) can be set on the same namespace. Here is a list of modes:&lt;/p>
&lt;ul>
&lt;li>&lt;code>enforce&lt;/code> — Any Pods that violate the policy will be rejected&lt;/li>
&lt;li>&lt;code>audit&lt;/code> — Violations will be recorded as an annotation in the audit logs, but don't affect whether the pod is allowed.&lt;/li>
&lt;li>&lt;code>warn&lt;/code> — Violations will send a warning message back to the user, but don't affect whether the pod is allowed.&lt;/li>
&lt;/ul>
&lt;p>In addition to modes you can also pin the policy to a specific version (for example v1.22). Pinning to a specific version allows the behavior to remain consistent if the policy definition changes in future Kubernetes releases.&lt;/p>
&lt;h2 id="hands-on-demo">Hands on demo&lt;/h2>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">KinD&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/tools/">kubectl&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/get-docker/">Docker&lt;/a> or &lt;a href="https://podman.io/getting-started/installation">Podman&lt;/a> container runtime &amp;amp; CLI&lt;/li>
&lt;/ul>
&lt;h3 id="deploy-a-kind-cluster">Deploy a kind cluster&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kind create cluster --image kindest/node:v1.23.0
&lt;/code>&lt;/pre>&lt;/div>&lt;p>It might take a while to start and once it's started it might take a minute or so before the node becomes ready.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl cluster-info --context kind-kind
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Wait for the node STATUS to become ready.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>NAME STATUS ROLES AGE VERSION
kind-control-plane Ready control-plane,master 54m v1.23.0
&lt;/code>&lt;/pre>&lt;h3 id="confirm-pod-security-is-enabled">Confirm Pod Security is enabled&lt;/h3>
&lt;p>The best way to &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default">confirm the API's default enabled plugins&lt;/a> is to check the Kubernetes API container's help arguments.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl -n kube-system &lt;span style="color:#a2f">exec&lt;/span> kube-apiserver-kind-control-plane -it -- kube-apiserver -h | grep &lt;span style="color:#b44">&amp;#34;default enabled ones&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>...
--enable-admission-plugins strings
admission plugins that should be enabled in addition
to default enabled ones (NamespaceLifecycle, LimitRanger,
ServiceAccount, TaintNodesByCondition, PodSecurity, Priority,
DefaultTolerationSeconds, DefaultStorageClass,
StorageObjectInUseProtection, PersistentVolumeClaimResize,
RuntimeClass, CertificateApproval, CertificateSigning,
CertificateSubjectRestriction, DefaultIngressClass,
MutatingAdmissionWebhook, ValidatingAdmissionWebhook,
ResourceQuota).
...
&lt;/code>&lt;/pre>&lt;p>&lt;code>PodSecurity&lt;/code> is listed in the group of default enabled admission plugins.&lt;/p>
&lt;p>If using a cloud provider, or if you don't have access to the API server, the best way to check would be to run a quick end-to-end test:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create namespace verify-pod-security
kubectl label namespace verify-pod-security pod-security.kubernetes.io/enforce&lt;span style="color:#666">=&lt;/span>restricted
&lt;span style="color:#080;font-style:italic"># The following command does NOT create a workload (--dry-run=server)&lt;/span>
kubectl -n verify-pod-security run &lt;span style="color:#a2f">test&lt;/span> --dry-run&lt;span style="color:#666">=&lt;/span>server --image&lt;span style="color:#666">=&lt;/span>busybox --privileged
kubectl delete namespace verify-pod-security
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>Error from server (Forbidden): pods &amp;quot;test&amp;quot; is forbidden: violates PodSecurity &amp;quot;restricted:latest&amp;quot;: privileged (container &amp;quot;test&amp;quot; must not set securityContext.privileged=true), allowPrivilegeEscalation != false (container &amp;quot;test&amp;quot; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container &amp;quot;test&amp;quot; must set securityContext.capabilities.drop=[&amp;quot;ALL&amp;quot;]), runAsNonRoot != true (pod or container &amp;quot;test&amp;quot; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &amp;quot;test&amp;quot; must set securityContext.seccompProfile.type to &amp;quot;RuntimeDefault&amp;quot; or &amp;quot;Localhost&amp;quot;)
&lt;/code>&lt;/pre>&lt;h3 id="configure-pod-security">Configure Pod Security&lt;/h3>
&lt;p>Policies are applied to a namespace via labels. These labels are as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;code>pod-security.kubernetes.io/&amp;lt;MODE&amp;gt;: &amp;lt;LEVEL&amp;gt;&lt;/code> (required to enable pod security)&lt;/li>
&lt;li>&lt;code>pod-security.kubernetes.io/&amp;lt;MODE&amp;gt;-version: &amp;lt;VERSION&amp;gt;&lt;/code> (&lt;em>optional&lt;/em>, defaults to latest)&lt;/li>
&lt;/ul>
&lt;p>A specific version can be supplied for each enforcement mode. The version pins the policy to the version that was shipped as part of the Kubernetes release. Pinning to a specific Kubernetes version allows for deterministic policy behavior while allowing flexibility for future updates to Pod Security Standards. The possible &amp;lt;MODE(S)&amp;gt; are &lt;code>enforce&lt;/code>, &lt;code>audit&lt;/code> and &lt;code>warn&lt;/code>.&lt;/p>
&lt;h3 id="when-to-use-warn">When to use &lt;code>warn&lt;/code>?&lt;/h3>
&lt;p>The typical uses for &lt;code>warn&lt;/code> are to get ready for a future change where you want to enforce a different policy. The most two common cases would be:&lt;/p>
&lt;ul>
&lt;li>&lt;code>warn&lt;/code> at the same level but a different version (e.g. pin &lt;code>enforce&lt;/code> to &lt;em>restricted+v1.23&lt;/em> and &lt;code>warn&lt;/code> at &lt;em>restricted+latest&lt;/em>)&lt;/li>
&lt;li>&lt;code>warn&lt;/code> at a stricter level (e.g. &lt;code>enforce&lt;/code> baseline, &lt;code>warn&lt;/code> restricted)&lt;/li>
&lt;/ul>
&lt;p>It's not recommended to use &lt;code>warn&lt;/code> for the exact same level+version of the policy as &lt;code>enforce&lt;/code>. In the admission sequence, if &lt;code>enforce&lt;/code> fails, the entire sequence fails before evaluating the &lt;code>warn&lt;/code>.&lt;/p>
&lt;p>First, create a namespace called &lt;code>verify-pod-security&lt;/code> if not created earlier. For the demo, &lt;code>--overwrite&lt;/code> is used when labeling to allow repurposing a single namespace for multiple examples.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create namespace verify-pod-security
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="deploy-demo-workloads">Deploy demo workloads&lt;/h3>
&lt;p>Each workload represents a higher level of security that would not pass the profile that comes after it.&lt;/p>
&lt;p>For the following examples, use the &lt;code>busybox&lt;/code> container runs a &lt;code>sleep&lt;/code> command for 1 million seconds (≅11 days) or until deleted. Pod Security is not interested in which container image you chose, but rather the Pod level settings and their implications for security.&lt;/p>
&lt;h3 id="privileged-level-and-workload">Privileged level and workload&lt;/h3>
&lt;p>For the privileged pod, use the &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#privileged">privileged policy&lt;/a>. This allows the process inside a container to gain new processes (also known as &amp;quot;privilege escalation&amp;quot;) and can be dangerous if untrusted.&lt;/p>
&lt;p>First, let's apply a restricted Pod Security level for a test.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># enforces a &amp;#34;restricted&amp;#34; security policy and audits on restricted&lt;/span>
kubectl label --overwrite ns verify-pod-security &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/enforce&lt;span style="color:#666">=&lt;/span>restricted &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/audit&lt;span style="color:#666">=&lt;/span>restricted
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, try to deploy a privileged workload in the namespace.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n verify-pod-security apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-privileged
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> allowPrivilegeEscalation: true
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>Error from server (Forbidden): error when creating &amp;quot;STDIN&amp;quot;: pods &amp;quot;busybox-privileged&amp;quot; is forbidden: violates PodSecurity &amp;quot;restricted:latest&amp;quot;: allowPrivilegeEscalation != false (container &amp;quot;busybox&amp;quot; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container &amp;quot;busybox&amp;quot; must set securityContext.capabilities.drop=[&amp;quot;ALL&amp;quot;]), runAsNonRoot != true (pod or container &amp;quot;busybox&amp;quot; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &amp;quot;busybox&amp;quot; must set securityContext.seccompProfile.type to &amp;quot;RuntimeDefault&amp;quot; or &amp;quot;Localhost&amp;quot;)
&lt;/code>&lt;/pre>&lt;p>Now let's apply the privileged Pod Security level and try again.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># enforces a &amp;#34;privileged&amp;#34; security policy and warns / audits on baseline&lt;/span>
kubectl label --overwrite ns verify-pod-security &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/enforce&lt;span style="color:#666">=&lt;/span>privileged &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/warn&lt;span style="color:#666">=&lt;/span>baseline &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/audit&lt;span style="color:#666">=&lt;/span>baseline
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n verify-pod-security apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-privileged
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> allowPrivilegeEscalation: true
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>pod/busybox-privileged created
&lt;/code>&lt;/pre>&lt;p>We can run &lt;code>kubectl -n verify-pod-security get pods&lt;/code> to verify it is running. Clean up with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl -n verify-pod-security delete pod busybox-privileged
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="baseline-level-and-workload">Baseline level and workload&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline">baseline policy&lt;/a> demonstrates sensible defaults while preventing common container exploits.&lt;/p>
&lt;p>Let's revert back to a restricted Pod Security level for a quick test.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># enforces a &amp;#34;restricted&amp;#34; security policy and audits on restricted&lt;/span>
kubectl label --overwrite ns verify-pod-security &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/enforce&lt;span style="color:#666">=&lt;/span>restricted &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/audit&lt;span style="color:#666">=&lt;/span>restricted
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Apply the workload.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n verify-pod-security apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-baseline
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> allowPrivilegeEscalation: false
&lt;/span>&lt;span style="color:#b44"> capabilities:
&lt;/span>&lt;span style="color:#b44"> add:
&lt;/span>&lt;span style="color:#b44"> - NET_BIND_SERVICE
&lt;/span>&lt;span style="color:#b44"> - CHOWN
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>Error from server (Forbidden): error when creating &amp;quot;STDIN&amp;quot;: pods &amp;quot;busybox-baseline&amp;quot; is forbidden: violates PodSecurity &amp;quot;restricted:latest&amp;quot;: unrestricted capabilities (container &amp;quot;busybox&amp;quot; must set securityContext.capabilities.drop=[&amp;quot;ALL&amp;quot;]; container &amp;quot;busybox&amp;quot; must not include &amp;quot;CHOWN&amp;quot; in securityContext.capabilities.add), runAsNonRoot != true (pod or container &amp;quot;busybox&amp;quot; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &amp;quot;busybox&amp;quot; must set securityContext.seccompProfile.type to &amp;quot;RuntimeDefault&amp;quot; or &amp;quot;Localhost&amp;quot;)
&lt;/code>&lt;/pre>&lt;p>Let's apply the baseline Pod Security level and try again.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># enforces a &amp;#34;baseline&amp;#34; security policy and warns / audits on restricted&lt;/span>
kubectl label --overwrite ns verify-pod-security &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/enforce&lt;span style="color:#666">=&lt;/span>baseline &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/warn&lt;span style="color:#666">=&lt;/span>restricted &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/audit&lt;span style="color:#666">=&lt;/span>restricted
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n verify-pod-security apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-baseline
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> allowPrivilegeEscalation: false
&lt;/span>&lt;span style="color:#b44"> capabilities:
&lt;/span>&lt;span style="color:#b44"> add:
&lt;/span>&lt;span style="color:#b44"> - NET_BIND_SERVICE
&lt;/span>&lt;span style="color:#b44"> - CHOWN
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to the following. Note that the warnings match the error message from the test above, but the pod is still successfully created.&lt;/p>
&lt;pre>&lt;code>Warning: would violate PodSecurity &amp;quot;restricted:latest&amp;quot;: unrestricted capabilities (container &amp;quot;busybox&amp;quot; must set securityContext.capabilities.drop=[&amp;quot;ALL&amp;quot;]; container &amp;quot;busybox&amp;quot; must not include &amp;quot;CHOWN&amp;quot; in securityContext.capabilities.add), runAsNonRoot != true (pod or container &amp;quot;busybox&amp;quot; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &amp;quot;busybox&amp;quot; must set securityContext.seccompProfile.type to &amp;quot;RuntimeDefault&amp;quot; or &amp;quot;Localhost&amp;quot;)
pod/busybox-baseline created
&lt;/code>&lt;/pre>&lt;p>Remember, we set the &lt;code>verify-pod-security&lt;/code> namespace to &lt;code>warn&lt;/code> based on the restricted profile. We can run &lt;code>kubectl -n verify-pod-security get pods&lt;/code> to verify it is running. Clean up with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl -n verify-pod-security delete pod busybox-baseline
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="restricted-level-and-workload">Restricted level and workload&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted">restricted policy&lt;/a> requires rejection of all privileged parameters. It is the most secure with a trade-off for complexity.
The restricted policy allows containers to add the &lt;code>NET_BIND_SERVICE&lt;/code> capability only.&lt;/p>
&lt;p>While we've already tested restricted as a blocking function, let's try to get something running that meets all the criteria.&lt;/p>
&lt;p>First we need to reapply the restricted profile, for the last time.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># enforces a &amp;#34;restricted&amp;#34; security policy and audits on restricted&lt;/span>
kubectl label --overwrite ns verify-pod-security &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/enforce&lt;span style="color:#666">=&lt;/span>restricted &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> pod-security.kubernetes.io/audit&lt;span style="color:#666">=&lt;/span>restricted
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n verify-pod-security apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-restricted
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> allowPrivilegeEscalation: false
&lt;/span>&lt;span style="color:#b44"> capabilities:
&lt;/span>&lt;span style="color:#b44"> add:
&lt;/span>&lt;span style="color:#b44"> - NET_BIND_SERVICE
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>Error from server (Forbidden): error when creating &amp;quot;STDIN&amp;quot;: pods &amp;quot;busybox-restricted&amp;quot; is forbidden: violates PodSecurity &amp;quot;restricted:latest&amp;quot;: unrestricted capabilities (container &amp;quot;busybox&amp;quot; must set securityContext.capabilities.drop=[&amp;quot;ALL&amp;quot;]), runAsNonRoot != true (pod or container &amp;quot;busybox&amp;quot; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &amp;quot;busybox&amp;quot; must set securityContext.seccompProfile.type to &amp;quot;RuntimeDefault&amp;quot; or &amp;quot;Localhost&amp;quot;)
&lt;/code>&lt;/pre>&lt;p>This is because the restricted profile explicitly requires that certain values are set to the most secure parameters.&lt;/p>
&lt;p>By requiring explicit values, manifests become more declarative and your entire security model can shift left. With the &lt;code>restricted&lt;/code> level of enforcement, a company could audit their cluster's compliance based on permitted manifests.&lt;/p>
&lt;p>Let's fix each warning resulting in the following file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n verify-pod-security apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-restricted
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> seccompProfile:
&lt;/span>&lt;span style="color:#b44"> type: RuntimeDefault
&lt;/span>&lt;span style="color:#b44"> runAsNonRoot: true
&lt;/span>&lt;span style="color:#b44"> allowPrivilegeEscalation: false
&lt;/span>&lt;span style="color:#b44"> capabilities:
&lt;/span>&lt;span style="color:#b44"> drop:
&lt;/span>&lt;span style="color:#b44"> - ALL
&lt;/span>&lt;span style="color:#b44"> add:
&lt;/span>&lt;span style="color:#b44"> - NET_BIND_SERVICE
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>pod/busybox-restricted created
&lt;/code>&lt;/pre>&lt;p>Run &lt;code>kubectl -n verify-pod-security get pods&lt;/code> to verify it is running. The output is similar to this:&lt;/p>
&lt;pre>&lt;code>NAME READY STATUS RESTARTS AGE
busybox-restricted 0/1 CreateContainerConfigError 0 2m26s
&lt;/code>&lt;/pre>&lt;p>Let's figure out why the container is not starting with &lt;code>kubectl -n verify-pod-security describe pod busybox-restricted&lt;/code>. The output is similar to this:&lt;/p>
&lt;pre>&lt;code>Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Warning Failed 2m29s (x8 over 3m55s) kubelet Error: container has runAsNonRoot and image will run as root (pod: &amp;quot;busybox-restricted_verify-pod-security(a4c6a62d-2166-41a9-b288-20df17cf5c90)&amp;quot;, container: busybox)
&lt;/code>&lt;/pre>&lt;p>To solve this, set the effective UID (&lt;code>runAsUser&lt;/code>) to a non-zero (root) value or use the &lt;code>nobody&lt;/code> UID (65534).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># delete the original pod&lt;/span>
kubectl -n verify-pod-security delete pod busybox-restricted
&lt;span style="color:#080;font-style:italic"># create the pod again with new runAsUser&lt;/span>
cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n verify-pod-security apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-restricted
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> runAsUser: 65534
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> seccompProfile:
&lt;/span>&lt;span style="color:#b44"> type: RuntimeDefault
&lt;/span>&lt;span style="color:#b44"> runAsNonRoot: true
&lt;/span>&lt;span style="color:#b44"> allowPrivilegeEscalation: false
&lt;/span>&lt;span style="color:#b44"> capabilities:
&lt;/span>&lt;span style="color:#b44"> drop:
&lt;/span>&lt;span style="color:#b44"> - ALL
&lt;/span>&lt;span style="color:#b44"> add:
&lt;/span>&lt;span style="color:#b44"> - NET_BIND_SERVICE
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Run &lt;code>kubectl -n verify-pod-security get pods&lt;/code> to verify it is running. The output is similar to this:&lt;/p>
&lt;pre>&lt;code>NAME READY STATUS RESTARTS AGE
busybox-restricted 1/1 Running 0 25s
&lt;/code>&lt;/pre>&lt;p>Clean up the demo (restricted pod and namespace) with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl delete namespace verify-pod-security
&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this point, if you wanted to dive deeper into linux permissions or what is permitted for a certain container, exec into the control plane and play around with &lt;code>containerd&lt;/code> and &lt;code>crictl inspect&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># if using docker, shell into the control plane&lt;/span>
docker &lt;span style="color:#a2f">exec&lt;/span> -it kind-control-plane bash
&lt;span style="color:#080;font-style:italic"># list running containers&lt;/span>
crictl ps
&lt;span style="color:#080;font-style:italic"># inspect each one by container ID&lt;/span>
crictl inspect &amp;lt;CONTAINER ID&amp;gt;
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="applying-a-cluster-wide-policy">Applying a cluster-wide policy&lt;/h3>
&lt;p>In addition to applying labels to namespaces to configure policy you can also configure cluster-wide policies and exemptions using the AdmissionConfiguration resource.&lt;/p>
&lt;p>Using this resource, policy definitions are applied cluster-wide by default and any policy that is applied via namespace labels will take precedence.&lt;/p>
&lt;p>There is no runtime configurable API for the &lt;code>AdmissionConfiguration&lt;/code> configuration file so a cluster administrator would need to specify a path to the file below via the &lt;code>--admission-control-config-file&lt;/code> flag on the API server.&lt;/p>
&lt;p>In the following resource we are enforcing the baseline policy and warning and auditing the baseline policy. We are also making the kube-system namespace exempt from this policy.&lt;/p>
&lt;p>It's not recommended to alter control plane / clusters after install, so let's build a new cluster with a default policy on all namespaces.&lt;/p>
&lt;p>First, delete the current cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kind delete cluster
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create a Pod Security configuration that &lt;code>enforce&lt;/code> and &lt;code>audit&lt;/code> baseline policies while using a restricted profile to &lt;code>warn&lt;/code> the end user.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF &amp;gt; pod-security.yaml
&lt;/span>&lt;span style="color:#b44">apiVersion: apiserver.config.k8s.io/v1
&lt;/span>&lt;span style="color:#b44">kind: AdmissionConfiguration
&lt;/span>&lt;span style="color:#b44">plugins:
&lt;/span>&lt;span style="color:#b44">- name: PodSecurity
&lt;/span>&lt;span style="color:#b44"> configuration:
&lt;/span>&lt;span style="color:#b44"> apiVersion: pod-security.admission.config.k8s.io/v1beta1
&lt;/span>&lt;span style="color:#b44"> kind: PodSecurityConfiguration
&lt;/span>&lt;span style="color:#b44"> defaults:
&lt;/span>&lt;span style="color:#b44"> enforce: &amp;#34;baseline&amp;#34;
&lt;/span>&lt;span style="color:#b44"> enforce-version: &amp;#34;latest&amp;#34;
&lt;/span>&lt;span style="color:#b44"> audit: &amp;#34;baseline&amp;#34;
&lt;/span>&lt;span style="color:#b44"> audit-version: &amp;#34;latest&amp;#34;
&lt;/span>&lt;span style="color:#b44"> warn: &amp;#34;restricted&amp;#34;
&lt;/span>&lt;span style="color:#b44"> warn-version: &amp;#34;latest&amp;#34;
&lt;/span>&lt;span style="color:#b44"> exemptions:
&lt;/span>&lt;span style="color:#b44"> # Array of authenticated usernames to exempt.
&lt;/span>&lt;span style="color:#b44"> usernames: []
&lt;/span>&lt;span style="color:#b44"> # Array of runtime class names to exempt.
&lt;/span>&lt;span style="color:#b44"> runtimeClasses: []
&lt;/span>&lt;span style="color:#b44"> # Array of namespaces to exempt.
&lt;/span>&lt;span style="color:#b44"> namespaces: [kube-system]
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>For additional options, check out the official &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller">&lt;em>standards admission controller&lt;/em>&lt;/a> docs.&lt;/p>
&lt;p>We now have a default baseline policy. Next pass it to the kind configuration to enable the &lt;code>--admission-control-config-file&lt;/code> API server argument and pass the policy file. To pass a file to a kind cluster, use a configuration file to pass additional setup instructions. Kind uses &lt;code>kubeadm&lt;/code> to provision the cluster and the configuration file has the ability to pass &lt;code>kubeadmConfigPatches&lt;/code> for further customization. In our case, the local file is mounted into the control plane node as &lt;code>/etc/kubernetes/policies/pod-security.yaml&lt;/code> which is then mounted into the &lt;code>apiServer&lt;/code> container. We also pass the &lt;code>--admission-control-config-file&lt;/code> argument pointing to the policy's location.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF &amp;gt; kind-config.yaml
&lt;/span>&lt;span style="color:#b44">kind: Cluster
&lt;/span>&lt;span style="color:#b44">apiVersion: kind.x-k8s.io/v1alpha4
&lt;/span>&lt;span style="color:#b44">nodes:
&lt;/span>&lt;span style="color:#b44">- role: control-plane
&lt;/span>&lt;span style="color:#b44"> kubeadmConfigPatches:
&lt;/span>&lt;span style="color:#b44"> - |
&lt;/span>&lt;span style="color:#b44"> kind: ClusterConfiguration
&lt;/span>&lt;span style="color:#b44"> apiServer:
&lt;/span>&lt;span style="color:#b44"> # enable admission-control-config flag on the API server
&lt;/span>&lt;span style="color:#b44"> extraArgs:
&lt;/span>&lt;span style="color:#b44"> admission-control-config-file: /etc/kubernetes/policies/pod-security.yaml
&lt;/span>&lt;span style="color:#b44"> # mount new file / directories on the control plane
&lt;/span>&lt;span style="color:#b44"> extraVolumes:
&lt;/span>&lt;span style="color:#b44"> - name: policies
&lt;/span>&lt;span style="color:#b44"> hostPath: /etc/kubernetes/policies
&lt;/span>&lt;span style="color:#b44"> mountPath: /etc/kubernetes/policies
&lt;/span>&lt;span style="color:#b44"> readOnly: true
&lt;/span>&lt;span style="color:#b44"> pathType: &amp;#34;DirectoryOrCreate&amp;#34;
&lt;/span>&lt;span style="color:#b44"> # mount the local file on the control plane
&lt;/span>&lt;span style="color:#b44"> extraMounts:
&lt;/span>&lt;span style="color:#b44"> - hostPath: ./pod-security.yaml
&lt;/span>&lt;span style="color:#b44"> containerPath: /etc/kubernetes/policies/pod-security.yaml
&lt;/span>&lt;span style="color:#b44"> readOnly: true
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create a new cluster using the kind configuration file defined above.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kind create cluster --image kindest/node:v1.23.0 --config kind-config.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let's look at the default namespace.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl describe namespace default
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>Name: default
Labels: kubernetes.io/metadata.name=default
Annotations: &amp;lt;none&amp;gt;
Status: Active
No resource quota.
No LimitRange resource.
&lt;/code>&lt;/pre>&lt;p>Let's create a new namespace and see if the labels apply there.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create namespace test-defaults
kubectl describe namespace test-defaults
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Same.&lt;/p>
&lt;pre>&lt;code>Name: test-defaults
Labels: kubernetes.io/metadata.name=test-defaults
Annotations: &amp;lt;none&amp;gt;
Status: Active
No resource quota.
No LimitRange resource.
&lt;/code>&lt;/pre>&lt;p>Can a privileged workload be deployed?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n test-defaults apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-privileged
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> securityContext:
&lt;/span>&lt;span style="color:#b44"> allowPrivilegeEscalation: true
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Hmm... yep. The default &lt;code>warn&lt;/code> level is working at least.&lt;/p>
&lt;pre>&lt;code>Warning: would violate PodSecurity &amp;quot;restricted:latest&amp;quot;: allowPrivilegeEscalation != false (container &amp;quot;busybox&amp;quot; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container &amp;quot;busybox&amp;quot; must set securityContext.capabilities.drop=[&amp;quot;ALL&amp;quot;]), runAsNonRoot != true (pod or container &amp;quot;busybox&amp;quot; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &amp;quot;busybox&amp;quot; must set securityContext.seccompProfile.type to &amp;quot;RuntimeDefault&amp;quot; or &amp;quot;Localhost&amp;quot;)
pod/busybox-privileged created
&lt;/code>&lt;/pre>&lt;p>Let's delete the pod with &lt;code>kubectl -n test-defaults delete pod/busybox-privileged&lt;/code>.&lt;/p>
&lt;p>Is my config even working?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># if using docker, shell into the control plane&lt;/span>
docker &lt;span style="color:#a2f">exec&lt;/span> -it kind-control-plane bash
&lt;span style="color:#080;font-style:italic"># cat out the file we mounted&lt;/span>
cat /etc/kubernetes/policies/pod-security.yaml
&lt;span style="color:#080;font-style:italic"># check the api server logs&lt;/span>
cat /var/log/containers/kube-apiserver*.log
&lt;span style="color:#080;font-style:italic"># check the api server config&lt;/span>
cat /etc/kubernetes/manifests/kube-apiserver.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>UPDATE:&lt;/strong> The baseline policy permits &lt;code>allowPrivilegeEscalation&lt;/code>. While I cannot see the Pod Security default levels of enforcement, they are there. Let's try to provide a manifest that violates the baseline by requesting hostNetwork access.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># delete the original pod&lt;/span>
kubectl -n test-defaults delete pod busybox-privileged
cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF | kubectl -n test-defaults apply -f -
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: busybox-privileged
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: busybox
&lt;/span>&lt;span style="color:#b44"> image: busybox
&lt;/span>&lt;span style="color:#b44"> args:
&lt;/span>&lt;span style="color:#b44"> - sleep
&lt;/span>&lt;span style="color:#b44"> - &amp;#34;1000000&amp;#34;
&lt;/span>&lt;span style="color:#b44"> hostNetwork: true
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code>Error from server (Forbidden): error when creating &amp;quot;STDIN&amp;quot;: pods &amp;quot;busybox-privileged&amp;quot; is forbidden: violates PodSecurity &amp;quot;baseline:latest&amp;quot;: host namespaces (hostNetwork=true)
&lt;/code>&lt;/pre>&lt;h4 id="it-worked">Yes!!! It worked! 🎉🎉🎉&lt;/h4>
&lt;p>I later found out, another way to check if things are operating as intended is to check the raw API server metrics endpoint.&lt;/p>
&lt;p>Run the following command:&lt;/p>
&lt;pre>&lt;code>kubectl get --raw /metrics | grep pod_security_evaluations_total
&lt;/code>&lt;/pre>&lt;p>The output is similar to this:&lt;/p>
&lt;pre>&lt;code># HELP pod_security_evaluations_total [ALPHA] Number of policy evaluations that occurred, not counting ignored or exempt requests.
# TYPE pod_security_evaluations_total counter
pod_security_evaluations_total{decision=&amp;quot;allow&amp;quot;,mode=&amp;quot;enforce&amp;quot;,policy_level=&amp;quot;baseline&amp;quot;,policy_version=&amp;quot;latest&amp;quot;,request_operation=&amp;quot;create&amp;quot;,resource=&amp;quot;pod&amp;quot;,subresource=&amp;quot;&amp;quot;} 2
pod_security_evaluations_total{decision=&amp;quot;allow&amp;quot;,mode=&amp;quot;enforce&amp;quot;,policy_level=&amp;quot;privileged&amp;quot;,policy_version=&amp;quot;latest&amp;quot;,request_operation=&amp;quot;create&amp;quot;,resource=&amp;quot;pod&amp;quot;,subresource=&amp;quot;&amp;quot;} 0
pod_security_evaluations_total{decision=&amp;quot;allow&amp;quot;,mode=&amp;quot;enforce&amp;quot;,policy_level=&amp;quot;privileged&amp;quot;,policy_version=&amp;quot;latest&amp;quot;,request_operation=&amp;quot;update&amp;quot;,resource=&amp;quot;pod&amp;quot;,subresource=&amp;quot;&amp;quot;} 0
pod_security_evaluations_total{decision=&amp;quot;deny&amp;quot;,mode=&amp;quot;audit&amp;quot;,policy_level=&amp;quot;baseline&amp;quot;,policy_version=&amp;quot;latest&amp;quot;,request_operation=&amp;quot;create&amp;quot;,resource=&amp;quot;pod&amp;quot;,subresource=&amp;quot;&amp;quot;} 1
pod_security_evaluations_total{decision=&amp;quot;deny&amp;quot;,mode=&amp;quot;enforce&amp;quot;,policy_level=&amp;quot;baseline&amp;quot;,policy_version=&amp;quot;latest&amp;quot;,request_operation=&amp;quot;create&amp;quot;,resource=&amp;quot;pod&amp;quot;,subresource=&amp;quot;&amp;quot;} 1
pod_security_evaluations_total{decision=&amp;quot;deny&amp;quot;,mode=&amp;quot;warn&amp;quot;,policy_level=&amp;quot;restricted&amp;quot;,policy_version=&amp;quot;latest&amp;quot;,request_operation=&amp;quot;create&amp;quot;,resource=&amp;quot;controller&amp;quot;,subresource=&amp;quot;&amp;quot;} 2
pod_security_evaluations_total{decision=&amp;quot;deny&amp;quot;,mode=&amp;quot;warn&amp;quot;,policy_level=&amp;quot;restricted&amp;quot;,policy_version=&amp;quot;latest&amp;quot;,request_operation=&amp;quot;create&amp;quot;,resource=&amp;quot;pod&amp;quot;,subresource=&amp;quot;&amp;quot;} 2
&lt;/code>&lt;/pre>&lt;p>A monitoring tool could ingest these metrics too for reporting, assessments, or measuring trends.&lt;/p>
&lt;h2 id="clean-up">Clean up&lt;/h2>
&lt;p>When finished, delete the kind cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kind delete cluster
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="auditing">Auditing&lt;/h2>
&lt;p>Auditing is another way to track what policies are being enforced in your cluster. To set up auditing with kind, review the official docs for &lt;a href="https://kind.sigs.k8s.io/docs/user/auditing/">enabling auditing&lt;/a>. As of &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.11.md#sig-auth">version 1.11&lt;/a>, Kubernetes audit logs include two annotations that indicate whether or not a request was authorized (&lt;code>authorization.k8s.io/decision&lt;/code>) and the reason for the decision (&lt;code>authorization.k8s.io/reason&lt;/code>). Audit events can be streamed to a webhook for monitoring, tracking, or alerting.&lt;/p>
&lt;p>The audit events look similar to the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">{&lt;span style="color:#b44">&amp;#34;authorization.k8s.io/decision&amp;#34;&lt;/span>:&lt;span style="color:#b44">&amp;#34;allow&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;authorization.k8s.io/reason&amp;#34;&lt;/span>:&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;pod-security.kubernetes.io/audit&amp;#34;&lt;/span>:&lt;span style="color:#b44">&amp;#34;allowPrivilegeEscalation != false (container \&amp;#34;busybox\&amp;#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \&amp;#34;busybox\&amp;#34; must set securityContext.capabilities.drop=[\&amp;#34;ALL\&amp;#34;]), runAsNonRoot != true (pod or container \&amp;#34;busybox\&amp;#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \&amp;#34;busybox\&amp;#34; must set securityContext.seccompProfile.type to \&amp;#34;RuntimeDefault\&amp;#34; or \&amp;#34;Localhost\&amp;#34;)&amp;#34;&lt;/span>}}&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Auditing is also a good first step in evaluating your cluster's current compliance with Pod Security. The Kubernetes Enhancement Proposal (KEP) hints at a future where &lt;code>baseline&lt;/code> &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/2579-psp-replacement/README.md#rollout-of-baseline-by-default-for-unlabeled-namespaces">could be the default for unlabeled namespaces&lt;/a>.&lt;/p>
&lt;p>Example &lt;code>audit-policy.yaml&lt;/code> configuration tuned for Pod Security events:&lt;/p>
&lt;pre>&lt;code>apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: RequestResponse
resources:
- group: &amp;quot;&amp;quot; # core API group
resources: [&amp;quot;pods&amp;quot;, &amp;quot;pods/ephemeralcontainers&amp;quot;, &amp;quot;podtemplates&amp;quot;, &amp;quot;replicationcontrollers&amp;quot;]
- group: &amp;quot;apps&amp;quot;
resources: [&amp;quot;daemonsets&amp;quot;, &amp;quot;deployments&amp;quot;, &amp;quot;replicasets&amp;quot;, &amp;quot;statefulsets&amp;quot;]
- group: &amp;quot;batch&amp;quot;
resources: [&amp;quot;cronjobs&amp;quot;, &amp;quot;jobs&amp;quot;]
verbs: [&amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;]
omitStages:
- &amp;quot;RequestReceived&amp;quot;
- &amp;quot;ResponseStarted&amp;quot;
- &amp;quot;Panic&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Once auditing is enabled, look at the configured local file if using &lt;code>--audit-log-path&lt;/code> or the destination of a webhook if using &lt;code>--audit-webhook-config-file&lt;/code>.&lt;/p>
&lt;p>If using a file (&lt;code>--audit-log-path&lt;/code>), run &lt;code>cat /PATH/TO/API/AUDIT.log | grep &amp;quot;is forbidden:&amp;quot;&lt;/code> to see all rejected workloads audited.&lt;/p>
&lt;h2 id="psp-migrations">PSP migrations&lt;/h2>
&lt;p>If you're already using PSP, SIG Auth has created a guide and &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">published the steps to migrate off of PSP&lt;/a>.&lt;/p>
&lt;p>To summarize the process:&lt;/p>
&lt;ul>
&lt;li>Update all existing PSPs to be non-mutating&lt;/li>
&lt;li>Apply Pod Security policies in &lt;code>warn&lt;/code> or &lt;code>audit&lt;/code> mode&lt;/li>
&lt;li>Upgrade Pod Security policies to &lt;code>enforce&lt;/code> mode&lt;/li>
&lt;li>Remove &lt;code>PodSecurityPolicy&lt;/code> from &lt;code>--enable-admission-plugins&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Listed as &amp;quot;optional future extensions&amp;quot; and currently out of scope, SIG Auth has kicked around the idea of providing a tool to assist with migrations. More &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/2579-psp-replacement/README.md#automated-psp-migration-tooling">details in the KEP&lt;/a>.&lt;/p>
&lt;h2 id="wrap-up">Wrap up&lt;/h2>
&lt;p>Pod Security is a promising new feature that provides an out-of-the-box way to allow users to improve the security posture of their workloads. Like any new enhancement that has matured to beta, we ask that you try it out, provide feedback, or share your experience via either raising a Github issue or joining SIG Auth community meetings. It's our hope that Pod Security will be deployed on every cluster in our ongoing pursuit as a community to make Kubernetes security a priority.&lt;/p>
&lt;p>For a step by step guide on how to enable &amp;quot;baseline&amp;quot; Pod Security Standards with Pod Security Admission feature please refer to these dedicated &lt;a href="https://kubernetes.io/docs/tutorials/security/">tutorials&lt;/a> that cover the configuration needed at cluster level and namespace level.&lt;/p>
&lt;h2 id="additional-resources">Additional resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Official Pod Security Docs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">Enforce Pod Security Standards with Namespace Labels&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/">Enforce Pod Security Standards by Configuring the Built-in Admission Controller&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/2579-psp-replacement/README.md">Official Kubernetes Enhancement Proposal&lt;/a> (KEP)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://medium.com/@LachlanEvenson/hands-on-with-kubernetes-pod-security-admission-b6cac495cd11">Hands on with Kubernetes Pod Security&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.23: Dual-stack IPv4/IPv6 Networking Reaches GA</title><link>https://kubernetes.io/blog/2021/12/08/dual-stack-networking-ga/</link><pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/08/dual-stack-networking-ga/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Bridget Kromhout (Microsoft)&lt;/p>
&lt;p>&amp;quot;When will Kubernetes have IPv6?&amp;quot; This question has been asked with increasing frequency ever since alpha support for IPv6 was first added in k8s v1.9. While Kubernetes has supported IPv6-only clusters since v1.18, migration from IPv4 to IPv6 was not yet possible at that point. At long last, &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack/">dual-stack IPv4/IPv6 networking&lt;/a> has reached general availability (GA) in Kubernetes v1.23.&lt;/p>
&lt;p>What does dual-stack networking mean for you? Let’s take a look…&lt;/p>
&lt;h2 id="service-api-updates">Service API updates&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services&lt;/a> were single-stack before 1.20, so using both IP families meant creating one Service per IP family. The user experience was simplified in 1.20, when Services were re-implemented to allow both IP families, meaning a single Service can handle both IPv4 and IPv6 workloads. Dual-stack load balancing is possible between services running any combination of IPv4 and IPv6.&lt;/p>
&lt;p>The Service API now has new fields to support dual-stack, replacing the single ipFamily field.&lt;/p>
&lt;ul>
&lt;li>You can select your choice of IP family by setting &lt;code>ipFamilyPolicy&lt;/code> to one of three options: SingleStack, PreferDualStack, or RequireDualStack. A service can be changed between single-stack and dual-stack (within some limits).&lt;/li>
&lt;li>Setting &lt;code>ipFamilies&lt;/code> to a list of families assigned allows you to set the order of families used.&lt;/li>
&lt;li>&lt;code>clusterIPs&lt;/code> is inclusive of the previous &lt;code>clusterIP&lt;/code> but allows for multiple entries, so it’s no longer necessary to run duplicate services, one in each of the two IP families. Instead, you can assign cluster IP addresses in both IP families.&lt;/li>
&lt;/ul>
&lt;p>Note that Pods are also dual-stack. For a given pod, there is no possibility of setting multiple IP addresses in the same family.&lt;/p>
&lt;h2 id="default-behavior-remains-single-stack">Default behavior remains single-stack&lt;/h2>
&lt;p>Starting in 1.20 with the re-implementation of dual-stack services as alpha, the underlying networking for Kubernetes has included dual-stack whether or not a cluster was configured with the feature flag to enable dual-stack.&lt;/p>
&lt;p>Kubernetes 1.23 removed that feature flag as part of graduating the feature to stable. Dual-stack networking is always available if you want to configure it. You can set your cluster network to operate as single-stack IPv4, as single-stack IPv6, or as dual-stack IPv4/IPv6.&lt;/p>
&lt;p>While Services are set according to what you configure, Pods default to whatever the CNI plugin sets. If your CNI plugin assigns single-stack IPs, you will have single-stack unless &lt;code>ipFamilyPolicy&lt;/code> specifies PreferDualStack or RequireDualStack. If your CNI plugin assigns dual-stack IPs, &lt;code>pod.status.PodIPs&lt;/code> defaults to dual-stack.&lt;/p>
&lt;p>Even though dual-stack is possible, it is not mandatory to use it. Examples in the documentation show the variety possible in &lt;a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios">dual-stack service configurations&lt;/a>.&lt;/p>
&lt;h2 id="try-dual-stack-right-now">Try dual-stack right now&lt;/h2>
&lt;p>While upstream Kubernetes now supports &lt;a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/">dual-stack networking&lt;/a> as a GA or stable feature, each provider’s support of dual-stack Kubernetes may vary. Nodes need to be provisioned with routable IPv4/IPv6 network interfaces. Pods need to be dual-stack. The &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin&lt;/a> is what assigns the IP addresses to the Pods, so it's the network plugin being used for the cluster that needs to support dual-stack. Some Container Network Interface (CNI) plugins support dual-stack, as does kubenet.&lt;/p>
&lt;p>Ecosystem support of dual-stack is increasing; you can create &lt;a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/dual-stack-support/">dual-stack clusters with kubeadm&lt;/a>, try a &lt;a href="https://kind.sigs.k8s.io/docs/user/configuration/#ip-family">dual-stack cluster locally with KIND&lt;/a>, and deploy dual-stack clusters in cloud providers (after checking docs for CNI or kubenet availability).&lt;/p>
&lt;h2 id="get-involved-with-sig-network">Get involved with SIG Network&lt;/h2>
&lt;p>SIG-Network wants to learn from community experiences with dual-stack networking to find out more about evolving needs and your use cases. The &lt;a href="https://www.youtube.com/watch?v=uZ0WLxpmBbY&amp;amp;list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP&amp;amp;index=4">SIG-network update video from KubeCon NA 2021&lt;/a> summarizes the SIG’s recent updates, including dual-stack going to stable in 1.23.&lt;/p>
&lt;p>The current SIG-Network &lt;a href="https://github.com/orgs/kubernetes/projects/10">KEPs&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork">issues&lt;/a> on GitHub illustrate the SIG’s areas of emphasis. The &lt;a href="https://github.com/kubernetes/enhancements/issues/2438">dual-stack API server&lt;/a> is one place to consider contributing.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network#meetings">SIG-Network meetings&lt;/a> are a friendly, welcoming venue for you to connect with the community and share your ideas. Looking forward to hearing from you!&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>The dual-stack networking feature represents the work of many Kubernetes contributors. Thanks to all who contributed code, experience reports, documentation, code reviews, and everything in between. Bridget Kromhout details this community effort in &lt;a href="https://containerjournal.com/features/dual-stack-networking-in-kubernetes/">Dual-Stack Networking in Kubernetes&lt;/a>. KubeCon keynotes by Tim Hockin &amp;amp; Khaled (Kal) Henidak in 2019 (&lt;a href="https://www.youtube.com/watch?v=o-oMegdZcg4">The Long Road to IPv4/IPv6 Dual-stack Kubernetes&lt;/a>) and by Lachlan Evenson in 2021 (&lt;a href="https://www.youtube.com/watch?v=lVrt8F2B9CM">And Here We Go: Dual-stack Networking in Kubernetes&lt;/a>) talk about the dual-stack journey, spanning five years and a great many lines of code.&lt;/p></description></item><item><title>Blog: Kubernetes 1.23: The Next Frontier</title><link>https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.23/release-team.md">Kubernetes 1.23 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the release of Kubernetes 1.23, the last release of 2021!&lt;/p>
&lt;p>This release consists of 47 enhancements: 11 enhancements have graduated to stable, 17 enhancements are moving to beta, and 19 enhancements are entering alpha. Also, 1 feature has been deprecated.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="deprecation-of-flexvolume">Deprecation of FlexVolume&lt;/h3>
&lt;p>FlexVolume is deprecated. The out-of-tree CSI driver is the recommended way to write volume drivers in Kubernetes. See &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors">this doc&lt;/a> for more information. Maintainers of FlexVolume drivers should implement a CSI driver and move users of FlexVolume to CSI. Users of FlexVolume should move their workloads to the CSI driver.&lt;/p>
&lt;h3 id="deprecation-of-klog-specific-flags">Deprecation of klog specific flags&lt;/h3>
&lt;p>To simplify the code base, several &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog">logging flags were marked as deprecated&lt;/a> in Kubernetes 1.23. The code which implements them will be removed in a future release, so users of those need to start replacing the deprecated flags with some alternative solutions.&lt;/p>
&lt;h3 id="software-supply-chain-slsa-level-1-compliance-in-the-kubernetes-release-process">Software Supply Chain SLSA Level 1 Compliance in the Kubernetes Release Process&lt;/h3>
&lt;p>Kubernetes releases now generate provenance attestation files describing the staging and release phases of the release process. Artifacts are now verified as they are handed over from one phase to the next. This final piece completes the work needed to comply with Level 1 of the &lt;a href="https://slsa.dev/">SLSA security framework&lt;/a> (Supply-chain Levels for Software Artifacts).&lt;/p>
&lt;h3 id="ipv4-ipv6-dual-stack-networking-graduates-to-ga">IPv4/IPv6 Dual-stack Networking graduates to GA&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack">IPv4/IPv6 dual-stack networking&lt;/a> graduates to GA. Since 1.21, Kubernetes clusters have been enabled to support dual-stack networking by default. In 1.23, the &lt;code>IPv6DualStack&lt;/code> feature gate is removed. The use of dual-stack networking is not mandatory. Although clusters are enabled to support dual-stack networking, Pods and Services continue to default to single-stack. To use dual-stack networking Kubernetes nodes must have routable IPv4/IPv6 network interfaces, a dual-stack capable CNI network plugin must be used, Pods must be configured to be dual-stack and Services must have their &lt;code>.spec.ipFamilyPolicy&lt;/code> field set to either &lt;code>PreferDualStack&lt;/code> or &lt;code>RequireDualStack&lt;/code>.&lt;/p>
&lt;h3 id="horizontalpodautoscaler-v2-graduates-to-ga">HorizontalPodAutoscaler v2 graduates to GA&lt;/h3>
&lt;p>The HorizontalPodAutscaler &lt;code>autoscaling/v2&lt;/code> stable API moved to GA in 1.23. The HorizontalPodAutoscaler &lt;code>autoscaling/v2beta2&lt;/code> API has been deprecated.&lt;/p>
&lt;h3 id="generic-ephemeral-volume-feature-graduates-to-ga">Generic Ephemeral Volume feature graduates to GA&lt;/h3>
&lt;p>The generic ephemeral volume feature moved to GA in 1.23. This feature allows any existing storage driver that supports dynamic provisioning to be used as an ephemeral volume with the volume’s lifecycle bound to the Pod. All StorageClass parameters for volume provisioning and all features supported with PersistentVolumeClaims are supported.&lt;/p>
&lt;h3 id="skip-volume-ownership-change-graduates-to-ga">Skip Volume Ownership change graduates to GA&lt;/h3>
&lt;p>The feature to configure volume permission and ownership change policy for Pods moved to GA in 1.23. This allows users to skip recursive permission changes on mount and speeds up the pod start up time.&lt;/p>
&lt;h3 id="allow-csi-drivers-to-opt-in-to-volume-ownership-and-permission-change-graduates-to-ga">Allow CSI drivers to opt-in to volume ownership and permission change graduates to GA&lt;/h3>
&lt;p>The feature to allow CSI Drivers to declare support for fsGroup based permissions graduates to GA in 1.23.&lt;/p>
&lt;h3 id="podsecurity-graduates-to-beta">PodSecurity graduates to Beta&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">PodSecurity&lt;/a> moves to Beta. &lt;code>PodSecurity&lt;/code> replaces the deprecated &lt;code>PodSecurityPolicy&lt;/code> admission controller. &lt;code>PodSecurity&lt;/code> is an admission controller that enforces Pod Security Standards on Pods in a Namespace based on specific namespace labels that set the enforcement level. In 1.23, the &lt;code>PodSecurity&lt;/code> feature gate is enabled by default.&lt;/p>
&lt;h3 id="container-runtime-interface-cri-v1-is-default">Container Runtime Interface (CRI) v1 is default&lt;/h3>
&lt;p>The Kubelet now supports the CRI &lt;code>v1&lt;/code> API, which is now the project-wide default.
If a container runtime does not support the &lt;code>v1&lt;/code> API, Kubernetes will fall back to the &lt;code>v1alpha2&lt;/code> implementation. There is no intermediate action required by end-users, because &lt;code>v1&lt;/code> and &lt;code>v1alpha2&lt;/code> do not differ in their implementation. It is likely that &lt;code>v1alpha2&lt;/code> will be removed in one of the future Kubernetes releases to be able to develop &lt;code>v1&lt;/code>.&lt;/p>
&lt;h3 id="structured-logging-graduate-to-beta">Structured logging graduate to Beta&lt;/h3>
&lt;p>Structured logging reached its Beta milestone. Most log messages from kubelet and kube-scheduler have been converted. Users are encouraged to try out JSON output or parsing of the structured text format and provide feedback on possible solutions for the open issues, such as handling of multi-line strings in log values.&lt;/p>
&lt;h3 id="simplified-multi-point-plugin-configuration-for-scheduler">Simplified Multi-point plugin configuration for scheduler&lt;/h3>
&lt;p>The kube-scheduler is adding a new, simplified config field for Plugins to allow multiple extension points to be enabled in one spot. The new &lt;code>multiPoint&lt;/code> plugin field is intended to simplify most scheduler setups for administrators. Plugins that are enabled via &lt;code>multiPoint&lt;/code> will automatically be registered for each individual extension point that they implement. For example, a plugin that implements Score and Filter extensions can be simultaneously enabled for both. This means entire plugins can be enabled and disabled without having to manually edit individual extension point settings. These extension points can now be abstracted away due to their irrelevance for most users.&lt;/p>
&lt;h3 id="csi-migration-updates">CSI Migration updates&lt;/h3>
&lt;p>CSI Migration enables the replacement of existing in-tree storage plugins such as &lt;code>kubernetes.io/gce-pd&lt;/code> or &lt;code>kubernetes.io/aws-ebs&lt;/code> with a corresponding CSI driver.
If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference.
After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.&lt;/p>
&lt;ul>
&lt;li>CSI Migration feature is turned on by default but stays in Beta for GCE PD, AWS EBS, and Azure Disk in 1.23.&lt;/li>
&lt;li>CSI Migration is introduced as an Alpha feature for Ceph RBD and Portworx in 1.23.&lt;/li>
&lt;/ul>
&lt;h3 id="expression-language-validation-for-crd-is-alpha">Expression language validation for CRD is alpha&lt;/h3>
&lt;p>Expression language validation for CRD is in alpha starting in 1.23. If the &lt;code>CustomResourceValidationExpressions&lt;/code> feature gate is enabled, custom resources will be validated by validation rules using the &lt;a href="https://github.com/google/cel-spec">Common Expression Language (CEL)&lt;/a>.&lt;/p>
&lt;h3 id="server-side-field-validation-is-alpha">Server Side Field Validation is Alpha&lt;/h3>
&lt;p>If the &lt;code>ServerSideFieldValidation&lt;/code> feature gate is enabled starting 1.23, users will receive warnings from the server when they send Kubernetes objects in the request that contain unknown or duplicate fields. Previously unknown fields and all but the last duplicate fields would be dropped by the server.&lt;/p>
&lt;p>With the feature gate enabled, we also introduce the &lt;code>fieldValidation&lt;/code> query parameter so that users can specify the desired behavior of the server on a per request basis. Valid values for the &lt;code>fieldValidation&lt;/code> query parameter are:&lt;/p>
&lt;ul>
&lt;li>Ignore (default when feature gate is disabled, same as pre-1.23 behavior of dropping/ignoring unkonwn fields)&lt;/li>
&lt;li>Warn (default when feature gate is enabled).&lt;/li>
&lt;li>Strict (this will fail the request with an Invalid Request error)&lt;/li>
&lt;/ul>
&lt;h3 id="openapi-v3-is-alpha">OpenAPI v3 is Alpha&lt;/h3>
&lt;p>If the &lt;code>OpenAPIV3&lt;/code> feature gate is enabled starting 1.23, users will be able to request the OpenAPI v3.0 spec for all Kubernetes types. OpenAPI v3 aims to be fully transparent and includes support for a set of fields that are dropped when publishing OpenAPI v2: &lt;code>default&lt;/code>, &lt;code>nullable&lt;/code>, &lt;code>oneOf&lt;/code>, &lt;code>anyOf&lt;/code>. A separate spec is published per Kubernetes group version (at the &lt;code>$cluster/openapi/v3/apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;&lt;/code> endpoint) for improved performance and discovery, for all group versions can be found at the &lt;code>$cluster/openapi/v3&lt;/code> path.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/563">IPv4/IPv6 Dual-Stack Support&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/695">Skip Volume Ownership Change&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/592">TTL After Finished Controller&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1682">Config FSGroup Policy in CSI Driver object&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1698">Generic Ephemeral Inline Volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1933">Defend Against Logging Secrets via Static Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2365">Namespace Scoped Ingress Class Parameters&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2420">Reducing Kubernetes Build Maintenance&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2702">Graduate HPA API to GA&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1040">Priority and Fairness for API Server Requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="release-notes">Release Notes&lt;/h3>
&lt;p>Check out the full details of the Kubernetes 1.23 release in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md">release notes&lt;/a>.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes 1.23 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.23.0">GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local Kubernetes clusters using Docker container “nodes” with &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>. You can also easily install 1.23 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release Team&lt;/h3>
&lt;p>This release was made possible by a very dedicated group of individuals, who came together as a team to deliver technical content, documentation, code, and a host of other components that go into every Kubernetes release.&lt;/p>
&lt;p>A huge thank you to the release lead Rey Lejano for leading us through a successful release cycle, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.23 release for the community.&lt;/p>
&lt;h3 id="release-theme-and-logo">Release Theme and Logo&lt;/h3>
&lt;p>&lt;strong>Kubernetes 1.23: The Next Frontier&lt;/strong>&lt;/p>
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2021-12-07-kubernetes-release-1.23/kubernetes-1.23.png"/>
&lt;/figure>
&lt;p>&amp;quot;The Next Frontier&amp;quot; theme represents the new and graduated enhancements in 1.23, Kubernetes' history of Star Trek references, and the growth of community members in the release team.&lt;/p>
&lt;p>Kubernetes has a history of Star Trek references. The original codename for Kubernetes within Google is Project 7, a reference to Seven of Nine from Star Trek Voyager. And of course Borg was the name for the predecessor to Kubernetes. &amp;quot;The Next Frontier&amp;quot; theme continues the Star Trek references. &amp;quot;The Next Frontier&amp;quot; is a fusion of two Star Trek titles, Star Trek V: The Final Frontier and Star Trek the Next Generation.&lt;/p>
&lt;p>&amp;quot;The Next Frontier&amp;quot; represents a line in the SIG Release charter, &amp;quot;Ensure there is a consistent group of community members in place to support the release process across time.&amp;quot; With each release team, we grow the community with new release team members and for many it's their first contribution in their open source frontier.&lt;/p>
&lt;p>Reference: &lt;a href="https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/">https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/&lt;/a>
Reference: &lt;a href="https://github.com/kubernetes/community/blob/master/sig-release/charter.md">https://github.com/kubernetes/community/blob/master/sig-release/charter.md&lt;/a>&lt;/p>
&lt;p>The Kubernetes 1.23 release logo continues with the theme's Star Trek reference. Every star is a helm from the Kubernetes logo. The ship represents the collective teamwork of the release team.&lt;/p>
&lt;p>Rey Lejano designed the logo.&lt;/p>
&lt;h3 id="user-highlights">User Highlights&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.cncf.io/announcements/2021/09/22/cncf-end-user-technology-radar-provides-insights-into-devsecops/">Findings of the latest CNCF End User Technology Radar&lt;/a> were themed around DevSecOps. Check out the &lt;a href="https://radar.cncf.io/">Radar Page&lt;/a> for the full details and findings.&lt;/li>
&lt;li>Learn about how &lt;a href="https://www.cncf.io/case-studies/aegon-life-india/">end user Aegon Life India migrated core processes from its traditional monolith to a microservice-based architecture&lt;/a> in its effort to transform into a leading digital service company.&lt;/li>
&lt;li>Utilizing multiple cloud native projects, &lt;a href="https://www.cncf.io/case-studies/seagate/">Seagate engineered edgerX to run Real-time Analytics at the Edge&lt;/a>.&lt;/li>
&lt;li>Check out how &lt;a href="https://www.cncf.io/case-studies/zambon/">Zambon worked with SparkFabrik to develop 16 websites, with cloud native technologies, to enable stakeholders to easily update content while maintaining a consistent brand identity&lt;/a>.&lt;/li>
&lt;li>Using Kubernetes, &lt;a href="https://www.cncf.io/case-studies/influxdata/">InfluxData was able to deliver on the promise of multi-cloud, multi-region service availability&lt;/a> by creating a true cloud abstraction layer that allows for the seamless delivery of InfluxDB as a single application to multiple global clusters across three major cloud providers.&lt;/li>
&lt;/ul>
&lt;h3 id="ecosystem-updates">Ecosystem Updates&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.cncf.io/events/kubecon-cloudnativecon-north-america-2021/">KubeCon + CloudNativeCon NA 2021&lt;/a> was held in October 2021, both online and in person. All talks are &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP">now available on-demand&lt;/a> for anyone that would like to catch up!&lt;/li>
&lt;li>&lt;a href="https://www.cncf.io/announcements/2021/11/18/kubernetes-and-cloud-native-essentials-training-and-kcna-certification-now-available/">Kubernetes and Cloud Native Essentials Training and KCNA Certification are now generally available for enrollment and scheduling&lt;/a>. Additionally, a new online training course, &lt;a href="https://www.cncf.io/announcements/2021/10/13/entry-level-kubernetes-certification-to-help-advance-cloud-careers/">Kubernetes and Cloud Native Essentials (LFS250)&lt;/a>, has been released to both prepare individuals for entry-level cloud roles and to sit for the KCNA exam.&lt;/li>
&lt;li>&lt;a href="https://www.cncf.io/announcements/2021/10/13/inclusive-naming-initiative-announces-new-community-resources-for-a-more-inclusive-future/">New resources are now available from the Inclusive Naming Initiative&lt;/a>, including an Inclusive Strategies for Open Source (LFC103) course, Language Evaluation Framework, and Implementation Path.&lt;/li>
&lt;/ul>
&lt;h3 id="project-velocity">Project Velocity&lt;/h3>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;amp;refresh=15m">CNCF K8s DevStats&lt;/a> project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.23 release cycle, which ran for 16 weeks (August 23 to December 7), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.22.0%20-%20now&amp;amp;var-metric=contributions">1032 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.22.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&amp;amp;var-repo_name=kubernetes%2Fkubernetes">1084 individuals&lt;/a>.&lt;/p>
&lt;h3 id="event-update">Event Update&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.lfasiallc.com/kubecon-cloudnativecon-open-source-summit-china/">KubeCon + CloudNativeCon China 2021&lt;/a> is happening this month from December 9 - 11. After taking a break last year, the event will be virtual this year and includes 105 sessions. Check out the event schedule &lt;a href="https://www.lfasiallc.com/kubecon-cloudnativecon-open-source-summit-china/program/schedule/">here&lt;/a>.&lt;/li>
&lt;li>KubeCon + CloudNativeCon Europe 2022 will take place in Valencia, Spain, May 4 – 7, 2022! You can find more information about the conference and registration on the &lt;a href="https://events.linuxfoundation.org/archive/2021/kubecon-cloudnativecon-europe/">event site&lt;/a>.&lt;/li>
&lt;li>Kubernetes Community Days has upcoming events scheduled in Pakistan, Brazil, Chengdu, and in Australia.&lt;/li>
&lt;/ul>
&lt;h3 id="upcoming-release-webinar">Upcoming Release Webinar&lt;/h3>
&lt;p>Join members of the Kubernetes 1.23 release team on January 4, 2022 to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the &lt;a href="https://community.cncf.io/e/mrey9h/">event page&lt;/a> on the CNCF Online Programs site.&lt;/p>
&lt;h3 id="get-involved">Get Involved&lt;/h3>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributors&lt;/a> website&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for the latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Contribution, containers and cricket: the Kubernetes 1.22 release interview</title><link>https://kubernetes.io/blog/2021/12/01/contribution-containers-and-cricket-the-kubernetes-1.22-release-interview/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/01/contribution-containers-and-cricket-the-kubernetes-1.22-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>The Kubernetes release train rolls on, and we look ahead to the release of 1.23 next week. &lt;a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">As is our tradition&lt;/a>, I'm pleased to bring you a look back at the process that brought us the previous version.&lt;/p>
&lt;p>The release team for 1.22 was led by &lt;a href="https://twitter.com/coffeeartgirl">Savitha Raghunathan&lt;/a>, who was, at the time, a Senior Platform Engineer at MathWorks. &lt;a href="https://kubernetespodcast.com/episode/157-kubernetes-1.22/">I spoke to Savitha&lt;/a> on the &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a>, the weekly&lt;super>*&lt;/super> show covering the Kubernetes and Cloud Native ecosystem.&lt;/p>
&lt;p>Our release conversations shine a light on the team that puts together each Kubernetes release. Make sure you &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe, wherever you get your podcasts&lt;/a> so you catch the story of 1.23.&lt;/p>
&lt;p>And in case you're interested in why the show has been on a hiatus the last few weeks, all will be revealed in the next episode!&lt;/p>
&lt;p>&lt;em>This transcript has been lightly edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: Welcome to the show, Savitha.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Hey, Craig. Thanks for having me on the show. How are you today?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I'm very well, thank you. I've interviewed a lot of people on the show, and you're actually the first person who's asked that of me.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I'm glad. It's something that I always do. I just want to make sure the other person is good and happy.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That's very kind of you. Thank you for kicking off on a wonderful foot there. I want to ask first of all — you grew up in Chennai. My association with Chennai is the &lt;a href="https://en.wikipedia.org/wiki/Chennai_Super_Kings">Super Kings cricket team&lt;/a>. Was cricket part of your upbringing?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah. Actually, a lot. My mom loves watching cricket. I have a younger brother, and when we were growing up, we used to play cricket on the terrace. Everyone surrounding me, my best friends — and even now, my partner — loves watching cricket, too. Cricket is a part of my life.&lt;/p>
&lt;p>I stopped watching it a while ago, but I still enjoy a good game.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's probably a bit harder in the US. Everything's in a different time zone. I find, with my cricket team being on the other side of the world, that it's a lot easier when they're playing near me, as opposed to trying to keep up with what they're doing when they're playing at 3:00 in the morning.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: That is actually one of the things that made me lose touch with cricket. I'm going to give you a piece of interesting information. I never supported Chennai Super Kings. I always supported &lt;a href="https://en.wikipedia.org/wiki/Royal_Challengers_Bangalore">Royal Challengers of Bangalore&lt;/a>.&lt;/p>
&lt;p>I once went to the stadium, and it was a match between the Chennai Super Kings and the RCB. I was the only one who was cheering whenever the RCB hit a 6, or when they were scoring. I got the stares of thousands of people looking at me. I'm like, &amp;quot;what are you doing?&amp;quot; My friends are like, &amp;quot;you're going to get us killed! Just stop screaming!&amp;quot;&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I hear you. As a New Zealander in the UK, there are a lot of international cricket matches I've been to where I am one of the few people dressed in the full beige kit. But I have to ask, why an affiliation with a different team?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I'm not sure. When the IPL came out, I really liked Virat Kohli. He was playing for RCB at that time, and I think pretty much that's it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Well, what I know about the Chennai Super Kings is that their coach is New Zealand's finest batsmen and &lt;a href="https://www.youtube.com/watch?v=vSZAaUCAclw">air conditioning salesman&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Stephen_Fleming">Stephen Fleming&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Oh, really?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yeah, he's a dead ringer for the guy who played the &lt;a href="https://s1.reutersmedia.net/resources/r/?m=02&amp;amp;d=20061130&amp;amp;t=2&amp;amp;i=153531&amp;amp;w=&amp;amp;fh=545px&amp;amp;fw=&amp;amp;ll=&amp;amp;pl=&amp;amp;sq=&amp;amp;r=153531">yellow Wiggle&lt;/a> back in the day.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Oh, interesting. I remember the name, but I cannot put the picture and the name together. I stopped watching cricket once I moved to the States. Then, all my focus was on studies and extracurriculars. I have always been an introvert. The campus — it was a new thing for me — they had international festivals.&lt;/p>
&lt;p>And every week, they'd have some kind of new thing going on, so I'd go check them out. I wouldn't participate, but I did go out and check them out. That was a big feat for me around that time because a lot of people — and still, even now, a lot of people — they kind of scare me. I don't know how to make a conversation with everyone.&lt;/p>
&lt;p>I'll just go and say, &amp;quot;hi, how are you? OK, I'm good. I'm just going to move on&amp;quot;. And I'll just go to the next person. And after two hours, I'm out of that place.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Perhaps a pleasant side effect of the last 12 months — a lot fewer gatherings of people.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Could be that, but I'm so excited about KubeCon. But when I think about it, I'm like &amp;quot;oh my God. There's going to be a lot of people. What am I going to do? I'm going to meet all my friends over there&amp;quot;.&lt;/p>
&lt;p>Sometimes I have social anxiety like, what's going to happen?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What's going to happen is you're going to ask them how they are at the beginning, and they're immediately going to be set at ease.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: &lt;em>laughs&lt;/em> I hope so.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk a little bit, then, about your transition from India to the US. You did your undergraduate degree in computer science at the SSN College of Engineering. How did you end up at Arizona State?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I always wanted to pursue higher studies when I was in India, and I didn't have the opportunity immediately. Once I graduated from my school there, I went and I worked for a couple of years. My aim was always to get out of there and come here, do my graduate studies.&lt;/p>
&lt;p>Eventually, I want to do a PhD. I have an idea of what I want to do. I always wanted to keep studying. If there's an option that I could just keep studying and not do work or anything of that sort, I'd just pick that other one — I'll just keep studying.&lt;/p>
&lt;p>But unfortunately, you need money and other things to live and sustain in this world. So I'm like, OK, I'll take a break from studies, and I will work for a while.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The road to success is littered with dreams of PhDs. I have a lot of friends who thought that that was the path they were going to take, and they've had a beautiful career and probably aren't going to go back to study. Did you use the &lt;a href="https://en.wikipedia.org/wiki/MATLAB">Matlab&lt;/a> software at all while you were going through your schooling?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: No, unfortunately. That is a question that everyone asks. I have not used Matlab. I haven't used it even now. I don't use it for work. I didn't have any necessity for my school work. I didn't have anything to do with Matlab. I never analysed, or did data processing, or anything, with Matlab. So unfortunately, no.&lt;/p>
&lt;p>Everyone asks me like, you're working at &lt;a href="https://en.wikipedia.org/wiki/MathWorks">MathWorks&lt;/a>. Have you used Matlab? I'm like, no.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Fair enough. Nor have I. But it's been around since the late 1970s, so I imagine there are a lot of people who will have come across it at some point. Do you work with a lot of people who have been working on it that whole time?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Kind of. Not all the time, but I get to meet some folks who work on the product itself. Most of my interactions are with the infrastructure team and platform engineering teams at MathWorks. One other interesting fact is that when I joined the company — MathWorks has an extensive internal curriculum for training and learning, which I really love. They have an &amp;quot;Intro to Matlab&amp;quot; course, and that's on my bucket of things to do.&lt;/p>
&lt;p>It was like 500 years ago. I added it, and I never got to it. I'm like, OK, maybe this year at least I want to get to it and I want to learn something new. My partner used Matlab extensively. He misses it right now at his current employer. And he's like, &amp;quot;you have the entire licence! You have access to the entire suite and you haven't used it?&amp;quot; I'm like, &amp;quot;no!&amp;quot;&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Well, I have bad news for the idea of you doing a PhD, I'm sorry.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Another thing is that none of my family knew about the company MathWorks and Matlab. The only person who knew was my younger brother. He was so proud. He was like, &amp;quot;oh my God&amp;quot;.&lt;/p>
&lt;p>When he was 12 years old, he started getting involved in robotics and all that stuff. That's how he got introduced to Matlab. He goes absolutely bananas for the swag. So all the t-shirts, all the hoodies — any swag that I get from MathWorks goes to him, without saying.&lt;/p>
&lt;p>Over the five, six years, the things that I've got — there was only one sweatshirt that I kept for myself. Everything else I've just given to him. And he cherishes it. He's the only one in my family who knew about Matlab and MathWorks.&lt;/p>
&lt;p>Now, everyone knows, because I'm working there. They were initially like, I don't even know that company name. Is it like Amazon? I'm like, no, we make software that can send people to the moon. And we also make software that can do amazing robotic surgeries and even make a car drive on its own. That's something that I take immense pride in.&lt;/p>
&lt;p>I know I don't directly work on the product, but I'm enabling the people who are creating the product. I'm really, really proud of that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think Jeff Bezos is working on at least two out of three of those disciplines that you mentioned before, so it's maybe a little bit like Amazon. One thing I've always thought about Matlab is that, because it's called Matlab, it solves that whole problem where &lt;a href="https://www.grammar.com/math_vs._maths">Americans call it math, and the rest of the world call it maths&lt;/a>. Why do Americans think there's only one math?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Definitely. I had trouble — growing up in India, it's always British English. And I had so much trouble when I moved here. So many things changed.&lt;/p>
&lt;p>One of the things is maths. I always got used to writing maths, physics, and everything.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: They don't call it &amp;quot;physic&amp;quot; in the US, do they?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: No, no, they don't. Luckily, they don't. That still stays &amp;quot;physics&amp;quot;. But math — I had trouble. It's maths. Even when you do the full abbreviations like mathematics and you are still calling it math, I'm like, mm.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: They can do the computer science abbreviation thing and call it math-7-S or whatever the number of letters is.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Just like Kubernetes. K-8-s.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Your path to Kubernetes is through MathWorks. They started out as a company making software which was distributed in a physical sense — boxed copies, if you will. I understand now there is a cloud version. Can I assume that that is where the two worlds intersect?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Kind of. I have interaction with the team that supports Matlab on the cloud, but I don't get to work with them on a day-to-day basis. They use Docker containers, and they are building the platform using Kubernetes. So yeah, a little bit of that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: So what exactly is the platform that you are engineering day to day?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Providing Kubernetes as a platform, obviously — that goes without saying — to some of the internal development teams. In the future we might expand it to more teams within the company. That is a focus area right now, so that's what we are doing. In the process, we might even get to work with the people who are deploying Matlab on the cloud, which is exciting.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, your path to contribution to Kubernetes, you've said before, was through &lt;a href="https://github.com/kubernetes/website/pull/15588">fixing a 404 error on the Kubernetes.io website&lt;/a>. Do you remember what the page was?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I do. I was going to something for work, and I came across this changelog. In Kubernetes there's a nice page — once you got to the release page, there would be a long list of changelogs.&lt;/p>
&lt;p>One of the things that I fixed was, the person who worked on the feature had changed their GitHub handle, and that wasn't reflected on this page. So that was my first. I got curious and clicked on the links. One of the links was the handle, and that went to a 404. And I was like &amp;quot;Yeah, I'll just fix that. They have done all the hard work. They can get the credit that's due&amp;quot;.&lt;/p>
&lt;p>It was easy. It wasn't overwhelming for me to pick it up as my first issue. Before that I logged on around Kubernetes for about six to eight months without doing anything because it was just a lot.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the other things that you said about your initial contribution is that you had to learn how to use Git. As a very powerful tool, I find Git is a high barrier to entry for even contributing code to a project. When you want to contribute a blog post or documentation or a fix like you did before, I find it almost impossible to think how a new user would come along and do that. What was your process? Do you think that there's anything we can do to make that barrier lower for new contributors?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Of course. There are more and more tutorials available these days. There is a new contributor workshop. They actually have a &lt;a href="https://www.kubernetes.dev/docs/guide/github-workflow/">GitHub workflow section&lt;/a>, &lt;a href="https://www.kubernetes.dev/docs/guide/pull-requests/">how to do a pull request&lt;/a> and stuff like that. I know a couple of folks from SIG Docs that are working on which Git commands that you need, or how to get to writing something small and getting it committed. But more tutorials or more links to intro to Git would definitely help.&lt;/p>
&lt;p>The thing is also, someone like a documentation writer — they don't actually want to know the entirety of Git. Honestly, it's an ocean. I don't know how to do it. Most of the time, I still ask for help even though I work with Git on a day to day basis. There are several articles and a lot of help is available already within the community. Maybe we could just add a couple more to &lt;a href="https://kubernetes.dev/">kubernetes.dev&lt;/a>. That is an amazing site for all the new contributors and existing contributors who want to build code, who want to write documentation.&lt;/p>
&lt;p>We could just add a tutorial there like, &amp;quot;hey, don't know Git, you are new to Git? You just need to know these main things&amp;quot;.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I find it a shame, to be honest, that people need to use Git for that, by comparison to Wikipedia where you can come along, and even though it might be written in Markdown or something like it, it seems like the barrier is a lot lower. Similar to you, I always have to look up anything more complicated than the five or six Git commands that I use on a day to day basis. Even to do simple things, I basically just go and follow a recipe which I find on the internet.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: This is how I got introduced to one of the amazing mentors in Kubernetes. Everyone knows him by his handle, Dims. It was my second PR to the Kubernetes website, and I made a mistake. I destroyed the Git history. I could not push my reviews and comments — I addressed them. I couldn't push them back.&lt;/p>
&lt;p>My immediate thought was to delete it and recreate, do another pull request. But then I was like, &amp;quot;what happens to others who have already put effort into reviewing them?&amp;quot; I asked for help, and Dims was there.&lt;/p>
&lt;p>I would say I just got lucky he was there. And he was like, &amp;quot;OK, let me walk you through&amp;quot;. We did troubleshooting through Slack messages. I copied and pasted all the errors. Every single command that he said, I copied and pasted. And then he was like, &amp;quot;OK, run this one. Try this one. And do this one&amp;quot;.&lt;/p>
&lt;p>Finally, I got it fixed. So you know what I did? I went and I stored the command history somewhere local for the next time when I run into this problem. Luckily, I haven't. But I find the contributors so helpful. They are busy. They have a lot of things to do, but they take moments to stop and help someone who's new.&lt;/p>
&lt;p>That is also another part of the reason why I stay — I want to contribute more. It's mainly the community. It's the Kubernetes community. I know you asked me about Git, and I just took the conversation to the Kubernetes community. That's how my brain works.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A lot of people in the community do that and think that's fantastic, obviously, people like Dims who are just floating around on Slack and seem to have endless time. I don't know how they do it.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I really want to know the secret for endless time. If I only had 48 hours in a day. I would sleep for 16 hours, and I would use the rest of the time for doing the things that I want.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: If I had a chance to sleep up to 48 hours a day, I think it'd be a lot more than 16.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Now, one of the areas that you've been contributing to Kubernetes is in the release team. In 1.18, you were a shadow for the docs role. You led that role in 1.19. And you were a release lead shadow for versions 1,20 and 1.21 before finally leading this release, 1.22, which we will talk about soon.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>How did you get involved? And how did you decide which roles to take as you went through that process?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: That is a topic I love to talk about. This was fresh when I started learning about Kubernetes and using Kubernetes at work. And I got so much help from the community, I got interested in contributing back.&lt;/p>
&lt;p>At the first KubeCon that I attended in 2018, in Seattle, they had a speed mentoring session. Now they call it &amp;quot;pod mentoring&amp;quot;. I went to the session, and said, &amp;quot;hey, I want to contribute. I don't know where to start&amp;quot;. And I got a lot of information on how to get started.&lt;/p>
&lt;p>One of the places was SIG Release and the release team. I came back and diligently attended all the SIG Release meetings for four to six months. And in between, I applied to the Kubernetes release team — 1.14 and 1.15. I didn't get through. So I took a little bit of a break, and I focused on doing some documentation work. Then I applied for 1.18.&lt;/p>
&lt;p>Since I was already working on some kinds of — not like full fledged &amp;quot;documentation&amp;quot; documentation, I still don't write. I eventually want to write something really nice and full fledged documentation like other awesome folks.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You'll need a lot more than 48 hours in your day to do that.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: &lt;em>laughing&lt;/em> That's how I applied for the docs role, because I know a little bit about the website. I've done a few pull requests and commits. That's how I got started. I applied for that one role, and I got selected for the 1.18 team. That's how my journey just took off.&lt;/p>
&lt;p>And the next release, I was leading the documentation team. And as everyone knows, the pandemic hit. It was one of the longest releases. I could lean back on the community. I would just wait for the release team meetings.&lt;/p>
&lt;p>It was my way of coping with the pandemic. It took my mind off. It was actually more than a release team, they were people. They were all people first, and we took care of each other. So it felt good.&lt;/p>
&lt;p>And then, I became a release lead shadow for 1.20 and 1.21 because I wanted to know more. I wanted to learn more. I wasn't ready. I still don't feel ready, but I have led 1.22. So if I could do it, anyone could do it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How much of this work is day job?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I am lucky to be blessed with an awesome team. I do most of my work after work, but there have been times where I have to take meetings and attend to immediate urgent stuff. During the time of exception requests and stuff like that, I take a little bit of time from my work.&lt;/p>
&lt;p>My team has been wonderful: they support me in all possible ways, and the management as well. Other than the meetings, I don't do much of the work during the day job. It just takes my focus and attention away too much, and I end up having to spend a lot of time sitting in front of the computer, which I don't like.&lt;/p>
&lt;p>Before the pandemic I had a good work life balance. I'd just go to work at 7:00, 7:30, and I'd be back by 4 o'clock. I never touched my laptop ever again. I left all work behind when I came home. So right now, I'm still learning how to get through.&lt;/p>
&lt;p>I try to limit the amount of open source work that I do during work time. The release lead shadow and the release lead job — they require a lot of time, effort. So on average, I'd be spending two to three hours post work time on the release activities.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Before the pandemic, everyone was worried that if we let people work from home, they wouldn't work enough. I think the opposite has actually happened, is that now we're worried that if we let people work from home, they will just get on the computer in the morning and you'll have to pry it out of their hands at midnight.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah, I think the productivity has increased at least twofold, I would say, for everyone, once they started working from home.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But at the expense of work-life balance, though, because as you say, when you're sitting in the same chair in front of, perhaps, the same computer doing your MathWorks work and then your open source work, they kind of can blur into one perhaps?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: That is a challenge. I face it every day. But so many others are also facing it. I implemented a few little tricks to help me. When I used to come back home from work, the first thing I would do is remove my watch. That was an indication that OK, I'm done.&lt;/p>
&lt;p>That's the thing that I still do. I just remove my watch, and I just keep it right where my workstation is. And I just close the door so that I never look back. Even going past the room, I don't get a glimpse of my work office. I start implementing tiny little things like that to avoid burnout.&lt;/p>
&lt;p>I think I'm still facing a little bit of burnout. I don't know if I have fully recovered from it. I constantly feel like I need a vacation. And I could just take a vacation for like a month or two. If it's possible, I will just do it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I do hope that travel opens up for everyone as an opportunity because I know that, for a lot of people, it's not so much they've been working from home but they've been living at work. The idea of taking vacation effectively means, well, I've been stuck in the same place, if I've been under a lockdown. It's hard to justify that. It will be good as things improve worldwide for us to be able to start focusing more on mental health and perhaps getting away from the &amp;quot;everything room,&amp;quot; as I sometimes call it.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I'm totally looking forward to it. I hope that travel opens up and I could go home and I could meet my siblings and my aunt and my parents.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Catch a cricket match?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah. Probably yes, if I have company and if there is anything interesting happening around the time. I don't mind going back to the Chepauk Stadium and catching a match or two.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's turn now to the recently released &lt;a href="https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/">Kubernetes 1.22&lt;/a>. Congratulations on the launch.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Thank you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Each launch comes with a theme and a mascot or a logo. What is the theme for this release?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: The theme for the release is reaching new peaks. I am fascinated with a lot of space travel and chasing stars, the Milky Way. The best place to do that is over the top of a mountain. So that is the release logo, basically. It's a mountain — Mount Rainier. On top of that, there is a Kubernetes flag, and it's overlooking the Milky Way.&lt;/p>
&lt;p>It's also symbolic that with every release, that we are achieving something new, bigger, and better, and we are making the release awesome. So I just wanted to incorporate that into the team as to say, we are achieving new things with every release. That's the &amp;quot;reaching new peaks&amp;quot; theme.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The last couple of releases have both been incrementally larger — as a result, perhaps, of the fact there are now only three releases per year rather than four. There were also changes to the process, where the work has been driven a lot more by the SIGs than by the release team having to go and ask the SIGs what was going on. What can you say about the size and scope of the 1.22 release?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: The 1.22 release is the largest release to date. We have 56 enhancements if I'm not wrong, and we have a good amount of features that's graduated as stable. You can now say that Kubernetes as a project has become more mature because you see new features coming in. At the same time, you see the features that weren't used getting deprecated — we have like three deprecations in this release.&lt;/p>
&lt;p>Aside from that fact, we also have a big team that's supporting one of the longest releases. This is the first official release cycle after the cadence KEP got approved. Officially, we are at four months, even though 1.19 was six months, and 1.21 was like 3 and 1/2 months, I think, this is the first one after the official KEP approval.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What changes did you make to the process knowing that you had that extra month?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: One of the things the community had asked for is more time for development. We tried to incorporate that in the release schedule. We had about six weeks between the enhancements freeze and the code freeze. That's one.&lt;/p>
&lt;p>It might not be visible to everyone, but one of the things that I wanted to make sure of was the health of the team — since it was a long, long release, we had time to plan out, and not have everyone work during the weekends or during their evenings or time off. That actually helped everyone keep their sanity, and also in making good progress and delivering good results at the end of the release. That's one of the process improvements that I'd call out.&lt;/p>
&lt;p>We got better by making a post during the exception request process. Everyone works around the world. People from the UK start a little earlier than the people in the US East Coast. The West Coast starts three hours later than the East Coast. We used to make a post every Friday evening saying &amp;quot;hey, we actually received this many requests. We have addressed a number of them. We are waiting on a couple, or whatever. All the release team members are done for the day. We will see you around on Monday. Have a good weekend.&amp;quot; Something like that.&lt;/p>
&lt;p>We set the expectations from the community as well. We understand things are really important and urgent, but we are done. This gave everyone their time back. They don't have to worry over the weekend thinking like, hey, what's happening? What's happening in the release? They could spend time with their family, or they could do whatever they want to do, like go on a hike, or just sit and watch TV.&lt;/p>
&lt;p>There have been weekends that I just did that. I just binge-watched a series. That's what I did.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Any recommendations?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I'm a big fan of Marvel, so I have watched the new &lt;a href="https://en.wikipedia.org/wiki/Loki_(TV_series)">Loki&lt;/a>, which I really love. Loki is one of my favourite characters in Marvel. And I also liked &lt;a href="https://en.wikipedia.org/wiki/WandaVision">WandaVision&lt;/a>. That was good, too.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I've not seen Loki yet, but I've heard it described as the best series of Doctor Who in the last few years.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Really?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There must be an element of time-travelling in there if that's how people are describing it.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: You should really go and watch it whenever you have time. It's really amazing. I might go back and watch it again because I might have missed bits and pieces. That always happens in Marvel movies and the episodes; you need to watch them a couple of times to catch, &amp;quot;oh, this is how they relate&amp;quot;.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes, the mark of good media that you want to immediately go back and watch it again once you've seen it.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Let's look now at some of the new features in Kubernetes 1.22. A couple of things that have graduated to general availability — server-side apply, external credential providers, a couple of new security features — the replacement for pod security policy has been announced, and seccomp is now available by default.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Do you have any favourite features in 1.22 that you'd like to discuss?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I have a lot of them. All my favourite features are related to security. OK, one of them is not security, but a major theme of my favourite KEPs is security. I'll start with the &lt;a href="https://github.com/kubernetes/enhancements/issues/2413">default seccomp&lt;/a>. I think it will help make clusters secure by default, and may assist in preventing more vulnerabilities, which means less headaches for the cluster administrators.&lt;/p>
&lt;p>This is close to my heart because the base of the MathWorks platform is provisioning Kubernetes clusters. Knowing that they are secure by default will definitely provide me with some good sleep. And also, I'm paranoid about security most of the time. I'm super interested in making everything secure. It might get in the way of making the users of the platform angry because it's not usable in any way.&lt;/p>
&lt;p>My next one is &lt;a href="https://github.com/kubernetes/enhancements/issues/2033">rootless Kubelet&lt;/a>. That feature's going to enable the cluster admin, the platform developers to deploy Kubernetes components to run in a user namespace. And I think that is also a great addition.&lt;/p>
&lt;p>Like you mention, the most awaited drop in for the PSP replacement is here. It's &lt;a href="https://github.com/kubernetes/enhancements/issues/2579">pod admission control&lt;/a>. It lets cluster admins apply the pod security standards. And I think it's just not related to the cluster admins. I might have to go back and check on that. Anyone can probably use it — the developers and the admins alike.&lt;/p>
&lt;p>It also supports various modes, which is most welcome. There are times where you don't want to just cut the users off because they are trying to do something which is not securely correct. You just want to warn them, hey, this is what you are doing. This might just cause a security issue later, so you might want to correct it. But you just don't want to cut them off from using the platform, or them trying to attempt to do something — deploy their workload and get their day-to-day job done. That is something that I really like, that it also supports a warning mechanism.&lt;/p>
&lt;p>Another one which is not security is &lt;a href="https://github.com/kubernetes/enhancements/issues/2400">node swap support&lt;/a>. Kubernetes didn't have support for swap before, but it is taken into consideration now. This is an alpha feature. With this, you can take advantage of the swap, which is provisioned on the Linux VMs.&lt;/p>
&lt;p>Some of the workloads — when they are deployed, they might need a lot of swap for the start-up — example, like Node and Java applications, which I just took out of their KEP user stories. So if anyone's interested, they can go and look in the KEP. That's useful. And it also increases the node stability and whatnot. So I think it's going to be beneficial for a lot of folks.&lt;/p>
&lt;p>We know how Java and containers work. I think it has gotten better, but five years ago, it was so hard to get a Java application to fit in a small container. It always needed a lot of memory, swap, and everything to start up and run. I think this will help the users and help the admins and keep the cost low, and it will tie into so many other things as well. I'm excited about that feature.&lt;/p>
&lt;p>Another feature that I want to just call out — I don't use Windows that much, but I just want to give a shout out to the folks who are doing an amazing job bringing all the Kubernetes features to Windows as well, to give a seamless experience.&lt;/p>
&lt;p>One of the things is &lt;a href="https://github.com/kubernetes/enhancements/issues/1981">Windows privileged containers&lt;/a>. I think it went alpha this release. And that is a wonderful addition, if you ask me. It can take advantage of whatever that's happening on the Linux side. And they can also port it over and see, OK, I can now run Windows containers in a privileged mode.&lt;/p>
&lt;p>So whatever they are trying to achieve, they can do it. So that's a noteworthy mention. I need to give a shout out for the folks who work and make things happen in the Windows ecosystem as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the things that's great about the release process is the continuity between groups and teams. There's always an emeritus advisor who was a lead from a previous release. One thing that I always ask when I do these interviews is, what is the advice that you give to the next person? When &lt;a href="https://kubernetespodcast.com/episode/146-kubernetes-1.21/">we talked to Nabarun for the 1.21 interview&lt;/a>, he said that his advice to you would be &amp;quot;do, delegate, and defer&amp;quot;. Figure out what you can do, figure out what you can ask other people to do, and figure out what doesn't need to be done. Were you able to take that advice on board?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah, you won't believe it. &lt;a href="https://twitter.com/KubernetesPod/status/1423188323347177474/photo/3">I have it right here stuck to my monitor.&lt;/a>&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Next to your Git cheat sheet?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: &lt;em>laughs&lt;/em> Absolutely. I just have it stuck there. I just took a look at it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Someone that you will have been able to delegate and defer to is Rey Lejano from Rancher Labs and SUSE, who is the release lead to be for 1.23.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I want to tell Rey to beware of the team's mental health. Schedule in such a way that it avoids burnout. Check in, and make sure that everyone is doing good. If they need some kind of help, create a safe space where they can actually ask for help, if they want to step back, if they need someone to cover.&lt;/p>
&lt;p>I think that is most important. The releases are successful based on the thousands and thousands of contributors. But when it comes to a release team, you need to have a healthy team where people feel they are in a good place and they just want to make good contributions, which means they want to be heard. That's one thing that I want to tell Rey.&lt;/p>
&lt;p>Also collaborate and learn from each other. I constantly learn. I think the team was 39 folks, including me. Every day I learned something or the other, even starting from how to interact.&lt;/p>
&lt;p>Sometimes I have learned more leadership skills from my release lead shadows. They are awesome, and they are mature. I constantly learn from them, and I admire them a lot.&lt;/p>
&lt;p>It also helps to have good, strong individuals in the team who can step up and help when needed. For example, unfortunately, we lost one of our teammates after the start of the release cycle. That was tragic. His name was &lt;a href="https://github.com/cncf/memorials/blob/main/peeyush-gupta.md">Peeyush Gupta&lt;/a>. He was an awesome and wonderful human — very warm.&lt;/p>
&lt;p>I didn't get more of a chance to interact with him. I had exchanged a few Slack messages, but I got his warm personality. I just want to take a couple of seconds to remember him. He was awesome.&lt;/p>
&lt;p>After we lost him, we had this strong person from the team step up and lead the communications, who had never been a part of the release team before at all. He was a shadow for the first time. His name is Jesse Butler. So he stepped up, and he just took it away. He ran the comms show for 1.22.&lt;/p>
&lt;p>That's what the community is about. You take care of team members, and the team will take care of you. So that's one other thing that I want to let Rey know, and maybe whoever — I think it's applicable overall.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's a link to a &lt;a href="https://milaap.org/fundraisers/support-peeyush-gupta-family-education">family education fund for Peeyush Gupta&lt;/a>, which you can find in the show notes.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Five releases in a row now you've been a member of the release team. Will you be putting your feet up now for 1.23?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I am going to take a break for a while. In the future, I want to be contributing, if not the release team, the SIG Release and the release management effort. But right now, I have been there for five releases. And I feel like, OK, I just need a little bit of fresh air.&lt;/p>
&lt;p>And also the pandemic and the burnout has caught up, so I'm going to take a break from certain contributions. You will see me in the future. I will be around, but I might not be actively participating in the release team activities. I will be around the community. Anyone can reach out to me. They all know my Slack, so they can just reach out to me via Slack or Twitter.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes, your Twitter handle is CoffeeArtGirl. Does that mean that you'll be spending some time working on your lattes?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I am very bad at making lattes. The coffee art means that I used to &lt;a href="https://twitter.com/KubernetesPod/status/1423188323347177474/photo/1">make art with coffee&lt;/a>. You get instant coffee powder and just mix it with water. You get the colours, very beautiful brown colours. I used to make art using that.&lt;/p>
&lt;p>And I love coffee. So I just combined all the words together. And I had to come up with it in a span of one hour or so because I was joining this 'meet our contributors' panel. And Paris asked me, &amp;quot;do you have a Twitter handle?&amp;quot; I was planning to create one, but I didn't have the time.&lt;/p>
&lt;p>I'm like, well, let me just think what I could just come up with real quick. So I just came up with that. So that's the story behind my Twitter handle. Everyone's interested in it. You are not the first person you have asked me or mentioned about it. So many others are like, why coffee art?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And you are also interested in art with perhaps other materials?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yes. My interests keep changing. I used to do pebble art. It's just collecting pebbles from wherever I go, and I used to paint on them. I used to use watercolour, but I want to come back to watercolour sometime.&lt;/p>
&lt;p>My recent interests are coloured pencils, which came back. When I was very young, I used to do a lot of coloured pencils. And then I switched to watercolours and oil painting. So I just go around in circles.&lt;/p>
&lt;p>One of the hobbies that I picked up during a pandemic is crochet. I made a scarf for Mother's Day. My mum and my dad were here last year. They got stuck because of the pandemic, and they couldn't go back home. So they stayed with me for 10 months. That is the jackpot that I had, that I got to spend so much time with my parents after I moved to the US.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And they got rewarded with a scarf.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One to share between them.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I started making a blanket for my dad. And it became so heavy, I might have to just pick up some lighter yarn. I still don't know the differences between different kinds of yarns, but I'm getting better.&lt;/p>
&lt;p>I started out because I wanted to make these little toys. They call them &lt;a href="https://en.wikipedia.org/wiki/Amigurumi">amigurumi&lt;/a> in the crochet world. I wanted to make them. That's why I started out. I'm trying. I made &lt;a href="https://twitter.com/KubernetesPod/status/1423188323347177474/photo/2">a little cat&lt;/a> which doesn't look like a cat, but it is a cat. I have to tell everyone that it's a cat so that they don't mock me later, but.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's an artistic interpretation of a cat.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: It definitely is!&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/coffeeartgirl">Savitha Raghunathan&lt;/a>, now a Senior Software Engineer at Red Hat, served as the Kubernetes 1.22 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Quality-of-Service for Memory Resources</title><link>https://kubernetes.io/blog/2021/11/26/qos-memory-resources/</link><pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/11/26/qos-memory-resources/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Tim Xu (Tencent Cloud)&lt;/p>
&lt;p>Kubernetes v1.22, released in August 2021, introduced a new alpha feature that improves how Linux nodes implement memory resource requests and limits.&lt;/p>
&lt;p>In prior releases, Kubernetes did not support memory quality guarantees.
For example, if you set container resources as follows:&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: example
spec:
containers:
- name: nginx
resources:
requests:
memory: &amp;quot;64Mi&amp;quot;
cpu: &amp;quot;250m&amp;quot;
limits:
memory: &amp;quot;64Mi&amp;quot;
cpu: &amp;quot;500m&amp;quot;
&lt;/code>&lt;/pre>&lt;p>&lt;code>spec.containers[].resources.requests&lt;/code>(e.g. cpu, memory) is designed for scheduling. When you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum capacity for each of the resource types: the amount of CPU and memory it can provide for Pods. The scheduler ensures that, for each resource type, the sum of the resource requests of the scheduled Containers is less than the capacity of the node.&lt;/p>
&lt;p>&lt;code>spec.containers[].resources.limits&lt;/code> is passed to the container runtime when the kubelet starts a container. CPU is considered a &amp;quot;compressible&amp;quot; resource. If your app starts hitting your CPU limits, Kubernetes starts throttling your container, giving your app potentially worse performance. However, it won’t be terminated. That is what &amp;quot;compressible&amp;quot; means.&lt;/p>
&lt;p>In cgroup v1, and prior to this feature, the container runtime never took into account and effectively ignored spec.containers[].resources.requests[&amp;quot;memory&amp;quot;]. This is unlike CPU, in which the container runtime consider both requests and limits. Furthermore, memory actually can't be compressed in cgroup v1. Because there is no way to throttle memory usage, if a container goes past its memory limit it will be terminated by the kernel with an OOM (Out of Memory) kill.&lt;/p>
&lt;p>Fortunately, cgroup v2 brings a new design and implementation to achieve full protection on memory. The new feature relies on cgroups v2 which most current operating system releases for Linux already provide. With this experimental feature, &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/">quality-of-service for pods and containers&lt;/a> extends to cover not just CPU time but memory as well.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>Memory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes. Memory requests and limits of containers in pod are used to set specific interfaces &lt;code>memory.min&lt;/code> and &lt;code>memory.high&lt;/code> provided by the memory controller. When &lt;code>memory.min&lt;/code> is set to memory requests, memory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures the availability of memory for Kubernetes pods. And if memory limits are set in the container, this means that the system needs to limit container memory usage, Memory QoS uses &lt;code>memory.high&lt;/code> to throttle workload approaching it's memory limit, ensuring that the system is not overwhelmed by instantaneous memory allocation.&lt;/p>
&lt;p>&lt;img src="./memory-qos-cal.svg" alt="">&lt;/p>
&lt;p>The following table details the specific functions of these two parameters and how they correspond to Kubernetes container resources.&lt;/p>
&lt;table>
&lt;tr>
&lt;th style="text-align:center">File&lt;/th>
&lt;th style="text-align:center">Description&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>memory.min&lt;/td>
&lt;td>&lt;code>memory.min&lt;/code> specifies a minimum amount of memory the cgroup must always retain, i.e., memory that can never be reclaimed by the system. If the cgroup's memory usage reaches this low limit and can’t be increased, the system OOM killer will be invoked.
&lt;br>
&lt;br>
&lt;i>We map it to the container's memory request&lt;/i>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory.high&lt;/td>
&lt;td>&lt;code>memory.high&lt;/code> is the memory usage throttle limit. This is the main mechanism to control a cgroup's memory use. If a cgroup's memory use goes over the high boundary specified here, the cgroup’s processes are throttled and put under heavy reclaim pressure. The default is max, meaning there is no limit.
&lt;br>
&lt;br>
&lt;i>We use a formula to calculate &lt;code>memory.high&lt;/code>, depending on container's memory limit or node allocatable memory (if container's memory limit is empty) and a throttling factor. Please refer to the KEP for more details on the formula.&lt;/i>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>When container memory requests are made, kubelet passes &lt;code>memory.min&lt;/code> to the back-end CRI runtime (possibly containerd, cri-o) via the &lt;code>Unified&lt;/code> field in CRI during container creation. The &lt;code>memory.min&lt;/code> in container level cgroup will be set to:&lt;/p>
&lt;p>&lt;img src="./container-memory-min.svg" alt="">&lt;br>
&lt;sub>i: the i&lt;sup>th&lt;/sup> container in one pod&lt;/sub>&lt;/p>
&lt;p>Since the &lt;code>memory.min&lt;/code> interface requires that the ancestor cgroup directories are all set, the pod and node cgroup directories need to be set correctly.&lt;/p>
&lt;p>&lt;code>memory.min&lt;/code> in pod level cgroup:&lt;br>
&lt;img src="./pod-memory-min.svg" alt="">&lt;br>
&lt;sub>i: the i&lt;sup>th&lt;/sup> container in one pod&lt;/sub>&lt;/p>
&lt;p>&lt;code>memory.min&lt;/code> in node level cgroup:&lt;br>
&lt;img src="./node-memory-min.svg" alt="">&lt;br>
&lt;sub>i: the i&lt;sup>th&lt;/sup> pod in one node, j: the j&lt;sup>th&lt;/sup> container in one pod&lt;/sub>&lt;/p>
&lt;p>Kubelet will manage the cgroup hierarchy of the pod level and node level cgroups directly using runc libcontainer library, while container cgroup limits are managed by the container runtime.&lt;/p>
&lt;p>For memory limits, in addition to the original way of limiting memory usage, Memory QoS adds an additional feature of throttling memory allocation. A throttling factor is introduced as a multiplier (default is 0.8). If the result of multiplying memory limits by the factor is greater than memory requests, kubelet will set &lt;code>memory.high&lt;/code> to the value and use &lt;code>Unified&lt;/code> via CRI. And if the container does not specify memory limits, kubelet will use node allocatable memory instead. The &lt;code>memory.high&lt;/code> in container level cgroup is set to:&lt;/p>
&lt;p>&lt;img src="./container-memory-high.svg" alt="">&lt;br>
&lt;sub>i: the i&lt;sup>th&lt;/sup> container in one pod&lt;/sub>&lt;/p>
&lt;p>This can can help improve stability when pod memory usage increases, ensuring that memory is throttled as it approaches the memory limit.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>Here are the prerequisites for enabling Memory QoS on your Linux node, some of these are related to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2254-cgroup-v2">Kubernetes support for cgroup v2&lt;/a>.&lt;/p>
&lt;ol>
&lt;li>Kubernetes since v1.22&lt;/li>
&lt;li>&lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> since v1.0.0-rc93; &lt;a href="https://containerd.io/">containerd&lt;/a> since 1.4; &lt;a href="https://cri-o.io/">cri-o&lt;/a> since 1.20&lt;/li>
&lt;li>Linux kernel minimum version: 4.15, recommended version: 5.2+&lt;/li>
&lt;li>Linux image with cgroupv2 enabled or enabling cgroupv2 unified_cgroup_hierarchy manually&lt;/li>
&lt;/ol>
&lt;p>OCI runtimes such as runc and crun already support cgroups v2 &lt;a href="https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#unified">&lt;code>Unified&lt;/code>&lt;/a>, and Kubernetes CRI has also made the desired changes to support passing &lt;a href="https://github.com/kubernetes/kubernetes/pull/102578">&lt;code>Unified&lt;/code>&lt;/a>. However, CRI Runtime support is required as well. Memory QoS in Alpha phase is designed to support containerd and cri-o. Related PR &lt;a href="https://github.com/containerd/containerd/pull/5627">Feature: containerd-cri support LinuxContainerResources.Unified #5627&lt;/a> has been merged and will be released in containerd 1.6. CRI-O &lt;a href="https://github.com/cri-o/cri-o/pull/5207">implement kube alpha features for 1.22 #5207&lt;/a> is still in WIP.&lt;/p>
&lt;p>With those prerequisites met, you can enable the memory QoS feature gate (see &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">Set kubelet parameters via a config file&lt;/a>).&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>You can find more details as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2570-memory-qos/#readme">Support Memory QoS with cgroup v2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2254-cgroup-v2/#readme">cgroup v2&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>You can reach SIG Node by several means:&lt;/p>
&lt;ul>
&lt;li>Slack: &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fnode">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>You can also contact me directly:&lt;/p>
&lt;ul>
&lt;li>GitHub / Slack: @xiaoxubeii&lt;/li>
&lt;li>Email: &lt;a href="mailto:xiaoxubeii@gmail.com">xiaoxubeii@gmail.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Dockershim removal is coming. Are you ready?</title><link>https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sergey Kanzhelev, Google. With reviews from Davanum Srinivas, Elana Hashman, Noah Kantrowitz, Rey Lejano.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Poll closed&lt;/h4>
This poll closed on January 7, 2022.
&lt;/div>
&lt;p>Last year we &lt;a href="https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">announced&lt;/a>
that Kubernetes' dockershim component (which provides a built-in integration for
Docker Engine) is deprecated.&lt;/p>
&lt;p>&lt;em>Update: There's a &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Dockershim Deprecation FAQ&lt;/a>
with more information, and you can also discuss the deprecation via a dedicated
&lt;a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub issue&lt;/a>.&lt;/em>&lt;/p>
&lt;p>Our current plan is to remove dockershim from the Kubernetes codebase soon.
We are looking for feedback from you whether you are ready for dockershim
removal and to ensure that you are ready when the time comes.&lt;/p>
&lt;p>&lt;del>Please fill out this survey: &lt;a href="https://forms.gle/svCJmhvTv78jGdSx8">https://forms.gle/svCJmhvTv78jGdSx8&lt;/a>&lt;/del>&lt;/p>
&lt;p>The dockershim component that enables Docker as a Kubernetes container runtime is
being deprecated in favor of runtimes that directly use the &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface&lt;/a>
created for Kubernetes. Many Kubernetes users have migrated to
other container runtimes without problems. However we see that dockershim is
still very popular. You may see some public numbers in recent &lt;a href="https://www.datadoghq.com/container-report/#8">Container Report&lt;/a> from DataDog.
Some Kubernetes hosting vendors just recently enabled other runtimes support
(especially for Windows nodes). And we know that many third party tools vendors
are still not ready: &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/#telemetry-and-security-agent-vendors">migrating telemetry and security agents&lt;/a>.&lt;/p>
&lt;p>At this point, we believe that there is feature parity between Docker and the
other runtimes. Many end-users have used our &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">migration guide&lt;/a>
and are running production workload using these different runtimes. The plan of
record today is that dockershim will be removed in version 1.24, slated for
release around April of next year. For those developing or running alpha and
beta versions, dockershim will be removed in December at the beginning of the
1.24 release development cycle.&lt;/p>
&lt;p>There is only one month left to give us feedback. We want you to tell us how
ready you are.&lt;/p>
&lt;p>&lt;del>We are collecting opinions through this survey: &lt;a href="https://forms.gle/svCJmhvTv78jGdSx8">https://forms.gle/svCJmhvTv78jGdSx8&lt;/a>&lt;/del>
To better understand preparedness for the dockershim removal, our survey is
asking the version of Kubernetes you are currently using, and an estimate of
when you think you will adopt Kubernetes 1.24. All the aggregated information
on dockershim removal readiness will be published.
Free form comments will be reviewed by SIG Node leadership. If you want to
discuss any details of migrating from dockershim, report bugs or adoption
blockers, you can use one of the SIG Node contact options any time:
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">https://github.com/kubernetes/community/tree/master/sig-node#contact&lt;/a>&lt;/p>
&lt;p>Kubernetes is a mature project. This deprecation is another
step in the effort to get away from permanent beta features and providing more
stability and compatibility guarantees. With the migration from dockershim you
will get more flexibility and choice of container runtime features as well as
less dependencies of your apps on specific underlying technology. Please take
time to review the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">dockershim migration documentation&lt;/a>
and consult your Kubernetes hosting vendor (if you have one) what container runtime options are available for you.
Read up &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#container-runtimes">container runtime documentation with instructions on how to use containerd and CRI-O&lt;/a>
to help prepare you when you're ready to upgrade to 1.24. CRI-O, containerd, and
Docker with &lt;a href="https://github.com/Mirantis/cri-dockerd">Mirantis cri-dockerd&lt;/a> are
not the only container runtime options, we encourage you to explore the &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape on container runtimes&lt;/a>
in case another suits you better.&lt;/p>
&lt;p>Thank you!&lt;/p></description></item><item><title>Blog: Non-root Containers And Devices</title><link>https://kubernetes.io/blog/2021/11/09/non-root-containers-and-devices/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/11/09/non-root-containers-and-devices/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Mikko Ylinen (Intel)&lt;/p>
&lt;p>The user/group ID related security settings in Pod's &lt;code>securityContext&lt;/code> trigger a problem when users want to
deploy containers that use accelerator devices (via &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Kubernetes Device Plugins&lt;/a>) on Linux. In this blog
post I talk about the problem and describe the work done so far to address it. It's not meant to be a long story about getting the &lt;a href="https://github.com/kubernetes/kubernetes/issues/92211">k/k issue&lt;/a> fixed.&lt;/p>
&lt;p>Instead, this post aims to raise awareness of the issue and to highlight important device use-cases too. This is needed as Kubernetes works on new related features such as support for user namespaces.&lt;/p>
&lt;h2 id="why-non-root-containers-can-t-use-devices-and-why-it-matters">Why non-root containers can't use devices and why it matters&lt;/h2>
&lt;p>One of the key security principles for running containers in Kubernetes is the
principle of least privilege. The Pod/container &lt;code>securityContext&lt;/code> specifies the config
options to set, e.g., Linux capabilities, MAC policies, and user/group ID values to achieve this.&lt;/p>
&lt;p>Furthermore, the cluster admins are supported with tools like &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/">PodSecurityPolicy&lt;/a> (deprecated) or
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a> (alpha) to enforce the desired security settings for pods that are being deployed in
the cluster. These settings could, for instance, require that containers must be &lt;code>runAsNonRoot&lt;/code> or
that they are forbidden from running with root's group ID in &lt;code>runAsGroup&lt;/code> or &lt;code>supplementalGroups&lt;/code>.&lt;/p>
&lt;p>In Kubernetes, the kubelet builds the list of &lt;a href="https://pkg.go.dev/k8s.io/cri-api@v0.22.1/pkg/apis/runtime/v1#Device">&lt;code>Device&lt;/code>&lt;/a> resources to be made available to a container
(based on inputs from the Device Plugins) and the list is included in the CreateContainer CRI message
sent to the CRI container runtime. Each &lt;code>Device&lt;/code> contains little information: host/container device
paths and the desired devices cgroups permissions.&lt;/p>
&lt;p>The &lt;a href="https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md">OCI Runtime Spec for Linux Container Configuration&lt;/a>
expects that in addition to the devices cgroup fields, more detailed information about the devices
must be provided:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;type&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;lt;string&amp;gt;&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;path&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;lt;string&amp;gt;&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;major&amp;#34;: &lt;/span>&amp;lt;int64&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;minor&amp;#34;: &lt;/span>&amp;lt;int64&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;fileMode&amp;#34;: &lt;/span>&amp;lt;uint32&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;uid&amp;#34;: &lt;/span>&amp;lt;uint32&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;gid&amp;#34;: &lt;/span>&amp;lt;uint32&amp;gt;&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>},&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The CRI container runtimes (containerd, CRI-O) are responsible for obtaining this information
from the host for each &lt;code>Device&lt;/code>. By default, the runtimes copy the host device's user and group IDs:&lt;/p>
&lt;ul>
&lt;li>&lt;code>uid&lt;/code> (uint32, OPTIONAL) - id of device owner in the container namespace.&lt;/li>
&lt;li>&lt;code>gid&lt;/code> (uint32, OPTIONAL) - id of device group in the container namespace.&lt;/li>
&lt;/ul>
&lt;p>Similarly, the runtimes prepare other mandatory &lt;code>config.json&lt;/code> sections based on the CRI fields,
including the ones defined in &lt;code>securityContext&lt;/code>: &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code>, which become part of the POSIX
platforms user structure via:&lt;/p>
&lt;ul>
&lt;li>&lt;code>uid&lt;/code> (int, REQUIRED) specifies the user ID in the container namespace.&lt;/li>
&lt;li>&lt;code>gid&lt;/code> (int, REQUIRED) specifies the group ID in the container namespace.&lt;/li>
&lt;li>&lt;code>additionalGids&lt;/code> (array of ints, OPTIONAL) specifies additional group IDs in the container namespace to be added to the process.&lt;/li>
&lt;/ul>
&lt;p>However, the resulting &lt;code>config.json&lt;/code> triggers a problem when trying to run containers with
both devices added and with non-root uid/gid set via &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code>: the container user process
has no permission to use the device even when its group id (gid, copied from host) was permissive to
non-root groups. This is because the container user does not belong to that host group (e.g., via &lt;code>additionalGids&lt;/code>).&lt;/p>
&lt;p>Being able to run applications that use devices as non-root user is normal and expected to work so that
the security principles can be met. Therefore, several alternatives were considered to get the gap filled with what the PodSec/CRI/OCI supports today.&lt;/p>
&lt;h2 id="what-was-done-to-solve-the-issue">What was done to solve the issue?&lt;/h2>
&lt;p>You might have noticed from the problem definition that it would at least be possible to workaround
the problem by manually adding the device gid(s) to &lt;code>supplementalGroups&lt;/code>, or in
the case of just one device, set &lt;code>runAsGroup&lt;/code> to the device's group id. However, this is problematic because the device gid(s) may have
different values depending on the nodes' distro/version in the cluster. For example, with GPUs the following commands for different distros and versions return different gids:&lt;/p>
&lt;p>Fedora 33:&lt;/p>
&lt;pre>&lt;code>$ ls -l /dev/dri/
total 0
drwxr-xr-x. 2 root root 80 19.10. 10:21 by-path
crw-rw----+ 1 root video 226, 0 19.10. 10:42 card0
crw-rw-rw-. 1 root render 226, 128 19.10. 10:21 renderD128
$ grep -e video -e render /etc/group
video:x:39:
render:x:997:
&lt;/code>&lt;/pre>&lt;p>Ubuntu 20.04:&lt;/p>
&lt;pre>&lt;code>$ ls -l /dev/dri/
total 0
drwxr-xr-x 2 root root 80 19.10. 17:36 by-path
crw-rw---- 1 root video 226, 0 19.10. 17:36 card0
crw-rw---- 1 root render 226, 128 19.10. 17:36 renderD128
$ grep -e video -e render /etc/group
video:x:44:
render:x:133:
&lt;/code>&lt;/pre>&lt;p>Which number to choose in your &lt;code>securityContext&lt;/code>? Also, what if the &lt;code>runAsGroup&lt;/code>/&lt;code>runAsUser&lt;/code> values cannot be hard-coded because
they are automatically assigned during pod admission time via external security policies?&lt;/p>
&lt;p>Unlike volumes with &lt;code>fsGroup&lt;/code>, the devices have no official notion of &lt;code>deviceGroup&lt;/code>/&lt;code>deviceUser&lt;/code> that the CRI runtimes (or kubelet)
would be able to use. We considered using container annotations set by the device plugins (e.g., &lt;code>io.kubernetes.cri.hostDeviceSupplementalGroup/&lt;/code>) to get custom OCI &lt;code>config.json&lt;/code> uid/gid values.
This would have required changes to all existing device plugins which was not ideal.&lt;/p>
&lt;p>Instead, a solution that is &lt;em>seamless&lt;/em> to end-users without getting the device plugin vendors involved was preferred. The selected approach was
to re-use &lt;code>runAsUser&lt;/code> and &lt;code>runAsGroup&lt;/code> values in &lt;code>config.json&lt;/code> for devices:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;type&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;c&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;path&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;/dev/foo&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;major&amp;#34;: &lt;/span>&lt;span style="color:#666">123&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;minor&amp;#34;: &lt;/span>&lt;span style="color:#666">4&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;fileMode&amp;#34;: &lt;/span>&lt;span style="color:#666">438&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;uid&amp;#34;: &lt;/span>&amp;lt;runAsUser&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;gid&amp;#34;: &lt;/span>&amp;lt;runAsGroup&amp;gt;&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>},&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With &lt;code>runc&lt;/code> OCI runtime (in non-rootless mode), the device is created (&lt;code>mknod(2)&lt;/code>) in
the container namespace and the ownership is changed to &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code> using &lt;code>chmod(2)&lt;/code>.&lt;/p>
&lt;p>&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/">Rootless mode&lt;/a> and devices is not supported.
&lt;/div>
Having the ownership updated in the container namespace is justified as the user process is the only one accessing the device. Only &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code>
are taken into account, and, e.g., the &lt;code>USER&lt;/code> setting in the container is currently ignored.&lt;/p>
&lt;p>While it is likely that the &amp;quot;faulty&amp;quot; deployments (i.e., non-root &lt;code>securityContext&lt;/code> + devices) do not exist, to be absolutely sure no
deployments break, an opt-in config entry in both containerd and CRI-O to enable the new behavior was added. The following:&lt;/p>
&lt;p>&lt;code>device_ownership_from_security_context (bool)&lt;/code>&lt;/p>
&lt;p>defaults to &lt;code>false&lt;/code> and must be enabled to use the feature.&lt;/p>
&lt;h2 id="see-non-root-containers-using-devices-after-the-fix">See non-root containers using devices after the fix&lt;/h2>
&lt;p>To demonstrate the new behavior, let's use a Data Plane Development Kit (DPDK) application using hardware accelerators, Kubernetes CPU manager, and HugePages as an example. The cluster runs containerd with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-toml" data-lang="toml">[plugins]
[plugins.&lt;span style="color:#b44">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>]
device_ownership_from_security_context = &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>or CRI-O with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-toml" data-lang="toml">[crio.runtime]
device_ownership_from_security_context = &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>and the &lt;code>Guaranteed&lt;/code> QoS Class Pod that runs DPDK's crypto-perf test utility with this YAML:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">...&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>qat-dpdk&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsUser&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>crypto-perf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>intel/crypto-perf:devel&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;3&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;128Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">qat.intel.com/generic&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;4&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hugepages-2Mi&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;128Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;3&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;128Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">qat.intel.com/generic&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;4&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hugepages-2Mi&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;128Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To verify the results, check the user and group ID that the container runs as:&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it qat-dpdk -c crypto-perf -- id
&lt;/code>&lt;/pre>&lt;p>They are set to non-zero values as expected:&lt;/p>
&lt;pre>&lt;code>uid=1000 gid=2000 groups=2000,3000
&lt;/code>&lt;/pre>&lt;p>Next, check the device node permissions (&lt;code>qat.intel.com/generic&lt;/code> exposes &lt;code>/dev/vfio/&lt;/code> devices) are accessible to &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it qat-dpdk -c crypto-perf -- ls -la /dev/vfio
total 0
drwxr-xr-x 2 root root 140 Sep 7 10:55 .
drwxr-xr-x 7 root root 380 Sep 7 10:55 ..
crw------- 1 1000 2000 241, 0 Sep 7 10:55 58
crw------- 1 1000 2000 241, 2 Sep 7 10:55 60
crw------- 1 1000 2000 241, 10 Sep 7 10:55 68
crw------- 1 1000 2000 241, 11 Sep 7 10:55 69
crw-rw-rw- 1 1000 2000 10, 196 Sep 7 10:55 vfio
&lt;/code>&lt;/pre>&lt;p>Finally, check the non-root container is also allowed to create HugePages:&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it qat-dpdk -c crypto-perf -- ls -la /dev/hugepages/
&lt;/code>&lt;/pre>&lt;p>&lt;code>fsGroup&lt;/code> gives a &lt;code>runAsUser&lt;/code> writable HugePages emptyDir mountpoint:&lt;/p>
&lt;pre>&lt;code>total 0
drwxrwsr-x 2 root 3000 0 Sep 7 10:55 .
drwxr-xr-x 7 root root 380 Sep 7 10:55 ..
&lt;/code>&lt;/pre>&lt;h2 id="help-us-test-it-and-provide-feedback">Help us test it and provide feedback!&lt;/h2>
&lt;p>The functionality described here is expected to help with cluster security and the configurability of device permissions. To allow
non-root containers to use devices requires cluster admins to opt-in to the functionality by setting
&lt;code>device_ownership_from_security_context = true&lt;/code>. To make it a default setting, please test it and provide your feedback (via SIG-Node meetings or issues)!
The flag is available in CRI-O v1.22 release and queued for containerd v1.6.&lt;/p>
&lt;p>More work is needed to get it &lt;em>properly&lt;/em> supported. It is known to work with &lt;code>runc&lt;/code> but it also needs to be made to function
with other OCI runtimes too, where applicable. For instance, Kata Containers supports device passthrough and allows it to make devices
available to containers in VM sandboxes too.&lt;/p>
&lt;p>Moreover, the additional challenge comes with support of user names and devices. This problem is still &lt;a href="https://github.com/kubernetes/enhancements/pull/2101">open&lt;/a>
and requires more brainstorming.&lt;/p>
&lt;p>Finally, it needs to be understood whether &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code> are enough or if device specific settings similar to &lt;code>fsGroups&lt;/code> are needed in PodSpec/CRI v2.&lt;/p>
&lt;h2 id="thanks">Thanks&lt;/h2>
&lt;p>My thanks goes to Mike Brown (IBM, containerd), Peter Hunt (Redhat, CRI-O), and Alexander Kanevskiy (Intel) for providing all the feedback and good conversations.&lt;/p></description></item><item><title>Blog: Announcing the 2021 Steering Committee Election Results</title><link>https://kubernetes.io/blog/2021/11/08/steering-committee-results-2021/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/11/08/steering-committee-results-2021/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Kaslin Fields&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes/community/tree/master/events/elections/2021">2021 Steering Committee Election&lt;/a> is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2021. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.&lt;/p>
&lt;p>This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their &lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">charter&lt;/a>.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Stephen Augustus (&lt;a href="https://github.com/justaugustus">@justaugustus&lt;/a>), Cisco&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>), Apple&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Tim Pepper (&lt;a href="https://github.com/tpepper">@tpepper&lt;/a>), VMware&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>They join continuing members:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>), VMware&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Jordan Liggitt (&lt;a href="https://github.com/liggitt">@liggitt&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Bob Killen (&lt;a href="https://github.com/mrbobbytables">@mrbobbytables&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Paris Pittman and Christoph Blecker are returning Steering Committee Members.&lt;/p>
&lt;h2 id="big-thanks">Big Thanks&lt;/h2>
&lt;p>Thank you and congratulations on a successful election to this round’s election officers:&lt;/p>
&lt;ul>
&lt;li>Alison Dowdney, (&lt;a href="https://github.com/alisondy">@alisondy&lt;/a>)&lt;/li>
&lt;li>Noah Kantrowitz (&lt;a href="https://github.com/coderanger">@coderanger&lt;/a>)&lt;/li>
&lt;li>Josh Berkus (&lt;a href="https://github.com/jberkus">@jberkus&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>Special thanks to Arnaud Meukam (&lt;a href="https://github.com/ameukam">@ameukam&lt;/a>), k8s-infra liaison, who enabled our voting software on community-owned infrastructure.&lt;/p>
&lt;p>Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:&lt;/p>
&lt;ul>
&lt;li>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>)&lt;/li>
&lt;li>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>And thank you to all the candidates who came forward to run for election.&lt;/p>
&lt;h2 id="get-involved-with-the-steering-committee">Get Involved with the Steering Committee&lt;/h2>
&lt;p>This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee &lt;a href="https://github.com/kubernetes/steering/projects/1">backlog items&lt;/a> and weigh in by filing an issue or creating a PR against their &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>. They have an open meeting on &lt;a href="https://github.com/kubernetes/steering">the first Monday at 9:30am PT of every month&lt;/a> and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a>.&lt;/p>
&lt;p>You can see what the Steering Committee meetings are all about by watching past meetings on the &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>This post was written by the &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing">Upstream Marketing Working Group&lt;/a>. If you want to write stories about the Kubernetes community, learn more about us.&lt;/em>&lt;/p></description></item><item><title>Blog: Use KPNG to Write Specialized kube-proxiers</title><link>https://kubernetes.io/blog/2021/10/18/use-kpng-to-write-specialized-kube-proxiers/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/10/18/use-kpng-to-write-specialized-kube-proxiers/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Lars Ekman (Ericsson)&lt;/p>
&lt;p>The post will show you how to create a specialized service kube-proxy
style network proxier using Kubernetes Proxy NG
&lt;a href="https://github.com/kubernetes-sigs/kpng">kpng&lt;/a> without interfering
with the existing kube-proxy. The kpng project aims at renewing the
the default Kubernetes Service implementation, the &amp;quot;kube-proxy&amp;quot;. An
important feature of kpng is that it can be used as a library to
create proxiers outside K8s. While this is useful for CNI-plugins that
replaces the kube-proxy it also opens the possibility for anyone to
create a proxier for a special purpose.&lt;/p>
&lt;h2 id="define-a-service-that-uses-a-specialized-proxier">Define a service that uses a specialized proxier&lt;/h2>
&lt;pre>&lt;code>apiVersion: v1
kind: Service
metadata:
name: kpng-example
labels:
service.kubernetes.io/service-proxy-name: kpng-example
spec:
clusterIP: None
ipFamilyPolicy: RequireDualStack
externalIPs:
- 10.0.0.55
- 1000::55
selector:
app: kpng-alpine
ports:
- port: 6000
&lt;/code>&lt;/pre>&lt;p>If the &lt;code>service.kubernetes.io/service-proxy-name&lt;/code> label is defined the
&lt;code>kube-proxy&lt;/code> will ignore the service. A custom controller can watch
services with the label set to it's own name, &amp;quot;kpng-example&amp;quot; in
this example, and setup specialized load-balancing.&lt;/p>
&lt;p>The &lt;code>service.kubernetes.io/service-proxy-name&lt;/code> label is &lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#servicekubernetesioservice-proxy-name">not
new&lt;/a>,
but so far is has been quite hard to write a specialized proxier.&lt;/p>
&lt;p>The common use for a specialized proxier is assumed to be handling
external traffic for some use-case not supported by K8s. In that
case &lt;code>ClusterIP&lt;/code> is not needed, so we use a &amp;quot;headless&amp;quot; service in this
example.&lt;/p>
&lt;h2 id="specialized-proxier-using-kpng">Specialized proxier using kpng&lt;/h2>
&lt;p>A &lt;a href="https://github.com/kubernetes-sigs/kpng">kpng&lt;/a> based proxier
consists of the &lt;code>kpng&lt;/code> controller handling all the K8s api related
functions, and a &amp;quot;backend&amp;quot; implementing the load-balancing. The
backend can be linked with the &lt;code>kpng&lt;/code> controller binary or be a
separate program communicating with the controller using gRPC.&lt;/p>
&lt;pre>&lt;code>kpng kube --service-proxy-name=kpng-example to-api
&lt;/code>&lt;/pre>&lt;p>This starts the &lt;code>kpng&lt;/code> controller and tell it to watch only services
with the &amp;quot;kpng-example&amp;quot; service proxy name. The &amp;quot;to-api&amp;quot; parameter
will open a gRPC server for backends.&lt;/p>
&lt;p>You can test this yourself outside your cluster. Please see the example
below.&lt;/p>
&lt;p>Now we start a backend that simply prints the updates from the
controller.&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f kpng-example.yaml
$ kpng-json | jq # (this is the backend)
{
&amp;quot;Service&amp;quot;: {
&amp;quot;Namespace&amp;quot;: &amp;quot;default&amp;quot;,
&amp;quot;Name&amp;quot;: &amp;quot;kpng-example&amp;quot;,
&amp;quot;Type&amp;quot;: &amp;quot;ClusterIP&amp;quot;,
&amp;quot;IPs&amp;quot;: {
&amp;quot;ClusterIPs&amp;quot;: {},
&amp;quot;ExternalIPs&amp;quot;: {
&amp;quot;V4&amp;quot;: [
&amp;quot;10.0.0.55&amp;quot;
],
&amp;quot;V6&amp;quot;: [
&amp;quot;1000::55&amp;quot;
]
},
&amp;quot;Headless&amp;quot;: true
},
&amp;quot;Ports&amp;quot;: [
{
&amp;quot;Protocol&amp;quot;: 1,
&amp;quot;Port&amp;quot;: 6000,
&amp;quot;TargetPort&amp;quot;: 6000
}
]
},
&amp;quot;Endpoints&amp;quot;: [
{
&amp;quot;IPs&amp;quot;: {
&amp;quot;V6&amp;quot;: [
&amp;quot;1100::202&amp;quot;
]
},
&amp;quot;Local&amp;quot;: true
},
{
&amp;quot;IPs&amp;quot;: {
&amp;quot;V4&amp;quot;: [
&amp;quot;11.0.2.2&amp;quot;
]
},
&amp;quot;Local&amp;quot;: true
},
{
&amp;quot;IPs&amp;quot;: {
&amp;quot;V4&amp;quot;: [
&amp;quot;11.0.1.2&amp;quot;
]
}
},
{
&amp;quot;IPs&amp;quot;: {
&amp;quot;V6&amp;quot;: [
&amp;quot;1100::102&amp;quot;
]
}
}
]
}
&lt;/code>&lt;/pre>&lt;p>A real backend would use some mechanism to load-balance traffic from
the external IPs to the endpoints.&lt;/p>
&lt;h2 id="writing-a-backend">Writing a backend&lt;/h2>
&lt;p>The &lt;code>kpng-json&lt;/code> backend looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> main
&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#b44">&amp;#34;os&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;encoding/json&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;sigs.k8s.io/kpng/client&amp;#34;&lt;/span>
)
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">main&lt;/span>() {
client.&lt;span style="color:#00a000">Run&lt;/span>(jsonPrint)
}
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">jsonPrint&lt;/span>(items []&lt;span style="color:#666">*&lt;/span>client.ServiceEndpoints) {
enc &lt;span style="color:#666">:=&lt;/span> json.&lt;span style="color:#00a000">NewEncoder&lt;/span>(os.Stdout)
&lt;span style="color:#a2f;font-weight:bold">for&lt;/span> _, item &lt;span style="color:#666">:=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">range&lt;/span> items {
_ = enc.&lt;span style="color:#00a000">Encode&lt;/span>(item)
}
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>(yes, that is the entire program)&lt;/p>
&lt;p>A real backend would of course be much more complex, but this
illustrates how &lt;code>kpng&lt;/code> let you focus on load-balancing.&lt;/p>
&lt;p>You can have several backends connected to a &lt;code>kpng&lt;/code> controller, so
during development or debug it can be useful to let something like the
&lt;code>kpng-json&lt;/code> backend run in parallel with your real backend.&lt;/p>
&lt;h2 id="example">Example&lt;/h2>
&lt;p>The complete example can be found &lt;a href="https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec">here&lt;/a>.&lt;/p>
&lt;p>As an example we implement an &amp;quot;all-ip&amp;quot; backend. It direct all traffic
for the externalIPs to a local endpoint, regardless of ports and upper
layer protocols. There is a
&lt;a href="https://github.com/kubernetes/enhancements/pull/2611">KEP&lt;/a> for this
function and this example is a much simplified version.&lt;/p>
&lt;p>To direct all traffic from an external address to a local POD &lt;a href="https://github.com/kubernetes/enhancements/pull/2611#issuecomment-895061013">only
one iptables rule is
needed&lt;/a>,
for instance;&lt;/p>
&lt;pre>&lt;code>ip6tables -t nat -A PREROUTING -d 1000::55/128 -j DNAT --to-destination 1100::202
&lt;/code>&lt;/pre>&lt;p>As you can see the addresses are in the call to the backend and all it
have to do is:&lt;/p>
&lt;ul>
&lt;li>Extract the addresses with &lt;code>Local: true&lt;/code>&lt;/li>
&lt;li>Setup iptables rules for the &lt;code>ExternalIPs&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>A script doing that may look like:&lt;/p>
&lt;pre>&lt;code>xip=$(cat /tmp/out | jq -r .Service.IPs.ExternalIPs.V6[0])
podip=$(cat /tmp/out | jq -r '.Endpoints[]|select(.Local == true)|select(.IPs.V6 != null)|.IPs.V6[0]')
ip6tables -t nat -A PREROUTING -d $xip/128 -j DNAT --to-destination $podip
&lt;/code>&lt;/pre>&lt;p>Assuming the JSON output above is stored in &lt;code>/tmp/out&lt;/code> (&lt;a href="https://stedolan.github.io/jq/">jq&lt;/a> is an &lt;em>awesome&lt;/em> program!).&lt;/p>
&lt;p>As this is an example we make it really simple for ourselves by using
a minor variation of the &lt;code>kpng-json&lt;/code> backend above. Instead of just
printing, a program is called and the JSON output is passed as &lt;code>stdin&lt;/code>
to that program. The backend can be tested stand-alone:&lt;/p>
&lt;pre>&lt;code>CALLOUT=jq kpng-callout
&lt;/code>&lt;/pre>&lt;p>Where &lt;code>jq&lt;/code> can be replaced with your own program or script. A script
may look like the example above. For more info and the complete
example please see &lt;a href="https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec">https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec&lt;/a>.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>While &lt;a href="https://github.com/kubernetes-sigs/kpng">kpng&lt;/a> is in early
stage of development this post wants to show how you may build your
own specialized K8s proxiers in the future. The only thing your
applications need to do is to add the
&lt;code>service.kubernetes.io/service-proxy-name&lt;/code> label in the Service
manifest.&lt;/p>
&lt;p>It is a tedious process to get new features into the &lt;code>kube-proxy&lt;/code> and
it is not unlikely that they will be rejected, so to write a
specialized proxier may be the only option.&lt;/p></description></item><item><title>Blog: Introducing ClusterClass and Managed Topologies in Cluster API</title><link>https://kubernetes.io/blog/2021/10/08/capi-clusterclass-and-managed-topologies/</link><pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/10/08/capi-clusterclass-and-managed-topologies/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Fabrizio Pandini (VMware)&lt;/p>
&lt;p>The &lt;a href="https://cluster-api.sigs.k8s.io/">Cluster API community&lt;/a> is happy to announce the implementation of &lt;em>ClusterClass and Managed Topologies&lt;/em>, a new feature that will greatly simplify how you can provision, upgrade, and operate multiple Kubernetes clusters in a declarative way.&lt;/p>
&lt;h2 id="a-little-bit-of-context">A little bit of context…&lt;/h2>
&lt;p>Before getting into the details, let's take a step back and look at the history of Cluster API.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/cluster-api/">Cluster API project&lt;/a> started three years ago, and the first releases focused on extensibility and implementing a declarative API that allows a seamless experience across infrastructure providers. This was a success with many cloud providers: AWS, Azure, Digital Ocean, GCP, Metal3, vSphere and still counting.&lt;/p>
&lt;p>With extensibility addressed, the focus shifted to features, like automatic control plane and etcd management, health-based machine remediation, machine rollout strategies and more.&lt;/p>
&lt;p>Fast forwarding to 2021, with lots of companies using Cluster API to manage fleets of Kubernetes clusters running workloads in production, the community focused its effort on stabilization of both code, APIs, documentation, and on extensive test signals which inform Kubernetes releases.&lt;/p>
&lt;p>With solid foundations in place, and a vibrant and welcoming community that still continues to grow, it was time to plan another iteration on our UX for both new and advanced users.&lt;/p>
&lt;p>Enter ClusterClass and Managed Topologies, tada!&lt;/p>
&lt;h2 id="clusterclass">ClusterClass&lt;/h2>
&lt;p>As the name suggests, ClusterClass and managed topologies are built in two parts.&lt;/p>
&lt;p>The idea behind ClusterClass is simple: define the shape of your cluster once, and reuse it many times, abstracting the complexities and the internals of a Kubernetes cluster away.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-10-08-clusterclass-and-managed-topologies/clusterclass.svg" alt="Defining a ClusterClass">&lt;/p>
&lt;p>ClusterClass, at its heart, is a collection of Cluster and Machine templates. You can use it as a “stamp” that can be leveraged to create many clusters of a similar shape.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-amazing-cluster-class&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>controlplane.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmControlPlaneTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>high-availability-control-plane&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">machineInfrastructure&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>control-plane-machine&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">workers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">machineDeployments&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type1-workers&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">bootstrap&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bootstrap.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmConfigTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type1-bootstrap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">infrastructure&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type1-machine&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type2-workers&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">bootstrap&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bootstrap.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmConfigTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type2-bootstrap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">infrastructure&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type2-machine&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">infrastructure&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerClusterTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster-infrastructure&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The possibilities are endless; you can get a default ClusterClass from the community, “off-the-shelf” classes from your vendor of choice, “certified” classes from the platform admin in your company, or even create custom ones for advanced scenarios.&lt;/p>
&lt;h2 id="managed-topologies">Managed Topologies&lt;/h2>
&lt;p>Managed Topologies let you put the power of ClusterClass into action.&lt;/p>
&lt;p>Given a ClusterClass, you can create many Clusters of a similar shape by providing a single resource, the Cluster.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-10-08-clusterclass-and-managed-topologies/create-cluster.svg" alt="Create a Cluster with ClusterClass">&lt;/p>
&lt;p>Here is an example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Cluster&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-amazing-cluster&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">topology&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># define a managed topology&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-amazing-cluster-class&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># use the ClusterClass mentioned earlier&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1.21.2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">workers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">machineDeployments&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type1-workers&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>big-pool-of-machines&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">5&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type2-workers&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>small-pool-of-machines&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But there is more than simplified cluster creation. Now the Cluster acts as a single control point for your entire topology.&lt;/p>
&lt;p>All the power of Cluster API, extensibility, lifecycle automation, stability, all the features required for managing an enterprise grade Kubernetes cluster on the infrastructure provider of your choice are now at your fingertips: you can create your Cluster, add new machines, upgrade to the next Kubernetes version, and all from a single place.&lt;/p>
&lt;p>It is just as simple as it looks!&lt;/p>
&lt;h2 id="what-s-next">What’s next&lt;/h2>
&lt;p>While the amazing Cluster API community is working hard to deliver the first version of ClusterClass and managed topologies later this year, we are already looking forward to what comes next for the project and its ecosystem.&lt;/p>
&lt;p>There are a lot of great ideas and opportunities ahead!&lt;/p>
&lt;p>We want to make managed topologies even more powerful and flexible, allowing users to dynamically change bits of a ClusterClass according to the specific needs of a Cluster; this will ensure the same simple and intuitive UX for solving complex problems like e.g. selecting machine image for a specific Kubernetes version and for a specific region of your infrastructure provider, or injecting proxy configurations in the entire Cluster, and so on.&lt;/p>
&lt;p>Stay tuned for what comes next, and if you have any questions, comments or suggestions:&lt;/p>
&lt;ul>
&lt;li>Chat with us on the Kubernetes &lt;a href="http://slack.k8s.io/">Slack&lt;/a>:&lt;a href="https://kubernetes.slack.com/archives/C8TSNPY4T">#cluster-api&lt;/a>&lt;/li>
&lt;li>Join the SIG Cluster Lifecycle &lt;a href="https://groups.google.com/g/kubernetes-sig-cluster-lifecycle">Google Group&lt;/a> to receive calendar invites and gain access to documents&lt;/li>
&lt;li>Join our &lt;a href="https://zoom.us/j/861487554">Zoom meeting&lt;/a>, every Wednesday at 10:00 Pacific Time&lt;/li>
&lt;li>Check out the &lt;a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">ClusterClass quick-start&lt;/a> for the Docker provider (CAPD) in the Cluster API book.&lt;/li>
&lt;li>&lt;em>UPDATE&lt;/em>: Check out the &lt;a href="https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/index.html">ClusterClass experimental feature&lt;/a> documentation in the Cluster API book.&lt;/li>
&lt;/ul></description></item><item><title>Blog: A Closer Look at NSA/CISA Kubernetes Hardening Guidance</title><link>https://kubernetes.io/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jim Angel (Google), Pushkar Joglekar (VMware), and Savitha
Raghunathan (Red Hat)&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Disclaimer&lt;/h4>
The open source tools listed in this article are to serve as examples only
and are in no way a direct recommendation from the Kubernetes community or authors.
&lt;/div>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>USA's National Security Agency (NSA) and the Cybersecurity and Infrastructure
Security Agency (CISA)
released, &amp;quot;&lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF">Kubernetes Hardening Guidance&lt;/a>&amp;quot;
on August 3rd, 2021. The guidance details threats to Kubernetes environments
and provides secure configuration guidance to minimize risk.&lt;/p>
&lt;p>The following sections of this blog correlate to the sections in the NSA/CISA guidance.
Any missing sections are skipped because of limited opportunities to add
anything new to the existing content.&lt;/p>
&lt;p>&lt;em>Note&lt;/em>: This blog post is not a substitute for reading the guide. Reading the published
guidance is recommended before proceeding as the following content is
complementary.&lt;/p>
&lt;h2 id="introduction-and-threat-model">Introduction and Threat Model&lt;/h2>
&lt;p>Note that the threats identified as important by the NSA/CISA, or the intended audience of this guidance, may be different from the threats that other enterprise users of Kubernetes consider important. This section
is still useful for organizations that care about data, resource theft and
service unavailability.&lt;/p>
&lt;p>The guidance highlights the following three sources of compromises:&lt;/p>
&lt;ul>
&lt;li>Supply chain risks&lt;/li>
&lt;li>Malicious threat actors&lt;/li>
&lt;li>Insider threats (administrators, users, or cloud service providers)&lt;/li>
&lt;/ul>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Threat_model">threat model&lt;/a> tries to take a step back and review threats that not only
exist within the boundary of a Kubernetes cluster but also include the underlying
infrastructure and surrounding workloads that Kubernetes does not manage.&lt;/p>
&lt;p>For example, when a workload outside the cluster shares the same physical
network, it has access to the kubelet and to control plane components: etcd, controller manager, scheduler and API
server. Therefore, the guidance recommends having network level isolation
separating Kubernetes clusters from other workloads that do not need connectivity
to Kubernetes control plane nodes. Specifically, scheduler, controller-manager,
etcd only need to be accessible to the API server. Any interactions with Kubernetes
from outside the cluster can happen by providing access to API server port.&lt;/p>
&lt;p>List of ports and protocols for each of these components are
defined in &lt;a href="https://kubernetes.io/docs/reference/ports-and-protocols/">Ports and Protocols&lt;/a>
within the Kubernetes documentation.&lt;/p>
&lt;blockquote>
&lt;p>Special note: kube-scheduler and kube-controller-manager uses different ports than the ones mentioned in the guidance&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;a href="https://cnsmap.netlify.app/threat-modelling">Threat modelling&lt;/a> section
from the CNCF &lt;a href="https://github.com/cncf/tag-security/tree/main/security-whitepaper">Cloud Native Security Whitepaper + Map&lt;/a>
provides another perspective on approaching threat modelling Kubernetes, from a
cloud native lens.&lt;/p>
&lt;h2 id="kubernetes-pod-security">Kubernetes Pod security&lt;/h2>
&lt;p>Kubernetes by default does not guarantee strict workload isolation between pods
running in the same node in a cluster. However, the guidance provides several
techniques to enhance existing isolation and reduce the attack surface in case of a
compromise.&lt;/p>
&lt;h3 id="non-root-containers-and-rootless-container-engines">&amp;quot;Non-root&amp;quot; containers and &amp;quot;rootless&amp;quot; container engines&lt;/h3>
&lt;p>Several best practices related to basic security principle of least privilege
i.e. provide only the permissions are needed; no more, no less, are worth a
second look.&lt;/p>
&lt;p>The guide recommends setting non-root user at build time instead of relying on
setting &lt;code>runAsUser&lt;/code> at runtime in your Pod spec. This is a good practice and provides
some level of defense in depth. For example, if the container image is built with user &lt;code>10001&lt;/code>
and the Pod spec misses adding the &lt;code>runAsuser&lt;/code> field in its &lt;code>Deployment&lt;/code> object. In this
case there are certain edge cases that are worth exploring for awareness:&lt;/p>
&lt;ol>
&lt;li>Pods can fail to start, if the user defined at build time is different from
the one defined in pod spec and some files are as a result inaccessible.&lt;/li>
&lt;li>Pods can end up sharing User IDs unintentionally. This can be problematic
even if the User IDs are non-zero in a situation where a container escape to
host file system is possible. Once the attacker has access to the host file
system, they get access to all the file resources that are owned by other
unrelated pods that share the same UID.&lt;/li>
&lt;li>Pods can end up sharing User IDs, with other node level processes not managed
by Kubernetes e.g. node level daemons for auditing, vulnerability scanning,
telemetry. The threat is similar to the one above where host file system
access can give attacker full access to these node level daemons without
needing to be root on the node.&lt;/li>
&lt;/ol>
&lt;p>However, none of these cases will have as severe an impact as a container
running as root being able to escape as a root user on the host, which can provide
an attacker with complete control of the worker node, further allowing lateral
movement to other worker or control plane nodes.&lt;/p>
&lt;p>Kubernetes 1.22 introduced
an &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/">alpha feature&lt;/a>
that specifically reduces the impact of such a control plane component running
as root user to a non-root user through user namespaces.&lt;/p>
&lt;p>That (&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">alpha stage&lt;/a>) support for user namespaces / rootless mode is available with
the following container runtimes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.docker.com/engine/security/rootless/">Docker Engine&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics">Podman&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Some distributions support running in rootless mode, like the following:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kind.sigs.k8s.io/docs/user/rootless/">kind&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental">k3s&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/rootless-containers/usernetes">Usernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="immutable-container-filesystems">Immutable container filesystems&lt;/h3>
&lt;p>The NSA/CISA Kubernetes Hardening Guidance highlights an often overlooked feature &lt;code>readOnlyRootFileSystem&lt;/code>, with a
working example in &lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=42">Appendix B&lt;/a>. This example limits execution and tampering of
containers at runtime. Any read/write activity can then be limited to few
directories by using &lt;code>tmpfs&lt;/code> volume mounts.&lt;/p>
&lt;p>However, some applications that modify the container filesystem at runtime, like exploding a WAR or JAR file at container startup,
could face issues when enabling this feature. To avoid this issue, consider making minimal changes to the filesystem at runtime
when possible.&lt;/p>
&lt;h3 id="building-secure-container-images">Building secure container images&lt;/h3>
&lt;p>Kubernetes Hardening Guidance also recommends running a scanner at deploy time as an admission controller,
to prevent vulnerable or misconfigured pods from running in the cluster.
Theoretically, this sounds like a good approach but there are several caveats to
consider before this can be implemented in practice:&lt;/p>
&lt;ul>
&lt;li>Depending on network bandwidth, available resources and scanner of choice,
scanning for vulnerabilities for an image can take an indeterminate amount of
time. This could lead to slower or unpredictable pod start up times, which
could result in spikes of unavailability when apps are serving peak load.&lt;/li>
&lt;li>If the policy that allows or denies pod startup is made using incorrect or
incomplete data it could result in several false positive or false negative
outcomes like the following:
&lt;ul>
&lt;li>inside a container image, the &lt;code>openssl&lt;/code> package is detected as vulnerable. However,
the application is written in Golang and uses the Go &lt;code>crypto&lt;/code> package for TLS. Therefore, this vulnerability
is not in the code execution path and as such has minimal impact if it
remains unfixed.&lt;/li>
&lt;li>A vulnerability is detected in the &lt;code>openssl&lt;/code> package for a Debian base image.
However, the upstream Debian community considers this as a Minor impact
vulnerability and as a result does not release a patch fix for this
vulnerability. The owner of this image is now stuck with a vulnerability that
cannot be fixed and a cluster that does not allow the image to run because
of predefined policy that does not take into account whether the fix for a
vulnerability is available or not&lt;/li>
&lt;li>A Golang app is built on top of a &lt;a href="https://github.com/GoogleContainerTools/distroless">distroless&lt;/a>
image, but it is compiled with a Golang version that uses a vulnerable &lt;a href="https://pkg.go.dev/std">standard library&lt;/a>.
The scanner has
no visibility into golang version but only on OS level packages. So it
allows the pod to run in the cluster in spite of the image containing an
app binary built on vulnerable golang.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>To be clear, relying on vulnerability scanners is absolutely a good idea but
policy definitions should be flexible enough to allow:&lt;/p>
&lt;ul>
&lt;li>Creation of exception lists for images or vulnerabilities through labelling&lt;/li>
&lt;li>Overriding the severity with a risk score based on impact of a vulnerability&lt;/li>
&lt;li>Applying the same policies at build time to catch vulnerable images with
fixable vulnerabilities before they can be deployed into Kubernetes clusters&lt;/li>
&lt;/ul>
&lt;p>Special considerations like offline vulnerability database fetch, may also be
needed, if the clusters run in an air-gapped environment and the scanners
require internet access to update the vulnerability database.&lt;/p>
&lt;h3 id="pod-security-policies">Pod Security Policies&lt;/h3>
&lt;p>Since Kubernetes v1.21, the &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/">PodSecurityPolicy&lt;/a>
API and related features are &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">deprecated&lt;/a>,
but some of the guidance in this section will still apply for the next few years, until cluster operators
upgrade their clusters to newer Kubernetes versions.&lt;/p>
&lt;p>The Kubernetes project is working on a replacement for PodSecurityPolicy.
Kubernetes v1.22 includes an alpha feature called &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a>
that is intended to allow enforcing a minimum level of isolation between pods.&lt;/p>
&lt;p>The built-in isolation levels for Pod Security Admission are derived
from &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>, which is a superset of all the components mentioned in Table I &lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=17">page 10&lt;/a> of
the guidance.&lt;/p>
&lt;p>Information about migrating from PodSecurityPolicy to the Pod Security
Admission feature is available
in
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller&lt;/a>.&lt;/p>
&lt;p>One important behavior mentioned in the guidance that remains the same between
Pod Security Policy and its replacement is that enforcing either of them does
not affect pods that are already running. With both PodSecurityPolicy and Pod Security Admission,
the enforcement happens during the pod creation
stage.&lt;/p>
&lt;h3 id="hardening-container-engines">Hardening container engines&lt;/h3>
&lt;p>Some container workloads are less trusted than others but may need to run in the
same cluster. In those cases, running them on dedicated nodes that include
hardened container runtimes that provide stricter pod isolation boundaries can
act as a useful security control.&lt;/p>
&lt;p>Kubernetes supports
an API called &lt;a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">RuntimeClass&lt;/a> that is
stable / GA (and, therefore, enabled by default) stage as of Kubernetes v1.20.
RuntimeClass allows you to ensure that Pods requiring strong isolation are scheduled onto
nodes that can offer it.&lt;/p>
&lt;p>Some third-party projects that you can use in conjunction with RuntimeClass are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kata-containers/kata-containers/blob/main/docs/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md#create-runtime-class-for-kata-containers">kata containers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gvisor.dev/docs/user_guide/containerd/quick_start/">gvisor&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>As discussed here and in the guidance, many features and tooling exist in and around
Kubernetes that can enhance the isolation boundaries between
pods. Based on relevant threats and risk posture, you should pick and choose
between them, instead of trying to apply all the recommendations. Having said that, cluster
level isolation i.e. running workloads in dedicated clusters, remains the strictest workload
isolation mechanism, in spite of improvements mentioned earlier here and in the guide.&lt;/p>
&lt;h2 id="network-separation-and-hardening">Network Separation and Hardening&lt;/h2>
&lt;p>Kubernetes Networking can be tricky and this section focuses on how to secure
and harden the relevant configurations. The guide identifies the following as key
takeaways:&lt;/p>
&lt;ul>
&lt;li>Using NetworkPolicies to create isolation between resources,&lt;/li>
&lt;li>Securing the control plane&lt;/li>
&lt;li>Encrypting traffic and sensitive data&lt;/li>
&lt;/ul>
&lt;h3 id="network-policies">Network Policies&lt;/h3>
&lt;p>Network policies can be created with the help of network plugins. In order to
make the creation and visualization easier for users, Cilium supports
a &lt;a href="https://editor.cilium.io">web GUI tool&lt;/a>. That web GUI lets you create Kubernetes
NetworkPolicies (a generic API that nevertheless requires a compatible CNI plugin),
and / or Cilium network policies (CiliumClusterwideNetworkPolicy and CiliumNetworkPolicy,
which only work in clusters that use the Cilium CNI plugin).
You can use these APIs to restrict network traffic between pods, and therefore minimize the
attack vector.&lt;/p>
&lt;p>Another scenario that is worth exploring is the usage of external IPs. Some
services, when misconfigured, can create random external IPs. An attacker can take
advantage of this misconfiguration and easily intercept traffic. This vulnerability
has been reported
in &lt;a href="https://www.cvedetails.com/cve/CVE-2020-8554/">CVE-2020-8554&lt;/a>.
Using &lt;a href="https://github.com/kubernetes-sigs/externalip-webhook">externalip-webhook&lt;/a>
can mitigate this vulnerability by preventing the services from using random
external IPs. &lt;a href="https://github.com/kubernetes-sigs/externalip-webhook">externalip-webhook&lt;/a>
only allows creation of services that don't require external IPs or whose
external IPs are within the range specified by the administrator.&lt;/p>
&lt;blockquote>
&lt;p>CVE-2020-8554 - Kubernetes API server in all versions allow an attacker
who is able to create a ClusterIP service and set the &lt;code>spec.externalIPs&lt;/code> field,
to intercept traffic to that IP address. Additionally, an attacker who is able to
patch the &lt;code>status&lt;/code> (which is considered a privileged operation and should not
typically be granted to users) of a LoadBalancer service can set the
&lt;code>status.loadBalancer.ingress.ip&lt;/code> to similar effect.&lt;/p>
&lt;/blockquote>
&lt;h3 id="resource-policies">Resource Policies&lt;/h3>
&lt;p>In addition to configuring ResourceQuotas and limits, consider restricting how many process
IDs (PIDs) a given Pod can use, and also to reserve some PIDs for node-level use to avoid
resource exhaustion. More details to apply these limits can be
found in &lt;a href="https://kubernetes.io/docs/concepts/policy/pid-limiting/">Process ID Limits And Reservations&lt;/a>.&lt;/p>
&lt;h3 id="control-plane-hardening">Control Plane Hardening&lt;/h3>
&lt;p>In the next section, the guide covers control plane hardening. It is worth
noting that
from &lt;a href="https://github.com/kubernetes/kubernetes/issues/91506">Kubernetes 1.20&lt;/a>,
insecure port from API server, has been removed.&lt;/p>
&lt;h3 id="etcd">Etcd&lt;/h3>
&lt;p>As a general rule, the etcd server should be configured to only trust
certificates assigned to the API server. It limits the attack surface and prevents a
malicious attacker from gaining access to the cluster. It might be beneficial to
use a separate CA for etcd, as it by default trusts all the certificates issued
by the root CA.&lt;/p>
&lt;h3 id="kubeconfig-files">Kubeconfig Files&lt;/h3>
&lt;p>In addition to specifying the token and certificates directly, &lt;code>.kubeconfig&lt;/code>
supports dynamic retrieval of temporary tokens using auth provider plugins.
Beware of the possibility of malicious
shell &lt;a href="https://banzaicloud.com/blog/kubeconfig-security/">code execution&lt;/a> in a
&lt;code>kubeconfig&lt;/code> file. Once attackers gain access to the cluster, they can steal ssh
keys/secrets or more.&lt;/p>
&lt;h3 id="secrets">Secrets&lt;/h3>
&lt;p>Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets&lt;/a> is the native way of managing secrets as a Kubernetes
API object. However, in some scenarios such as a desire to have a single source of truth for all app secrets, irrespective of whether they run on Kubernetes or not, secrets can be managed loosely coupled with
Kubernetes and consumed by pods through side-cars or init-containers with minimal usage of Kubernetes Secrets API.&lt;/p>
&lt;p>&lt;a href="https://github.com/external-secrets/kubernetes-external-secrets">External secrets providers&lt;/a>
and &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">csi-secrets-store&lt;/a>
are some of these alternatives to Kubernetes Secrets&lt;/p>
&lt;h2 id="log-auditing">Log Auditing&lt;/h2>
&lt;p>The NSA/CISA guidance stresses monitoring and alerting based on logs. The key points
include logging at the host level, application level, and on the cloud. When
running Kubernetes in production, it's important to understand who's
responsible, and who's accountable, for each layer of logging.&lt;/p>
&lt;h3 id="kubernetes-api-auditing">Kubernetes API auditing&lt;/h3>
&lt;p>One area that deserves more focus is what exactly should alert or be logged. The
document outlines a sample policy in &lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=55">Appendix L: Audit Policy&lt;/a> that logs all
RequestResponse's including metadata and request / response bodies. While helpful for a demo, it may not be practical for production.&lt;/p>
&lt;p>Each organization needs to evaluate their
own threat model and build an audit policy that complements or helps troubleshooting incident response. Think
about how someone would attack your organization and what audit trail could identify it. Review more advanced options for tuning audit logs in the official &lt;a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy">audit logging documentation&lt;/a>.
It's crucial to tune your audit logs to only include events that meet your threat model. A minimal audit policy that logs everything at &lt;code>metadata&lt;/code> level can also be a good starting point.&lt;/p>
&lt;p>Audit logging configurations can also be tested with
kind following these &lt;a href="https://kind.sigs.k8s.io/docs/user/auditing">instructions&lt;/a>.&lt;/p>
&lt;h3 id="streaming-logs-and-auditing">Streaming logs and auditing&lt;/h3>
&lt;p>Logging is important for threat and anomaly detection. As the document outlines,
it's a best practice to scan and alert on logs as close to real time as possible
and to protect logs from tampering if a compromise occurs. It's important to
reflect on the various levels of logging and identify the critical areas such as
API endpoints.&lt;/p>
&lt;p>Kubernetes API audit logging can stream to a webhook and there's an example in &lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=58">Appendix N: Webhook configuration&lt;/a>. Using a webhook could be a method that
stores logs off cluster and/or centralizes all audit logs. Once logs are
centrally managed, look to enable alerting based on critical events. Also ensure
you understand what the baseline is for normal activities.&lt;/p>
&lt;h3 id="alert-identification">Alert identification&lt;/h3>
&lt;p>While the guide stressed the importance of notifications, there is not a blanket
event list to alert from. The alerting requirements vary based on your own
requirements and threat model. Examples include the following events:&lt;/p>
&lt;ul>
&lt;li>Changes to the &lt;code>securityContext&lt;/code> of a Pod&lt;/li>
&lt;li>Updates to admission controller configs&lt;/li>
&lt;li>Accessing certain files / URLs&lt;/li>
&lt;/ul>
&lt;h3 id="additional-logging-resources">Additional logging resources&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=OPuu8wsu2Zc">Seccomp Security Profiles and You: A Practical Guide - Duffie Cooley&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=ZJgaGJm9NJE">TGI Kubernetes 119: Gatekeeper and OPA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.lacework.com/blog/hiding-in-plaintext-sight-abusing-the-lack-of-kubernetes-auditing-policies/">Abusing The Lack of Kubernetes Auditing Policies&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2021/08/25/seccomp-default/">Enable seccomp for all workloads with a new v1.22 alpha feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.twitch.tv/videos/1147889860">This Week in Cloud Native: Auditing / Pod Security&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="upgrading-and-application-security-practices">Upgrading and Application Security practices&lt;/h2>
&lt;p>Kubernetes releases three times per year, so upgrade-related toil is a common problem for
people running production clusters. In addition to this, operators must
regularly upgrade the underlying node's operating system and running
applications. This is a best practice to ensure continued support and to reduce
the likelihood of bugs or vulnerabilities.&lt;/p>
&lt;p>Kubernetes supports the three most recent stable releases. While each Kubernetes
release goes through a large number of tests before being published, some
teams aren't comfortable running the latest stable release until some time has
passed. No matter what version you're running, ensure that patch upgrades
happen frequently or automatically. More information can be found in
the &lt;a href="https://kubernetes.io/releases/version-skew-policy/">version skew&lt;/a> policy
pages.&lt;/p>
&lt;p>When thinking about how you'll manage node OS upgrades, consider ephemeral
nodes. Having the ability to destroy and add nodes allows your team to respond
quicker to node issues. In addition, having deployments that tolerate node
instability (and a culture that encourages frequent deployments) allows for
easier cluster upgrades.&lt;/p>
&lt;p>Additionally, it's worth reiterating from the guidance that periodic
vulnerability scans and penetration tests can be performed on the various system
components to proactively look for insecure configurations and vulnerabilities.&lt;/p>
&lt;h3 id="finding-release-security-information">Finding release &amp;amp; security information&lt;/h3>
&lt;p>To find the most recent Kubernetes supported versions, refer to
&lt;a href="https://k8s.io/releases">https://k8s.io/releases&lt;/a>, which includes minor versions. It's good to stay up to date with
your minor version patches.&lt;/p>
&lt;p>If you're running a managed Kubernetes offering, look for their release
documentation and find their various security channels.&lt;/p>
&lt;p>Subscribe to
the &lt;a href="https://groups.google.com/g/kubernetes-announce">Kubernetes Announce mailing list&lt;/a>.
The Kubernetes Announce mailing list is searchable for terms such
as &amp;quot;&lt;a href="https://groups.google.com/g/kubernetes-announce/search?q=%5BSecurity%20Advisory%5D">Security Advisories&lt;/a>&amp;quot;.
You can set up alerts and email notifications as long as you know what key
words to alert on.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In summary, it is fantastic to see security practitioners sharing this
level of detailed guidance in public. This guidance further highlights
Kubernetes going mainstream and how securing Kubernetes clusters and the
application containers running on Kubernetes continues to need attention and focus of
practitioners. Only a few weeks after the guidance was published, an open source
tool &lt;a href="https://github.com/armosec/kubescape">kubescape&lt;/a> to validate cluster
against this guidance became available.&lt;/p>
&lt;p>This tool can be a great starting point to check the current state of your
clusters, after which you can use the information in this blog post and in the guidance to assess
where improvements can be made.&lt;/p>
&lt;p>Finally, it is worth reiterating that not all controls in this guidance will
make sense for all practitioners. The best way to know which controls matter is
to rely on the threat model of your own Kubernetes environment.&lt;/p>
&lt;p>&lt;em>A special shout out and thanks to Rory McCune (@raesene) for his inputs to this blog post&lt;/em>&lt;/p></description></item><item><title>Blog: How to Handle Data Duplication in Data-Heavy Kubernetes Environments</title><link>https://kubernetes.io/blog/2021/09/29/how-to-handle-data-duplication-in-data-heavy-kubernetes-environments/</link><pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/09/29/how-to-handle-data-duplication-in-data-heavy-kubernetes-environments/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong>
Augustinas Stirbis (CAST AI)&lt;/p>
&lt;h2 id="why-duplicate-data">Why Duplicate Data?&lt;/h2>
&lt;p>It’s convenient to create a copy of your application with a copy of its state for each team.
For example, you might want a separate database copy to test some significant schema changes
or develop other disruptive operations like bulk insert/delete/update...&lt;/p>
&lt;p>&lt;strong>Duplicating data takes a lot of time.&lt;/strong> That’s because you need first to download
all the data from a source block storage provider to compute and then send
it back to a storage provider again. There’s a lot of network traffic and CPU/RAM used in this process.
Hardware acceleration by offloading certain expensive operations to dedicated hardware is
&lt;strong>always a huge performance boost&lt;/strong>. It reduces the time required to complete an operation by orders
of magnitude.&lt;/p>
&lt;h2 id="volume-snapshots-to-the-rescue">Volume Snapshots to the rescue&lt;/h2>
&lt;p>Kubernetes introduced &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">VolumeSnapshots&lt;/a> as alpha in 1.12,
beta in 1.17, and the Generally Available version in 1.20.
VolumeSnapshots use specialized APIs from storage providers to duplicate volume of data.&lt;/p>
&lt;p>Since data is already in the same storage device (array of devices), duplicating data is usually
a metadata operation for storage providers with local snapshots (majority of on-premise storage providers).
All you need to do is point a new disk to an immutable snapshot and only
save deltas (or let it do a full-disk copy). As an operation that is inside the storage back-end,
it’s much quicker and usually doesn’t involve sending traffic over the network.
Public Clouds storage providers under the hood work a bit differently. They save snapshots
to Object Storage and then copy back from Object storage to Block storage when &amp;quot;duplicating&amp;quot; disk.
Technically there is a lot of Compute and network resources spent on Cloud providers side,
but from Kubernetes user perspective VolumeSnapshots work the same way whether is it local or
remote snapshot storage provider and no Compute and Network resources are involved in this operation.&lt;/p>
&lt;h2 id="sounds-like-we-have-our-solution-right">Sounds like we have our solution, right?&lt;/h2>
&lt;p>Actually, VolumeSnapshots are namespaced, and Kubernetes protects namespaced data from
being shared between tenants (Namespaces). This Kubernetes limitation is a conscious design
decision so that a Pod running in a different namespace can’t mount another application’s
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim&lt;/a> (PVC).&lt;/p>
&lt;p>One way around it would be to create multiple volumes with duplicate data in one namespace.
However, you could easily reference the wrong copy.&lt;/p>
&lt;p>So the idea is to separate teams/initiatives by namespaces to avoid that and generally
limit access to the production namespace.&lt;/p>
&lt;h2 id="solution-creating-a-golden-snapshot-externally">Solution? Creating a Golden Snapshot externally&lt;/h2>
&lt;p>Another way around this design limitation is to create Snapshot externally (not through Kubernetes).
This is also called pre-provisioning a snapshot manually. Next, I will import it
as a multi-tenant golden snapshot that can be used for many namespaces. Below illustration will be
for AWS EBS (Elastic Block Storage) and GCE PD (Persistent Disk) services.&lt;/p>
&lt;h3 id="high-level-plan-for-preparing-the-golden-snapshot">High-level plan for preparing the Golden Snapshot&lt;/h3>
&lt;ol>
&lt;li>Identify Disk (EBS/Persistent Disk) that you want to clone with data in the cloud provider&lt;/li>
&lt;li>Make a Disk Snapshot (in cloud provider console)&lt;/li>
&lt;li>Get Disk Snapshot ID&lt;/li>
&lt;/ol>
&lt;h3 id="high-level-plan-for-cloning-data-for-each-team">High-level plan for cloning data for each team&lt;/h3>
&lt;ol>
&lt;li>Create Namespace “sandbox01”&lt;/li>
&lt;li>Import Disk Snapshot (ID) as VolumeSnapshotContent to Kubernetes&lt;/li>
&lt;li>Create VolumeSnapshot in the Namespace &amp;quot;sandbox01&amp;quot; mapped to VolumeSnapshotContent&lt;/li>
&lt;li>Create the PersistentVolumeClaim from VolumeSnapshot&lt;/li>
&lt;li>Install Deployment or StatefulSet with PVC&lt;/li>
&lt;/ol>
&lt;h2 id="step-1-identify-disk">Step 1: Identify Disk&lt;/h2>
&lt;p>First, you need to identify your golden source. In my case, it’s a PostgreSQL database
on PersistentVolumeClaim “postgres-pv-claim” in the “production” namespace.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl -n &amp;lt;namespace&amp;gt; get pvc &amp;lt;pvc-name&amp;gt; -o jsonpath='{.spec.volumeName}'
&lt;/code>&lt;/pre>&lt;p>The output will look similar to:&lt;/p>
&lt;pre>&lt;code>pvc-3096b3ba-38b6-4fd1-a42f-ec99176ed0d90
&lt;/code>&lt;/pre>&lt;h2 id="step-2-prepare-your-golden-source">Step 2: Prepare your golden source&lt;/h2>
&lt;p>You need to do this once or every time you want to refresh your golden data.&lt;/p>
&lt;h3 id="make-a-disk-snapshot">Make a Disk Snapshot&lt;/h3>
&lt;p>Go to AWS EC2 or GCP Compute Engine console and search for an EBS volume
(on AWS) or Persistent Disk (on GCP), that has a label matching the last output.
In this case I saw: &lt;code>pvc-3096b3ba-38b6-4fd1-a42f-ec99176ed0d9&lt;/code>.&lt;/p>
&lt;p>Click on Create snapshot and give it a name. You can do it in Console manually,
in AWS CloudShell / Google Cloud Shell, or in the terminal. To create a snapshot in the
terminal you must have the AWS CLI tool (&lt;code>aws&lt;/code>) or Google's CLI (&lt;code>gcloud&lt;/code>)
installed and configured.&lt;/p>
&lt;p>Here’s the command to create snapshot on GCP:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">gcloud compute disks snapshot &amp;lt;cloud-disk-id&amp;gt; --project=&amp;lt;gcp-project-id&amp;gt; --snapshot-names=&amp;lt;set-new-snapshot-name&amp;gt; --zone=&amp;lt;availability-zone&amp;gt; --storage-location=&amp;lt;region&amp;gt;
&lt;/code>&lt;/pre>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-09-07-data-duplication-in-data-heavy-k8s-env/create-volume-snapshot-gcp.png"
alt="Screenshot of a terminal showing volume snapshot creation on GCP"/> &lt;figcaption>
&lt;h4>GCP snapshot creation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>GCP identifies the disk by its PVC name, so it’s direct mapping. In AWS, you need to
find volume by the CSIVolumeName AWS tag with PVC name value first that will be used for snapshot creation.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-09-07-data-duplication-in-data-heavy-k8s-env/identify-volume-aws.png"
alt="Screenshot of AWS web console, showing EBS volume identification"/> &lt;figcaption>
&lt;h4>Identify disk ID on AWS&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Mark done Volume (volume-id) &lt;code>vol-00c7ecd873c6fb3ec&lt;/code> and ether create EBS snapshot in AWS Console, or use &lt;code>aws cli&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">aws ec2 create-snapshot --volume-id '&amp;lt;volume-id&amp;gt;' --description '&amp;lt;set-new-snapshot-name&amp;gt;' --tag-specifications 'ResourceType=snapshot'
&lt;/code>&lt;/pre>&lt;h2 id="step-3-get-your-disk-snapshot-id">Step 3: Get your Disk Snapshot ID&lt;/h2>
&lt;p>In AWS, the command above will output something similar to:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">&amp;quot;SnapshotId&amp;quot;: &amp;quot;snap-09ed24a70bc19bbe4&amp;quot;
&lt;/code>&lt;/pre>&lt;p>If you’re using the GCP cloud, you can get the snapshot ID from the gcloud command by querying for the snapshot’s given name:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">gcloud compute snapshots --project=&amp;lt;gcp-project-id&amp;gt; describe &amp;lt;new-snapshot-name&amp;gt; | grep id:
&lt;/code>&lt;/pre>&lt;p>You should get similar output to:&lt;/p>
&lt;pre>&lt;code>id: 6645363163809389170
&lt;/code>&lt;/pre>&lt;h2 id="step-4-create-a-development-environment-for-each-team">Step 4: Create a development environment for each team&lt;/h2>
&lt;p>Now I have my Golden Snapshot, which is immutable data. Each team will get a copy
of this data, and team members can modify it as they see fit, given that a new EBS/persistent
disk will be created for each team.&lt;/p>
&lt;p>Below I will define a manifest for each namespace. To save time, you can replace
the namespace name (such as changing “sandbox01” → “sandbox42”) using tools
such as &lt;code>sed&lt;/code> or &lt;code>yq&lt;/code>, with Kubernetes-aware templating tools like
&lt;a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/">Kustomize&lt;/a>,
or using variable substitution in a CI/CD pipeline.&lt;/p>
&lt;p>Here's an example manifest:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Retain&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pd.csi.storage.gke.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;gcp/projects/staging-eu-castai-vt5hy2/global/snapshots/6645363163809389170&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-snap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-snap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotContentName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In Kubernetes, VolumeSnapshotContent (VSC) objects are not namespaced.
However, I need a separate VSC for each different namespace to use, so the
&lt;code>metadata.name&lt;/code> of each VSC must also be different. To make that straightfoward,
I used the target namespace as part of the name.&lt;/p>
&lt;p>Now it’s time to replace the driver field with the CSI (Container Storage Interface) driver
installed in your K8s cluster. Major cloud providers have CSI driver for block storage that
support VolumeSnapshots but quite often CSI drivers are not installed by default, consult
with your Kubernetes provider.&lt;/p>
&lt;p>That manifest above defines a VSC that works on GCP.
On AWS, driver and SnashotHandle values might look like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ebs.csi.aws.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;snap-07ff83d328c981c98&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this point, I need to use the &lt;em>Retain&lt;/em> policy, so that the CSI driver doesn’t try to
delete my manually created EBS disk snapshot.&lt;/p>
&lt;p>For GCP, you will have to build this string by hand - add a full project ID and snapshot ID.
For AWS, it’s just a plain snapshot ID.&lt;/p>
&lt;p>VSC also requires specifying which VolumeSnapshot (VS) will use it, so VSC and VS are
referencing each other.&lt;/p>
&lt;p>Now I can create PersistentVolumeClaim from VS above. It’s important to set this first:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgres-pv-claim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSource&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-snap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>21Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If default StorageClass has &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode">WaitForFirstConsumer&lt;/a> policy,
then the actual Cloud Disk will be created from the Golden Snapshot only when some Pod bounds that PVC.&lt;/p>
&lt;p>Now I assign that PVC to my Pod (in my case, it’s Postgresql) as I would with any other PVC.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl -n &amp;lt;namespace&amp;gt; get volumesnapshotContent,volumesnapshot,pvc,pod
&lt;/code>&lt;/pre>&lt;p>Both VS and VSC should be &lt;em>READYTOUSE&lt;/em> true, PVC bound, and the Pod (from Deployment or StatefulSet) running.&lt;/p>
&lt;p>&lt;strong>To keep on using data from my Golden Snapshot, I just need to repeat this for the
next namespace and voilà! No need to waste time and compute resources on the duplication process.&lt;/strong>&lt;/p></description></item><item><title>Blog: Spotlight on SIG Node</title><link>https://kubernetes.io/blog/2021/09/27/sig-node-spotlight-2021/</link><pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/09/27/sig-node-spotlight-2021/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Dewan Ahmed, Red Hat&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In Kubernetes, a &lt;em>Node&lt;/em> is a representation of a single machine in your cluster. &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> owns that very important Node component and supports various subprojects such as Kubelet, Container Runtime Interface (CRI) and more to support how the pods and host resources interact. In this blog, we have summarized our conversation with &lt;a href="https://twitter.com/ehashdn">Elana Hashman (EH)&lt;/a> &amp;amp; &lt;a href="https://twitter.com/SergeyKanzhelev">Sergey Kanzhelev (SK)&lt;/a>, who walk us through the various aspects of being a part of the SIG and share some insights about how others can get involved.&lt;/p>
&lt;h2 id="a-summary-of-our-conversation">A summary of our conversation&lt;/h2>
&lt;h3 id="could-you-tell-us-a-little-about-what-sig-node-does">Could you tell us a little about what SIG Node does?&lt;/h3>
&lt;p>SK: SIG Node is a vertical SIG responsible for the components that support the controlled interactions between the pods and host resources. We manage the lifecycle of pods that are scheduled to a node. This SIG's focus is to enable a broad set of workload types, including workloads with hardware specific or performance sensitive requirements. All while maintaining isolation boundaries between pods on a node, as well as the pod and the host. This SIG maintains quite a few components and has many external dependencies (like container runtimes or operating system features), which makes the complexity we deal with huge. We tame the complexity and aim to continuously improve node reliability.&lt;/p>
&lt;h3 id="sig-node-is-a-vertical-sig-could-you-explain-a-bit-more">&amp;quot;SIG Node is a vertical SIG&amp;quot; could you explain a bit more?&lt;/h3>
&lt;p>EH: There are two kinds of SIGs: horizontal and vertical. Horizontal SIGs are concerned with a particular function of every component in Kubernetes: for example, SIG Security considers security aspects of every component in Kubernetes, or SIG Instrumentation looks at the logs, metrics, traces and events of every component in Kubernetes. Such SIGs don't tend to own a lot of code.&lt;/p>
&lt;p>Vertical SIGs, on the other hand, own a single component, and are responsible for approving and merging patches to that code base. SIG Node owns the &amp;quot;Node&amp;quot; vertical, pertaining to the kubelet and its lifecycle. This includes the code for the kubelet itself, as well as the node controller, the container runtime interface, and related subprojects like the node problem detector.&lt;/p>
&lt;h3 id="how-did-the-ci-subproject-start-is-this-specific-to-sig-node-and-how-does-it-help-the-sig">How did the CI subproject start? Is this specific to SIG Node and how does it help the SIG?&lt;/h3>
&lt;p>SK: The subproject started as a follow up after one of the releases was blocked by numerous test failures of critical tests. These tests haven’t started falling all at once, rather continuous lack of attention led to slow degradation of tests quality. SIG Node was always prioritizing quality and reliability, and forming of the subproject was a way to highlight this priority.&lt;/p>
&lt;h3 id="as-the-3rd-largest-sig-in-terms-of-number-of-issues-and-prs-how-does-your-sig-juggle-so-much-work">As the 3rd largest SIG in terms of number of issues and PRs, how does your SIG juggle so much work?&lt;/h3>
&lt;p>EH: It helps to be organized. When I increased my contributions to the SIG in January of 2021, I found myself overwhelmed by the volume of pull requests and issues and wasn't sure where to start. We were already tracking test-related issues and pull requests on the CI subproject board, but that was missing a lot of our bugfixes and feature work. So I began putting together a triage board for the rest of our pull requests, which allowed me to sort each one by status and what actions to take, and documented its use for other contributors. We closed or merged over 500 issues and pull requests tracked by our two boards in each of the past two releases. The Kubernetes devstats showed that we have significantly increased our velocity as a result.&lt;/p>
&lt;p>In June, we ran our first bug scrub event to work through the backlog of issues filed against SIG Node, ensuring they were properly categorized. We closed over 130 issues over the course of this 48 hour global event, but as of writing we still have 333 open issues.&lt;/p>
&lt;h3 id="why-should-new-and-existing-contributors-consider-joining-sig-node">Why should new and existing contributors consider joining SIG Node?&lt;/h3>
&lt;p>SK: Being a SIG Node contributor gives you skills and recognition that are rewarding and useful. Understanding under the hood of a kubelet helps architecting better apps, tune and optimize those apps, and gives leg up in issues troubleshooting. If you are a new contributor, SIG Node gives you the foundational knowledge that is key to understanding why other Kubernetes components are designed the way they are. Existing contributors may benefit as many features will require SIG Node changes one way or another. So being a SIG Node contributor helps building features in other SIGs faster.&lt;/p>
&lt;p>SIG Node maintains numerous components, many of which have dependency on external projects or OS features. This makes the onboarding process quite lengthy and demanding. But if you are up for a challenge, there is always a place for you, and a group of people to support.&lt;/p>
&lt;h3 id="what-do-you-do-to-help-new-contributors-get-started">What do you do to help new contributors get started?&lt;/h3>
&lt;p>EH: Getting started in SIG Node can be intimidating, since there is so much work to be done, our SIG meetings are very large, and it can be hard to find a place to start.&lt;/p>
&lt;p>I always encourage new contributors to work on things that they have some investment in already. In SIG Node, that might mean volunteering to help fix a bug that you have personally been affected by, or helping to triage bugs you care about by priority.&lt;/p>
&lt;p>To come up to speed on any open source code base, there are two strategies you can take: start by exploring a particular issue deeply, and follow that to expand the edges of your knowledge as needed, or briefly review as many issues and change requests as you possibly can to get a higher level picture of how the component works. Ultimately, you will need to do both if you want to become a Node reviewer or approver.&lt;/p>
&lt;p>&lt;a href="https://twitter.com/dims">Davanum Srinivas&lt;/a> and I each ran a cohort of group mentoring to help teach new contributors the skills to become Node reviewers, and if there's interest we can work to find a mentor to run another session. I also encourage new contributors to attend our Node CI Subproject meeting: it's a smaller audience and we don't record the triage sessions, so it can be a less intimidating way to get started with the SIG.&lt;/p>
&lt;h3 id="are-there-any-particular-skills-you-d-like-to-recruit-for-what-skills-are-contributors-to-sig-usability-likely-to-learn">Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?&lt;/h3>
&lt;p>SK: SIG Node works on many workstreams in very different areas. All of these areas are on system level. For the typical code contributions you need to have a passion for building and utilizing low level APIs and writing performant and reliable components. Being a contributor you will learn how to debug and troubleshoot, profile, and monitor these components, as well as user workload that is run by these components. Often, with the limited to no access to Nodes, as they are running production workloads.&lt;/p>
&lt;p>The other way of contribution is to help document SIG node features. This type of contribution requires a deep understanding of features, and ability to explain them in simple terms.&lt;/p>
&lt;p>Finally, we are always looking for feedback on how best to run your workload. Come and explain specifics of it, and what features in SIG Node components may help to run it better.&lt;/p>
&lt;h3 id="what-are-you-getting-positive-feedback-on-and-what-s-coming-up-next-for-sig-node">What are you getting positive feedback on, and what’s coming up next for SIG Node?&lt;/h3>
&lt;p>EH: Over the past year SIG Node has adopted some new processes to help manage our feature development and Kubernetes enhancement proposals, and other SIGs have looked to us for inspiration in managing large workloads. I hope that this is an area we can continue to provide leadership in and further iterate on.&lt;/p>
&lt;p>We have a great balance of new features and deprecations in flight right now. Deprecations of unused or difficult to maintain features help us keep technical debt and maintenance load under control, and examples include the dockershim and DynamicKubeletConfiguration deprecations. New features will unlock additional functionality in end users' clusters, and include exciting features like support for cgroups v2, swap memory, graceful node shutdowns, and device management policies.&lt;/p>
&lt;h3 id="any-closing-thoughts-resources-you-d-like-to-share">Any closing thoughts/resources you’d like to share?&lt;/h3>
&lt;p>SK/EH: It takes time and effort to get to any open source community. SIG Node may overwhelm you at first with the number of participants, volume of work, and project scope. But it is totally worth it. Join our welcoming community! &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node GitHub Repo&lt;/a> contains many useful resources including Slack, mailing list and other contact info.&lt;/p>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>SIG Node hosted a &lt;a href="https://www.youtube.com/watch?v=z5aY4e2RENA">KubeCon + CloudNativeCon Europe 2021 talk&lt;/a> with an intro and deep dive to their awesome SIG. Join the SIG's meetings to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream Node team as a contributor!&lt;/p></description></item><item><title>Blog: Introducing Single Pod Access Mode for PersistentVolumes</title><link>https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/</link><pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Chris Henzie (Google)&lt;/p>
&lt;p>Last month's release of Kubernetes v1.22 introduced a new ReadWriteOncePod access mode for &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes">PersistentVolumes&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims&lt;/a>.
With this alpha feature, Kubernetes allows you to restrict volume access to a single pod in the cluster.&lt;/p>
&lt;h2 id="what-are-access-modes-and-why-are-they-important">What are access modes and why are they important?&lt;/h2>
&lt;p>When using storage, there are different ways to model how that storage is consumed.&lt;/p>
&lt;p>For example, a storage system like a network file share can have many users all reading and writing data simultaneously.
In other cases maybe everyone is allowed to read data but not write it.
For highly sensitive data, maybe only one user is allowed to read and write data but nobody else.&lt;/p>
&lt;p>In the world of Kubernetes, &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">access modes&lt;/a> are the way you can define how durable storage is consumed.
These access modes are a part of the spec for PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>shared-cache&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteMany&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Allow many nodes to access shared-cache simultaneously.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before v1.22, Kubernetes offered three access modes for PVs and PVCs:&lt;/p>
&lt;ul>
&lt;li>ReadWriteOnce – the volume can be mounted as read-write by a single node&lt;/li>
&lt;li>ReadOnlyMany – the volume can be mounted read-only by many nodes&lt;/li>
&lt;li>ReadWriteMany – the volume can be mounted as read-write by many nodes&lt;/li>
&lt;/ul>
&lt;p>These access modes are enforced by Kubernetes components like the &lt;code>kube-controller-manager&lt;/code> and &lt;code>kubelet&lt;/code> to ensure only certain pods are allowed to access a given PersistentVolume.&lt;/p>
&lt;h2 id="what-is-this-new-access-mode-and-how-does-it-work">What is this new access mode and how does it work?&lt;/h2>
&lt;p>Kubernetes v1.22 introduced a fourth access mode for PVs and PVCs, that you can use for CSI volumes:&lt;/p>
&lt;ul>
&lt;li>ReadWriteOncePod – the volume can be mounted as read-write by a single pod&lt;/li>
&lt;/ul>
&lt;p>If you create a pod with a PVC that uses the ReadWriteOncePod access mode, Kubernetes ensures that pod is the only pod across your whole cluster that can read that PVC or write to it.&lt;/p>
&lt;p>If you create another pod that references the same PVC with this access mode, the pod will fail to start because the PVC is already in use by another pod.
For example:&lt;/p>
&lt;pre>&lt;code>Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Warning FailedScheduling 1s default-scheduler 0/1 nodes are available: 1 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode.
&lt;/code>&lt;/pre>&lt;h3 id="how-is-this-different-than-the-readwriteonce-access-mode">How is this different than the ReadWriteOnce access mode?&lt;/h3>
&lt;p>The ReadWriteOnce access mode restricts volume access to a single &lt;em>node&lt;/em>, which means it is possible for multiple pods on the same node to read from and write to the same volume.
This could potentially be a major problem for some applications, especially if they require at most one writer for data safety guarantees.&lt;/p>
&lt;p>With ReadWriteOncePod these issues go away.
Set the access mode on your PVC, and Kubernetes guarantees that only a single pod has access.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>The ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes.
As a first step you need to enable the ReadWriteOncePod &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates">feature gate&lt;/a> for &lt;code>kube-apiserver&lt;/code>, &lt;code>kube-scheduler&lt;/code>, and &lt;code>kubelet&lt;/code>.
You can enable the feature by setting command line arguments:&lt;/p>
&lt;pre>&lt;code>--feature-gates=&amp;quot;...,ReadWriteOncePod=true&amp;quot;
&lt;/code>&lt;/pre>&lt;p>You also need to update the following CSI sidecars to these versions or greater:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0">csi-provisioner:v3.0.0+&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0">csi-attacher:v3.3.0+&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0">csi-resizer:v1.3.0+&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="creating-a-persistentvolumeclaim">Creating a PersistentVolumeClaim&lt;/h3>
&lt;p>In order to use the ReadWriteOncePod access mode for your PVs and PVCs, you will need to create a new PVC with the access mode:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>single-writer-only&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOncePod&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Allow only a single pod to access single-writer-only.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If your storage plugin supports &lt;a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/">dynamic provisioning&lt;/a>, new PersistentVolumes will be created with the ReadWriteOncePod access mode applied.&lt;/p>
&lt;h4 id="migrating-existing-persistentvolumes">Migrating existing PersistentVolumes&lt;/h4>
&lt;p>If you have existing PersistentVolumes, they can be migrated to use ReadWriteOncePod.&lt;/p>
&lt;p>In this example, we already have a &amp;quot;cat-pictures-pvc&amp;quot; PersistentVolumeClaim that is bound to a &amp;quot;cat-pictures-pv&amp;quot; PersistentVolume, and a &amp;quot;cat-pictures-writer&amp;quot; Deployment that uses this PersistentVolumeClaim.&lt;/p>
&lt;p>As a first step, you need to edit your PersistentVolume's &lt;code>spec.persistentVolumeReclaimPolicy&lt;/code> and set it to &lt;code>Retain&lt;/code>.
This ensures your PersistentVolume will not be deleted when we delete the corresponding PersistentVolumeClaim:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch pv cat-pictures-pv -p &lt;span style="color:#b44">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;persistentVolumeReclaimPolicy&amp;#34;:&amp;#34;Retain&amp;#34;}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next you need to stop any workloads that are using the PersistentVolumeClaim bound to the PersistentVolume you want to migrate, and then delete the PersistentVolumeClaim.&lt;/p>
&lt;p>Once that is done, you need to clear your PersistentVolume's &lt;code>spec.claimRef.uid&lt;/code> to ensure PersistentVolumeClaims can bind to it upon recreation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl scale --replicas&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">0&lt;/span> deployment cat-pictures-writer
kubectl delete pvc cat-pictures-pvc
kubectl patch pv cat-pictures-pv -p &lt;span style="color:#b44">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;claimRef&amp;#34;:{&amp;#34;uid&amp;#34;:&amp;#34;&amp;#34;}}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After that you need to replace the PersistentVolume's access modes with ReadWriteOncePod:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch pv cat-pictures-pv -p &lt;span style="color:#b44">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;accessModes&amp;#34;:[&amp;#34;ReadWriteOncePod&amp;#34;]}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> The ReadWriteOncePod access mode cannot be combined with other access modes.
Make sure ReadWriteOncePod is the only access mode on the PersistentVolume when updating, otherwise the request will fail.
&lt;/div>
&lt;p>Next you need to modify your PersistentVolumeClaim to set ReadWriteOncePod as the only access mode.
You should also set your PersistentVolumeClaim's &lt;code>spec.volumeName&lt;/code> to the name of your PersistentVolume.&lt;/p>
&lt;p>Once this is done, you can recreate your PersistentVolumeClaim and start up your workloads:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># IMPORTANT: Make sure to edit your PVC in cat-pictures-pvc.yaml before applying. You need to:&lt;/span>
&lt;span style="color:#080;font-style:italic"># - Set ReadWriteOncePod as the only access mode&lt;/span>
&lt;span style="color:#080;font-style:italic"># - Set spec.volumeName to &amp;#34;cat-pictures-pv&amp;#34;&lt;/span>
kubectl apply -f cat-pictures-pvc.yaml
kubectl apply -f cat-pictures-writer-deployment.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Lastly you may edit your PersistentVolume's &lt;code>spec.persistentVolumeReclaimPolicy&lt;/code> and set to it back to &lt;code>Delete&lt;/code> if you previously changed it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch pv cat-pictures-pv -p &lt;span style="color:#b44">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;persistentVolumeReclaimPolicy&amp;#34;:&amp;#34;Delete&amp;#34;}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can read &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">Configure a Pod to Use a PersistentVolume for Storage&lt;/a> for more details on working with PersistentVolumes and PersistentVolumeClaims.&lt;/p>
&lt;h2 id="what-volume-plugins-support-this">What volume plugins support this?&lt;/h2>
&lt;p>The only volume plugins that support this are CSI drivers.
SIG Storage does not plan to support this for in-tree plugins because they are being deprecated as part of &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#what-is-the-timeline-status">CSI migration&lt;/a>.
Support may be considered for beta for users that prefer to use the legacy in-tree volume APIs with CSI migration enabled.&lt;/p>
&lt;h2 id="as-a-storage-vendor-how-do-i-add-support-for-this-access-mode-to-my-csi-driver">As a storage vendor, how do I add support for this access mode to my CSI driver?&lt;/h2>
&lt;p>The ReadWriteOncePod access mode will work out of the box without any required updates to CSI drivers, but &lt;a href="#update-your-csi-sidecars">does require updates to CSI sidecars&lt;/a>.
With that being said, if you would like to stay up to date with the latest changes to the CSI specification (v1.5.0+), read on.&lt;/p>
&lt;p>Two new access modes were introduced to the CSI specification in order to disambiguate the legacy &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L418-L420">&lt;code>SINGLE_NODE_WRITER&lt;/code>&lt;/a> access mode.
They are &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L437-L447">&lt;code>SINGLE_NODE_SINGLE_WRITER&lt;/code> and &lt;code>SINGLE_NODE_MULTI_WRITER&lt;/code>&lt;/a>.
In order to communicate to sidecars (like the &lt;a href="https://github.com/kubernetes-csi/external-provisioner">external-provisioner&lt;/a>) that your driver understands and accepts these two new CSI access modes, your driver will also need to advertise the &lt;code>SINGLE_NODE_MULTI_WRITER&lt;/code> capability for the &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L1073-L1081">controller service&lt;/a> and &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L1515-L1524">node service&lt;/a>.&lt;/p>
&lt;p>If you'd like to read up on the motivation for these access modes and capability bits, you can also read the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md#csi-specification-changes-volume-capabilities">CSI Specification Changes, Volume Capabilities&lt;/a> section of KEP-2485 (ReadWriteOncePod PersistentVolume Access Mode).&lt;/p>
&lt;h3 id="update-your-csi-driver-to-use-the-new-interface">Update your CSI driver to use the new interface&lt;/h3>
&lt;p>As a first step you will need to update your driver's &lt;code>container-storage-interface&lt;/code> dependency to v1.5.0+, which contains support for these new access modes and capabilities.&lt;/p>
&lt;h3 id="accept-new-csi-access-modes">Accept new CSI access modes&lt;/h3>
&lt;p>If your CSI driver contains logic for validating CSI access modes for requests , it may need updating.
If it currently accepts &lt;code>SINGLE_NODE_WRITER&lt;/code>, it should be updated to also accept &lt;code>SINGLE_NODE_SINGLE_WRITER&lt;/code> and &lt;code>SINGLE_NODE_MULTI_WRITER&lt;/code>.&lt;/p>
&lt;p>Using the &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/v1.2.2/pkg/gce-pd-csi-driver/utils.go#L116-L130">GCP PD CSI driver validation logic&lt;/a> as an example, here is how it can be extended:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-diff" data-lang="diff">&lt;span style="color:#000080;font-weight:bold">diff --git a/pkg/gce-pd-csi-driver/utils.go b/pkg/gce-pd-csi-driver/utils.go
&lt;/span>&lt;span style="color:#000080;font-weight:bold">index 281242c..b6c5229 100644
&lt;/span>&lt;span style="color:#000080;font-weight:bold">&lt;/span>&lt;span style="color:#a00000">--- a/pkg/gce-pd-csi-driver/utils.go
&lt;/span>&lt;span style="color:#a00000">&lt;/span>&lt;span style="color:#00a000">+++ b/pkg/gce-pd-csi-driver/utils.go
&lt;/span>&lt;span style="color:#00a000">&lt;/span>&lt;span style="color:#800080;font-weight:bold">@@ -123,6 +123,8 @@ func validateAccessMode(am *csi.VolumeCapability_AccessMode) error {
&lt;/span>&lt;span style="color:#800080;font-weight:bold">&lt;/span> case csi.VolumeCapability_AccessMode_SINGLE_NODE_READER_ONLY:
case csi.VolumeCapability_AccessMode_MULTI_NODE_READER_ONLY:
case csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER:
&lt;span style="color:#00a000">+ case csi.VolumeCapability_AccessMode_SINGLE_NODE_SINGLE_WRITER:
&lt;/span>&lt;span style="color:#00a000">+ case csi.VolumeCapability_AccessMode_SINGLE_NODE_MULTI_WRITER:
&lt;/span>&lt;span style="color:#00a000">&lt;/span> default:
return fmt.Errorf(&amp;#34;%v access mode is not supported for for PD&amp;#34;, am.GetMode())
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="advertise-new-csi-controller-and-node-service-capabilities">Advertise new CSI controller and node service capabilities&lt;/h3>
&lt;p>Your CSI driver will also need to return the new &lt;code>SINGLE_NODE_MULTI_WRITER&lt;/code> capability as part of the &lt;code>ControllerGetCapabilities&lt;/code> and &lt;code>NodeGetCapabilities&lt;/code> RPCs.&lt;/p>
&lt;p>Using the &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/v1.2.2/pkg/gce-pd-csi-driver/gce-pd-driver.go#L54-L77">GCP PD CSI driver capability advertisement logic&lt;/a> as an example, here is how it can be extended:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-diff" data-lang="diff">&lt;span style="color:#000080;font-weight:bold">diff --git a/pkg/gce-pd-csi-driver/gce-pd-driver.go b/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span>&lt;span style="color:#000080;font-weight:bold">index 45903f3..0d7ea26 100644
&lt;/span>&lt;span style="color:#000080;font-weight:bold">&lt;/span>&lt;span style="color:#a00000">--- a/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span>&lt;span style="color:#a00000">&lt;/span>&lt;span style="color:#00a000">+++ b/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span>&lt;span style="color:#00a000">&lt;/span>&lt;span style="color:#800080;font-weight:bold">@@ -56,6 +56,8 @@ func (gceDriver *GCEDriver) SetupGCEDriver(name, vendorVersion string, extraVolu
&lt;/span>&lt;span style="color:#800080;font-weight:bold">&lt;/span> csi.VolumeCapability_AccessMode_SINGLE_NODE_WRITER,
csi.VolumeCapability_AccessMode_MULTI_NODE_READER_ONLY,
csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER,
&lt;span style="color:#00a000">+ csi.VolumeCapability_AccessMode_SINGLE_NODE_SINGLE_WRITER,
&lt;/span>&lt;span style="color:#00a000">+ csi.VolumeCapability_AccessMode_SINGLE_NODE_MULTI_WRITER,
&lt;/span>&lt;span style="color:#00a000">&lt;/span> }
gceDriver.AddVolumeCapabilityAccessModes(vcam)
csc := []csi.ControllerServiceCapability_RPC_Type{
&lt;span style="color:#800080;font-weight:bold">@@ -67,12 +69,14 @@ func (gceDriver *GCEDriver) SetupGCEDriver(name, vendorVersion string, extraVolu
&lt;/span>&lt;span style="color:#800080;font-weight:bold">&lt;/span> csi.ControllerServiceCapability_RPC_EXPAND_VOLUME,
csi.ControllerServiceCapability_RPC_LIST_VOLUMES,
csi.ControllerServiceCapability_RPC_LIST_VOLUMES_PUBLISHED_NODES,
&lt;span style="color:#00a000">+ csi.ControllerServiceCapability_RPC_SINGLE_NODE_MULTI_WRITER,
&lt;/span>&lt;span style="color:#00a000">&lt;/span> }
gceDriver.AddControllerServiceCapabilities(csc)
ns := []csi.NodeServiceCapability_RPC_Type{
csi.NodeServiceCapability_RPC_STAGE_UNSTAGE_VOLUME,
csi.NodeServiceCapability_RPC_EXPAND_VOLUME,
csi.NodeServiceCapability_RPC_GET_VOLUME_STATS,
&lt;span style="color:#00a000">+ csi.NodeServiceCapability_RPC_SINGLE_NODE_MULTI_WRITER,
&lt;/span>&lt;span style="color:#00a000">&lt;/span> }
gceDriver.AddNodeServiceCapabilities(ns)
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="implement-nodepublishvolume-behavior">Implement &lt;code>NodePublishVolume&lt;/code> behavior&lt;/h3>
&lt;p>The CSI spec outlines expected behavior for the &lt;code>NodePublishVolume&lt;/code> RPC when called more than once for the same volume but with different arguments (like the target path).
Please refer to &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/spec.md#nodepublishvolume">the second table in the NodePublishVolume section of the CSI spec&lt;/a> for more details on expected behavior when implementing in your driver.&lt;/p>
&lt;h3 id="update-your-csi-sidecars">Update your CSI sidecars&lt;/h3>
&lt;p>When deploying your CSI drivers, you must update the following CSI sidecars to versions that depend on CSI spec v1.5.0+ and the Kubernetes v1.22 API.
The minimum required versions are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0">csi-provisioner:v3.0.0+&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0">csi-attacher:v3.3.0+&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0">csi-resizer:v1.3.0+&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>As part of the beta graduation for this feature, SIG Storage plans to update the Kubenetes scheduler to support pod preemption in relation to ReadWriteOncePod storage.
This means if two pods request a PersistentVolumeClaim with ReadWriteOncePod, the pod with highest priority will gain access to the PersistentVolumeClaim and any pod with lower priority will be preempted from the node and be unable to access the PersistentVolumeClaim.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>Please see &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md">KEP-2485&lt;/a> for more details on the ReadWriteOncePod access mode and motivations for CSI spec changes.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>The &lt;a href="https://kubernetes.slack.com/messages/csi">Kubernetes #csi Slack channel&lt;/a> and any of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">standard SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and the CSI teams.&lt;/p>
&lt;p>Special thanks to the following people for their insightful reviews and design considerations:&lt;/p>
&lt;ul>
&lt;li>Abdullah Gharaibeh (ahg-g)&lt;/li>
&lt;li>Aldo Culquicondor (alculquicondor)&lt;/li>
&lt;li>Ben Swartzlander (bswartz)&lt;/li>
&lt;li>Deep Debroy (ddebroy)&lt;/li>
&lt;li>Hemant Kumar (gnufied)&lt;/li>
&lt;li>Humble Devassy Chirammal (humblec)&lt;/li>
&lt;li>James DeFelice (jdef)&lt;/li>
&lt;li>Jan Šafránek (jsafrane)&lt;/li>
&lt;li>Jing Xu (jingxu97)&lt;/li>
&lt;li>Jordan Liggitt (liggitt)&lt;/li>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;li>Saad Ali (saad-ali)&lt;/li>
&lt;li>Tim Hockin (thockin)&lt;/li>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;/ul>
&lt;p>If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG).
We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Alpha in Kubernetes v1.22: API Server Tracing</title><link>https://kubernetes.io/blog/2021/09/03/api-server-tracing/</link><pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/09/03/api-server-tracing/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> David Ashpole (Google)&lt;/p>
&lt;p>In distributed systems, it can be hard to figure out where problems are. You grep through one component's logs just to discover that the source of your problem is in another component. You search there only to discover that you need to enable debug logs to figure out what really went wrong... And it goes on. The more complex the path your request takes, the harder it is to answer questions about where it went. I've personally spent many hours doing this dance with a variety of Kubernetes components. Distributed tracing is a tool which is designed to help in these situations, and the Kubernetes API Server is, perhaps, the most important Kubernetes component to be able to debug. At Kubernetes' Sig Instrumentation, our mission is to make it easier to understand what's going on in your cluster, and we are happy to announce that distributed tracing in the Kubernetes API Server reached alpha in 1.22.&lt;/p>
&lt;h2 id="what-is-tracing">What is Tracing?&lt;/h2>
&lt;p>Distributed tracing links together a bunch of super-detailed information from multiple different sources, and structures that telemetry into a single tree for that request. Unlike logging, which limits the quantity of data ingested by using log levels, tracing collects all of the details and uses sampling to collect only a small percentage of requests. This means that once you have a trace which demonstrates an issue, you should have all the information you need to root-cause the problem--no grepping for object UID required! My favorite aspect, though, is how useful the visualizations of traces are. Even if you don't understand the inner workings of the API Server, or don't have a clue what an etcd &amp;quot;Transaction&amp;quot; is, I'd wager you (yes, you!) could tell me roughly what the order of events was, and which components were involved in the request. If some step takes a long time, it is easy to tell where the problem is.&lt;/p>
&lt;h2 id="why-opentelemetry">Why OpenTelemetry?&lt;/h2>
&lt;p>It's important that Kubernetes works well for everyone, regardless of who manages your infrastructure, or which vendors you choose to integrate with. That is particularly true for Kubernetes' integrations with telemetry solutions. OpenTelemetry, being a CNCF project, shares these core values, and is creating exactly what we need in Kubernetes: A set of open standards for Tracing client library APIs and a standard trace format. By using OpenTelemetry, we can ensure users have the freedom to choose their backend, and ensure vendors have a level playing field. The timing couldn't be better: the OpenTelemetry golang API and SDK are very close to their 1.0 release, and will soon offer backwards-compatibility for these open standards.&lt;/p>
&lt;h2 id="why-instrument-the-api-server">Why instrument the API Server?&lt;/h2>
&lt;p>The Kubernetes API Server is a great candidate for tracing for a few reasons:&lt;/p>
&lt;ul>
&lt;li>It follows the standard &amp;quot;RPC&amp;quot; model (serve a request by making requests to downstream components), which makes it easy to instrument.&lt;/li>
&lt;li>Users are latency-sensitive: If a request takes more than 10 seconds to complete, many clients will time-out.&lt;/li>
&lt;li>It has a complex service topology: A single request could require consulting a dozen webhooks, or involve multiple requests to etcd.&lt;/li>
&lt;/ul>
&lt;h2 id="trying-out-apiserver-tracing-with-a-webhook">Trying out APIServer Tracing with a webhook&lt;/h2>
&lt;h3 id="enabling-api-server-tracing">Enabling API Server Tracing&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Enable the APIServerTracing &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature-gate&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Set our configuration for tracing by pointing the &lt;code>--tracing-config-file&lt;/code> flag on the kube-apiserver at our config file, which contains:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apiserver.config.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TracingConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># 1% sampling rate&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">samplingRatePerMillion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="enabling-etcd-tracing">Enabling Etcd Tracing&lt;/h3>
&lt;p>Add &lt;code>--experimental-enable-distributed-tracing&lt;/code>, &lt;code>--experimental-distributed-tracing-address=0.0.0.0:4317&lt;/code>, &lt;code>--experimental-distributed-tracing-service-name=etcd&lt;/code> flags to etcd to enable tracing. Note that this traces every request, so it will probably generate a lot of traces if you enable it.&lt;/p>
&lt;h3 id="example-trace-list-nodes">Example Trace: List Nodes&lt;/h3>
&lt;p>I could've used any trace backend, but decided to use Jaeger, since it is one of the most popular open-source tracing projects. I deployed &lt;a href="https://hub.docker.com/r/jaegertracing/all-in-one">the Jaeger All-in-one container&lt;/a> in my cluster, deployed &lt;a href="https://github.com/open-telemetry/opentelemetry-collector">the OpenTelemetry collector&lt;/a> on my control-plane node (&lt;a href="https://github.com/dashpole/dashpole_demos/tree/master/otel/controlplane">example&lt;/a>), and captured traces like this one:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-09-03-api-server-tracing/example-trace-1.png" alt="Jaeger screenshot showing API server and etcd trace" title="Jaeger screenshot showing API server and etcd trace">&lt;/p>
&lt;p>The teal lines are from the API Server, and includes it serving a request to &lt;code>/api/v1/nodes&lt;/code>, and issuing a grpc &lt;code>Range&lt;/code> RPC to ETCD. The yellow-ish line is from ETCD handling the &lt;code>Range&lt;/code> RPC.&lt;/p>
&lt;h3 id="example-trace-create-pod-with-mutating-webhook">Example Trace: Create Pod with Mutating Webhook&lt;/h3>
&lt;p>I instrumented the &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/tree/master/examples/builtins">example webhook&lt;/a> with OpenTelemetry (I had to &lt;a href="https://github.com/dashpole/controller-runtime/commit/85fdda7ba03dd2c22ef62c1a3dbdf5aa651f90da">patch&lt;/a> controller-runtime, but it makes a neat demo), and routed traces to Jaeger as well. I collected traces like this one:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-09-03-api-server-tracing/example-trace-2.png" alt="Jaeger screenshot showing API server, admission webhook, and etcd trace" title="Jaeger screenshot showing API server, admission webhook, and etcd trace">&lt;/p>
&lt;p>Compared with the previous trace, there are two new spans: A teal span from the API Server making a request to the admission webhook, and a brown span from the admission webhook serving the request. Even if you didn't instrument your webhook, you would still get the span from the API Server making the request to the webhook.&lt;/p>
&lt;h2 id="get-involved">Get involved!&lt;/h2>
&lt;p>As this is our first attempt at adding distributed tracing to a Kubernetes component, there is probably a lot we can improve! If my struggles resonated with you, or if you just want to try out the latest Kubernetes has to offer, please give the feature a try and open issues with any problem you encountered and ways you think the feature could be improved.&lt;/p>
&lt;p>This is just the very beginning of what we can do with distributed tracing in Kubernetes. If there are other components you think would benefit from distributed tracing, or want to help bring API Server Tracing to GA, join sig-instrumentation at our &lt;a href="https://github.com/kubernetes/community/tree/master/sig-instrumentation#instrumentation-special-interest-group">regular meetings&lt;/a> and get involved!&lt;/p></description></item><item><title>Blog: Kubernetes 1.22: A New Design for Volume Populators</title><link>https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/</link><pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong>
Ben Swartzlander (NetApp)&lt;/p>
&lt;p>Kubernetes v1.22, released earlier this month, introduced a redesigned approach for volume
populators. Originally implemented
in v1.18, the API suffered from backwards compatibility issues. Kubernetes v1.22 includes a new API
field called &lt;code>dataSourceRef&lt;/code> that fixes these problems.&lt;/p>
&lt;h2 id="data-sources">Data sources&lt;/h2>
&lt;p>Earlier Kubernetes releases already added a &lt;code>dataSource&lt;/code> field into the
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim&lt;/a> API,
used for cloning volumes and creating volumes from snapshots. You could use the &lt;code>dataSource&lt;/code> field when
creating a new PVC, referencing either an existing PVC or a VolumeSnapshot in the same namespace.
That also modified the normal provisioning process so that instead of yielding an empty volume, the
new PVC contained the same data as either the cloned PVC or the cloned VolumeSnapshot.&lt;/p>
&lt;p>Volume populators embrace the same design idea, but extend it to any type of object, as long
as there exists a &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resource&lt;/a>
to define the data source, and a populator controller to implement the logic. Initially,
the &lt;code>dataSource&lt;/code> field was directly extended to allow arbitrary objects, if the &lt;code>AnyVolumeDataSource&lt;/code>
feature gate was enabled on a cluster. That change unfortunately caused backwards compatibility
problems, and so the new &lt;code>dataSourceRef&lt;/code> field was born.&lt;/p>
&lt;p>In v1.22 if the &lt;code>AnyVolumeDataSource&lt;/code> feature gate is enabled, the &lt;code>dataSourceRef&lt;/code> field is
added, which behaves similarly to the &lt;code>dataSource&lt;/code> field except that it allows arbitrary
objects to be specified. The API server ensures that the two fields always have the same
contents, and neither of them are mutable. The differences is that at creation time
&lt;code>dataSource&lt;/code> allows only PVCs or VolumeSnapshots, and ignores all other values, while
&lt;code>dataSourceRef&lt;/code> allows most types of objects, and in the few cases it doesn't allow an
object (core objects other than PVCs) a validation error occurs.&lt;/p>
&lt;p>When this API change graduates to stable, we would deprecate using &lt;code>dataSource&lt;/code> and recommend
using &lt;code>dataSourceRef&lt;/code> field for all use cases.
In the v1.22 release, &lt;code>dataSourceRef&lt;/code> is available (as an alpha feature) specifically for cases
where you want to use for custom volume populators.&lt;/p>
&lt;h2 id="using-populators">Using populators&lt;/h2>
&lt;p>Every volume populator must have one or more CRDs that it supports. Administrators may
install the CRD and the populator controller and then PVCs with a &lt;code>dataSourceRef&lt;/code> specifies
a CR of the type that the populator supports will be handled by the populator controller
instead of the CSI driver directly.&lt;/p>
&lt;p>Underneath the covers, the CSI driver is still invoked to create an empty volume, which
the populator controller fills with the appropriate data. The PVC doesn't bind to the PV
until it's fully populated, so it's safe to define a whole application manifest including
pod and PVC specs and the pods won't begin running until everything is ready, just as if
the PVC was a clone of another PVC or VolumeSnapshot.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>PVCs with data sources are still noticed by the external-provisioner sidecar for the
related storage class (assuming a CSI provisioner is used), but because the sidecar
doesn't understand the data source kind, it doesn't do anything. The populator controller
is also watching for PVCs with data sources of a kind that it understands and when it
sees one, it creates a temporary PVC of the same size, volume mode, storage class,
and even on the same topology (if topology is used) as the original PVC. The populator
controller creates a worker pod that attaches to the volume and writes the necessary
data to it, then detaches from the volume and the populator controller rebinds the PV
from the temporary PVC to the orignal PVC.&lt;/p>
&lt;h2 id="trying-it-out">Trying it out&lt;/h2>
&lt;p>The following things are required to use volume populators:&lt;/p>
&lt;ul>
&lt;li>Enable the &lt;code>AnyVolumeDataSource&lt;/code> feature gate&lt;/li>
&lt;li>Install a CRD for the specific data source / populator&lt;/li>
&lt;li>Install the populator controller itself&lt;/li>
&lt;/ul>
&lt;p>Populator controllers may use the &lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a>
library to do most of the Kubernetes API level work. Individual populators only need to
provide logic for actually writing data into the volume based on a particular CR
instance. This library provides a sample populator implementation.&lt;/p>
&lt;p>These optional components improve user experience:&lt;/p>
&lt;ul>
&lt;li>Install the VolumePopulator CRD&lt;/li>
&lt;li>Create a VolumePopulator custom respource for each specific data source&lt;/li>
&lt;li>Install the &lt;a href="https://github.com/kubernetes-csi/volume-data-source-validator">volume data source validator&lt;/a>
controller (alpha)&lt;/li>
&lt;/ul>
&lt;p>The purpose of these components is to generate warning events on PVCs with data sources
for which there is no populator.&lt;/p>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>To see how this works, you can install the sample &amp;quot;hello&amp;quot; populator and try it
out.&lt;/p>
&lt;p>First install the volume-data-source-validator controller.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/master/client/config/crd/populator.storage.k8s.io_volumepopulators.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/master/deploy/kubernetes/rbac-data-source-validator.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/master/deploy/kubernetes/setup-data-source-validator.yaml
&lt;/code>&lt;/pre>&lt;p>Next install the example populator.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/master/example/hello-populator/crd.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/master/example/hello-populator/deploy.yaml
&lt;/code>&lt;/pre>&lt;p>Create an instance of the &lt;code>Hello&lt;/code> CR, with some text.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileContents&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello, world!&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create a PVC that refers to that CR as its data source.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10Mi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSourceRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, run a job that reads the file in the PVC.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-container&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>busybox:latest&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- cat&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /mnt/example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/mnt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeClaim&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Wait for the job to complete (including all of its dependencies).&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl wait --for=condition=Complete job/example-job
&lt;/code>&lt;/pre>&lt;p>And last examine the log from the job.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl logs job/example-job
Hello, world!
&lt;/code>&lt;/pre>&lt;p>Note that the volume already contained a text file with the string contents from
the CR. This is only the simplest example. Actual populators can set up the volume
to contain arbitrary contents.&lt;/p>
&lt;h2 id="how-to-write-your-own-volume-populator">How to write your own volume populator&lt;/h2>
&lt;p>Developers interested in writing new poplators are encouraged to use the
&lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a> library
and to only supply a small controller wrapper around the library, and a pod image
capable of attaching to volumes and writing the appropriate data to the volume.&lt;/p>
&lt;p>Individual populators can be extremely generic such that they work with every type
of PVC, or they can do vendor specific things to rapidly fill a volume with data
if the volume was provisioned by a specific CSI driver from the same vendor, for
example, by communicating directly with the storage for that volume.&lt;/p>
&lt;h2 id="the-future">The future&lt;/h2>
&lt;p>As this feature is still in alpha, we expect to update the out of tree controllers
with more tests and documentation. The community plans to eventually re-implement
the populator library as a sidecar, for ease of operations.&lt;/p>
&lt;p>We hope to see some official community-supported populators for some widely-shared
use cases. Also, we expect that volume populators will be used by backup vendors
as a way to &amp;quot;restore&amp;quot; backups to volumes, and possibly a standardized API to do
this will evolve.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>The enhancement proposal,
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators">Volume Populators&lt;/a>, includes lots of detail about the history and technical implementation
of this feature.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources">Volume populators and data sources&lt;/a>, within the documentation topic about persistent volumes,
explains how to use this feature in your cluster.&lt;/p>
&lt;p>Please get involved by joining the Kubernetes storage SIG to help us enhance this
feature. There are a lot of good ideas already and we'd be thrilled to have more!&lt;/p></description></item><item><title>Blog: Minimum Ready Seconds for StatefulSets</title><link>https://kubernetes.io/blog/2021/08/27/minreadyseconds-statefulsets/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/27/minreadyseconds-statefulsets/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Ravi Gudimetla (Red Hat), Maciej Szulik (Red Hat)&lt;/p>
&lt;p>This blog describes the notion of Availability for &lt;code>StatefulSet&lt;/code> workloads, and a new alpha feature in Kubernetes 1.22 which adds &lt;code>minReadySeconds&lt;/code> configuration for &lt;code>StatefulSets&lt;/code>.&lt;/p>
&lt;h2 id="what-problems-does-this-solve">What problems does this solve?&lt;/h2>
&lt;p>Prior to Kubernetes 1.22 release, once a &lt;code>StatefulSet&lt;/code> &lt;code>Pod&lt;/code> is in the &lt;code>Ready&lt;/code> state it is considered &lt;code>Available&lt;/code> to receive traffic. For some of the &lt;code>StatefulSet&lt;/code> workloads, it may not be the case. For example, a workload like Prometheus with multiple instances of Alertmanager, it should be considered &lt;code>Available&lt;/code> only when Alertmanager's state transfer is complete, not when the &lt;code>Pod&lt;/code> is in &lt;code>Ready&lt;/code> state. Since &lt;code>minReadySeconds&lt;/code> adds buffer, the state transfer may be complete before the &lt;code>Pod&lt;/code> becomes &lt;code>Available&lt;/code>. While this is not a fool proof way of identifying if the state transfer is complete or not, it gives a way to the end user to express their intention of waiting for sometime before the &lt;code>Pod&lt;/code> is considered &lt;code>Available&lt;/code> and it is ready to serve requests.&lt;/p>
&lt;p>Another case, where &lt;code>minReadySeconds&lt;/code> helps is when using &lt;code>LoadBalancer&lt;/code> &lt;code>Services&lt;/code> with cloud providers. Since &lt;code>minReadySeconds&lt;/code> adds latency after a &lt;code>Pod&lt;/code> is &lt;code>Ready&lt;/code>, it provides buffer time to prevent killing pods in rotation before new pods show up. Imagine a load balancer in unhappy path taking 10-15s to propagate. If you have 2 replicas then, you'd kill the second replica only after the first one is up but in reality, first replica cannot be seen because it is not yet ready to serve requests.&lt;/p>
&lt;p>So, in general, the notion of &lt;code>Availability&lt;/code> in &lt;code>StatefulSets&lt;/code> is pretty useful and this feature helps in solving the above problems. This is a feature that already exists for &lt;code>Deployments&lt;/code> and &lt;code>DaemonSets&lt;/code> and we now have them for &lt;code>StatefulSets&lt;/code> too to give users consistent workload experience.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>The statefulSet controller watches for both &lt;code>StatefulSets&lt;/code> and the &lt;code>Pods&lt;/code> associated with them. When the feature gate associated with this feature is enabled, the statefulSet controller identifies how long a particular &lt;code>Pod&lt;/code> associated with a &lt;code>StatefulSet&lt;/code> has been in the &lt;code>Running&lt;/code> state.&lt;/p>
&lt;p>If this value is greater than or equal to the time specified by the end user in &lt;code>.spec.minReadySeconds&lt;/code> field, the statefulSet controller updates a field called &lt;code>availableReplicas&lt;/code> in the &lt;code>StatefulSet&lt;/code>'s status subresource to include this &lt;code>Pod&lt;/code>. The &lt;code>status.availableReplicas&lt;/code> in &lt;code>StatefulSet&lt;/code>'s status is an integer field which tracks the number of pods that are &lt;code>Available&lt;/code>.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>You are required to prepare the following things in order to try out the feature:&lt;/p>
&lt;ul>
&lt;li>Download and install a kubectl greater than v1.22.0 version&lt;/li>
&lt;li>Switch on the feature gate with the command line flag &lt;code>--feature-gates=StatefulSetMinReadySeconds=true&lt;/code> on &lt;code>kube-apiserver&lt;/code> and &lt;code>kube-controller-manager&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>After successfully starting &lt;code>kube-apiserver&lt;/code> and &lt;code>kube-controller-manager&lt;/code>, you will see &lt;code>AvailableReplicas&lt;/code> in the status and &lt;code>minReadySeconds&lt;/code> of spec (with a default value of 0).&lt;/p>
&lt;p>Specify a value for &lt;code>minReadySeconds&lt;/code> for any StatefulSet and you can check if &lt;code>Pods&lt;/code> are available or not by checking &lt;code>AvailableReplicas&lt;/code> field using:
&lt;code>kubectl get statefulset/&amp;lt;name_of_the_statefulset&amp;gt; -o yaml&lt;/code>&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;ul>
&lt;li>Read the KEP: &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2599-minreadyseconds-for-statefulsets#readme">minReadySeconds for StatefulSets&lt;/a>&lt;/li>
&lt;li>Read the documentation: &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds">Minimum ready seconds&lt;/a> for StatefulSet&lt;/li>
&lt;li>Review the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/">API definition&lt;/a> for StatefulSet&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Please reach out to us in the &lt;a href="https://kubernetes.slack.com/archives/C18NZM5K9">#sig-apps&lt;/a> channel on Slack (visit &lt;a href="https://slack.k8s.io/">https://slack.k8s.io/&lt;/a> for an invitation if you need one), or on the SIG Apps mailing list: &lt;a href="mailto:kubernetes-sig-apps@googlegroups.com">kubernetes-sig-apps@googlegroups.com&lt;/a>&lt;/p></description></item><item><title>Blog: Enable seccomp for all workloads with a new v1.22 alpha feature</title><link>https://kubernetes.io/blog/2021/08/25/seccomp-default/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/25/seccomp-default/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Sascha Grunert, Red Hat&lt;/p>
&lt;p>This blog post is about a new Kubernetes feature introduced in v1.22, which adds
an additional security layer on top of the existing seccomp support. Seccomp is
a security mechanism for Linux processes to filter system calls (syscalls) based
on a set of defined rules. Applying seccomp profiles to containerized workloads
is one of the key tasks when it comes to enhancing the security of the
application deployment. Developers, site reliability engineers and
infrastructure administrators have to work hand in hand to create, distribute
and maintain the profiles over the applications life-cycle.&lt;/p>
&lt;p>You can use the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1">&lt;code>securityContext&lt;/code>&lt;/a> field of Pods and their
containers can be used to adjust security related configurations of the
workload. Kubernetes introduced dedicated &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1">seccomp related API
fields&lt;/a> in this &lt;code>SecurityContext&lt;/code> with the &lt;a href="https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/#graduated-to-stable">graduation of seccomp to
General Availability (GA)&lt;/a> in v1.19.0. This enhancement allowed an easier
way to specify if the whole pod or a specific container should run as:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Unconfined&lt;/code>: seccomp will not be enabled&lt;/li>
&lt;li>&lt;code>RuntimeDefault&lt;/code>: the container runtimes default profile will be used&lt;/li>
&lt;li>&lt;code>Localhost&lt;/code>: a node local profile will be applied, which is being referenced
by a relative path to the seccomp profile root (&lt;code>&amp;lt;kubelet-root-dir&amp;gt;/seccomp&lt;/code>)
of the kubelet&lt;/li>
&lt;/ul>
&lt;p>With the graduation of seccomp, nothing has changed from an overall security
perspective, because &lt;code>Unconfined&lt;/code> is still the default. This is totally fine if
you consider this from the upgrade path and backwards compatibility perspective of
Kubernetes releases. But it also means that it is more likely that a workload
runs without seccomp at all, which should be fixed in the long term.&lt;/p>
&lt;h2 id="seccompdefault-to-the-rescue">&lt;code>SeccompDefault&lt;/code> to the rescue&lt;/h2>
&lt;p>Kubernetes v1.22.0 introduces a new kubelet &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates">feature gate&lt;/a>
&lt;code>SeccompDefault&lt;/code>, which has been added in &lt;code>alpha&lt;/code> state as every other new
feature. This means that it is disabled by default and can be enabled manually
for every single Kubernetes node.&lt;/p>
&lt;p>What does the feature do? Well, it just changes the default seccomp profile from
&lt;code>Unconfined&lt;/code> to &lt;code>RuntimeDefault&lt;/code>. If not specified differently in the pod
manifest, then the feature will add a higher set of security constraints by
using the default profile of the container runtime. These profiles may differ
between runtimes like &lt;a href="https://github.com/cri-o/cri-o/blob/fe30d62/vendor/github.com/containers/common/pkg/seccomp/default_linux.go#L45">CRI-O&lt;/a> or &lt;a href="https://github.com/containerd/containerd/blob/e1445df/contrib/seccomp/seccomp_default.go#L51">containerd&lt;/a>. They also differ for
its used hardware architectures. But generally speaking, those default profiles
allow a common amount of syscalls while blocking the more dangerous ones, which
are unlikely or unsafe to be used in a containerized application.&lt;/p>
&lt;h3 id="enabling-the-feature">Enabling the feature&lt;/h3>
&lt;p>Two kubelet configuration changes have to be made to enable the feature:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Enable the feature&lt;/strong> gate by setting the &lt;code>SeccompDefault=true&lt;/code> via the command
line (&lt;code>--feature-gates&lt;/code>) or the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file">kubelet configuration&lt;/a> file.&lt;/li>
&lt;li>&lt;strong>Turn on the feature&lt;/strong> by enabling the feature by adding the
&lt;code>--seccomp-default&lt;/code> command line flag or via the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file">kubelet
configuration&lt;/a> file (&lt;code>seccompDefault: true&lt;/code>).&lt;/li>
&lt;/ol>
&lt;p>The kubelet will error on startup if only one of the above steps have been done.&lt;/p>
&lt;h3 id="trying-it-out">Trying it out&lt;/h3>
&lt;p>If the feature is enabled on a node, then you can create a new workload like
this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-container&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx:1.21&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now it is possible to inspect the used seccomp profile by using
&lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> while investigating the containers &lt;a href="https://github.com/opencontainers/runtime-spec/blob/0c021c1/config-linux.md#seccomp">runtime
specification&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#b8860b">CONTAINER_ID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>sudo crictl ps -q --name&lt;span style="color:#666">=&lt;/span>test-container&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
sudo crictl inspect &lt;span style="color:#b8860b">$CONTAINER_ID&lt;/span> | jq .info.runtimeSpec.linux.seccomp
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;defaultAction&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;SCMP_ACT_ERRNO&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;architectures&amp;#34;: &lt;/span>[&lt;span style="color:#b44">&amp;#34;SCMP_ARCH_X86_64&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;SCMP_ARCH_X86&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;SCMP_ARCH_X32&amp;#34;&lt;/span>],&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;syscalls&amp;#34;: &lt;/span>[&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;names&amp;#34;: &lt;/span>[&lt;span style="color:#b44">&amp;#34;_llseek&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;_newselect&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;accept&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>…, &amp;#34;write&amp;#34;, &amp;#34;writev&amp;#34;],&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;action&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;SCMP_ACT_ALLOW&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>},&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>…&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>}&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can see that the lower level container runtime (&lt;a href="https://github.com/cri-o/cri-o">CRI-O&lt;/a> and
&lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> in our case), successfully applied the default seccomp profile.
This profile denies all syscalls per default, while allowing commonly used ones
like &lt;a href="https://man7.org/linux/man-pages/man2/accept.2.html">&lt;code>accept&lt;/code>&lt;/a> or &lt;a href="https://man7.org/linux/man-pages/man2/write.2.html">&lt;code>write&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Please note that the feature will not influence any Kubernetes API for now.
Therefore, it is not possible to retrieve the used seccomp profile via &lt;code>kubectl&lt;/code>
&lt;code>get&lt;/code> or &lt;code>describe&lt;/code> if the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1">&lt;code>SeccompProfile&lt;/code>&lt;/a> field is unset within the
&lt;code>SecurityContext&lt;/code>.&lt;/p>
&lt;p>The feature also works when using multiple containers within a pod, for example
if you create a pod like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-container-nginx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx:1.21&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">seccompProfile&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Unconfined&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-container-redis&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>redis:6.2&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>then you should see that the &lt;code>test-container-nginx&lt;/code> runs without a seccomp profile:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo crictl inspect &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>sudo crictl ps -q --name&lt;span style="color:#666">=&lt;/span>test-container-nginx&lt;span style="color:#a2f;font-weight:bold">)&lt;/span> |
jq &lt;span style="color:#b44">&amp;#39;.info.runtimeSpec.linux.seccomp == null&amp;#39;&lt;/span>
&lt;span style="color:#a2f">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Whereas the container &lt;code>test-container-redis&lt;/code> runs with &lt;code>RuntimeDefault&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo crictl inspect &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>sudo crictl ps -q --name&lt;span style="color:#666">=&lt;/span>test-container-redis&lt;span style="color:#a2f;font-weight:bold">)&lt;/span> |
jq &lt;span style="color:#b44">&amp;#39;.info.runtimeSpec.linux.seccomp != null&amp;#39;&lt;/span>
&lt;span style="color:#a2f">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The same applies to the pod itself, which also runs with the default profile:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo crictl inspectp &lt;span style="color:#666">(&lt;/span>sudo crictl pods -q --name test-pod&lt;span style="color:#666">)&lt;/span> |
jq &lt;span style="color:#b44">&amp;#39;.info.runtimeSpec.linux.seccomp != null&amp;#39;&lt;/span>
&lt;span style="color:#a2f">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="upgrade-strategy">Upgrade strategy&lt;/h3>
&lt;p>It is recommended to enable the feature in multiple steps, whereas different
risks and mitigations exist for each one.&lt;/p>
&lt;h4 id="feature-gate-enabling">Feature gate enabling&lt;/h4>
&lt;p>Enabling the feature gate at the kubelet level will not turn on the feature, but
will make it possible by using the &lt;code>SeccompDefault&lt;/code> kubelet configuration or the
&lt;code>--seccomp-default&lt;/code> CLI flag. This can be done by an administrator for the whole
cluster or only a set of nodes.&lt;/p>
&lt;h4 id="testing-the-application">Testing the Application&lt;/h4>
&lt;p>If you're trying this within a dedicated test environment, you have to ensure
that the application code does not trigger syscalls blocked by the
&lt;code>RuntimeDefault&lt;/code> profile before enabling the feature on a node. This can be done
by:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Recommended&lt;/em>: Analyzing the code (manually or by running the application with
&lt;a href="https://man7.org/linux/man-pages/man1/strace.1.html">strace&lt;/a>) for any executed syscalls which may be blocked by the
default profiles. If that's the case, then you can override the default by
explicitly setting the pod or container to run as &lt;code>Unconfined&lt;/code>. Alternatively,
you can create a custom seccomp profile (see optional step below).
profile based on the default by adding the additional syscalls to the
&lt;code>&amp;quot;action&amp;quot;: &amp;quot;SCMP_ACT_ALLOW&amp;quot;&lt;/code> section.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Recommended&lt;/em>: Manually set the profile to the target workload and use a
rolling upgrade to deploy into production. Rollback the deployment if the
application does not work as intended.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Optional&lt;/em>: Run the application against an end-to-end test suite to trigger
all relevant code paths with &lt;code>RuntimeDefault&lt;/code> enabled. If a test fails, use
the same mitigation as mentioned above.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Optional&lt;/em>: Create a custom seccomp profile based on the default and change
its default action from &lt;code>SCMP_ACT_ERRNO&lt;/code> to &lt;code>SCMP_ACT_LOG&lt;/code>. This means that
the seccomp filter for unknown syscalls will have no effect on the application
at all, but the system logs will now indicate which syscalls may be blocked.
This requires at least a Kernel version 4.14 as well as a recent &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>
release. Monitor the application hosts audit logs (defaults to
&lt;code>/var/log/audit/audit.log&lt;/code>) or syslog entries (defaults to &lt;code>/var/log/syslog&lt;/code>)
for syscalls via &lt;code>type=SECCOMP&lt;/code> (for audit) or &lt;code>type=1326&lt;/code> (for syslog).
Compare the syscall ID with those &lt;a href="https://github.com/torvalds/linux/blob/7bb7f2a/arch/x86/entry/syscalls/syscall_64.tbl">listed in the Linux Kernel
sources&lt;/a> and add them to the custom profile. Be aware that custom
audit policies may lead into missing syscalls, depending on the configuration
of auditd.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Optional&lt;/em>: Use cluster additions like the &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator">Security Profiles Operator&lt;/a>
for profiling the application via its &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/c90ef3a/installation-usage.md#record-profiles-from-workloads-with-profilerecordings">log enrichment&lt;/a> capabilities or
recording a profile by using its &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/c90ef3a/installation-usage.md#using-the-log-enricher">recording feature&lt;/a>. This makes the
above mentioned manual log investigation obsolete.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="deploying-the-modified-application">Deploying the modified application&lt;/h4>
&lt;p>Based on the outcome of the application tests, it may be required to change the
application deployment by either specifying &lt;code>Unconfined&lt;/code> or a custom seccomp
profile. This is not the case if the application works as intended with
&lt;code>RuntimeDefault&lt;/code>.&lt;/p>
&lt;h4 id="enable-the-kubelet-configuration">Enable the kubelet configuration&lt;/h4>
&lt;p>If everything went well, then the feature is ready to be enabled by the kubelet
configuration or its corresponding CLI flag. This should be done on a per-node
basis to reduce the overall risk of missing a syscall during the investigations
when running the application tests. If it's possible to monitor audit logs
within the cluster, then it's recommended to do this for eventually missed
seccomp events. If the application works as intended then the feature can be
enabled for further nodes within the cluster.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Thank you for reading this blog post! I hope you enjoyed to see how the usage of
seccomp profiles has been evolved in Kubernetes over the past releases as much
as I do. On your own cluster, change the default seccomp profile to
&lt;code>RuntimeDefault&lt;/code> (using this new feature) and see the security benefits, and, of
course, feel free to reach out any time for feedback or questions.&lt;/p>
&lt;hr>
&lt;p>&lt;em>Editor's note: If you have any questions or feedback about this blog post, feel
free to reach out via the &lt;a href="https://kubernetes.slack.com/messages/sig-node">Kubernetes slack in #sig-node&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Blog: Alpha in v1.22: Windows HostProcess Containers</title><link>https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Brandon Smith (Microsoft)&lt;/p>
&lt;p>Kubernetes v1.22 introduced a new alpha feature for clusters that
include Windows nodes: HostProcess containers.&lt;/p>
&lt;p>HostProcess containers aim to extend the Windows container model to enable a wider
range of Kubernetes cluster management scenarios. HostProcess containers run
directly on the host and maintain behavior and access similar to that of a regular
process. With HostProcess containers, users can package and distribute management
operations and functionalities that require host access while retaining versioning
and deployment methods provided by containers. This allows Windows containers to
be used for a variety of device plugin, storage, and networking management scenarios
in Kubernetes. With this comes the enablement of host network mode—allowing
HostProcess containers to be created within the host's network namespace instead of
their own. HostProcess containers can also be built on top of existing Windows server
2019 (or later) base images, managed through the Windows container runtime, and run
as any user that is available on or in the domain of the host machine.&lt;/p>
&lt;p>Linux privileged containers are currently used for a variety of key scenarios in
Kubernetes, including kube-proxy (via kubeadm), storage, and networking scenarios.
Support for these scenarios in Windows previously required workarounds via proxies
or other implementations. Using HostProcess containers, cluster operators no longer
need to log onto and individually configure each Windows node for administrative
tasks and management of Windows services. Operators can now utilize the container
model to deploy management logic to as many clusters as needed with ease.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>Windows HostProcess containers are implemented with Windows &lt;em>Job Objects&lt;/em>, a break from the
previous container model using server silos. Job objects are components of the Windows OS which offer the ability to
manage a group of processes as a group (a.k.a. &lt;em>jobs&lt;/em>) and assign resource constraints to the
group as a whole. Job objects are specific to the Windows OS and are not associated with the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job API&lt;/a>. They have no process or file system isolation,
enabling the privileged payload to view and edit the host file system with the
correct permissions, among other host resources. The init process, and any processes
it launches or that are explicitly launched by the user, are all assigned to the
job object of that container. When the init process exits or is signaled to exit,
all the processes in the job will be signaled to exit, the job handle will be
closed and the storage will be unmounted.&lt;/p>
&lt;p>HostProcess and Linux privileged containers enable similar scenarios but differ
greatly in their implementation (hence the naming difference). HostProcess containers
have their own pod security policies. Those used to configure Linux privileged
containers &lt;strong>do not&lt;/strong> apply. Enabling privileged access to a Windows host is a
fundamentally different process than with Linux so the configuration and
capabilities of each differ significantly. Below is a diagram detailing the
overall architecture of Windows HostProcess containers:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/hostprocess-architecture.png"
alt="HostProcess Architecture"/>
&lt;/figure>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>HostProcess containers can be run from within a
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod">HostProcess Pod&lt;/a>.
With the feature enabled on Kubernetes version 1.22, a containerd container runtime of
1.5.4 or higher, and the latest version of hcsshim, deploying a pod spec with the
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/#before-you-begin">correct HostProcess configuration&lt;/a>
will enable you to run HostProcess containers. To get started with running
Windows containers see the general guidance for &lt;a href="https://kubernetes.io/docs/setup/production-environment/windows/">Windows in Kubernetes&lt;/a>&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Work through &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Read about Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Read the enhancement proposal &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-windows/1981-windows-privileged-container-support">Windows Privileged Containers and Host Networking Mode&lt;/a> (KEP-1981)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>HostProcess containers are in active development. SIG Windows welcomes suggestions from the community.
Get involved with &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">SIG Windows&lt;/a>
to contribute!&lt;/p></description></item><item><title>Blog: Kubernetes Memory Manager moves to beta</title><link>https://kubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/</link><pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Artyom Lukianov (Red Hat), Cezary Zukowski (Samsung)&lt;/p>
&lt;p>The blog post explains some of the internals of the &lt;em>Memory manager&lt;/em>, a beta feature
of Kubernetes 1.22. In Kubernetes, the Memory Manager is a
&lt;a href="https://kubernetes.io/docs/concepts/overview/components/#kubelet">kubelet&lt;/a> subcomponent.
The memory manage provides guaranteed memory (and hugepages)
allocation for pods in the &lt;code>Guaranteed&lt;/code> &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes">QoS class&lt;/a>.&lt;/p>
&lt;p>This blog post covers:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="#Why-do-you-need-it?">Why do you need it?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#How-does-it-work?">The internal details of how the &lt;strong>MemoryManager&lt;/strong> works&lt;/a>&lt;/li>
&lt;li>&lt;a href="#Current-limitations">Current limitations of the &lt;strong>MemoryManager&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="#Future-work-for-the-Memory-Manager">Future work for the &lt;strong>MemoryManager&lt;/strong>&lt;/a>&lt;/li>
&lt;/ol>
&lt;h2 id="why-do-you-need-it">Why do you need it?&lt;/h2>
&lt;p>Some Kubernetes workloads run on nodes with
&lt;a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">non-uniform memory access&lt;/a> (NUMA).
Suppose you have NUMA nodes in your cluster. In that case, you'll know about the potential for extra latency when
compute resources need to access memory on the different NUMA locality.&lt;/p>
&lt;p>To get the best performance and latency for your workload, container CPUs,
peripheral devices, and memory should all be aligned to the same NUMA
locality.
Before Kubernetes v1.22, the kubelet already provided a set of managers to
align CPUs and PCI devices, but you did not have a way to align memory.
The Linux kernel was able to make best-effort attempts to allocate
memory for tasks from the same NUMA node where the container is
executing are placed, but without any guarantee about that placement.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>The memory manager is doing two main things:&lt;/p>
&lt;ul>
&lt;li>provides the topology hint to the Topology Manager&lt;/li>
&lt;li>allocates the memory for containers and updates the state&lt;/li>
&lt;/ul>
&lt;p>The overall sequence of the Memory Manager under the Kubelet&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/MemoryManagerDiagram.svg" alt="MemoryManagerDiagram" title="MemoryManagerDiagram">&lt;/p>
&lt;p>During the Admission phase:&lt;/p>
&lt;ol>
&lt;li>When first handling a new pod, the kubelet calls the TopologyManager's &lt;code>Admit()&lt;/code> method.&lt;/li>
&lt;li>The Topology Manager is calling &lt;code>GetTopologyHints()&lt;/code> for every hint provider including the Memory Manager.&lt;/li>
&lt;li>The Memory Manager calculates all possible NUMA nodes combinations for every container inside the pod and returns hints to the Topology Manager.&lt;/li>
&lt;li>The Topology Manager calls to &lt;code>Allocate()&lt;/code> for every hint provider including the Memory Manager.&lt;/li>
&lt;li>The Memory Manager allocates the memory under the state according to the hint that the Topology Manager chose.&lt;/li>
&lt;/ol>
&lt;p>During Pod creation:&lt;/p>
&lt;ol>
&lt;li>The kubelet calls &lt;code>PreCreateContainer()&lt;/code>.&lt;/li>
&lt;li>For each container, the Memory Manager looks the NUMA nodes where it allocated the
memory for the container and then returns that information to the kubelet.&lt;/li>
&lt;li>The kubelet creates the container, via CRI, using a container specification
that incorporates information from the Memory Manager information.&lt;/li>
&lt;/ol>
&lt;h3 id="let-s-talk-about-the-configuration">Let's talk about the configuration&lt;/h3>
&lt;p>By default, the Memory Manager runs with the &lt;code>None&lt;/code> policy, meaning it will just
relax and not do anything. To make use of the Memory Manager, you should set
two command line options for the kubelet:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--memory-manager-policy=Static&lt;/code>&lt;/li>
&lt;li>&lt;code>--reserved-memory=&amp;quot;&amp;lt;numaNodeID&amp;gt;:&amp;lt;resourceName&amp;gt;=&amp;lt;quantity&amp;gt;&amp;quot;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The value for &lt;code>--memory-manager-policy&lt;/code> is straightforward: &lt;code>Static&lt;/code>. Deciding what to specify for &lt;code>--reserved-memory&lt;/code> takes more thought. To configure it correctly, you should follow two main rules:&lt;/p>
&lt;ul>
&lt;li>The amount of reserved memory for the &lt;code>memory&lt;/code> resource must be greater than zero.&lt;/li>
&lt;li>The amount of reserved memory for the resource type must be equal
to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">NodeAllocatable&lt;/a>
(&lt;code>kube-reserved + system-reserved + eviction-hard&lt;/code>) for the resource.
You can read more about memory reservations in &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/ReservedMemory.svg" alt="Reserved memory">&lt;/p>
&lt;h2 id="current-limitations">Current limitations&lt;/h2>
&lt;p>The 1.22 release and promotion to beta brings along enhancements and fixes, but the Memory Manager still has several limitations.&lt;/p>
&lt;h3 id="single-vs-cross-numa-node-allocation">Single vs Cross NUMA node allocation&lt;/h3>
&lt;p>The NUMA node can not have both single and cross NUMA node allocations. When the container memory is pinned to two or more NUMA nodes, we can not know from which NUMA node the container will consume the memory.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/SingleCrossNUMAAllocation.svg" alt="Single vs Cross NUMA allocation" title="SingleCrossNUMAAllocation">&lt;/p>
&lt;ol>
&lt;li>The &lt;code>container1&lt;/code> started on the NUMA node 0 and requests &lt;em>5Gi&lt;/em> of the memory but currently is consuming only &lt;em>3Gi&lt;/em> of the memory.&lt;/li>
&lt;li>For container2 the memory request is 10Gi, and no single NUMA node can satisfy it.&lt;/li>
&lt;li>The &lt;code>container2&lt;/code> consumes &lt;em>3.5Gi&lt;/em> of the memory from the NUMA node 0, but once the &lt;code>container1&lt;/code> will require more memory, it will not have it, and the kernel will kill one of the containers with the &lt;em>OOM&lt;/em> error.&lt;/li>
&lt;/ol>
&lt;p>To prevent such issues, the Memory Manager will fail the admission of the &lt;code>container2&lt;/code> until the machine has two NUMA nodes without a single NUMA node allocation.&lt;/p>
&lt;h3 id="works-only-for-guaranteed-pods">Works only for Guaranteed pods&lt;/h3>
&lt;p>The Memory Manager can not guarantee memory allocation for Burstable pods,
also when the Burstable pod has specified equal memory limit and request.&lt;/p>
&lt;p>Let's assume you have two Burstable pods: &lt;code>pod1&lt;/code> has containers with
equal memory request and limits, and &lt;code>pod2&lt;/code> has containers only with a
memory request set. You want to guarantee memory allocation for the &lt;code>pod1&lt;/code>.
To the Linux kernel, processes in either pod have the same &lt;em>OOM score&lt;/em>,
once the kernel finds that it does not have enough memory, it can kill
processes that belong to pod &lt;code>pod1&lt;/code>.&lt;/p>
&lt;h3 id="memory-fragmentation">Memory fragmentation&lt;/h3>
&lt;p>The sequence of Pods and containers that start and stop can fragment the memory on NUMA nodes.
The alpha implementation of the Memory Manager does not have any mechanism to balance pods and defragment memory back.&lt;/p>
&lt;h2 id="future-work-for-the-memory-manager">Future work for the Memory Manager&lt;/h2>
&lt;p>We do not want to stop with the current state of the Memory Manager and are looking to
make improvements, including in the following areas.&lt;/p>
&lt;h3 id="make-the-memory-manager-allocation-algorithm-smarter">Make the Memory Manager allocation algorithm smarter&lt;/h3>
&lt;p>The current algorithm ignores distances between NUMA nodes during the
calculation of the allocation. If same-node placement isn't available, we can still
provide better performance compared to the current implementation, by changing the
Memory Manager to prefer the closest NUMA nodes for cross-node allocation.&lt;/p>
&lt;h3 id="reduce-the-number-of-admission-errors">Reduce the number of admission errors&lt;/h3>
&lt;p>The default Kubernetes scheduler is not aware of the node's NUMA topology, and it can be a reason for many admission errors during the pod start.
We're hoping to add a KEP (Kubernetes Enhancement Proposal) to cover improvements in this area.
Follow &lt;a href="https://github.com/kubernetes/enhancements/issues/2044">Topology aware scheduler plugin in kube-scheduler&lt;/a> to see how this idea progresses.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>With the promotion of the Memory Manager to beta in 1.22, we encourage everyone to give it a try and look forward to any feedback you may have. While there are still several limitations, we have a set of enhancements planned to address them and look forward to providing you with many new features in upcoming releases.
If you have ideas for additional enhancements or a desire for certain features, please let us know. The team is always open to suggestions to enhance and improve the Memory Manager.
We hope you have found this blog informative and helpful! Let us know if you have any questions or comments.&lt;/p>
&lt;p>You can contact us via:&lt;/p>
&lt;ul>
&lt;li>The Kubernetes &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node &lt;/a>
channel in Slack (visit &lt;a href="https://slack.k8s.io/">https://slack.k8s.io/&lt;/a> for an invitation if you need one)&lt;/li>
&lt;li>The SIG Node mailing list, &lt;a href="https://groups.google.com/g/kubernetes-sig-node">kubernetes-sig-node@googlegroups.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.22: CSI Windows Support (with CSI Proxy) reaches GA</title><link>https://kubernetes.io/blog/2021/08/09/csi-windows-support-with-csi-proxy-reaches-ga/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/09/csi-windows-support-with-csi-proxy-reaches-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Mauricio Poppe (Google), Jing Xu (Google), and Deep Debroy (Apple)&lt;/p>
&lt;p>&lt;em>The stable version of CSI Proxy for Windows has been released alongside Kubernetes 1.22. CSI Proxy enables CSI Drivers running on Windows nodes to perform privileged storage operations.&lt;/em>&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Container Storage Interface (CSI) for Kubernetes went GA in the Kubernetes 1.13 release. CSI has become the standard for exposing block and file storage to containerized workloads on Container Orchestration systems (COs) like Kubernetes. It enables third-party storage providers to write and deploy plugins without the need to alter the core Kubernetes codebase. Legacy in-tree drivers are deprecated and new storage features are introduced in CSI, therefore it is important to get CSI Drivers to work on Windows.&lt;/p>
&lt;p>A CSI Driver in Kubernetes has two main components: a controller plugin which runs in the control plane and a node plugin which runs on every node.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The controller plugin generally does not need direct access to the host and can perform all its operations through the Kubernetes API and external control plane services.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The node plugin, however, requires direct access to the host for making block devices and/or file systems available to the Kubernetes kubelet. Due to the missing capability of running privileged operations from containers on Windows nodes &lt;a href="https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/">CSI Proxy was introduced as alpha in Kubernetes 1.18&lt;/a> as a way to enable containers to perform privileged storage operations. This enables containerized CSI Drivers to run on Windows nodes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-csi-proxy-and-how-do-csi-drivers-interact-with-it">What's CSI Proxy and how do CSI drivers interact with it?&lt;/h2>
&lt;p>When a workload that uses persistent volumes is scheduled, it'll go through a sequence of steps defined in the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI Spec&lt;/a>. First, the workload will be scheduled to run on a node. Then the controller component of a CSI Driver will attach the persistent volume to the node. Finally the node component of a CSI Driver will mount the persistent volume on the node.&lt;/p>
&lt;p>The node component of a CSI Driver needs to run on Windows nodes to support Windows workloads. Various privileged operations like scanning of disk devices, mounting of file systems, etc. cannot be done from a containerized application running on Windows nodes yet (&lt;a href="https://github.com/kubernetes/enhancements/issues/1981">Windows HostProcess containers&lt;/a> introduced in Kubernetes 1.22 as alpha enable functionalities that require host access like the operations mentioned before). However, we can perform these operations through a binary (CSI Proxy) that's pre-installed on the Window nodes. CSI Proxy has a client-server architecture and allows CSI drivers to issue privileged storage operations through a gRPC interface exposed over named pipes created during the startup of CSI Proxy.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-09-csi-windows-support-with-csi-proxy-reaches-ga/csi-proxy.png" alt="CSI Proxy Architecture">&lt;/p>
&lt;h2 id="csi-proxy-reaches-ga">CSI Proxy reaches GA&lt;/h2>
&lt;p>The CSI Proxy development team has worked closely with storage vendors, many of whom started integrating CSI Proxy into their CSI Drivers and provided feedback as early as CSI Proxy design proposal. This cooperation uncovered use cases where additional APIs were needed, found bugs, and identified areas for documentation improvement.&lt;/p>
&lt;p>The CSI Proxy design &lt;a href="https://github.com/kubernetes/enhancements/pull/2737">KEP&lt;/a> has been updated to reflect the current CSI Proxy architecture. Additional &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/DEVELOPMENT.md">development documentation&lt;/a> is included for contributors interested in helping with new features or bug fixes.&lt;/p>
&lt;p>Before we reached GA we wanted to make sure that our API is simple and consistent. We went through an extensive API review of the v1beta API groups where we made sure that the CSI Proxy API methods and messages are consistent with the naming conventions defined in the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI Spec&lt;/a>. As part of this effort we're graduating the &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/disk_v1.md">Disk&lt;/a>, &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/filesystem_v1.md">Filesystem&lt;/a>, &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/smb_v1.md">SMB&lt;/a> and &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/volume_v1.md">Volume&lt;/a> API groups to v1.&lt;/p>
&lt;p>Additional Windows system APIs to get information from the Windows nodes and support to mount iSCSI targets in Windows nodes, are available as alpha APIs in the &lt;a href="https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/client/api/system/v1alpha1">System API&lt;/a> and the &lt;a href="https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/client/api/iscsi/v1alpha2">iSCSI API&lt;/a>. These APIs will continue to be improved before we graduate them to v1.&lt;/p>
&lt;p>CSI Proxy v1 is compatible with all the previous v1betaX releases. The GA &lt;code>csi-proxy.exe&lt;/code> binary can handle requests from v1betaX clients thanks to the autogenerated conversion layer that transforms any versioned client request to a version-agnostic request that the server can process. Several &lt;a href="https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/integrationtests">integration tests&lt;/a> were added for all the API versions of the API groups that are graduating to v1 to ensure that CSI Proxy is backwards compatible.&lt;/p>
&lt;p>Version drift between CSI Proxy and the CSI Drivers that interact with it was also carefully considered. A &lt;a href="https://github.com/kubernetes-csi/csi-proxy/pull/124">connection fallback mechanism&lt;/a> has been provided for CSI Drivers to handle multiple versions of CSI Proxy for a smooth upgrade to v1. This allows CSI Drivers, like the GCE PD CSI Driver, &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/pull/738">to recognize which version of the CSI Proxy binary is running&lt;/a> and handle multiple versions of the CSI Proxy binary deployed on the node.&lt;/p>
&lt;p>CSI Proxy v1 is already being used by many CSI Drivers, including the &lt;a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/pull/966">AWS EBS CSI Driver&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver/pull/919">Azure Disk CSI Driver&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/pull/738">GCE PD CSI Driver&lt;/a>, and &lt;a href="https://github.com/kubernetes-csi/csi-driver-smb/pull/319">SMB CSI Driver&lt;/a>.&lt;/p>
&lt;h2 id="future-plans">Future plans&lt;/h2>
&lt;p>We're very excited for the future of CSI Proxy. With the upcoming &lt;a href="https://github.com/kubernetes/enhancements/issues/1981">Windows HostProcess containers&lt;/a>, we are considering converting the CSI Proxy in to a library consumed by CSI Drivers in addition to the current client/server design. This will allow us to iterate faster on new features because the &lt;code>csi-proxy.exe&lt;/code> binary will no longer be needed.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. Those interested in getting involved with the design and development of CSI Proxy, or any part of the Kubernetes Storage system, may join the Kubernetes Storage Special Interest Group (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>For those interested in more details about CSI support in Windows please reach out in the &lt;a href="https://kubernetes.slack.com/messages/csi-windows">#csi-windows&lt;/a> Kubernetes slack channel.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>CSI-Proxy received many contributions from members of the Kubernetes community. We thank all of the people that contributed to CSI Proxy with design reviews, bug reports, bug fixes, and for their continuous support in reaching this milestone:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/andyzhangx">Andy Zhang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jmpfar">Dan Ilan&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ddebroy">Deep Debroy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/humblec">Humble Devassy Chirammal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jingxu97">Jing Xu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/wk8">Jean Rougé&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/liggitt">Jordan Liggitt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ksubrmnn">Kalya Subramanian&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kkmsft">Krishnakumar R&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/manueltellez">Manuel Tellez&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/marosset">Mark Rossetti&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/mauriciopoppe">Mauricio Poppe&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/wongma7">Matthew Wong&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/msau42">Michelle Au&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/PatrickLang">Patrick Lang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/saad-ali">Saad Ali&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/yujuhong">Yuju Hong&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: New in Kubernetes v1.22: alpha support for using swap memory</title><link>https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Elana Hashman (Red Hat)&lt;/p>
&lt;p>The 1.22 release introduced alpha support for configuring swap memory usage for
Kubernetes workloads on a per-node basis.&lt;/p>
&lt;p>In prior releases, Kubernetes did not support the use of swap memory on Linux,
as it is difficult to provide guarantees and account for pod memory utilization
when swap is involved. As part of Kubernetes' earlier design, swap support was
considered out of scope, and a kubelet would by default fail to start if swap
was detected on a node.&lt;/p>
&lt;p>However, there are a number of &lt;a href="https://github.com/kubernetes/enhancements/blob/9d127347773ad19894ca488ee04f1cd3af5774fc/keps/sig-node/2400-node-swap/README.md#user-stories">use cases&lt;/a>
that would benefit from Kubernetes nodes supporting swap, including improved
node stability, better support for applications with high memory overhead but
smaller working sets, the use of memory-constrained devices, and memory
flexibility.&lt;/p>
&lt;p>Hence, over the past two releases, &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#readme">SIG Node&lt;/a> has
been working to gather appropriate use cases and feedback, and propose a design
for adding swap support to nodes in a controlled, predictable manner so that
Kubernetes users can perform testing and provide data to continue building
cluster capabilities on top of swap. The alpha graduation of swap memory
support for nodes is our first milestone towards this goal!&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>There are a number of possible ways that one could envision swap use on a node.
To keep the scope manageable for this initial implementation, when swap is
already provisioned and available on a node, &lt;a href="https://github.com/kubernetes/enhancements/blob/9d127347773ad19894ca488ee04f1cd3af5774fc/keps/sig-node/2400-node-swap/README.md#proposal">we have proposed&lt;/a>
the kubelet should be able to be configured such that:&lt;/p>
&lt;ul>
&lt;li>It can start with swap on.&lt;/li>
&lt;li>It will direct the Container Runtime Interface to allocate zero swap memory
to Kubernetes workloads by default.&lt;/li>
&lt;li>You can configure the kubelet to specify swap utilization for the entire
node.&lt;/li>
&lt;/ul>
&lt;p>Swap configuration on a node is exposed to a cluster admin via the
&lt;a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">&lt;code>memorySwap&lt;/code> in the KubeletConfiguration&lt;/a>.
As a cluster administrator, you can specify the node's behaviour in the
presence of swap memory by setting &lt;code>memorySwap.swapBehavior&lt;/code>.&lt;/p>
&lt;p>This is possible through the addition of a &lt;code>memory_swap_limit_in_bytes&lt;/code> field
to the container runtime interface (CRI). The kubelet's config will control how
much swap memory the kubelet instructs the container runtime to allocate to
each container via the CRI. The container runtime will then write the swap
settings to the container level cgroup.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>On a node where swap memory is already provisioned, Kubernetes use of swap on a
node can be enabled by enabling the &lt;code>NodeSwap&lt;/code> feature gate on the kubelet, and
disabling the &lt;code>failSwapOn&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration">configuration setting&lt;/a>
or the &lt;code>--fail-swap-on&lt;/code> command line flag.&lt;/p>
&lt;p>You can also optionally configure &lt;code>memorySwap.swapBehavior&lt;/code> in order to
specify how a node will use swap memory. For example,&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">memorySwap&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">swapBehavior&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>LimitedSwap&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The available configuration options for &lt;code>swapBehavior&lt;/code> are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>LimitedSwap&lt;/code> (default): Kubernetes workloads are limited in how much swap
they can use. Workloads on the node not managed by Kubernetes can still swap.&lt;/li>
&lt;li>&lt;code>UnlimitedSwap&lt;/code>: Kubernetes workloads can use as much swap memory as they
request, up to the system limit.&lt;/li>
&lt;/ul>
&lt;p>If configuration for &lt;code>memorySwap&lt;/code> is not specified and the feature gate is
enabled, by default the kubelet will apply the same behaviour as the
&lt;code>LimitedSwap&lt;/code> setting.&lt;/p>
&lt;p>The behaviour of the &lt;code>LimitedSwap&lt;/code> setting depends if the node is running with
v1 or v2 of control groups (also known as &amp;quot;cgroups&amp;quot;):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>cgroups v1:&lt;/strong> Kubernetes workloads can use any combination of memory and
swap, up to the pod's memory limit, if set.&lt;/li>
&lt;li>&lt;strong>cgroups v2:&lt;/strong> Kubernetes workloads cannot use swap memory.&lt;/li>
&lt;/ul>
&lt;h3 id="caveats">Caveats&lt;/h3>
&lt;p>Having swap available on a system reduces predictability. Swap's performance is
worse than regular memory, sometimes by many orders of magnitude, which can
cause unexpected performance regressions. Furthermore, swap changes a system's
behaviour under memory pressure, and applications cannot directly control what
portions of their memory usage are swapped out. Since enabling swap permits
greater memory usage for workloads in Kubernetes that cannot be predictably
accounted for, it also increases the risk of noisy neighbours and unexpected
packing configurations, as the scheduler cannot account for swap memory usage.&lt;/p>
&lt;p>The performance of a node with swap memory enabled depends on the underlying
physical storage. When swap memory is in use, performance will be significantly
worse in an I/O operations per second (IOPS) constrained environment, such as a
cloud VM with I/O throttling, when compared to faster storage mediums like
solid-state drives or NVMe.&lt;/p>
&lt;p>Hence, we do not recommend the use of swap for certain performance-constrained
workloads or environments. Cluster administrators and developers should
benchmark their nodes and applications before using swap in production
scenarios, and &lt;a href="#how-do-i-get-involved">we need your help&lt;/a> with that!&lt;/p>
&lt;h2 id="looking-ahead">Looking ahead&lt;/h2>
&lt;p>The Kubernetes 1.22 release introduces alpha support for swap memory on nodes,
and we will continue to work towards beta graduation in the 1.23 release. This
will include:&lt;/p>
&lt;ul>
&lt;li>Adding support for controlling swap consumption at the Pod level via cgroups.
&lt;ul>
&lt;li>This will include the ability to set a system-reserved quantity of swap
from what kubelet detects on the host.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Determining a set of metrics for node QoS in order to evaluate the
performance and stability of nodes with and without swap enabled.&lt;/li>
&lt;li>Collecting feedback from test user cases.
&lt;ul>
&lt;li>We will consider introducing new configuration modes for swap, such as a
node-wide swap limit for workloads.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>You can review the current &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory">documentation&lt;/a>
on the Kubernetes website.&lt;/p>
&lt;p>For more information, and to assist with testing and provide feedback, please
see &lt;a href="https://github.com/kubernetes/enhancements/issues/2400">KEP-2400&lt;/a> and its
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md">design proposal&lt;/a>.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Your feedback is always welcome! SIG Node &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#meetings">meets regularly&lt;/a>
and &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">can be reached&lt;/a>
via &lt;a href="https://slack.k8s.io/">Slack&lt;/a> (channel &lt;strong>#sig-node&lt;/strong>), or the SIG's
&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">mailing list&lt;/a>.
Feel free to reach out to me, Elana Hashman (&lt;strong>@ehashman&lt;/strong> on Slack and GitHub)
if you'd like to help.&lt;/p></description></item><item><title>Blog: Kubernetes 1.22: Server Side Apply moves to GA</title><link>https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jeffrey Ying, Google &amp;amp; Joe Betz, Google&lt;/p>
&lt;p>Server-side Apply (SSA) has been promoted to GA in the Kubernetes v1.22 release. The GA milestone means you can depend on the feature and its API, without fear of future backwards-incompatible changes. GA features are protected by the Kubernetes &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a>.&lt;/p>
&lt;h2 id="what-is-server-side-apply">What is Server-side Apply?&lt;/h2>
&lt;p>Server-side Apply helps users and controllers manage their resources through declarative configurations. Server-side Apply replaces the client side apply feature implemented by “kubectl apply” with a server-side implementation, permitting use by tools/clients other than kubectl. Server-side Apply is a new merging algorithm, as well as tracking of field ownership, running on the Kubernetes api-server. Server-side Apply enables new features like conflict detection, so the system knows when two actors are trying to edit the same field. Refer to the &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side Apply Documentation&lt;/a> and &lt;a href="https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/">Beta 2 release announcement&lt;/a> for more information.&lt;/p>
&lt;h2 id="what-s-new-since-beta">What’s new since Beta?&lt;/h2>
&lt;p>Since the &lt;a href="https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/">Beta 2 release&lt;/a> subresources support has been added, and both client-go and Kubebuilder have added comprehensive support for Server-side Apply. This completes the Server-side Apply functionality required to make controller development practical.&lt;/p>
&lt;h3 id="support-for-subresources">Support for subresources&lt;/h3>
&lt;p>Server-side Apply now fully supports subresources like &lt;code>status&lt;/code> and &lt;code>scale&lt;/code>. This is particularly important for &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">controllers&lt;/a>, which are often responsible for writing to subresources.&lt;/p>
&lt;h2 id="server-side-apply-support-in-client-go">Server-side Apply support in client-go&lt;/h2>
&lt;p>Previously, Server-side Apply could only be called from the client-go typed client using the &lt;code>Patch&lt;/code> function, with &lt;code>PatchType&lt;/code> set to &lt;code>ApplyPatchType&lt;/code>. Now, &lt;code>Apply&lt;/code> functions are included in the client to allow for a more direct and typesafe way of calling Server-side Apply. Each &lt;code>Apply&lt;/code> function takes an &amp;quot;apply configuration&amp;quot; type as an argument, which is a structured representation of an Apply request. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#666">...&lt;/span>
v1ac &lt;span style="color:#b44">&amp;#34;k8s.io/client-go/applyconfigurations/autoscaling/v1&amp;#34;&lt;/span>
)
hpaApplyConfig &lt;span style="color:#666">:=&lt;/span> v1ac.&lt;span style="color:#00a000">HorizontalPodAutoscaler&lt;/span>(autoscalerName, ns).
&lt;span style="color:#00a000">WithSpec&lt;/span>(v1ac.&lt;span style="color:#00a000">HorizontalPodAutoscalerSpec&lt;/span>().
&lt;span style="color:#00a000">WithMinReplicas&lt;/span>(&lt;span style="color:#666">0&lt;/span>)
)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> hpav1client.&lt;span style="color:#00a000">Apply&lt;/span>(ctx, hpaApplyConfig, metav1.ApplyOptions{FieldManager: &lt;span style="color:#b44">&amp;#34;mycontroller&amp;#34;&lt;/span>, Force: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>})
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note in this example that &lt;code>HorizontalPodAutoscaler&lt;/code> is imported from an &amp;quot;applyconfigurations&amp;quot; package. Each &amp;quot;apply configuration&amp;quot; type represents the same Kubernetes object kind as the corresponding go struct, but where all fields are pointers to make them optional, allowing apply requests to be accurately represented. For example, when the apply configuration in the above example is marshalled to YAML, it produces:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>autoscaling/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HorizontalPodAutoscaler&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myHPA&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myNamespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">minReplicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To understand why this is needed, the above YAML cannot be produced by the v1.HorizontalPodAutoscaler go struct. Take for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">hpa &lt;span style="color:#666">:=&lt;/span> v1.HorizontalPodAutoscaler{
TypeMeta: metav1.TypeMeta{
APIVersion: &lt;span style="color:#b44">&amp;#34;autoscaling/v1&amp;#34;&lt;/span>,
Kind: &lt;span style="color:#b44">&amp;#34;HorizontalPodAutoscaler&amp;#34;&lt;/span>,
},
ObjectMeta: ObjectMeta{
Namespace: ns,
Name: autoscalerName,
},
Spec: v1.HorizontalPodAutoscalerSpec{
MinReplicas: pointer.&lt;span style="color:#00a000">Int32Ptr&lt;/span>(&lt;span style="color:#666">0&lt;/span>),
},
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above code attempts to declare the same apply configuration as shown in the previous examples, but when marshalled to YAML, produces:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HorizontalPodAutoscaler&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>autoscaling/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>metadata&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myHPA&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myNamespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">creationTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">null&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">scaleTargetRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">minReplicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxReplicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Which, among other things, contains &lt;code>spec.maxReplicas&lt;/code> set to &lt;code>0&lt;/code>. This is almost certainly not what the caller intended (the intended apply configuration says nothing about the &lt;code>maxReplicas&lt;/code> field), and could have serious consequences on a production system: it directs the autoscaler to downscale to zero pods. The problem here originates from the fact that the go structs contain required fields that are zero valued if not set explicitly. The go structs work as intended for create and update operations, but are fundamentally incompatible with apply, which is why we have introduced the generated &amp;quot;apply configuration&amp;quot; types.&lt;/p>
&lt;p>The &amp;quot;apply configurations&amp;quot; also have convenience &lt;code>With&amp;lt;FieldName&amp;gt;&lt;/code> functions that make it easier to build apply requests. This allows developers to set fields without having to deal with the fact that all the fields in the &amp;quot;apply configuration&amp;quot; types are pointers, and are inconvenient to set using go. For example &lt;code>MinReplicas: &amp;amp;0&lt;/code> is not legal go code, so without the &lt;code>With&lt;/code> functions, developers would work around this problem by using a library, e.g. &lt;code>MinReplicas: pointer.Int32Ptr(0)&lt;/code>, but string enumerations like &lt;code>corev1.Protocol&lt;/code> are still a problem since they cannot be supported by a general purpose library. In addition to the convenience, the &lt;code>With&lt;/code> functions also isolate developers from the underlying representation, which makes it safer for the underlying representation to be changed to support additional features in the future.&lt;/p>
&lt;h2 id="using-server-side-apply-in-a-controller">Using Server-side Apply in a controller&lt;/h2>
&lt;p>You can use the new support for Server-side Apply no matter how you implemented your controller. However, the new client-go support makes it easier to use Server-side Apply in controllers.&lt;/p>
&lt;p>When authoring new controllers to use Server-side Apply, a good approach is to have the controller recreate the apply configuration for an object each time it reconciles that object. This ensures that the controller fully reconciles all the fields that it is responsible for. Controllers typically should unconditionally set all the fields they own by setting &lt;code>Force: true&lt;/code> in the &lt;code>ApplyOptions&lt;/code>. Controllers must also provide a &lt;code>FieldManager&lt;/code> name that is unique to the reconciliation loop that apply is called from.&lt;/p>
&lt;p>When upgrading existing controllers to use Server-side Apply the same approach often works well--migrate the controllers to recreate the apply configuration each time it reconciles any object. Unfortunately, the controller might have multiple code paths that update different parts of an object depending on various conditions. Migrating a controller like this to Server-side Apply can be risky because if the controller forgets to include any fields in an apply configuration that is included in a previous apply request, a field can be accidently deleted. To ease this type of migration, client-go apply support provides a way to replace any controller reconciliation code that performs a &amp;quot;read/modify-in-place/update&amp;quot; (or patch) workflow with a &amp;quot;extract/modify-in-place/apply&amp;quot; workflow. Here's an example of the new workflow:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">fieldMgr &lt;span style="color:#666">:=&lt;/span> &lt;span style="color:#b44">&amp;#34;my-field-manager&amp;#34;&lt;/span>
deploymentClient &lt;span style="color:#666">:=&lt;/span> clientset.&lt;span style="color:#00a000">AppsV1&lt;/span>().&lt;span style="color:#00a000">Deployments&lt;/span>(&lt;span style="color:#b44">&amp;#34;default&amp;#34;&lt;/span>)
&lt;span style="color:#080;font-style:italic">// read, could also be read from a shared informer
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>deployment, err &lt;span style="color:#666">:=&lt;/span> deploymentClient.&lt;span style="color:#00a000">Get&lt;/span>(ctx, &lt;span style="color:#b44">&amp;#34;example-deployment&amp;#34;&lt;/span>, metav1.GetOptions{})
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#080;font-style:italic">// handle error
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>}
&lt;span style="color:#080;font-style:italic">// extract
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>deploymentApplyConfig, err &lt;span style="color:#666">:=&lt;/span> appsv1ac.&lt;span style="color:#00a000">ExtractDeployment&lt;/span>(deployment, fieldMgr)
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#080;font-style:italic">// handle error
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>}
&lt;span style="color:#080;font-style:italic">// modify-in-place
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>deploymentApplyConfig.Spec.Template.Spec.&lt;span style="color:#00a000">WithContainers&lt;/span>(corev1ac.&lt;span style="color:#00a000">Container&lt;/span>().
&lt;span style="color:#00a000">WithName&lt;/span>(&lt;span style="color:#b44">&amp;#34;modify-slice&amp;#34;&lt;/span>).
&lt;span style="color:#00a000">WithImage&lt;/span>(&lt;span style="color:#b44">&amp;#34;nginx:1.14.2&amp;#34;&lt;/span>),
)
&lt;span style="color:#080;font-style:italic">// apply
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>applied, err &lt;span style="color:#666">:=&lt;/span> deploymentClient.&lt;span style="color:#00a000">Apply&lt;/span>(ctx, deploymentApplyConfig, metav1.ApplyOptions{FieldManager: fieldMgr})
&lt;/code>&lt;/pre>&lt;/div>&lt;p>For developers using Custom Resource Definitions (CRDs), the Kubebuilder apply support will provide the same capabilities. Documentation will be included in the Kubebuilder book when available.&lt;/p>
&lt;h2 id="server-side-apply-and-customresourcedefinitions">Server-side Apply and CustomResourceDefinitions&lt;/h2>
&lt;p>It is strongly recommended that all &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions&lt;/a> (CRDs) have a schema. CRDs without a schema are treated as unstructured data by Server-side Apply. Keys are treated as fields in a struct and lists are assumed to be atomic.&lt;/p>
&lt;p>CRDs that specify a schema are able to specify additional annotations in the schema. Please refer to the documentation on the full list of available annotations.&lt;/p>
&lt;p>New annotations since beta:&lt;/p>
&lt;p>&lt;strong>Defaulting:&lt;/strong> Values for fields that appliers do not express explicit interest in should be defaulted. This prevents an applier from unintentionally owning a defaulted field that might cause conflicts with other appliers. If unspecified, the default value is nil or the nil equivalent for the corresponding type.&lt;/p>
&lt;ul>
&lt;li>Usage: see the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting">CRD Defaulting&lt;/a> documentation for more details.&lt;/li>
&lt;li>Golang: &lt;code>+default=&amp;lt;value&amp;gt;&lt;/code>&lt;/li>
&lt;li>OpenAPI extension: &lt;code>default: &amp;lt;value&amp;gt;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Atomic for maps and structs:&lt;/p>
&lt;p>&lt;strong>Maps:&lt;/strong> By default maps are granular. A different manager is able to manage each map entry. They can also be configured to be atomic such that a single manager owns the entire map.&lt;/p>
&lt;ul>
&lt;li>Usage: Refer to &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/#merge-strategy">Merge Strategy&lt;/a> for a more detailed overview&lt;/li>
&lt;li>Golang: &lt;code>+mapType=granular/atomic&lt;/code>&lt;/li>
&lt;li>OpenAPI extension: &lt;code>x-kubernetes-map-type: granular/atomic&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Structs:&lt;/strong> By default structs are granular and a separate applier may own each field. For certain kinds of structs, atomicity may be desired. This is most commonly seen in small coordinate-like structs such as Field/Object/Namespace Selectors, Object References, RGB values, Endpoints (Protocol/Port pairs), etc.&lt;/p>
&lt;ul>
&lt;li>Usage: Refer to &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/#merge-strategy">Merge Strategy&lt;/a> for a more detailed overview&lt;/li>
&lt;li>Golang: &lt;code>+structType=granular/atomic&lt;/code>&lt;/li>
&lt;li>OpenAPI extension: &lt;code>x-kubernetes-map-type:atomic/granular&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-next">What's Next?&lt;/h2>
&lt;p>After Server Side Apply, the next focus for the API Expression working-group is around improving the expressiveness and size of the published Kubernetes API schema. To see the full list of items we are working on, please join our working group and refer to the work items document.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>The working-group for apply is &lt;a href="https://github.com/kubernetes/community/tree/master/wg-api-expression">wg-api-expression&lt;/a>. It is available on slack &lt;a href="https://kubernetes.slack.com/archives/C0123CNN8F3">#wg-api-expression&lt;/a>, through the &lt;a href="https://groups.google.com/g/kubernetes-wg-api-expression">mailing list&lt;/a> and we also meet every other Tuesday at 9.30 PT on Zoom.&lt;/p>
&lt;p>We would also like to use the opportunity to thank the hard work of all the contributors involved in making this promotion to GA possible:&lt;/p>
&lt;ul>
&lt;li>Andrea Nodari&lt;/li>
&lt;li>Antoine Pelisse&lt;/li>
&lt;li>Daniel Smith&lt;/li>
&lt;li>Jeffrey Ying&lt;/li>
&lt;li>Jenny Buckley&lt;/li>
&lt;li>Joe Betz&lt;/li>
&lt;li>Julian Modesto&lt;/li>
&lt;li>Kevin Delgado&lt;/li>
&lt;li>Kevin Wiesmüller&lt;/li>
&lt;li>Maria Ntalla&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.22: Reaching New Peaks</title><link>https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/</link><pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.22/release-team.md">Kubernetes 1.22 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the release of Kubernetes 1.22, the second release of 2021!&lt;/p>
&lt;p>This release consists of 53 enhancements: 13 enhancements have graduated to stable, 24 enhancements are moving to beta, and 16 enhancements are entering alpha. Also, three features have been deprecated.&lt;/p>
&lt;p>In April of this year, the Kubernetes release cadence was officially changed from four to three releases yearly. This is the first longer-cycle release related to that change. As the Kubernetes project matures, the number of enhancements per cycle grows. This means more work, from version to version, for the contributor community and Release Engineering team, and it can put pressure on the end-user community to stay up-to-date with releases containing increasingly more features.&lt;/p>
&lt;p>Changing the release cadence from four to three releases yearly balances many aspects of the project, both in how contributions and releases are managed, and also in the community's ability to plan for upgrades and stay up to date.&lt;/p>
&lt;p>You can read more in the official blog post &lt;a href="https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/">Kubernetes Release Cadence Change: Here’s What You Need To Know&lt;/a>.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="server-side-apply-graduates-to-ga">Server-side Apply graduates to GA&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side Apply&lt;/a> is a new field ownership and object merge algorithm running on the Kubernetes API server. Server-side Apply helps users and controllers manage their resources via declarative configurations. It allows them to create and/or modify their objects declaratively, simply by sending their fully specified intent. After being in beta for a couple releases, Server-side Apply is now generally available.&lt;/p>
&lt;h3 id="external-credential-providers-now-stable">External credential providers now stable&lt;/h3>
&lt;p>Support for Kubernetes client &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">credential plugins&lt;/a> has been in beta since 1.11, and with the release of Kubernetes 1.22 now graduates to stable. The GA feature set includes improved support for plugins that provide interactive login flows, as well as a number of bug fixes. Aspiring plugin authors can look at &lt;a href="https://github.com/ankeesler/sample-exec-plugin">sample-exec-plugin&lt;/a> to get started.&lt;/p>
&lt;h3 id="etcd-moves-to-3-5-0">etcd moves to 3.5.0&lt;/h3>
&lt;p>Kubernetes' default backend storage, etcd, has a new release: 3.5.0. The new release comes with improvements to the security, performance, monitoring, and developer experience. There are numerous bug fixes and some critical new features like the migration to structured logging and built-in log rotation. The release comes with a detailed future roadmap to implement a solution to traffic overload. You can read a full and detailed list of changes in the &lt;a href="https://etcd.io/blog/2021/announcing-etcd-3.5/">3.5.0 release announcement&lt;/a>.&lt;/p>
&lt;h3 id="quality-of-service-for-memory-resources">Quality of Service for memory resources&lt;/h3>
&lt;p>Originally, Kubernetes used the v1 cgroups API. With that design, the QoS class for a &lt;code>Pod&lt;/code> only applied to CPU resources (such as &lt;code>cpu_shares&lt;/code>). As an alpha feature, Kubernetes v1.22 can now use the cgroups v2 API to control memory allocation and isolation. This feature is designed to improve workload and node availability when there is contention for memory resources, and to improve the predictability of container lifecycle.&lt;/p>
&lt;h3 id="node-system-swap-support">Node system swap support&lt;/h3>
&lt;p>Every system administrator or Kubernetes user has been in the same boat regarding setting up and using Kubernetes: disable swap space. With the release of Kubernetes 1.22, alpha support is available to run nodes with swap memory. This change lets administrators opt in to configuring swap on Linux nodes, treating a portion of block storage as additional virtual memory.&lt;/p>
&lt;h3 id="windows-enhancements-and-capabilities">Windows enhancements and capabilities&lt;/h3>
&lt;p>Continuing to support the growing developer community, SIG Windows has released their &lt;a href="https://github.com/kubernetes-sigs/sig-windows-dev-tools/">Development Environment&lt;/a>. These new tools support multiple CNI providers and can run on multiple platforms. There is also a new way to run bleeding-edge Windows features from scratch by compiling the Windows kubelet and kube-proxy, then using them along with daily builds of other Kubernetes components.&lt;/p>
&lt;p>CSI support for Windows nodes moves to GA in the 1.22 release. In Kubernetes v1.22, Windows privileged containers are an alpha feature. To allow using CSI storage on Windows nodes, &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> enables CSI node plugins to be deployed as unprivileged pods, using the proxy to perform privileged storage operations on the node.&lt;/p>
&lt;h3 id="default-profiles-for-seccomp">Default profiles for seccomp&lt;/h3>
&lt;p>An alpha feature for default seccomp profiles has been added to the kubelet, along with a new command line flag and configuration. When in use, this new feature provides cluster-wide seccomp defaults, using the &lt;code>RuntimeDefault&lt;/code> seccomp profile rather than &lt;code>Unconfined&lt;/code> by default. This enhances the default security of the Kubernetes Deployment. Security administrators will now sleep better knowing that workloads are more secure by default. To learn more about the feature, please refer to the official &lt;a href="https://kubernetes.io/docs/tutorials/clusters/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">seccomp tutorial&lt;/a>.&lt;/p>
&lt;h3 id="more-secure-control-plane-with-kubeadm">More secure control plane with kubeadm&lt;/h3>
&lt;p>A new alpha feature allows running the &lt;code>kubeadm&lt;/code> control plane components as non-root users. This is a long requested security measure in &lt;code>kubeadm&lt;/code>. To try it you must enable the &lt;code>kubeadm&lt;/code> specific RootlessControlPlane feature gate. When you deploy a cluster using this alpha feature, your control plane runs with lower privileges.&lt;/p>
&lt;p>For &lt;code>kubeadm&lt;/code>, Kubernetes 1.22 also brings a new &lt;a href="https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/">v1beta3 configuration API&lt;/a>. This iteration adds some long requested features and deprecates some existing ones. The v1beta3 version is now the preferred API version; the v1beta2 API also remains available and is not yet deprecated.&lt;/p>
&lt;h2 id="major-changes">Major Changes&lt;/h2>
&lt;h3 id="removal-of-several-deprecated-beta-apis">Removal of several deprecated beta APIs&lt;/h3>
&lt;p>A number of deprecated beta APIs have been removed in 1.22 in favor of the GA version of those same APIs. All existing objects can be interacted with via stable APIs. This removal includes beta versions of the &lt;code>Ingress&lt;/code>, &lt;code>IngressClass&lt;/code>, &lt;code>Lease&lt;/code>, &lt;code>APIService&lt;/code>, &lt;code>ValidatingWebhookConfiguration&lt;/code>, &lt;code>MutatingWebhookConfiguration&lt;/code>, &lt;code>CustomResourceDefinition&lt;/code>, &lt;code>TokenReview&lt;/code>, &lt;code>SubjectAccessReview&lt;/code>, and &lt;code>CertificateSigningRequest&lt;/code> APIs.&lt;/p>
&lt;p>For the full list, check out the &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22">Deprecated API Migration Guide&lt;/a> as well as the blog post &lt;a href="https://blog.k8s.io/2021/07/14/upcoming-changes-in-kubernetes-1-22/">Kubernetes API and Feature Removals In 1.22: Here’s What You Need To Know&lt;/a>.&lt;/p>
&lt;h3 id="api-changes-and-improvements-for-ephemeral-containers">API changes and improvements for ephemeral containers&lt;/h3>
&lt;p>The API used to create &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">Ephemeral Containers&lt;/a> changes in 1.22. The Ephemeral Containers feature is alpha and disabled by default, and the new API does not work with clients that attempt to use the old API.&lt;/p>
&lt;p>For stable features, the kubectl tool follows the Kubernetes &lt;a href="https://kubernetes.io/releases/version-skew-policy/">version skew policy&lt;/a>; however, kubectl v1.21 and older do not support the new API for ephemeral containers. If you plan to use &lt;code>kubectl debug&lt;/code> to create ephemeral containers, and your cluster is running Kubernetes v1.22, you cannot do so with kubectl v1.21 or earlier. Please update kubectl to 1.22 if you wish to use &lt;code>kubectl debug&lt;/code> with a mix of cluster versions.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/542">Bound Service Account Token Volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2047">CSI Service Account Token&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1122">Windows Support for CSI Plugins&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1693">Warning mechanism for deprecated API use&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/85">PodDisruptionBudget Eviction&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="notable-feature-updates">Notable Feature Updates&lt;/h3>
&lt;ul>
&lt;li>A new &lt;a href="https://github.com/kubernetes/enhancements/issues/2579">PodSecurity admission&lt;/a> alpha feature is introduced, intended as a replacement for PodSecurityPolicy&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1769">The Memory Manager&lt;/a> moves to beta&lt;/li>
&lt;li>A new alpha feature to enable &lt;a href="https://github.com/kubernetes/enhancements/issues/647">API Server Tracing&lt;/a>&lt;/li>
&lt;li>A new v1beta3 version of the &lt;a href="https://github.com/kubernetes/enhancements/issues/970">kubeadm configuration&lt;/a> format&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1495">Generic data populators&lt;/a> for PersistentVolumes are now available in alpha&lt;/li>
&lt;li>The Kubernetes control plane will now always use the &lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs v2 controller&lt;/a>&lt;/li>
&lt;li>As an alpha feature, all Kubernetes node components (including the kubelet, kube-proxy, and container runtime) can be &lt;a href="https://github.com/kubernetes/enhancements/issues/2033">run as a non-root user&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="release-notes">Release notes&lt;/h1>
&lt;p>You can check out the full details of the 1.22 release in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md">release notes&lt;/a>.&lt;/p>
&lt;h1 id="availability-of-release">Availability of release&lt;/h1>
&lt;p>Kubernetes 1.22 is &lt;a href="https://kubernetes.io/releases/download/">available for download&lt;/a> and also &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.22.0">on the GitHub project&lt;/a>.&lt;/p>
&lt;p>There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a>. If you’d like to try building a cluster from scratch, check out the &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> tutorial by Kelsey Hightower.&lt;/p>
&lt;h1 id="release-team">Release Team&lt;/h1>
&lt;p>This release was made possible by a very dedicated group of individuals, who came together as a team to deliver technical content, documentation, code, and a host of other components that go into every Kubernetes release.&lt;/p>
&lt;p>A huge thank you to the release lead Savitha Raghunathan for leading us through a successful release cycle, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.22 release for the community.&lt;/p>
&lt;p>We would also like to take this opportunity to remember Peeyush Gupta, a member of our team that we lost earlier this year. Peeyush was actively involved in SIG ContribEx and the Kubernetes Release Team, most recently serving as the 1.22 Communications lead. His contributions and efforts will continue to reflect in the community he helped build. A &lt;a href="https://github.com/cncf/memorials/blob/main/peeyush-gupta.md">CNCF memorial&lt;/a> page has been created where thoughts and memories can be shared by the community.&lt;/p>
&lt;h1 id="release-logo">Release Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-04-kubernetes-release-1.22/kubernetes-1.22.png" alt="Kubernetes 1.22 Release Logo">&lt;/p>
&lt;p>Amidst the ongoing pandemic, natural disasters, and ever-present shadow of burnout, the 1.22 release of Kubernetes includes 53 enhancements. This makes it the largest release to date. This accomplishment was only made possible due to the hard-working and passionate Release Team members and the amazing contributors of the Kubernetes ecosystem. The release logo is our reminder to keep reaching for new milestones and setting new records. And it is dedicated to all the Release Team members, hikers, and stargazers!&lt;/p>
&lt;p>The logo is designed by &lt;a href="https://www.instagram.com/boris.z.man/">Boris Zotkin&lt;/a>. Boris is a Mac/Linux Administrator at the MathWorks. He enjoys simple things in life and loves spending time with his family. This tech-savvy individual is always up for a challenge and happy to help a friend!&lt;/p>
&lt;h1 id="user-highlights">User Highlights&lt;/h1>
&lt;ul>
&lt;li>In May, the CNCF welcomed 27 new organizations across the globe as members of the diverse cloud native ecosystem. These new &lt;a href="https://www.cncf.io/announcements/2021/05/05/27-new-members-join-the-cloud-native-computing-foundation/">members&lt;/a> will participate in CNCF events, including the upcoming &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">KubeCon + CloudNativeCon NA in Los Angeles&lt;/a> from October 12 – 15, 2021.&lt;/li>
&lt;li>The CNCF granted Spotify the &lt;a href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/">Top End User Award&lt;/a> during &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">KubeCon + CloudNativeCon EU – Virtual 2021&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h1 id="project-velocity">Project Velocity&lt;/h1>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/">CNCF K8s DevStats project&lt;/a> aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.22 release cycle, which ran for 15 weeks (April 26 to August 4), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.21.0%20-%20now&amp;amp;var-metric=contributions">1063 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.21.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">2054 individuals&lt;/a>.&lt;/p>
&lt;h1 id="ecosystem-updates">Ecosystem Updates&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">KubeCon + CloudNativeCon Europe 2021&lt;/a> was held in May, the third such event to be virtual. All talks are &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC">now available on-demand&lt;/a> for anyone that would like to catch up!&lt;/li>
&lt;li>&lt;a href="https://www.cncf.io/blog/2021/07/13/spring-term-lfx-program-largest-graduating-class-with-28-successful-cncf-interns">Spring Term LFX Program&lt;/a> had the largest graduating class with 28 successful CNCF interns!&lt;/li>
&lt;li>CNCF launched &lt;a href="https://www.cncf.io/blog/2021/06/03/cloud-native-community-goes-live-with-10-shows-on-twitch/">livestreaming on Twitch&lt;/a> at the beginning of the year targeting definitive interactive media experience for anyone wanting to learn, grow, and collaborate with others in the Cloud Native community from anywhere in the world.&lt;/li>
&lt;/ul>
&lt;h1 id="event-updates">Event Updates&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">KubeCon + CloudNativeCon North America 2021&lt;/a> will take place in Los Angeles, October 12 – 15, 2021! You can find more information about the conference and registration on the event site.&lt;/li>
&lt;li>&lt;a href="https://community.cncf.io/kubernetes-community-days/about-kcd/">Kubernetes Community Days&lt;/a> has upcoming events scheduled in Italy, the UK, and in Washington DC.&lt;/li>
&lt;/ul>
&lt;h1 id="upcoming-release-webinar">Upcoming release webinar&lt;/h1>
&lt;p>Join members of the Kubernetes 1.22 release team on October 5, 2021 to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the &lt;a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-122-release/">event page&lt;/a> on the CNCF Online Programs site.&lt;/p>
&lt;h1 id="get-involved">Get Involved&lt;/h1>
&lt;p>If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributors&lt;/a> website.&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Roorkee robots, releases and racing: the Kubernetes 1.21 release interview</title><link>https://kubernetes.io/blog/2021/07/29/roorkee-robots-releases-and-racing-the-kubernetes-1.21-release-interview/</link><pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/29/roorkee-robots-releases-and-racing-the-kubernetes-1.21-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>With Kubernetes 1.22 due out next week, now is a great time to look back on 1.21. The release team for that version was led by &lt;a href="https://twitter.com/theonlynabarun">Nabarun Pal&lt;/a> from VMware.&lt;/p>
&lt;p>Back in April I &lt;a href="https://kubernetespodcast.com/episode/146-kubernetes-1.21/">interviewed Nabarun&lt;/a> on the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a>; the latest in a series of release lead conversations that started back with 1.11, not long after the show started back in 2018.&lt;/p>
&lt;p>In these interviews we learn a little about the release, but also about the process behind it, and the story behind the person chosen to lead it. Getting to know a community member is my favourite part of the show each week, and so I encourage you to &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe wherever you get your podcasts&lt;/a>. With a release coming next week, you can probably guess what our next topic will be!&lt;/p>
&lt;p>&lt;em>This transcript has been edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: You have a Bachelor of Technology in Metallurgical and Materials Engineering. How are we doing at turning lead into gold?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Well, last I checked, we have yet to find the philosopher's stone!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the more important parts of the process?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We're not doing that well in terms of getting alchemists up and running. There is some improvement in nuclear technology, where you can turn lead into gold, but I would guess buying gold would be much more efficient.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Or Bitcoin? It depends what you want to do with the gold.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yeah, seeing the increasing prices of Bitcoin, you'd probably prefer to bet on that. But, don't take this as a suggestion. I'm not a registered investment advisor, and I don't give investment advice!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But you are, of course, a trained materials engineer. How did you get into that line of education?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We had a graded and equated exam structure, where you sit a single exam, and then based on your performance in that exam, you can try any of the universities which take those scores into account. I went to the Indian Institute of Technology, Roorkee.&lt;/p>
&lt;p>Materials engineering interested me a lot. I had a passion for computer science since childhood, but I also liked material science, so I wanted to explore that field. I did a lot of exploration around material science and metallurgy in my freshman and sophomore years, but then computing, since it was a passion, crept into the picture.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's dig in there a little bit. What did computing look like during your childhood?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It was a very interesting journey. I started exploring computers back when I was seven or eight. For my first programming language, if you call it a programming language, I explored LOGO.&lt;/p>
&lt;p>You have a turtle on the screen, and you issue commands to it, like move forward or rotate or pen up or pen down. You basically draw geometric figures. I could visually see how I could draw a square and how I could draw a triangle. It was an interesting journey after that. I learned BASIC, then went to some amount of HTML, JavaScript.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's interesting to me because Logo and BASIC were probably my first two programming languages, but I think there was probably quite a gap in terms of when HTML became a thing after those two! Did your love of computing always lead you down the path towards programming, or were you interested as a child in using computers for games or application software? What led you specifically into programming?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Programming came in late. Not just in computing, but in life, I'm curious with things. When my parents got me my first computer, I was curious. I was like, &amp;quot;how does this operating system work?&amp;quot; What even is running it? Using a television and using a computer is a different experience, but usability is kind of the same thing. The HCI device for a television is a remote, whereas with a computer, I had a keyboard and a mouse. I used to tinker with the box and reinstall operating systems.&lt;/p>
&lt;p>We used to get magazines back then. They used to bundle OpenSuse or Debian, and I used to install them. It was an interesting experience, 15 years back, how Linux used to be. I have been a tinkerer all around, and that's what eventually led me to programming.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: With an interest in both the physical and ethereal aspects of technology, you did a lot of robotics challenges during university. That's something that I am not surprised to hear from someone who has a background in Logo, to be honest. There's Mindstorms, and a lot of other technology that is based around robotics that a lot of LOGO people got into. How was that something that came about for you?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: When I joined my university, apart from studying materials, one of the things they used to really encourage was to get involved in a lot of extracurricular activities. One which interested me was robotics. I joined &lt;a href="https://github.com/marsiitr">my college robotics team&lt;/a> and participated in a lot of challenges.&lt;/p>
&lt;p>Predominantly, we used to participate in this competition called &lt;a href="https://en.wikipedia.org/wiki/ABU_Robocon">ABU Robocon&lt;/a>, which is an event conducted by the Asia-Pacific Broadcasting Union. What they used to do was, every year, one of the participating countries in the contest would provide a problem statement. For example, one year, they asked us to build a badminton-playing robot. They asked us to build a rugby playing robot or a Frisbee thrower, and there are some interesting problem statements around the challenge: you can't do this. You can't do that. Weight has to be like this. Dimensions have to be like that.&lt;/p>
&lt;p>I got involved in that, and most of my time at university, I used to spend there. Material science became kind of a backburner for me, and my hobby became my full time thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And you were not only involved there in terms of the project and contributions to it, but you got involved as a secretary of the team, effectively, doing a lot of the organization, which is a thread that will come up as we speak about Kubernetes.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Over the course of time, when I gained more knowledge into how the team works, it became very natural that I graduated up the ladder and then managed juniors. I became the joint secretary of the robotics club in our college. This was more of a broad, engaging role in evangelizing robotics at the university, to promote events, to help students to see the value in learning robotics - what you gain out of that mechanically or electronically, or how do you develop your logic by programming robots.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Your first job after graduation was working at a company called Algoshelf, but you were also an intern there while you were at school?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Algoshelf was known as Rorodata when I joined them as an intern. This was also an interesting opportunity for me in the sense that I was always interested in writing programs which people would use. One of the things that I did there was build an open source Function as a Service framework, if I may call it that - it was mostly turning Python functions into web servers without even writing any code. The interesting bit there was that it was targeted toward data scientists, and not towards programmers. We had to understand the pain of data scientists, that they had to learn a lot of programming in order to even deploy their machine learning models, and we wanted to solve that problem.&lt;/p>
&lt;p>They offered me a job after my internship, and I kept on working for them after I graduated from university. There, I got introduced to Kubernetes, so we pivoted into a product structure where the very same thing I told you, the Functions as a Service thing, could be deployed in Kubernetes. I was exploring Kubernetes to use it as a scalable platform. Instead of managing pets, we wanted to manage cattle, as in, we wanted to have a very highly distributed architecture.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Not actual cattle. I've been to India. There are a lot of cows around.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yeah, not actual cattle. That is a bit tough.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When Algoshelf we're looking at picking up Kubernetes, what was the evaluation process like? Were you looking at other tools at the time? Or had enough time passed that Kubernetes was clearly the platform that everyone was going to use?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Algoshelf was a natural evolution. Before Kubernetes, we used to deploy everything on a single big AWS server, using systemd. Everything was a systemd service, and everything was deployed using Fabric. Fabric is a Python package which essentially is like Ansible, but much leaner, as it does not have all the shims and things that Ansible has.&lt;/p>
&lt;p>Then we asked &amp;quot;what if we need to scale out to different machines?&amp;quot; Kubernetes was in the hype. We hopped onto the hype train to see whether Kubernetes was worth it for us. And that's where my journey started, exploring the ecosystem, exploring the community. How can we improve the community in essence?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A couple of times now you've mentioned as you've grown in a role, becoming part of the organization and the arranging of the group. You've talked about working in Python. You had submitted some talks to Pycon India. And I understand you're now a tech lead for that conference. What does the tech community look like in India and how do you describe your involvement in it?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: My involvement with the community began when I was at university. When I was working as an intern at Algoshelf, I was introduced to this-- I never knew about PyCon India, or tech conferences in general.&lt;/p>
&lt;p>The person that I was working with just asked me, like hey, did you submit a talk to PyCon India? It's very useful, the library that we were making. So I &lt;a href="https://www.nabarun.in/talk/2017/pyconindia/#1">submitted a talk&lt;/a> to PyCon India in 2017. Eventually the talk got selected. That was not my first speaking opportunity, it was my second. I also spoke at PyData Delhi on a similar thing that I worked on in my internship.&lt;/p>
&lt;p>It has been a journey since then. I talked about the same thing at FOSSASIA Summit in Singapore, and got really involved with the Python community because it was what I used to work on back then.&lt;/p>
&lt;p>After giving all those talks at conferences, I got also introduced to this amazing group called &lt;a href="https://dgplug.org/">dgplug&lt;/a>, which is an acronym for the Durgapur Linux Users Group. It is a group started in-- I don't remember the exact year, but it was around 12 to 13 years back, by someone called Kushal Das, with the ideology of &lt;a href="https://foss.training/">training students into being better open source contributors&lt;/a>.&lt;/p>
&lt;p>I liked the idea and got involved with in teaching last year. It is not limited to students. Professionals can also join in. It's about making anyone better at upstream contributions, making things sustainable. I started training people on Vim, on how to use text editors. so they are more efficient and productive. In general life, text editors are a really good tool.&lt;/p>
&lt;p>The other thing was the shell. How do you navigate around the Linux shell and command line? That has been a fun experience.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's very interesting to think about that, because my own involvement with a Linux User Group was probably around the year 2000. And back then we were teaching people how to install things-- Linux on CD was kinda new at that point in time. There was a lot more of, what is this new thing and how do we get involved? When the internet took off around that time, all of that stuff moved online - you no longer needed to go meet a group of people in a room to talk about Linux. And I haven't really given much thought to the concept of a LUG since then, but it's great to see it having turned into something that's now about contributing, rather than just about how you get things going for yourself.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Exactly. So as I mentioned earlier, my journey into Linux was installing SUSE from DVDs that came bundled with magazines. Back then it was a pain installing things because you did not get any instructions. There has certainly been a paradigm shift now. People are more open to reading instructions online, downloading ISOs, and then just installing them. So we really don't need to do that as part of LUGs.&lt;/p>
&lt;p>We have shifted more towards enabling people to contribute to whichever project that they use. For example, if you're using Fedora, contribute to Fedora; make things better. It's just about giving back to the community in any way possible.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You're also involved in the &lt;a href="https://www.meetup.com/Bangalore-Kubernetes-Meetup/">Kubernetes Bangalore meetup group&lt;/a>. Does that group have a similar mentality?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: The Kubernetes Bangalore meetup group is essentially focused towards spreading the knowledge of Kubernetes and the aligned products in the ecosystem, whatever there is in the Cloud Native Landscape, in various ways. For example, to evangelize about using them in your company or how people use them in existing ways.&lt;/p>
&lt;p>So a few months back in February, we did something like a &lt;a href="https://www.youtube.com/watch?v=FgsXbHBRYIc">Kubernetes contributor workshop&lt;/a>. It was one of its kind in India. It was the first one if I recall correctly. We got a lot of traction and community members interested in contributing to Kubernetes and a lot of other projects. And this is becoming a really valuable thing.&lt;/p>
&lt;p>I'm not much involved in the organization of the group. There are really great people already organizing it. I keep on being around and attending the meetups and trying to answer any questions if people have any.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One way that it is possible to contribute to the Kubernetes ecosystem is through the release process. You've &lt;a href="https://blog.naba.run/posts/release-enhancements-journey/">written a blog&lt;/a> which talks about your journey through that. It started in Kubernetes 1.17, where you took a shadow role for that release. Tell me about what it was like to first take that plunge.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Taking the plunge was a big step, I would say. It should not have been that way. After getting into the team, I saw that it is really encouraged that you should just apply to the team - but then write truthfully about yourself. What do you want? Write your passionate goal, why you want to be in the team.&lt;/p>
&lt;p>So even right now the shadow applications are open for the next release. I wanted to give that a small shoutout. If you want to contribute to the Kubernetes release team, please do apply. The form is pretty simple. You just need to say why do you want to contribute to the release team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What was your answer to that question?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It was a bit tricky. I have this philosophy of contributing to projects that I use in my day-to-day life. I use a lot of open source projects daily, and I started contributing to Kubernetes primarily because I was using the Kubernetes Python client. That was one of my first contributions.&lt;/p>
&lt;p>When I was contributing to that, I explored the release team and it interested me a lot, particularly how interesting and varied the mechanics of releasing Kubernetes are. For most software projects, it's usually whenever you decide that you have made meaningful progress in terms of features, you release it. But Kubernetes is not like that. We follow a regular release cadence. And all those aspects really interested me. I actually applied for the first time in Kubernetes 1.16, but got rejected.&lt;/p>
&lt;p>But I still applied to Kubernetes 1.17, and I got into the enhancements team. That team was led by &lt;a href="https://kubernetespodcast.com/episode/126-research-steering-honking/">MrBobbyTables, Bob Killen&lt;/a>, back then, and &lt;a href="https://kubernetespodcast.com/episode/131-kubernetes-1.20/">Jeremy Rickard&lt;/a> was one of my co-shadows in the team. I shadowed enhancements again. Then I lead enhancements in 1.19. I then shadowed the lead in 1.20 and eventually led the 1.21 team. That's what my journey has been.&lt;/p>
&lt;p>My suggestion to people is don't be afraid of failure. Even if you don't get selected, it's perfectly fine. You can still contribute to the release team. Just hop on the release calls, raise your hand, and introduce yourself.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Between the 1.20 and 1.21 releases, you moved to work on the upstream contribution team at VMware. I've noticed that VMware is hiring a lot of great upstream contributors at the moment. Is this something that &lt;a href="https://kubernetespodcast.com/episode/130-kubecon-na-2020/">Stephen Augustus&lt;/a> had his fingerprints all over? Is there something in the water?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: A lot of people have fingerprints on this process. Stephen certainly had his fingerprints on it, I would say. We are expanding the team of upstream contributors primarily because the product that we are working for is based on Kubernetes. It helps us a lot in driving processes upstream and helping out the community as a whole, because everyone then gets enabled and benefits from what we contribute to the community.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I understand that the Tanzu team is being built out in India at the moment, but I guess you probably haven't been able to meet them in person yet?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yes and no. I did not meet any of them after joining VMware, but I met a lot of my teammates, before I joined VMware, at KubeCons. For example, I met Nikhita, I met Dims, I met Stephen at KubeCon. I am yet to meet other members of the team and I'm really excited to catch up with them once everything comes out of lockdown and we go back to our normal lives.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes, everyone that I speak to who has changed jobs in the pandemic says it's a very odd experience, just nothing really being different. And the same perhaps for people who are working on open source moving companies as well. They're doing the same thing, perhaps just for a different employer.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: As we say in the community, see you in another Slack in some time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We now turn to the recent release of Kubernetes 1.21. First of all, congratulations on that.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Thank you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: &lt;a href="https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/">The announcement&lt;/a> says the release consists of 51 enhancements, 13 graduating to stable, 16 moving to beta, 20 entering alpha, and then two features that have been deprecated. How would you summarize this release?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: One of the big points for this release is that it is the largest release of all time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Really?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yep. 1.20 was the largest release back then, but 1.21 got more enhancements, primarily due to a lot of changes that we did to the process.&lt;/p>
&lt;p>In the 1.21 release cycle, we did a few things differently compared to other release cycles-- for example, in the enhancement process. An enhancement, in the Kubernetes context, is basically a feature proposal. You will hear the terminology &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/README.md">Kubernetes Enhancement Proposals&lt;/a>, or KEP, a lot in the community. An enhancement is a broad thing encapsulated in a specific document.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I like to think of it as a thing that's worth having a heading in the release notes.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Indeed. Until the 1.20 release cycle, what we used to do was-- the release team has a vertical called enhancements. The enhancements team members used to ping each of the enhancement issues and ask whether they want to be part of the release cycle or not. The authors would decide, or talk to their SIG, and then come back with the answer, as to whether they wanted to be part of the cycle.&lt;/p>
&lt;p>In this release, what we did was we eliminated that process and asked the SIGs proactively to discuss amongst themselves, what they wanted to pitch in for this release cycle. What set of features did they want to graduate this release? They may introduce things in alpha, graduate things to beta or stable, or they may also deprecate features.&lt;/p>
&lt;p>What this did was promote a lot of async processes, and at the same time, give power back to the community. The community decides what they want in the release and then comes back collectively. It also reduces a lot of stress on the release team who previously had to ask people consistently what they wanted to pitch in for the release. You now have a deadline. You discuss amongst your SIG what your roadmap is and what it looks like for the near future. Maybe this release, and the next two. And you put all of those answers into a Google spreadsheet. Spreadsheets are still a thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The Kubernetes ecosystem runs entirely on Google Spreadsheets.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It does, and a lot of Google Docs for meeting notes! We did a lot of process improvements, which essentially led to a better release. This release cycle we had 13 enhancements graduating to stable, 16 which moved to beta, and 20 enhancements which were net new features into the ecosystem, and came in as alpha.&lt;/p>
&lt;p>Along with that are features set for deprecation. One of them was PodSecurityPolicy. That has been a point of discussion in the Kubernetes user base and we also published &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">a blog post about it&lt;/a>. All credit to SIG Security who have been on top of things as to find a replacement for PodSecurityPolicy even before this release cycle ended, so that they could at least have a proposal of what will happen next.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk about some old things and some new things. You mentioned PodSecurityPolicy there. That's a thing that's been around a long time and is being deprecated. Two things that have been around a long time and that are now being promoted to stable are CronJobs and PodDisruptionBudgets, both of which were introduced in Kubernetes 1.4, which came out in 2016. Why do you think it took so long for them both to go stable?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I might not have a definitive answer to your question. One of the things that I feel is they might be already so good that nobody saw that they were beta features, and just kept on using them.&lt;/p>
&lt;p>One of the things that I noticed when reading for the CronJobs graduation from beta to stable was the new controller. Users might not see this, but there has been a drastic change in the CronJob controller v2. What it essentially does is goes from a poll-based method of checking what users have defined as CronJobs to a queue architecture, which is the modern method of defining controllers. That has been one of the really good improvements in the case of CronJobs. Instead of the controller working in O(N) time, you now have constant time complexity.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A lot of these features that have been in beta for a long time, like you say, people have an expectation that they are complete. With PodSecurityPolicy, it's being deprecated, which is allowed because it's a feature that never made it out of beta. But how do you think people will react to it going away? And does that say something about the need for the process to make sure that features don't just languish in beta forever, which has been introduced recently?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: That's true. One of the driving factors, when contributors are thinking of graduating beta features has been the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/1635-prevent-permabeta/README.md">&amp;quot;prevention of perma-beta&amp;quot; KEP&lt;/a>. Back in 1.19 we &lt;a href="https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/">introduced this process&lt;/a> where each of the beta resources were marked for deprecation and removal in a certain time frame-- three releases for deprecation and another release for removal. That's also a motivating factor for eventually rethinking as to how beta resources work for us in the community. That is also very effective, I would say.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do remember that Gmail was in beta for eight years.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I did not know that!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Nothing in Kubernetes is quite that old yet, but we'll get there. Of the 20 new enhancements, do you have a favorite or any that you'd like to call out?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: There are two specific features in 1.21 that I'm really interested in, and are coming as net new features. One of them is the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor">persistent volume health monitor&lt;/a>, which gives the users the capability to actually see whether the backing volumes, which power persistent volumes in Kubernetes, are deleted or not. For example, the volumes may get deleted due to an inadvertent event, or they may get corrupted. That information is basically surfaced out as a field so that the user can leverage it in any way.&lt;/p>
&lt;p>The other feature is the proposal for &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cli/859-kubectl-headers">adding headers with the command name to kubectl requests&lt;/a>. We have always set the user-agent information when doing those kind of requests, but the proposal is to add what command the user put in so that we can enable more telemetry, and cluster administrators can determine the usage patterns of how people are using the cluster. I'm really excited about these kind of features coming into play.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You're the first release lead from the Asia-Pacific region, or more accurately, outside of the US and Europe. Most meetings in the Kubernetes ecosystem are traditionally in the window of overlap between the US and Europe, in the morning in California and the evening here in the UK. What's it been like to work outside of the time zones that the community had previously been operating in?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It has been a fun and a challenging proposition, I would say. In the last two-ish years that I have been contributing to Kubernetes, the community has also transformed from a lot of early morning Pacific calls to more towards async processes. For example, we in the release team have transformed our processes so we don't do updates in the calls anymore. What we do is ask for updates ahead of time, and then in the call, we just discuss things which need to be discussed synchronously in the team.&lt;/p>
&lt;p>We leverage the meetings right now more for discussions. But we also don't come to decisions in those discussions, because if any stakeholder is not present on the call, it puts them at a disadvantage. We are trying to talk more on Slack, publicly, or talk on mailing lists. That's where most of the discussion should happen, and also to gain lazy consensus. What I mean by lazy consensus is come up with a pre-decision kind of thing, but then also invite feedback from the broader community about what people would like them to see about that specific thing being discussed. This is where we as a community are also transforming a lot, but there is a lot more headroom to grow.&lt;/p>
&lt;p>The release team also started to have EU/APAC burndown meetings. In addition to having one meeting focused towards the US and European time zones, we also do a meeting which is more suited towards European and Asia-Pacific time zones. One of the driving factors for those decisions was that the release team is seeing a lot of participation from a variety of time zones. To give you one metric, we had release team members this cycle from UTC+8 all through UTC-8 - 16 hours of span. It's really difficult to accommodate all of those zones in a single meeting. And it's not just those 16 hours of span - what about the other eight hours?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yeah, you're missing New Zealand. You could add another 5 hours of span right there.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Exactly. So we will always miss people in meetings, and that's why we should also innovate more, have different kinds of meetings. But that also may not be very sustainable in the future. Will people attend duplicate meetings? Will people follow both of the meetings? More meetings is one of the solutions.&lt;/p>
&lt;p>The other solution is you have threaded discussions on some medium, be it Slack or be it a mailing list. Then, people can just pitch in whenever it is work time for them. Then, at the end of the day, a 24-hour rolling period, you digest it, and then push it out as meeting notes. That's what the Contributor Experience Special Interest Group is doing - shout-out to them for moving to that process. I may be wrong here, but I think once every two weeks, they do async updates on Slack. And that is a really nice thing to have, improving variety of geographies that people can contribute from.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Once you've put everything together that you hope to be in your release, you create a release candidate build. How do you motivate people to test those?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: That's a very interesting question. It is difficult for us to motivate people into trying out these candidates. It's mostly people who are passionate about Kubernetes who try out the release candidates and see for themselves what the bugs are. I remember &lt;a href="https://twitter.com/dims/status/1377272238420934656">Dims tweeting out a call&lt;/a> that if somebody tries out the release candidate and finds a good bug or caveat, they could get a callout in the KubeCon keynote. That's one of the incentives - if you want to be called out in a KubeCon keynote, please try our release candidates.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Or get a new pair of Kubernetes socks?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We would love to give out goodies to people who try out our release candidates and find bugs. For example, if you want the brand new release team logo as a sticker, just hit me up. If you find a bug in a 1.22 release candidate, I would love to be able to send you some coupon codes for the store. Don't quote me on this, but do reach out.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now the release is out, is it time for you to put your feet up? What more things do you have to do, and how do you feel about the path ahead for yourself?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I was discussing this with the team yesterday. Even after the release, we had kind of a water-cooler conversation. I just pasted in a Zoom link to all the release team members and said, hey, do you want to chat? One of the things that I realized that I'm really missing is the daily burndowns right now. I will be around in the release team and the SIG Release meetings, helping out the new lead in transitioning. And even my job, right now, is not over. I'm working with Taylor, who is the emeritus advisor for 1.21, on figuring out some of the mechanics for the next release cycle. I'm also documenting what all we did as part of the process and as part of the process changes, and making sure the next release cycle is up and running.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've done a lot of these release lead interviews now, and there's a question which we always like to ask, which is, what will you write down in the transition envelope? Savitha Raghunathan is the release lead for 1.22. What is the advice that you will pass on to her?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Three words-- &lt;strong>Do, Delegate, and Defer&lt;/strong>. Categorize things into those three buckets as to what you should do right away, what you need to defer, and things that you can delegate to your shadows or other release team members. That's one of the mantras that works really well when leading a team. It is not just in the context of the release team, but it's in the context of managing any team.&lt;/p>
&lt;p>The other bit is &lt;strong>over-communicate&lt;/strong>. No amount of communication is enough. What I've realized is the community is always willing to help you. One of the big examples that I can give is the day before release was supposed to happen, we were seeing a lot of test failures, and then one of the community members had an idea-- why don't you just send an email? I was like, &amp;quot;that sounds good. We can send an email mentioning all the flakes and call out for help to the broader Kubernetes developer community.&amp;quot; And eventually, once we sent out the email, lots of people came in to help us in de-flaking the tests and trying to find out the root cause as to why those tests were failing so often. Big shout out to Antonio and all the SIG Network folks who came to pitch in.&lt;/p>
&lt;p>No matter how many names I mention, it will never be enough. A lot of people, even outside the release team, have helped us a lot with this release. And that's where the release theme comes in - &lt;strong>Power to the Community&lt;/strong>. I'm really stoked by how this community behaves and how people are willing to help you all the time. It's not about what they're telling you to do, but it's what they're also interested in, they're passionate about.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the things you're passionate about is Formula One. Do you think Lewis Hamilton is going to take it away this year?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It's a fair probability that Lewis will win the title this year as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Which would take him to eight all time career wins. And thus-- &lt;a href="https://www.nytimes.com/2020/11/15/sports/autoracing/lewis-hamilton-schumacher-formula-one-record.html">he's currently tied with Michael Schumacher&lt;/a>-- would pull him ahead.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yes. Michael Schumacher was my first favorite F1 driver, I would say. It feels a bit heartbreaking to see someone break Michael's record.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How do you feel about &lt;a href="https://www.formula1.com/en/latest/article.breaking-mick-schumacher-to-race-for-haas-in-2021-as-famous-surname-returns.66XTVfSt80GrZe91lvWVwJ.html">Michael Schumacher's son joining the contest?&lt;/a>&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I feel good. Mick Schumacher is in the fray right now. And I wish we could see him, in a few years, in a Ferrari. The Schumacher family back to Ferrari would be really great to see. But then, my fan favorite has always been McLaren, partly because I like the chemistry of Lando and Carlos over the last two years. It was heartbreaking to see Carlos go to Ferrari. But then we have Lando and Daniel Ricciardo in the team. They're also fun people.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/theonlynabarun">Nabarun Pal&lt;/a> is on the Tanzu team at VMware and served as the Kubernetes 1.21 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Updating NGINX-Ingress to use the stable Ingress API</title><link>https://kubernetes.io/blog/2021/07/26/update-with-ingress-nginx/</link><pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/26/update-with-ingress-nginx/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> James Strong, Ricardo Katz&lt;/p>
&lt;p>With all Kubernetes APIs, there is a process to creating, maintaining, and
ultimately deprecating them once they become GA. The networking.k8s.io API group is no
different. The upcoming Kubernetes 1.22 release will remove several deprecated APIs
that are relevant to networking:&lt;/p>
&lt;ul>
&lt;li>the &lt;code>networking.k8s.io/v1beta1&lt;/code> API version of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class">IngressClass&lt;/a>&lt;/li>
&lt;li>all beta versions of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>: &lt;code>extensions/v1beta1&lt;/code> and &lt;code>networking.k8s.io/v1beta1&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>On a v1.22 Kubernetes cluster, you'll be able to access Ingress and IngressClass
objects through the stable (v1) APIs, but access via their beta APIs won't be possible.
This change has been in
in discussion since
&lt;a href="https://github.com/kubernetes/kubernetes/issues/43214">2017&lt;/a>,
&lt;a href="https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/">2019&lt;/a> with
1.16 Kubernetes API deprecations, and most recently in
KEP-1453:
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1453-ingress-api#122">Graduate Ingress API to GA&lt;/a>.&lt;/p>
&lt;p>During community meetings, the networking Special Interest Group has decided to continue
supporting Kubernetes versions older than 1.22 with Ingress-NGINX version 0.47.0.
Support for Ingress-NGINX will continue for six months after Kubernetes 1.22
is released. Any additional bug fixes and CVEs for Ingress-NGINX will be
addressed on a need-by-need basis.&lt;/p>
&lt;p>Ingress-NGINX will have separate branches and releases of Ingress-NGINX to
support this model, mirroring the Kubernetes project process. Future
releases of the Ingress-NGINX project will track and support the latest
versions of Kubernetes.&lt;/p>
&lt;table>&lt;caption style="display: none;">Ingress NGINX supported version with Kubernetes Versions&lt;/caption>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Kubernetes version&lt;/th>
&lt;th style="text-align:left">Ingress-NGINX version&lt;/th>
&lt;th style="text-align:left">Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">v1.22&lt;/td>
&lt;td style="text-align:left">v1.0.0-alpha.2&lt;/td>
&lt;td style="text-align:left">New features, plus bug fixes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.21&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.20&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.19&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. Fixes only provided until 6 months after Kubernetes v1.22.0 is released.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Because of the updates in Kubernetes 1.22, &lt;strong>v0.47.0&lt;/strong> will not work with
Kubernetes 1.22.&lt;/p>
&lt;h1 id="what-you-need-to-do">What you need to do&lt;/h1>
&lt;p>The team is currently in the process of upgrading ingress-nginx to support
the v1 migration, you can track the progress
&lt;a href="https://github.com/kubernetes/ingress-nginx/pull/7156">here&lt;/a>.&lt;br>
We're not making feature improvements to &lt;code>ingress-nginx&lt;/code> until after the support for
Ingress v1 is complete.&lt;/p>
&lt;p>In the meantime to ensure no compatibility issues:&lt;/p>
&lt;ul>
&lt;li>Update to the latest version of Ingress-NGINX; currently
&lt;a href="https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v0.47.0">v0.47.0&lt;/a>&lt;/li>
&lt;li>After Kubernetes 1.22 is released, ensure you are using the latest version of
Ingress-NGINX that supports the stable APIs for Ingress and IngressClass.&lt;/li>
&lt;li>Test Ingress-NGINX version v1.0.0-alpha.2 with Cluster versions &amp;gt;= 1.19
and report any issues to the projects Github page.&lt;/li>
&lt;/ul>
&lt;p>The community’s feedback and support in this effort is welcome. The
Ingress-NGINX Sub-project regularly holds community meetings where we discuss
this and other issues facing the project. For more information on the sub-project,
please see &lt;a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes Release Cadence Change: Here’s What You Need To Know</title><link>https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Celeste Horgan, Adolfo García Veytia, James Laverack, Jeremy Rickard&lt;/p>
&lt;p>On April 23, 2021, the Release Team merged a Kubernetes Enhancement Proposal (KEP) changing the Kubernetes release cycle from four releases a year (once a quarter) to three releases a year.&lt;/p>
&lt;p>This blog post provides a high level overview about what this means for the Kubernetes community's contributors and maintainers.&lt;/p>
&lt;h2 id="what-s-changing-and-when">What's changing and when&lt;/h2>
&lt;p>Starting with the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.22">Kubernetes 1.22 release&lt;/a>, a lightweight policy will drive the creation of each release schedule. This policy states:&lt;/p>
&lt;ul>
&lt;li>The first Kubernetes release of a calendar year should start at the second or third
week of January to provide people more time for contributors coming back from the
end of year holidays.&lt;/li>
&lt;li>The last Kubernetes release of a calendar year should be finished by the middle of
December.&lt;/li>
&lt;li>A Kubernetes release cycle has a length of approximately 15 weeks.&lt;/li>
&lt;li>The week of KubeCon + CloudNativeCon is not considered a 'working week' for SIG Release. The Release Team will not hold meetings or make decisions in this period.&lt;/li>
&lt;li>An explicit SIG Release break of at least two weeks between each cycle will
be enforced.&lt;/li>
&lt;/ul>
&lt;p>As a result, Kubernetes will follow a three releases per year cadence. Kubernetes 1.23 will be the final release of the 2021 calendar year. This new policy results in a very predictable release schedule, allowing us to forecast upcoming release dates:&lt;/p>
&lt;p>&lt;em>Proposed Kubernetes Release Schedule for the remainder of 2021&lt;/em>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Week Number in Year&lt;/th>
&lt;th>Release Number&lt;/th>
&lt;th>Release Week&lt;/th>
&lt;th>Note&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>35&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1 (August 23)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>16 (December 07)&lt;/td>
&lt;td>KubeCon + CloudNativeCon NA Break (Oct 11-15)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;em>Proposed Kubernetes Release Schedule for 2022&lt;/em>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Week Number in Year&lt;/th>
&lt;th>Release Number&lt;/th>
&lt;th>Release Week&lt;/th>
&lt;th>Note&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1 (January 03)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>15 (April 12)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1 (April 26)&lt;/td>
&lt;td>KubeCon + CloudNativeCon EU likely to occur&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>15 (August 09)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>34&lt;/td>
&lt;td>1.26&lt;/td>
&lt;td>1 (August 22&lt;/td>
&lt;td>KubeCon + CloudNativeCon NA likely to occur&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>49&lt;/td>
&lt;td>1.26&lt;/td>
&lt;td>14 (December 06)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>These proposed dates reflect only the start and end dates, and they are subject to change. The Release Team will select dates for enhancement freeze, code freeze, and other milestones at the start of each release. For more information on these milestones, please refer to the &lt;a href="https://www.k8s.dev/resources/release/#phases">release phases&lt;/a> documentation. Feedback from prior releases will feed into this process.&lt;/p>
&lt;h2 id="what-this-means-for-end-users">What this means for end users&lt;/h2>
&lt;p>The major change end users will experience is a slower release cadence and a slower rate of enhancement graduation. Kubernetes release artifacts, release notes, and all other aspects of any given release will stay the same.&lt;/p>
&lt;p>Prior to this change an enhancement could graduate from alpha to stable in 9 months. With the change in cadence, this will stretch to 12 months. Additionally, graduation of features over the last few releases has in some part been driven by release team activities.&lt;/p>
&lt;p>With fewer releases, users can expect to see the rate of feature graduation slow. Users can also expect releases to contain a larger number of enhancements that they need to be aware of during upgrades. However, with fewer releases to consume per year, it's intended that end user organizations will spend less time on upgrades and gain more time on supporting their Kubernetes clusters. It also means that Kubernetes releases are in support for a slightly longer period of time, so bug fixes and security patches will be available for releases for a longer period of time.&lt;/p>
&lt;h2 id="what-this-means-for-kubernetes-contributors">What this means for Kubernetes contributors&lt;/h2>
&lt;p>With a lower release cadence, contributors have more time for project enhancements, feature development, planning, and testing. A slower release cadence also provides more room for maintaining their mental health, preparing for events like KubeCon + CloudNativeCon or work on downstream integrations.&lt;/p>
&lt;h2 id="why-we-decided-to-change-the-release-cadence">Why we decided to change the release cadence&lt;/h2>
&lt;p>The Kubernetes 1.19 cycle was far longer than usual. SIG Release extended it to lessen the burden on both Kubernetes contributors and end users due the COVID-19 pandemic. Following this extended release, the Kubernetes 1.20 release became the third, and final, release for 2020.&lt;/p>
&lt;p>As the Kubernetes project matures, the number of enhancements per cycle grows, along with the burden on contributors, the Release Engineering team. Downstream consumers and integrators also face increased challenges keeping up with &lt;a href="https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/">ever more feature-packed releases&lt;/a>. A wider project adoption means the complexity of supporting a rapidly evolving platform affects a bigger downstream chain of consumers.&lt;/p>
&lt;p>Changing the release cadence from four to three releases per year balances a variety of factors for stakeholders: while it's not strictly an LTS policy, consumers and integrators will get longer support terms for each minor version as the extended release cycles lead to the &lt;a href="https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/">previous three releases being supported&lt;/a> for a longer period. Contributors get more time to &lt;a href="https://www.cncf.io/blog/2021/04/12/enhancing-the-kubernetes-enhancements-process/">mature enhancements&lt;/a> and &lt;a href="https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md">get them ready for production&lt;/a>.&lt;/p>
&lt;p>Finally, the management overhead for SIG Release and the Release Engineering team diminishes allowing the team to spend more time on improving the quality of the software releases and the tooling that drives them.&lt;/p>
&lt;h2 id="how-you-can-help">How you can help&lt;/h2>
&lt;p>Join the &lt;a href="https://github.com/kubernetes/sig-release/discussions/1566">discussion&lt;/a> about communicating future release dates and be sure to be on the lookout for post release surveys.&lt;/p>
&lt;h2 id="where-you-can-find-out-more">Where you can find out more&lt;/h2>
&lt;ul>
&lt;li>Read the KEP &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-release/2572-release-cadence">here&lt;/a>&lt;/li>
&lt;li>Join the &lt;a href="https://groups.google.com/g/kubernetes-dev">kubernetes-dev&lt;/a> mailing list&lt;/li>
&lt;li>Join &lt;a href="https://slack.k8s.io">Kubernetes Slack&lt;/a> and follow the #announcements channel&lt;/li>
&lt;/ul></description></item><item><title>Blog: Spotlight on SIG Usability</title><link>https://kubernetes.io/blog/2021/07/15/sig-usability-spotlight-2021/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/15/sig-usability-spotlight-2021/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kunal Kushwaha, Civo&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Are you interested in learning about what &lt;a href="https://github.com/kubernetes/community/tree/master/sig-usability">SIG Usability&lt;/a> does and how you can get involved? Well, you're at the right place. SIG Usability is all about making Kubernetes more accessible to new folks, and its main activity is conducting user research for the community. In this blog, we have summarized our conversation with &lt;a href="https://twitter.com/morengab">Gaby Moreno&lt;/a>, who walks us through the various aspects of being a part of the SIG and shares some insights about how others can get involved.&lt;/p>
&lt;p>Gaby is a co-lead for SIG Usability. She works as a Product Designer at IBM and enjoys working on the user experience of open, hybrid cloud technologies like Kubernetes, OpenShift, Terraform, and Cloud Foundry.&lt;/p>
&lt;h2 id="a-summary-of-our-conversation">A summary of our conversation&lt;/h2>
&lt;h3 id="q-could-you-tell-us-a-little-about-what-sig-usability-does">Q. Could you tell us a little about what SIG Usability does?&lt;/h3>
&lt;p>A. SIG Usability at a high level started because there was no dedicated user experience team for Kubernetes. The extent of SIG Usability is focussed on the end-client ease of use of the Kubernetes project. The main activity is user research for the community, which includes speaking to Kubernetes users.&lt;/p>
&lt;p>This covers points like user experience and accessibility. The objectives of the SIG are to guarantee that the Kubernetes project is maximally usable by people of a wide range of foundations and capacities, such as incorporating internationalization and ensuring the openness of documentation.&lt;/p>
&lt;h3 id="q-why-should-new-and-existing-contributors-consider-joining-sig-usability">Q. Why should new and existing contributors consider joining SIG Usability?&lt;/h3>
&lt;p>A. There are plenty of territories where new contributors can begin. For example:&lt;/p>
&lt;ul>
&lt;li>User research projects, where people can help understand the usability of the end-user experiences, including error messages, end-to-end tasks, etc.&lt;/li>
&lt;li>Accessibility guidelines for Kubernetes community artifacts, examples include: internationalization of documentation, color choices for people with color blindness, ensuring compatibility with screen reader technology, user interface design for core components with user interfaces, and more.&lt;/li>
&lt;/ul>
&lt;h3 id="q-what-do-you-do-to-help-new-contributors-get-started">Q. What do you do to help new contributors get started?&lt;/h3>
&lt;p>A. New contributors can get started by shadowing one of the user interviews, going through user interview transcripts, analyzing them, and designing surveys.&lt;/p>
&lt;p>SIG Usability is also open to new project ideas. If you have an idea, we’ll do what we can to support it. There are regular SIG Meetings where people can ask their questions live. These meetings are also recorded for those who may not be able to attend. As always, you can reach out to us on Slack as well.&lt;/p>
&lt;h3 id="q-what-does-the-survey-include">Q. What does the survey include?&lt;/h3>
&lt;p>A. In simple terms, the survey gathers information about how people use Kubernetes, such as trends in learning to deploy a new system, error messages they receive, and workflows.&lt;/p>
&lt;p>One of our goals is to standardize the responses accordingly. The ultimate goal is to analyze survey responses for important user stories whose needs aren't being met.&lt;/p>
&lt;h3 id="q-are-there-any-particular-skills-you-d-like-to-recruit-for-what-skills-are-contributors-to-sig-usability-likely-to-learn">Q. Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?&lt;/h3>
&lt;p>A. Although contributing to SIG Usability does not have any pre-requisites as such, experience with user research, qualitative research, or prior experience with how to conduct an interview would be great plus points. Quantitative research, like survey design and screening, is also helpful and something that we expect contributors to learn.&lt;/p>
&lt;h3 id="q-what-are-you-getting-positive-feedback-on-and-what-s-coming-up-next-for-sig-usability">Q. What are you getting positive feedback on, and what’s coming up next for SIG Usability?&lt;/h3>
&lt;p>A. We have had new members joining and coming to monthly meetings regularly and showing interests in becoming a contributor and helping the community. We have also had a lot of people reach out to us via Slack showcasing their interest in the SIG.&lt;/p>
&lt;p>Currently, we are focused on finishing the study mentioned in our &lt;a href="https://www.youtube.com/watch?v=Byn0N_ZstE0">talk&lt;/a>, also our project for this year. We are always happy to have new contributors join us.&lt;/p>
&lt;h3 id="q-any-closing-thoughts-resources-you-d-like-to-share">Q: Any closing thoughts/resources you’d like to share?&lt;/h3>
&lt;p>A. We love meeting new contributors and assisting them in investigating different Kubernetes project spaces. We will work with and team up with other SIGs to facilitate engaging with end-users, running studies, and help them integrate accessible design practices into their development practices.&lt;/p>
&lt;p>Here are some resources for you to get started:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-usability">GitHub&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/g/kubernetes-sig-usability">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fusability">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.slack.com/archives/CLC5EF63T">Slack channel #sig-usability&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>SIG Usability hosted a &lt;a href="https://www.youtube.com/watch?v=Byn0N_ZstE0">KubeCon talk&lt;/a> about studying Kubernetes users' experiences. The talk focuses on updates to the user study projects, understanding who is using Kubernetes, what they are trying to achieve, how the project is addressing their needs, and where we need to improve the project and the client experience. Join the SIG's update to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream usability team as a contributor!&lt;/p></description></item><item><title>Blog: Kubernetes API and Feature Removals In 1.22: Here’s What You Need To Know</title><link>https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Krishna Kilari (Amazon Web Services), Tim Bannister (The Scale Factory)&lt;/p>
&lt;p>As the Kubernetes API evolves, APIs are periodically reorganized or upgraded.
When APIs evolve, the old APIs they replace are deprecated, and eventually removed.
See &lt;a href="#kubernetes-api-removals">Kubernetes API removals&lt;/a> to read more about Kubernetes'
policy on removing APIs.&lt;/p>
&lt;p>We want to make sure you're aware of some upcoming removals. These are
beta APIs that you can use in current, supported Kubernetes versions,
and they are already deprecated. The reason for all of these removals
is that they have been superseded by a newer, stable (“GA”) API.&lt;/p>
&lt;p>Kubernetes 1.22, due for release in August 2021, will remove a number of deprecated
APIs.
&lt;em>Update&lt;/em>:
&lt;a href="https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/">Kubernetes 1.22: Reaching New Peaks&lt;/a>
has details on the v1.22 release.&lt;/p>
&lt;h2 id="api-changes">API removals for Kubernetes v1.22&lt;/h2>
&lt;p>The &lt;strong>v1.22&lt;/strong> release will stop serving the API versions we've listed immediately below.
These are all beta APIs that were previously deprecated in favor of newer and more stable
API versions.&lt;/p>
&lt;!-- sorted by API group -->
&lt;ul>
&lt;li>Beta versions of the &lt;code>ValidatingWebhookConfiguration&lt;/code> and &lt;code>MutatingWebhookConfiguration&lt;/code> API (the &lt;strong>admissionregistration.k8s.io/v1beta1&lt;/strong> API versions)&lt;/li>
&lt;li>The beta &lt;code>CustomResourceDefinition&lt;/code> API (&lt;strong>apiextensions.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>APIService&lt;/code> API (&lt;strong>apiregistration.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>TokenReview&lt;/code> API (&lt;strong>authentication.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>Beta API versions of &lt;code>SubjectAccessReview&lt;/code>, &lt;code>LocalSubjectAccessReview&lt;/code>, &lt;code>SelfSubjectAccessReview&lt;/code> (API versions from &lt;strong>authorization.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>CertificateSigningRequest&lt;/code> API (&lt;strong>certificates.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>Lease&lt;/code> API (&lt;strong>coordination.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>All beta &lt;code>Ingress&lt;/code> APIs (the &lt;strong>extensions/v1beta1&lt;/strong> and &lt;strong>networking.k8s.io/v1beta1&lt;/strong> API versions)&lt;/li>
&lt;/ul>
&lt;p>The Kubernetes documentation covers these
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22">API removals for v1.22&lt;/a> and explains
how each of those APIs change between beta and stable.&lt;/p>
&lt;h2 id="what-to-do">What to do&lt;/h2>
&lt;p>We're going to run through each of the resources that are affected by these removals
and explain the steps you'll need to take.&lt;/p>
&lt;dl>
&lt;dt>&lt;code>Ingress&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>networking.k8s.io/v1&lt;/strong>
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress&lt;/a> API,
&lt;a href="https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/#ingress-graduates-to-general-availability">available since v1.19&lt;/a>.&lt;br>
The related API &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-class-v1/">IngressClass&lt;/a>
is designed to complement the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>
concept, allowing you to configure multiple kinds of Ingress within one cluster.
If you're currently using the deprecated
&lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-ingress-class-deprecated">&lt;code>kubernetes.io/ingress.class&lt;/code>&lt;/a>
annotation, plan to switch to using the &lt;code>.spec.ingressClassName&lt;/code> field instead.&lt;br>
On any cluster running Kubernetes v1.19 or later, you can use the v1 API to
retrieve or update existing Ingress objects, even if they were created using an
older API version.
&lt;p>When you convert an Ingress to the v1 API, you should review each rule in that Ingress.
Older Ingresses use the legacy &lt;code>ImplementationSpecific&lt;/code> path type. Instead of &lt;code>ImplementationSpecific&lt;/code>, switch &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types">path matching&lt;/a> to either &lt;code>Prefix&lt;/code> or &lt;code>Exact&lt;/code>. One of the benefits of moving to these alternative path types is that it becomes easier to migrate between different Ingress classes.&lt;/p>
&lt;p>&lt;strong>ⓘ&lt;/strong> As well as upgrading &lt;em>your&lt;/em> own use of the Ingress API as a client, make sure that
every ingress controller that you use is compatible with the v1 Ingress API.
Read &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#prerequisites">Ingress Prerequisites&lt;/a>
for more context about Ingress and ingress controllers.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>ValidatingWebhookConfiguration&lt;/code> and &lt;code>MutatingWebhookConfiguration&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>admissionregistration.k8s.io/v1&lt;/strong> API versions of
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/validating-webhook-configuration-v1/">ValidatingWebhookConfiguration&lt;/a>
and &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/mutating-webhook-configuration-v1/">MutatingWebhookConfiguration&lt;/a>,
available since v1.16.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created using an older API version.&lt;/dd>
&lt;dt>&lt;code>CustomResourceDefinition&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/custom-resource-definition-v1/">CustomResourceDefinition&lt;/a>
&lt;strong>apiextensions.k8s.io/v1&lt;/strong> API, available since v1.16.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version. If you defined any custom resources in your cluster, those
are still served after you upgrade.
&lt;p>If you're using external CustomResourceDefinitions, you can use
&lt;a href="#kubectl-convert">&lt;code>kubectl convert&lt;/code>&lt;/a> to translate existing manifests to use the newer API.
Because there are some functional differences between beta and stable CustomResourceDefinitions,
our advice is to test out each one to make sure it works how you expect after the upgrade.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>APIService&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>apiregistration.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/api-service-v1/">APIService&lt;/a>
API, available since v1.10.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created using an older API version.
If you already have API aggregation using an APIService object, this aggregation continues
to work after you upgrade.&lt;/dd>
&lt;dt>&lt;code>TokenReview&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>authentication.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-review-v1/">TokenReview&lt;/a>
API, available since v1.10.
&lt;p>As well as serving this API via HTTP, the Kubernetes API server uses the same format to
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">send&lt;/a>
TokenReviews to webhooks. The v1.22 release continues to use the v1beta1 API for TokenReviews
sent to webhooks by default. See &lt;a href="#looking-ahead">Looking ahead&lt;/a> for some specific tips about
switching to the stable API.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>SubjectAccessReview&lt;/code>, &lt;code>SelfSubjectAccessReview&lt;/code> and &lt;code>LocalSubjectAccessReview&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>authorization.k8s.io/v1&lt;/strong> versions of those
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/">authorization APIs&lt;/a>, available since v1.6.&lt;/dd>
&lt;dt>&lt;code>CertificateSigningRequest&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>certificates.k8s.io/v1&lt;/strong>
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/certificate-signing-request-v1/">CertificateSigningRequest&lt;/a>
API, available since v1.19.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version. Existing issued certificates retain their validity when you upgrade.&lt;/dd>
&lt;dt>&lt;code>Lease&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>coordination.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/">Lease&lt;/a>
API, available since v1.14.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version.&lt;/dd>
&lt;/dl>
&lt;h3 id="kubectl-convert">&lt;code>kubectl convert&lt;/code>&lt;/h3>
&lt;p>There is a plugin to &lt;code>kubectl&lt;/code> that provides the &lt;code>kubectl convert&lt;/code> subcommand.
It's an official plugin that you can download as part of Kubernetes.
See &lt;a href="https://kubernetes.io/releases/download/">Download Kubernetes&lt;/a> for more details.&lt;/p>
&lt;p>You can use &lt;code>kubectl convert&lt;/code> to update manifest files to use a different API
version. For example, if you have a manifest in source control that uses the beta
Ingress API, you can check that definition out,
and run
&lt;code>kubectl convert -f &amp;lt;manifest&amp;gt; --output-version &amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;&lt;/code>.
You can use the &lt;code>kubectl convert&lt;/code> command to automatically convert an
existing manifest.&lt;/p>
&lt;p>For example, to convert an older Ingress definition to
&lt;code>networking.k8s.io/v1&lt;/code>, you can run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl convert -f ./legacy-ingress.yaml --output-version networking.k8s.io/v1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The automatic conversion uses a similar technique to how the Kubernetes control plane
updates objects that were originally created using an older API version. Because it's
a mechanical conversion, you might need to go in and change the manifest to adjust
defaults etc.&lt;/p>
&lt;h3 id="rehearse-for-the-upgrade">Rehearse for the upgrade&lt;/h3>
&lt;p>If you manage your cluster's API server component, you can try out these API
removals before you upgrade to Kubernetes v1.22.&lt;/p>
&lt;p>To do that, add the following to the kube-apiserver command line arguments:&lt;/p>
&lt;p>&lt;code>--runtime-config=admissionregistration.k8s.io/v1beta1=false,apiextensions.k8s.io/v1beta1=false,apiregistration.k8s.io/v1beta1=false,authentication.k8s.io/v1beta1=false,authorization.k8s.io/v1beta1=false,certificates.k8s.io/v1beta1=false,coordination.k8s.io/v1beta1=false,extensions/v1beta1/ingresses=false,networking.k8s.io/v1beta1=false&lt;/code>&lt;/p>
&lt;p>(as a side effect, this also turns off v1beta1 of EndpointSlice - watch out for
that when you're testing).&lt;/p>
&lt;p>Once you've switched all the kube-apiservers in your cluster to use that setting,
those beta APIs are removed. You can test that API clients (&lt;code>kubectl&lt;/code>, deployment
tools, custom controllers etc) still work how you expect, and you can revert if
you need to without having to plan a more disruptive downgrade.&lt;/p>
&lt;h3 id="advice-for-software-authors">Advice for software authors&lt;/h3>
&lt;p>Maybe you're reading this because you're a developer of an addon or other
component that integrates with Kubernetes?&lt;/p>
&lt;p>If you develop an Ingress controller, webhook authenticator, an API aggregation, or
any other tool that relies on these deprecated APIs, you should already have started
to switch your software over.&lt;/p>
&lt;p>You can use the tips in
&lt;a href="#rehearse-for-the-upgrade">Rehearse for the upgrade&lt;/a> to run your own Kubernetes
cluster that only uses the new APIs, and make sure that your code works OK.
For your documentation, make sure readers are aware of any steps they should take
for the Kubernetes v1.22 upgrade.&lt;/p>
&lt;p>Where possible, give your users a hand to adopt the new APIs early - perhaps in a
test environment - so they can give you feedback about any problems.&lt;/p>
&lt;p>There are some &lt;a href="#looking-ahead">more deprecations&lt;/a> coming in Kubernetes v1.25,
so plan to have those covered too.&lt;/p>
&lt;h2 id="kubernetes-api-removals">Kubernetes API removals&lt;/h2>
&lt;p>Here's some background about why Kubernetes removes some APIs, and also a promise
about &lt;em>stable&lt;/em> APIs in Kubernetes.&lt;/p>
&lt;p>Kubernetes follows a defined
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a> for its
features, including the Kubernetes API. That policy allows for replacing stable
(“GA”) APIs from Kubernetes. Importantly, this policy means that a stable API only
be deprecated when a newer stable version of that same API is available.&lt;/p>
&lt;p>That stability guarantee matters: if you're using a stable Kubernetes API, there
won't ever be a new version released that forces you to switch to an alpha or beta
feature.&lt;/p>
&lt;p>Earlier stages are different. Alpha features are under test and potentially
incomplete. Almost always, alpha features are disabled by default.
Kubernetes releases can and do remove alpha features that haven't worked out.&lt;/p>
&lt;p>After alpha, comes beta. These features are typically enabled by default; if the
testing works out, the feature can graduate to stable. If not, it might need
a redesign.&lt;/p>
&lt;p>Last year, Kubernetes officially
&lt;a href="https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/#avoiding-permanent-beta">adopted&lt;/a>
a policy for APIs that have reached their beta phase:&lt;/p>
&lt;blockquote>
&lt;p>For Kubernetes REST APIs, when a new feature's API reaches beta, that starts
a countdown. The beta-quality API now has three releases …
to either:&lt;/p>
&lt;ul>
&lt;li>reach GA, and deprecate the beta, or&lt;/li>
&lt;li>have a new beta version (and deprecate the previous beta).&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;em>At the time of that article, three Kubernetes releases equated to roughly nine
calendar months. Later that same month, Kubernetes
adopted a new
release cadence of three releases per calendar year, so the countdown period is
now roughly twelve calendar months.&lt;/em>&lt;/p>
&lt;p>Whether an API removal is because of a beta feature graduating to stable, or
because that API hasn't proved successful, Kubernetes will continue to remove
APIs by following its deprecation policy and making sure that migration options
are documented.&lt;/p>
&lt;h3 id="looking-ahead">Looking ahead&lt;/h3>
&lt;p>There's a setting that's relevant if you use webhook authentication checks.
A future Kubernetes release will switch to sending TokenReview objects
to webhooks using the &lt;code>authentication.k8s.io/v1&lt;/code> API by default. At the moment,
the default is to send &lt;code>authentication.k8s.io/v1beta1&lt;/code> TokenReviews to webhooks,
and that's still the default for Kubernetes v1.22.
However, you can switch over to the stable API right now if you want:
add &lt;code>--authentication-token-webhook-version=v1&lt;/code> to the command line options for
the kube-apiserver, and check that webhooks for authentication still work how you
expected.&lt;/p>
&lt;p>Once you're happy it works OK, you can leave the &lt;code>--authentication-token-webhook-version=v1&lt;/code>
option set across your control plane.&lt;/p>
&lt;p>The &lt;strong>v1.25&lt;/strong> release that's planned for next year will stop serving beta versions of
several Kubernetes APIs that are stable right now and have been for some time.
The same v1.25 release will &lt;strong>remove&lt;/strong> PodSecurityPolicy, which is deprecated and won't
graduate to stable. See
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>
for more information.&lt;/p>
&lt;p>The official &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25">list of API removals&lt;/a>
planned for Kubernetes 1.25 is:&lt;/p>
&lt;ul>
&lt;li>The beta &lt;code>CronJob&lt;/code> API (&lt;strong>batch/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>EndpointSlice&lt;/code> API (&lt;strong>networking.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>PodDisruptionBudget&lt;/code> API (&lt;strong>policy/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>PodSecurityPolicy&lt;/code> API (&lt;strong>policy/v1beta1&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;h2 id="want-to-know-more">Want to know more?&lt;/h2>
&lt;p>Deprecations are announced in the Kubernetes release notes. You can see the announcements
of pending deprecations in the release notes for
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecations">1.19&lt;/a>,
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">1.20&lt;/a>,
and &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">1.21&lt;/a>.&lt;/p>
&lt;p>For information on the process of deprecation and removal, check out the official Kubernetes
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">deprecation policy&lt;/a>
document.&lt;/p></description></item><item><title>Blog: Announcing Kubernetes Community Group Annual Reports</title><link>https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/</link><pubDate>Mon, 28 Jun 2021 10:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Divya Mohan&lt;/p>
&lt;figure>&lt;a href="https://www.cncf.io/reports/kubernetes-community-annual-report-2020/">
&lt;img src="https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/k8s_annual_report_2020.svg"
alt="Community annual report 2020"/> &lt;/a>
&lt;/figure>
&lt;p>Given the growth and scale of the Kubernetes project, the existing reporting mechanisms were proving to be inadequate and challenging.
Kubernetes is a large open source project. With over 100000 commits just to the main k/kubernetes repository, hundreds of other code
repositories in the project, and thousands of contributors, there's a lot going on. In fact, there are 37 contributor groups at the time of
writing. We also value all forms of contribution and not just code changes.&lt;/p>
&lt;p>With that context in mind, the challenge of reporting on all this activity was a call to action for exploring better options. Therefore
inspired by the Apache Software Foundation’s &lt;a href="https://www.apache.org/foundation/board/reporting">open guide to PMC Reporting&lt;/a> and the
&lt;a href="https://www.cncf.io/cncf-annual-report-2020/">CNCF project Annual Reporting&lt;/a>, the Kubernetes project is proud to announce the
&lt;strong>Kubernetes Community Group Annual Reports for Special Interest Groups (SIGs) and Working Groups (WGs)&lt;/strong>. In its flagship edition,
the &lt;a href="https://www.cncf.io/reports/kubernetes-community-annual-report-2020/">2020 Summary report&lt;/a> focuses on bettering the
Kubernetes ecosystem by assessing and promoting the healthiness of the groups within the upstream community.&lt;/p>
&lt;p>Previously, the mechanisms for the Kubernetes project overall to report on groups and their activities were
&lt;a href="https://k8s.devstats.cncf.io/">devstats&lt;/a>, GitHub data, issues, to measure the healthiness of a given UG/WG/SIG/Committee. As a
project spanning several diverse communities, it was essential to have something that captured the human side of things. With 50,000+
contributors, it’s easy to assume that the project has enough help and this report surfaces more information than /help-wanted and
/good-first-issue for end users. This is how we sustain the project. Paraphrasing one of the Steering Committee members,
&lt;a href="https://github.com/parispittman">Paris Pittman&lt;/a>, “There was a requirement for tighter feedback loops - ones that involved more than just
GitHub data and issues. Given that Kubernetes, as a project, has grown in scale and number of contributors over the years, we have
outgrown the existing reporting mechanisms.&amp;quot;&lt;/p>
&lt;p>The existing communication channels between the Steering committee members and the folks leading the groups and committees were also required
to be made as open and as bi-directional as possible. Towards achieving this very purpose, every group and committee has been assigned a
liaison from among the steering committee members for kick off, help, or guidance needed throughout the process. According to
&lt;a href="https://github.com/dims">Davanum Srinivas a.k.a. dims&lt;/a>, “... That was one of the main motivations behind this report. People (leading the
groups/committees) know that they can reach out to us and there’s a vehicle for them to reach out to us… This is our way of setting up a
two-way feedback for them.&amp;quot; The progress on these action items would be updated and tracked on the monthly Steering Committee meetings
ensuring that this is not a one-off activity. Quoting &lt;a href="https://github.com/nikhita">Nikhita Raghunath&lt;/a>, one of the Steering Committee members,
“... Once we have a base, the liaisons will work with these groups to ensure that the problems are resolved. When we have a report next year,
we’ll have a look at the progress made and how we could still do better. But the idea is definitely to not stop at the report.”&lt;/p>
&lt;p>With this report, we hope to empower our end user communities with information that they can use to identify ways in which they can support
the project as well as a sneak peek into the roadmap for upcoming features. As a community, we thrive on feedback and would love to hear your
views about the report. You can get in touch with the &lt;a href="https://github.com/kubernetes/steering#contact">Steering Committee&lt;/a> via
&lt;a href="https://kubernetes.slack.com/messages/steering-committee">Slack&lt;/a> or via the &lt;a href="steering@kubernetes.io">mailing list&lt;/a>.&lt;/p></description></item></channel></rss>