<!doctype html><html lang=zh class=no-js>
<head>
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JPP6RFM2BP"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JPP6RFM2BP')</script>
<link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/scheduling-eviction/>
<link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/scheduling-eviction/>
<link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/scheduling-eviction/>
<link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/scheduling-eviction/>
<link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/scheduling-eviction/>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.87.0">
<link rel=canonical type=text/html href=https://kubernetes.io/zh/docs/concepts/scheduling-eviction/>
<link rel="shortcut icon" type=image/png href=/images/favicon.png>
<link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=manifest href=/manifest.webmanifest>
<link rel=apple-touch-icon href=/images/kubernetes-192x192.png>
<title>调度，抢占和驱逐 | Kubernetes</title><meta property="og:title" content="调度，抢占和驱逐">
<meta property="og:description" content="在Kubernetes中，调度 (scheduling) 指的是确保 Pods 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pods 以便高优先级的 Pods 可以 调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。
">
<meta property="og:type" content="website">
<meta property="og:url" content="https://kubernetes.io/zh/docs/concepts/scheduling-eviction/"><meta property="og:site_name" content="Kubernetes">
<meta itemprop=name content="调度，抢占和驱逐">
<meta itemprop=description content="在Kubernetes中，调度 (scheduling) 指的是确保 Pods 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pods 以便高优先级的 Pods 可以 调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。
"><meta name=twitter:card content="summary">
<meta name=twitter:title content="调度，抢占和驱逐">
<meta name=twitter:description content="在Kubernetes中，调度 (scheduling) 指的是确保 Pods 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pods 以便高优先级的 Pods 可以 调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。
">
<link href=/scss/main.css rel=stylesheet>
<script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script>
<meta name=theme-color content="#326ce5">
<link rel=stylesheet href=/css/feature-states.css>
<meta name=description content="在Kubernetes中，调度 (scheduling) 指的是确保 Pods 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pods 以便高优先级的 Pods 可以 调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。
">
<meta property="og:description" content="在Kubernetes中，调度 (scheduling) 指的是确保 Pods 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pods 以便高优先级的 Pods 可以 调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。
">
<meta name=twitter:description content="在Kubernetes中，调度 (scheduling) 指的是确保 Pods 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pods 以便高优先级的 Pods 可以 调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。
">
<meta property="og:url" content="https://kubernetes.io/zh/docs/concepts/scheduling-eviction/">
<meta property="og:title" content="调度，抢占和驱逐">
<meta name=twitter:title content="调度，抢占和驱逐">
<meta name=twitter:image content="https://kubernetes.io/images/favicon.png">
<meta name=twitter:image:alt content="Kubernetes">
<meta property="og:image" content="/images/kubernetes-horizontal-color.png">
<meta property="og:type" content="article">
<script src=/js/script.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary>
<a class=navbar-brand href=/zh/></a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-2 mb-lg-0">
<a class="nav-link active" href=/zh/docs/>文档</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/blog/>Kubernetes 博客</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/training/>培训</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/partners/>合作伙伴</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/community/>社区</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/zh/case-studies/>案例分析</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.27</a>
<a class=dropdown-item href=https://v1-26.docs.kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.26</a>
<a class=dropdown-item href=https://v1-25.docs.kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.23</a>
</div>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/docs/concepts/scheduling-eviction/>English</a>
<a class=dropdown-item href=/ko/docs/concepts/scheduling-eviction/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/concepts/scheduling-eviction/>日本語 Japanese</a>
<a class=dropdown-item href=/pt-br/docs/concepts/scheduling-eviction/>Português</a>
<a class=dropdown-item href=/id/docs/concepts/scheduling-eviction/>Bahasa Indonesia</a>
</div>
</li>
</ul>
</div>
<button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.
</p><p>
<a href=/zh/docs/concepts/scheduling-eviction/>返回本页常规视图</a>.
</p>
</div>
<h1 class=title>调度，抢占和驱逐</h1>
<div class=lead>在Kubernetes中，调度 (scheduling) 指的是确保 Pods 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pods 以便高优先级的 Pods 可以 调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。</div>
<ul>
<li>1: <a href=#pg-598f36d691ab197f9d995784574b0a12>Kubernetes 调度器</a></li>
<li>2: <a href=#pg-21169f516071aea5d16734a4c27789a5>将 Pod 指派给节点</a></li>
<li>3: <a href=#pg-da22fe2278df236f71efbe672f392677>Pod 开销</a></li>
<li>4: <a href=#pg-ede4960b56a3529ee0bfe7c8fe2d09a5>污点和容忍度</a></li>
<li>5: <a href=#pg-60e5a2861609e0848d58ce8bf99c4a31>Pod 优先级和抢占</a></li>
<li>6: <a href=#pg-78e0431b4b7516092662a7c289cbb304>节点压力驱逐</a></li>
<li>7: <a href=#pg-b87723bf81b079042860f0ebd37b0a64>API 发起的驱逐</a></li>
<li>8: <a href=#pg-961126cd43559012893979e568396a49>扩展资源的资源装箱</a></li>
<li>9: <a href=#pg-602208c95fe7b1f1170310ce993f5814>调度框架</a></li>
<li>10: <a href=#pg-d9574a30fcbc631b0d2a57850e161e89>调度器性能调优</a></li>
</ul>
<div class=content>
<h2 id=调度>调度</h2>
<ul>
<li><a href=/zh/docs/concepts/scheduling-eviction/kube-scheduler/>Kubernetes 调度器</a></li>
<li><a href=/zh/docs/concepts/scheduling-eviction/assign-pod-node/>将 Pods 指派到节点</a></li>
<li><a href=/zh/docs/concepts/scheduling-eviction/pod-overhead/>Pod 开销</a></li>
<li><a href=/zh/docs/concepts/scheduling-eviction/taint-and-toleration/>污点和容忍</a></li>
<li><a href=/zh/docs/concepts/scheduling-eviction/scheduling-framework>调度框架</a></li>
<li><a href=/zh/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>调度器的性能调试</a></li>
<li><a href=/zh/docs/concepts/scheduling-eviction/resource-bin-packing/>扩展资源的资源装箱</a></li>
</ul>
<h2 id=pod-干扰>Pod 干扰</h2>
<ul>
<li><a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级和抢占</a></li>
<li><a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>节点压力驱逐</a></li>
<li><a href=/zh/docs/concepts/scheduling-eviction/api-eviction/>API发起的驱逐</a></li>
</ul>
</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-598f36d691ab197f9d995784574b0a12>1 - Kubernetes 调度器</h1>
<p>在 Kubernetes 中，<em>调度</em> 是指将 <a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> 放置到合适的
<a class=glossary-tooltip title="Kubernetes 中的工作机器称作节点。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> 上，然后对应 Node 上的
<a class=glossary-tooltip title="一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。" data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> 才能够运行这些 pod。</p>
<h2 id=scheduling>调度概览</h2>
<p>调度器通过 kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。
调度器会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。
调度器会依据下文的调度原则来做出调度选择。</p>
<p>如果你想要理解 Pod 为什么会被调度到特定的 Node 上，或者你想要尝试实现
一个自定义的调度器，这篇文章将帮助你了解调度。</p>
<h2 id=kube-scheduler>kube-scheduler</h2>
<p><a href=/zh/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
是 Kubernetes 集群的默认调度器，并且是集群
<a class=glossary-tooltip title="控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。" data-toggle=tooltip data-placement=top href="/zh/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label=控制面>控制面</a> 的一部分。
如果你真的希望或者有这方面的需求，kube-scheduler 在设计上是允许
你自己写一个调度组件并替换原有的 kube-scheduler。</p>
<p>对每一个新创建的 Pod 或者是未被调度的 Pod，kube-scheduler 会选择一个最优的
Node 去运行这个 Pod。然而，Pod 内的每一个容器对资源都有不同的需求，而且
Pod 本身也有不同的资源需求。因此，Pod 在被调度到 Node 上之前，
根据这些特定的资源调度需求，需要对集群中的 Node 进行一次过滤。</p>
<p>在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 <em>可调度节点</em>。
如果没有任何一个 Node 能满足 Pod 的资源请求，那么这个 Pod 将一直停留在
未调度状态直到调度器能够找到合适的 Node。</p>
<p>调度器先在集群中找到一个 Pod 的所有可调度节点，然后根据一系列函数对这些可调度节点打分，
选出其中得分最高的 Node 来运行 Pod。之后，调度器将这个调度决定通知给
kube-apiserver，这个过程叫做 <em>绑定</em>。</p>
<p>在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、
亲和以及反亲和要求、数据局域性、负载间的干扰等等。</p>
<h2 id=kube-scheduler-implementation>kube-scheduler 调度流程</h2>
<p>kube-scheduler 给一个 pod 做调度选择包含两个步骤：</p>
<ol>
<li>过滤</li>
<li>打分</li>
</ol>
<p>过滤阶段会将所有满足 Pod 调度需求的 Node 选出来。
例如，PodFitsResources 过滤函数会检查候选 Node 的可用资源能否满足 Pod 的资源请求。
在过滤之后，得出一个 Node 列表，里面包含了所有可调度节点；通常情况下，
这个 Node 列表包含不止一个 Node。如果这个列表是空的，代表这个 Pod 不可调度。</p>
<p>在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node。
根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。</p>
<p>最后，kube-scheduler 会将 Pod 调度到得分最高的 Node 上。
如果存在多个得分最高的 Node，kube-scheduler 会从中随机选取一个。</p>
<p>支持以下两种方式配置调度器的过滤和打分行为：</p>
<ol>
<li><a href=/zh/docs/reference/scheduling/policies>调度策略</a> 允许你配置过滤的 <em>断言(Predicates)</em>
和打分的 <em>优先级(Priorities)</em> 。</li>
<li><a href=/zh/docs/reference/scheduling/config/#profiles>调度配置</a> 允许你配置实现不同调度阶段的插件，
包括：<code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code> 等等。
你也可以配置 kube-scheduler 运行不同的配置文件。</li>
</ol>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>阅读关于 <a href=/zh/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>调度器性能调优</a></li>
<li>阅读关于 <a href=/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/>Pod 拓扑分布约束</a></li>
<li>阅读关于 kube-scheduler 的 <a href=/zh/docs/reference/command-line-tools-reference/kube-scheduler/>参考文档</a></li>
<li>阅读 <a href=/zh/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler 配置参考 (v1beta3)</a></li>
<li>了解关于 <a href=/zh/docs/tasks/extend-kubernetes/configure-multiple-schedulers/>配置多个调度器</a> 的方式</li>
<li>了解关于 <a href=/zh/docs/tasks/administer-cluster/topology-manager/>拓扑结构管理策略</a></li>
<li>了解关于 <a href=/zh/docs/concepts/scheduling-eviction/pod-overhead/>Pod 额外开销</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-21169f516071aea5d16734a4c27789a5>2 - 将 Pod 指派给节点</h1>
<p>你可以约束一个 <a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>
只能在特定的<a class=glossary-tooltip title="Kubernetes 中的工作机器称作节点。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>上运行。
有几种方法可以实现这点，推荐的方法都是用
<a href=/zh/docs/concepts/overview/working-with-objects/labels/>标签选择算符</a>来进行选择。
通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 Pod 分散到节点上，
而不是将 Pod 放置在可用资源不足的节点上等等）。但在某些情况下，你可能需要进一步控制
Pod 被部署到的节点。例如，确保 Pod 最终落在连接了 SSD 的机器上，
或者将来自两个不同的服务且有大量通信的 Pods 被放置在同一个可用区。</p>
<p>你可以使用下列方法中的任何一种来选择 Kubernetes 对特定 Pod 的调度：</p>
<ul>
<li>与<a href=#built-in-node-labels>节点标签</a>匹配的 <a href=#nodeSelector>nodeSelector</a></li>
<li><a href=#affinity-and-anti-affinity>亲和性与反亲和性</a></li>
<li><a href=#nodename>nodeName</a> 字段</li>
</ul>
<h2 id=built-in-node-labels>节点标签 </h2>
<p>与很多其他 Kubernetes 对象类似，节点也有<a href=/zh/docs/concepts/overview/working-with-objects/labels/>标签</a>。
你可以<a href=/zh/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node>手动地添加标签</a>。
Kubernetes 也会为集群中所有节点添加一些标准的标签。
参见<a href=/zh/docs/reference/labels-annotations-taints/>常用的标签、注解和污点</a>以了解常见的节点标签。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>这些标签的取值是取决于云提供商的，并且是无法在可靠性上给出承诺的。
例如，<code>kubernetes.io/hostname</code> 的取值在某些环境中可能与节点名称相同，
而在其他环境中会取不同的值。
</div>
<h2 id=node-isolation-restriction>节点隔离/限制 </h2>
<p>通过为节点添加标签，你可以准备让 Pod 调度到特定节点或节点组上。
你可以使用这个功能来确保特定的 Pod 只能运行在具有一定隔离性，安全性或监管属性的节点上。</p>
<p>如果使用标签来实现节点隔离，建议选择节点上的
<a class=glossary-tooltip title="一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。" data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
无法修改的标签键。
这可以防止受感染的节点在自身上设置这些标签，进而影响调度器将工作负载调度到受感染的节点。</p>
<p><a href=/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction><code>NodeRestriction</code> 准入插件</a>防止
kubelet 使用 <code>node-restriction.kubernetes.io/</code> 前缀设置或修改标签。</p>
<p>要使用该标签前缀进行节点隔离：</p>
<ol>
<li>确保你在使用<a href=/zh/docs/reference/access-authn-authz/node/>节点鉴权</a>机制并且已经启用了
<a href=/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction 准入插件</a>。</li>
<li>将带有 <code>node-restriction.kubernetes.io/</code> 前缀的标签添加到 Node 对象，
然后在<a href=#nodeSelector>节点选择器</a>中使用这些标签。
例如，<code>example.com.node-restriction.kubernetes.io/fips=true</code> 或
<code>example.com.node-restriction.kubernetes.io/pci-dss=true</code>。</li>
</ol>
<h2 id=nodeselector>nodeSelector</h2>
<p><code>nodeSelector</code> 是节点选择约束的最简单推荐形式。你可以将 <code>nodeSelector</code> 字段添加到
Pod 的规约中设置你希望目标节点所具有的<a href=#built-in-node-labels>节点标签</a>。
Kubernetes 只会将 Pod 调度到拥有你所指定的每个标签的节点上。</p>
<p>进一步的信息可参见<a href=/zh/docs/tasks/configure-pod-container/assign-pods-nodes>将 Pod 指派给节点</a>。</p>
<h2 id=affinity-and-anti-affinity>亲和性与反亲和性 </h2>
<p><code>nodeSelector</code> 提供了一种最简单的方法来将 Pod 约束到具有特定标签的节点上。
亲和性和反亲和性扩展了你可以定义的约束类型。使用亲和性与反亲和性的一些好处有：</p>
<ul>
<li>亲和性、反亲和性语言的表达能力更强。<code>nodeSelector</code> 只能选择拥有所有指定标签的节点。
亲和性、反亲和性为你提供对选择逻辑的更强控制能力。</li>
<li>你可以标明某规则是“软需求”或者“偏好”，这样调度器在无法找到匹配节点时仍然调度该 Pod。</li>
<li>你可以使用节点上（或其他拓扑域中）运行的其他 Pod 的标签来实施调度约束，
而不是只能使用节点本身的标签。这个能力让你能够定义规则允许哪些 Pod 可以被放置在一起。</li>
</ul>
<h3 id=node-affinity>节点亲和性 </h3>
<p>节点亲和性概念上类似于 <code>nodeSelector</code>，
它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上。
节点亲和性有两种：</p>
<ul>
<li><code>requiredDuringSchedulingIgnoredDuringExecution</code>：
调度器只有在规则被满足的时候才能执行调度。此功能类似于 <code>nodeSelector</code>，
但其语法表达能力更强。</li>
<li><code>preferredDuringSchedulingIgnoredDuringExecution</code>：
调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>在上述类型中，<code>IgnoredDuringExecution</code> 意味着如果节点标签在 Kubernetes
调度 Pod 时发生了变更，Pod 仍将继续运行。
</div>
<p>你可以使用 Pod 规约中的 <code>.spec.affinity.nodeAffinity</code> 字段来设置节点亲和性。
例如，考虑下面的 Pod 规约：</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/zh/examples/pods/pod-with-node-affinity.yaml download=pods/pod-with-node-affinity.yaml><code>pods/pod-with-node-affinity.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-with-node-affinity-yaml')" title="Copy pods/pod-with-node-affinity.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-pod-with-node-affinity-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>kubernetes.io/os<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- linux<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>another-node-label-key<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- another-node-label-value<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:2.0</code></pre></div>
</div>
</div>
<p>在这一示例中，所应用的规则如下：</p>
<ul>
<li>节点必须包含键名为 <code>kubernetes.io/os</code> 的标签，并且其取值为 <code>linux</code>。</li>
<li>节点 <strong>最好</strong> 具有键名为 <code>another-node-label-key</code> 且取值为
<code>another-node-label-value</code> 的标签。</li>
</ul>
<p>你可以使用 <code>operator</code> 字段来为 Kubernetes 设置在解释规则时要使用的逻辑操作符。
你可以使用 <code>In</code>、<code>NotIn</code>、<code>Exists</code>、<code>DoesNotExist</code>、<code>Gt</code> 和 <code>Lt</code> 之一作为操作符。</p>
<p><code>NotIn</code> 和 <code>DoesNotExist</code> 可用来实现节点反亲和性行为。
你也可以使用<a href=/zh/docs/concepts/scheduling-eviction/taint-and-toleration/>节点污点</a>
将 Pod 从特定节点上驱逐。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>如果你同时指定了 <code>nodeSelector</code> 和 <code>nodeAffinity</code>，<strong>两者</strong> 必须都要满足，
才能将 Pod 调度到候选节点上。</p>
<p>如果你指定了多个与 <code>nodeAffinity</code> 类型关联的 <code>nodeSelectorTerms</code>，
只要其中一个 <code>nodeSelectorTerms</code> 满足的话，Pod 就可以被调度到节点上。</p>
<p>如果你指定了多个与同一 <code>nodeSelectorTerms</code> 关联的 <code>matchExpressions</code>，
则只有当所有 <code>matchExpressions</code> 都满足时 Pod 才可以被调度到节点上。</p>
</div>
<p>参阅<a href=/zh/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/>使用节点亲和性来为 Pod 指派节点</a>，
以了解进一步的信息。</p>
<h4 id=node-affinity-weight>节点亲和性权重 </h4>
<p>你可以为 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 亲和性类型的每个实例设置
<code>weight</code> 字段，其取值范围是 1 到 100。
当调度器找到能够满足 Pod 的其他调度请求的节点时，调度器会遍历节点满足的所有的偏好性规则，
并将对应表达式的 <code>weight</code> 值加和。</p>
<p>最终的加和值会添加到该节点的其他优先级函数的评分之上。
在调度器为 Pod 作出调度决定时，总分最高的节点的优先级也最高。</p>
<p>例如，考虑下面的 Pod 规约：</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/zh/examples/pods/pod-with-affinity-anti-affinity.yaml download=pods/pod-with-affinity-anti-affinity.yaml><code>pods/pod-with-affinity-anti-affinity.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-with-affinity-anti-affinity-yaml')" title="Copy pods/pod-with-affinity-anti-affinity.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-pod-with-affinity-anti-affinity-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-affinity-anti-affinity<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>kubernetes.io/os<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- linux<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>label-1<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- key-1<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>label-2<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- key-2<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:2.0</code></pre></div>
</div>
</div>
<p>如果存在两个候选节点，都满足 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 规则，
其中一个节点具有标签 <code>label-1:key-1</code>，另一个节点具有标签 <code>label-2:key-2</code>，
调度器会考察各个节点的 <code>weight</code> 取值，并将该权重值添加到节点的其他得分值之上，</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>如果你希望 Kubernetes 能够成功地调度此例中的 Pod，你必须拥有打了
<code>kubernetes.io/os=linux</code> 标签的节点。
</div>
<h4 id=node-affinity-per-scheduling-profile>逐个调度方案中设置节点亲和性 </h4>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code>
</div>
<p>在配置多个<a href=/zh/docs/reference/scheduling/config/#multiple-profiles>调度方案</a>时，
你可以将某个方案与节点亲和性关联起来，如果某个调度方案仅适用于某组特殊的节点时，
这样做是很有用的。
要实现这点，可以在<a href=/zh/docs/reference/scheduling/config/>调度器配置</a>中为
<a href=/zh/docs/reference/scheduling/config/#scheduling-plugins><code>NodeAffinity</code> 插件</a>的
<code>args</code> 字段添加 <code>addedAffinity</code>。例如：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>foo-scheduler<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeAffinity<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>addedAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>scheduler-profile<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span>- foo<span style=color:#bbb>
</span></code></pre></div>
<p>这里的 <code>addedAffinity</code> 除遵从 Pod 规约中设置的节点亲和性之外，还
适用于将 <code>.spec.schedulerName</code> 设置为 <code>foo-scheduler</code>。
换言之，为了匹配 Pod，节点需要满足 <code>addedAffinity</code> 和 Pod 的 <code>.spec.NodeAffinity</code>。</p>
<p>由于 <code>addedAffinity</code> 对最终用户不可见，其行为可能对用户而言是出乎意料的。
应该使用与调度方案名称有明确关联的节点标签。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>DaemonSet 控制器<a href=/zh/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler>为 DaemonSet 创建 Pods</a>，
但该控制器不理会调度方案。
DaemonSet 控制器创建 Pod 时，默认的 Kubernetes 调度器负责放置 Pod，
并遵从 DaemonSet 控制器中奢侈的 <code>nodeAffinity</code> 规则。
</div>
<h3 id=inter-pod-affinity-and-anti-affinity>pod 间亲和性与反亲和性 </h3>
<p>Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 <strong>Pod</strong> 的标签来约束
Pod 可以调度到的节点，而不是基于节点上的标签。</p>
<p>Pod 间亲和性与反亲和性的规则格式为“如果 X 上已经运行了一个或多个满足规则 Y 的 Pod，
则这个 Pod 应该（或者在反亲和性的情况下不应该）运行在 X 上”。
这里的 X 可以是节点、机架、云提供商可用区或地理区域或类似的拓扑域，
Y 则是 Kubernetes 尝试满足的规则。</p>
<p>你通过<a href=/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors>标签选择算符</a>
的形式来表达规则（Y），并可根据需要指定选关联的名字空间列表。
Pod 在 Kubernetes 中是名字空间作用域的对象，因此 Pod 的标签也隐式地具有名字空间属性。
针对 Pod 标签的所有标签选择算符都要指定名字空间，Kubernetes
会在指定的名字空间内寻找标签。</p>
<p>你会通过 <code>topologyKey</code> 来表达拓扑域（X）的概念，其取值是系统用来标示域的节点标签键。
相关示例可参见<a href=/zh/docs/reference/labels-annotations-taints/>常用标签、注解和污点</a>。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>Pod 间亲和性和反亲和性都需要相当的计算量，因此会在大规模集群中显著降低调度速度。
我们不建议在包含数百个节点的集群中使用这类设置。
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>Pod 反亲和性需要节点上存在一致性的标签。换言之，
集群中每个节点都必须拥有与 <code>topologyKey</code> 匹配的标签。
如果某些或者所有节点上不存在所指定的 <code>topologyKey</code> 标签，调度行为可能与预期的不同。
</div>
<h4 id=pod-间亲和性与反亲和性的类型>Pod 间亲和性与反亲和性的类型</h4>
<p>与<a href=#node-affinity>节点亲和性</a>类似，Pod 的亲和性与反亲和性也有两种类型：</p>
<ul>
<li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li>
<li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li>
</ul>
<p>例如，你可以使用 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 亲和性来告诉调度器，
将两个服务的 Pod 放到同一个云提供商可用区内，因为它们彼此之间通信非常频繁。
类似地，你可以使用 <code>preferredDuringSchedulingIgnoredDuringExecution</code>
反亲和性来将同一服务的多个 Pod 分布到多个云提供商可用区中。</p>
<p>要使用 Pod 间亲和性，可以使用 Pod 规约中的 <code>.affinity.podAffinity</code> 字段。
对于 Pod 间反亲和性，可以使用 Pod 规约中的 <code>.affinity.podAntiAffinity</code> 字段。</p>
<h4 id=an-example-of-a-pod-that-uses-pod-affinity>Pod 亲和性示例 </h4>
<p>考虑下面的 Pod 规约：</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/zh/examples/pods/pod-with-pod-affinity.yaml download=pods/pod-with-pod-affinity.yaml><code>pods/pod-with-pod-affinity.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-with-pod-affinity-yaml')" title="Copy pods/pod-with-pod-affinity.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-pod-with-pod-affinity-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- S1<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinityTerm</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- S2<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:2.0<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>本示例定义了一条 Pod 亲和性规则和一条 Pod 反亲和性规则。Pod 亲和性规则配置为
<code>requiredDuringSchedulingIgnoredDuringExecution</code>，而 Pod 反亲和性配置为
<code>preferredDuringSchedulingIgnoredDuringExecution</code>。</p>
<p>亲和性规则表示，仅当节点和至少一个已运行且有 <code>security=S1</code> 的标签的
Pod 处于同一区域时，才可以将该 Pod 调度到节点上。
更确切的说，调度器必须将 Pod 调度到具有 <code>topology.kubernetes.io/zone=V</code>
标签的节点上，并且集群中至少有一个位于该可用区的节点上运行着带有
<code>security=S1</code> 标签的 Pod。</p>
<p>反亲和性规则表示，如果节点处于 Pod 所在的同一可用区且至少一个 Pod 具有
<code>security=S2</code> 标签，则该 Pod 不应被调度到该节点上。
更确切地说， 如果同一可用区中存在其他运行着带有 <code>security=S2</code> 标签的 Pod 节点，
并且节点具有标签 <code>topology.kubernetes.io/zone=R</code>，Pod 不能被调度到该节点上。</p>
<p>查阅<a href=https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md>设计文档</a>
以了解 Pod 亲和性与反亲和性的更多示例。</p>
<p>你可以针对 Pod 间亲和性与反亲和性为其 <code>operator</code> 字段使用 <code>In</code>、<code>NotIn</code>、<code>Exists</code>、
<code>DoesNotExist</code> 等值。</p>
<p>原则上，<code>topologyKey</code> 可以是任何合法的标签键。出于性能和安全原因，<code>topologyKey</code>
有一些限制：</p>
<ul>
<li>对于 Pod 亲和性而言，在 <code>requiredDuringSchedulingIgnoredDuringExecution</code>
和 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 中，<code>topologyKey</code>
不允许为空。</li>
<li>对于 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 要求的 Pod 反亲和性，
准入控制器 <code>LimitPodHardAntiAffinityTopology</code> 要求 <code>topologyKey</code> 只能是
<code>kubernetes.io/hostname</code>。如果你希望使用其他定制拓扑逻辑，
你可以更改准入控制器或者禁用之。</li>
</ul>
<p>除了 <code>labelSelector</code> 和 <code>topologyKey</code>，你也可以指定 <code>labelSelector</code>
要匹配的命名空间列表，方法是在 <code>labelSelector</code> 和 <code>topologyKey</code>
所在层同一层次上设置 <code>namespaces</code>。
如果 <code>namespaces</code> 被忽略或者为空，则默认为 Pod 亲和性/反亲和性的定义所在的命名空间。</p>
<h4 id=namespace-selector>名字空间选择算符 </h4>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>
<p>用户也可以使用 <code>namespaceSelector</code> 选择匹配的名字空间，<code>namespaceSelector</code>
是对名字空间集合进行标签查询的机制。
亲和性条件会应用到 <code>namespaceSelector</code> 所选择的名字空间和 <code>namespaces</code> 字段中
所列举的名字空间之上。
注意，空的 <code>namespaceSelector</code>（<code>{}</code>）会匹配所有名字空间，而 null 或者空的
<code>namespaces</code> 列表以及 null 值 <code>namespaceSelector</code> 意味着“当前 Pod 的名字空间”。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>此功能特性是 Beta 版本的，默认是被启用的。你可以通过针对 kube-apiserver 和
kube-scheduler 设置<a href=/zh/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>
<code>PodAffinityNamespaceSelector</code> 来禁用此特性。
</div>
<h4 id=更实际的用例>更实际的用例</h4>
<p>Pod 间亲和性与反亲和性在与更高级别的集合（例如 ReplicaSet、StatefulSet、
Deployment 等）一起使用时，它们可能更加有用。
这些规则使得你可以配置一组工作负载，使其位于相同定义拓扑（例如，节点）中。</p>
<p>在下面的 Redis 缓存 Deployment 示例中，副本上设置了标签 <code>app=store</code>。
<code>podAntiAffinity</code> 规则告诉调度器避免将多个带有 <code>app=store</code> 标签的副本部署到同一节点上。
因此，每个独立节点上会创建一个缓存实例。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-cache<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-server<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis:3.2-alpine<span style=color:#bbb>
</span></code></pre></div>
<p>下面的 Deployment 用来提供 Web 服务器服务，会创建带有标签 <code>app=web-store</code> 的副本。
Pod 亲和性规则告诉调度器将副本放到运行有标签包含 <code>app=store</code> Pod 的节点上。
Pod 反亲和性规则告诉调度器不要在同一节点上放置多个 <code>app=web-store</code> 的服务器。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-server<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span>- web-store<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-app<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.16-alpine<span style=color:#bbb>
</span></code></pre></div>
<p>创建前面两个 Deployment 会产生如下的集群布局，每个 Web 服务器与一个缓存实例并置，
并分别运行在三个独立的节点上。</p>
<table>
<thead>
<tr>
<th style=text-align:center>node-1</th>
<th style=text-align:center>node-2</th>
<th style=text-align:center>node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:center><em>webserver-1</em></td>
<td style=text-align:center><em>webserver-2</em></td>
<td style=text-align:center><em>webserver-3</em></td>
</tr>
<tr>
<td style=text-align:center><em>cache-1</em></td>
<td style=text-align:center><em>cache-2</em></td>
<td style=text-align:center><em>cache-3</em></td>
</tr>
</tbody>
</table>
<p>参阅 <a href=/zh/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure>ZooKeeper 教程</a>
了解一个 StatefulSet 的示例，该 StatefulSet 配置了反亲和性以实现高可用，
所使用的是与此例相同的技术。</p>
<h2 id=nodename>nodeName</h2>
<p><code>nodeName</code> 是比亲和性或者 <code>nodeSelector</code> 更为直接的形式。<code>nodeName</code> 是 Pod
规约中的一个字段。如果 <code>nodeName</code> 字段不为空，调度器会忽略该 Pod，
而指定节点上的 kubelet 会尝试将 Pod 放到该节点上。
使用 <code>nodeName</code> 规则的优先级会高于使用 <code>nodeSelector</code> 或亲和性与非亲和性的规则。</p>
<p>使用 <code>nodeName</code> 来选择节点的方式有一些局限性：</p>
<ul>
<li>如果所指代的节点不存在，则 Pod 无法运行，而且在某些情况下可能会被自动删除。</li>
<li>如果所指代的节点无法提供用来运行 Pod 所需的资源，Pod 会失败，
而其失败原因中会给出是否因为内存或 CPU 不足而造成无法运行。</li>
<li>在云环境中的节点名称并不总是可预测的，也不总是稳定的。</li>
</ul>
<p>下面是一个使用 <code>nodeName</code> 字段的 Pod 规约示例：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeName</span>:<span style=color:#bbb> </span>kube-01<span style=color:#bbb>
</span></code></pre></div>
<p>上面的 Pod 只能运行在节点 <code>kube-01</code> 之上。</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>进一步阅读<a href=/zh/docs/concepts/scheduling-eviction/taint-and-toleration/>污点与容忍度</a>文档。</li>
<li>阅读<a href=https://git.k8s.io/community/contributors/design-proposals/scheduling/nodeaffinity.md>节点亲和性</a>
和<a href=https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md>Pod 间亲和性与反亲和性</a>
的设计文档。</li>
<li>了解<a href=/zh/docs/tasks/administer-cluster/topology-manager/>拓扑管理器</a>如何参与节点层面资源分配决定。</li>
<li>了解如何使用 <a href=/zh/docs/tasks/configure-pod-container/assign-pods-nodes/>nodeSelector</a>。</li>
<li>了解如何使用<a href=/zh/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/>亲和性和反亲和性</a>。</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-da22fe2278df236f71efbe672f392677>3 - Pod 开销</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>
<p>在节点上运行 Pod 时，Pod 本身占用大量系统资源。这些是运行 Pod 内容器所需资源之外的资源。
<em>POD 开销</em> 是一个特性，用于计算 Pod 基础设施在容器请求和限制之上消耗的资源。</p>
<p>在 Kubernetes 中，Pod 的开销是根据与 Pod 的 <a href=/zh/docs/concepts/containers/runtime-class/>RuntimeClass</a>
相关联的开销在<a href=/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks>准入</a>时设置的。</p>
<p>如果启用了 Pod Overhead，在调度 Pod 时，除了考虑容器资源请求的总和外，还要考虑 Pod 开销。
类似地，kubelet 将在确定 Pod cgroups 的大小和执行 Pod 驱逐排序时也会考虑 Pod 开销。</p>
<h2 id=set-up>启用 Pod 开销</h2>
<p>你需要确保在集群中启用了 <code>PodOverhead</code> <a href=/zh/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>
（在 1.18 默认是开启的），以及一个定义了 <code>overhead</code> 字段的 <code>RuntimeClass</code>。</p>
<h2 id=使用示例>使用示例</h2>
<p>要使用 PodOverhead 特性，需要一个定义了 <code>overhead</code> 字段的 RuntimeClass。
作为例子，下面的 RuntimeClass 定义中包含一个虚拟化所用的容器运行时，
RuntimeClass 如下，其中每个 Pod 大约使用 120MiB 用来运行虚拟机和寄宿操作系统：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>overhead</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podFixed</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;120Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<p>通过指定 <code>kata-fc</code> RuntimeClass 处理程序创建的工作负载会将内存和 CPU
开销计入资源配额计算、节点调度以及 Pod cgroup 尺寸确定。</p>
<p>假设我们运行下面给出的工作负载示例 test-pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>stdin</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tty</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>1500m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></code></pre></div>
<p>在准入阶段 RuntimeClass <a href=/zh/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>
更新工作负载的 PodSpec 以包含
RuntimeClass 中定义的 <code>overhead</code>。如果 PodSpec 中已定义该字段，该 Pod 将会被拒绝。
在这个例子中，由于只指定了 RuntimeClass 名称，所以准入控制器更新了 Pod，使之包含 <code>overhead</code>。</p>
<p>在 RuntimeClass 准入控制器之后，可以检验一下已更新的 PodSpec:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.overhead}&#39;</span>
</code></pre></div>
<p>输出：</p>
<pre><code>map[cpu:250m memory:120Mi]
</code></pre>
<p>如果定义了 ResourceQuata, 则容器请求的总量以及 <code>overhead</code> 字段都将计算在内。</p>
<p>当 kube-scheduler 决定在哪一个节点调度运行新的 Pod 时，调度器会兼顾该 Pod 的
<code>overhead</code> 以及该 Pod 的容器请求总量。在这个示例中，调度器将资源请求和开销相加，
然后寻找具备 2.25 CPU 和 320 MiB 内存可用的节点。</p>
<p>一旦 Pod 被调度到了某个节点， 该节点上的 kubelet 将为该 Pod 新建一个
<a class=glossary-tooltip title="一组具有可选资源隔离、审计和限制的 Linux 进程。" data-toggle=tooltip data-placement=top href="/zh/docs/reference/glossary/?all=true#term-cgroup" target=_blank aria-label=cgroup>cgroup</a>。 底层容器运行时将在这个
Pod 中创建容器。</p>
<p>如果该资源对每一个容器都定义了一个限制（定义了限制值的 Guaranteed QoS 或者
Burstable QoS），kubelet 会为与该资源（CPU 的 <code>cpu.cfs_quota_us</code> 以及内存的
<code>memory.limit_in_bytes</code>）
相关的 Pod cgroup 设定一个上限。该上限基于 PodSpec 中定义的容器限制总量与 <code>overhead</code> 之和。</p>
<p>对于 CPU，如果 Pod 的 QoS 是 Guaranteed 或者 Burstable，kubelet 会基于容器请求总量与
PodSpec 中定义的 <code>overhead</code> 之和设置 <code>cpu.shares</code>。</p>
<p>请看这个例子，验证工作负载的容器请求：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.containers[*].resources.limits}&#39;</span>
</code></pre></div>
<p>容器请求总计 2000m CPU 和 200MiB 内存：</p>
<pre><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre>
<p>对照从节点观察到的情况来检查一下：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl describe node | grep test-pod -B2
</code></pre></div>
<p>该输出显示请求了 2250m CPU 以及 320MiB 内存，包含了 PodOverhead 在内：</p>
<pre><code>  Namespace                   Name                CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------                   ----                ------------  ----------   ---------------  -------------  ---
  default                     test-pod            2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre>
<h2 id=验证-pod-cgroup-限制>验证 Pod cgroup 限制</h2>
<p>在工作负载所运行的节点上检查 Pod 的内存 cgroups。在接下来的例子中，
将在该节点上使用具备 CRI 兼容的容器运行时命令行工具
<a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md><code>crictl</code></a>。
这是一个显示 PodOverhead 行为的高级示例， 预计用户不需要直接在节点上检查 cgroups。
首先在特定的节点上确定该 Pod 的标识符：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># 在该 Pod 被调度到的节点上执行如下命令：</span>
<span style=color:#b8860b>POD_ID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>sudo crictl pods --name test-pod -q<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</code></pre></div>
<p>可以依此判断该 Pod 的 cgroup 路径：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># 在该 Pod 被调度到的节点上执行如下命令：</span>
sudo crictl inspectp -o<span style=color:#666>=</span>json <span style=color:#b8860b>$POD_ID</span> | grep cgroupsPath
</code></pre></div>
<p>执行结果的 cgroup 路径中包含了该 Pod 的 <code>pause</code> 容器。Pod 级别的 cgroup 在即上一层目录。</p>
<pre><code>        &quot;cgroupsPath&quot;: &quot;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a&quot;
</code></pre>
<p>在这个例子中，该 Pod 的 cgroup 路径是 <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>。
验证内存的 Pod 级别 cgroup 设置：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># 在该 Pod 被调度到的节点上执行这个命令。</span>
<span style=color:#080;font-style:italic># 另外，修改 cgroup 的名称以匹配为该 Pod 分配的 cgroup。</span>
 cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</code></pre></div>
<p>和预期的一样，这一数值为 320 MiB。</p>
<pre><code>335544320
</code></pre>
<h3 id=可观察性>可观察性</h3>
<p>在 <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a> 中可以通过
<code>kube_pod_overhead</code> 指标来协助确定何时使用 PodOverhead
以及协助观察以一个既定开销运行的工作负载的稳定性。
该特性在 kube-state-metrics 的 1.9 发行版本中不可用，不过预计将在后续版本中发布。
在此之前，用户需要从源代码构建 kube-state-metrics。</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=/zh/docs/concepts/containers/runtime-class/>RuntimeClass</a></li>
<li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>PodOverhead 设计</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ede4960b56a3529ee0bfe7c8fe2d09a5>4 - 污点和容忍度</h1>
<p><a href=/zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity><em>节点亲和性</em></a>
是 <a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> 的一种属性，它使 Pod
被吸引到一类特定的<a class=glossary-tooltip title="Kubernetes 中的工作机器称作节点。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>
（这可能出于一种偏好，也可能是硬性要求）。
<em>污点</em>（Taint）则相反——它使节点能够排斥一类特定的 Pod。</p>
<p>容忍度（Toleration）是应用于 Pod 上的，允许（但并不要求）Pod
调度到带有与之匹配的污点的节点上。</p>
<p>污点和容忍度（Toleration）相互配合，可以用来避免 Pod 被分配到不合适的节点上。
每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的 Pod，是不会被该节点接受的。</p>
<h2 id=概念>概念</h2>
<p>您可以使用命令 <a href=/docs/reference/generated/kubectl/kubectl-commands#taint>kubectl taint</a> 给节点增加一个污点。比如，</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</code></pre></div>
<p>给节点 <code>node1</code> 增加一个污点，它的键名是 <code>key1</code>，键值是 <code>value1</code>，效果是 <code>NoSchedule</code>。
这表示只有拥有和这个污点相匹配的容忍度的 Pod 才能够被分配到 <code>node1</code> 这个节点。</p>
<p>若要移除上述命令所添加的污点，你可以执行：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule-
</code></pre></div>
<p>您可以在 PodSpec 中定义 Pod 的容忍度。
下面两个容忍度均与上面例子中使用 <code>kubectl taint</code> 命令创建的污点相匹配，
因此如果一个 Pod 拥有其中的任何一个容忍度都能够被分配到 <code>node1</code> ：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<p>这里是一个使用了容忍度的 Pod：</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/zh/examples/pods/pod-with-toleration.yaml download=pods/pod-with-toleration.yaml><code>pods/pod-with-toleration.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-with-toleration-yaml')" title="Copy pods/pod-with-toleration.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-pod-with-toleration-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-key&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p><code>operator</code> 的默认值是 <code>Equal</code>。</p>
<p>一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果，并且：</p>
<ul>
<li>如果 <code>operator</code> 是 <code>Exists</code> （此时容忍度不能指定 <code>value</code>），或者</li>
<li>如果 <code>operator</code> 是 <code>Equal</code> ，则它们的 <code>value</code> 应该相等</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>存在两种特殊情况：</p>
<p>如果一个容忍度的 <code>key</code> 为空且 operator 为 <code>Exists</code>，
表示这个容忍度与任意的 key 、value 和 effect 都匹配，即这个容忍度能容忍任意 taint。</p>
<p>如果 <code>effect</code> 为空，则可以与所有键名 <code>key1</code> 的效果相匹配。</p>
</div>
<p>上述例子中 <code>effect</code> 使用的值为 <code>NoSchedule</code>，您也可以使用另外一个值 <code>PreferNoSchedule</code>。
这是“优化”或“软”版本的 <code>NoSchedule</code> —— 系统会 <em>尽量</em> 避免将 Pod 调度到存在其不能容忍污点的节点上，
但这不是强制的。<code>effect</code> 的值还可以设置为 <code>NoExecute</code>，下文会详细描述这个值。</p>
<p>您可以给一个节点添加多个污点，也可以给一个 Pod 添加多个容忍度设置。
Kubernetes 处理多个污点和容忍度的过程就像一个过滤器：从一个节点的所有污点开始遍历，
过滤掉那些 Pod 中存在与之相匹配的容忍度的污点。余下未被过滤的污点的 effect 值决定了
Pod 是否会被分配到该节点，特别是以下情况：</p>
<ul>
<li>如果未被过滤的污点中存在至少一个 effect 值为 <code>NoSchedule</code> 的污点，
则 Kubernetes 不会将 Pod 分配到该节点。</li>
<li>如果未被过滤的污点中不存在 effect 值为 <code>NoSchedule</code> 的污点，
但是存在 effect 值为 <code>PreferNoSchedule</code> 的污点，
则 Kubernetes 会 <em>尝试</em> 不将 Pod 分配到该节点。</li>
<li>如果未被过滤的污点中存在至少一个 effect 值为 <code>NoExecute</code> 的污点，
则 Kubernetes 不会将 Pod 分配到该节点（如果 Pod 还未在节点上运行），
或者将 Pod 从该节点驱逐（如果 Pod 已经在节点上运行）。</li>
</ul>
<p>例如，假设您给一个节点添加了如下污点</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoExecute
kubectl taint nodes node1 <span style=color:#b8860b>key2</span><span style=color:#666>=</span>value2:NoSchedule
</code></pre></div>
<p>假定有一个 Pod，它有两个容忍度：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<p>在这种情况下，上述 Pod 不会被分配到上述节点，因为其没有容忍度和第三个污点相匹配。
但是如果在给节点添加上述污点之前，该 Pod 已经在上述节点运行，
那么它还可以继续运行在该节点上，因为第三个污点是三个污点中唯一不能被这个 Pod 容忍的。</p>
<p>通常情况下，如果给一个节点添加了一个 effect 值为 <code>NoExecute</code> 的污点，
则任何不能忍受这个污点的 Pod 都会马上被驱逐，
任何可以忍受这个污点的 Pod 都不会被驱逐。
但是，如果 Pod 存在一个 effect 值为 <code>NoExecute</code> 的容忍度指定了可选属性
<code>tolerationSeconds</code> 的值，则表示在给节点添加了上述污点之后，
Pod 还能继续在节点上运行的时间。例如，</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></code></pre></div>
<p>这表示如果这个 Pod 正在运行，同时一个匹配的污点被添加到其所在的节点，
那么 Pod 还将继续在节点上运行 3600 秒，然后被驱逐。
如果在此之前上述污点被删除了，则 Pod 不会被驱逐。</p>
<h2 id=使用例子>使用例子</h2>
<p>通过污点和容忍度，可以灵活地让 Pod <em>避开</em> 某些节点或者将 Pod 从某些节点驱逐。下面是几个使用例子：</p>
<ul>
<li><strong>专用节点</strong>：如果您想将某些节点专门分配给特定的一组用户使用，您可以给这些节点添加一个污点（即，
<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>），
然后给这组用户的 Pod 添加一个相对应的 toleration（通过编写一个自定义的
<a href=/zh/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>，很容易就能做到）。
拥有上述容忍度的 Pod 就能够被分配到上述专用节点，同时也能够被分配到集群中的其它节点。
如果您希望这些 Pod 只能被分配到上述专用节点，那么您还需要给这些专用节点另外添加一个和上述
污点类似的 label （例如：<code>dedicated=groupName</code>），同时 还要在上述准入控制器中给 Pod
增加节点亲和性要求上述 Pod 只能被分配到添加了 <code>dedicated=groupName</code> 标签的节点上。</li>
</ul>
<ul>
<li><strong>配备了特殊硬件的节点</strong>：在部分节点配备了特殊硬件（比如 GPU）的集群中，
我们希望不需要这类硬件的 Pod 不要被分配到这些特殊节点，以便为后继需要这类硬件的 Pod 保留资源。
要达到这个目的，可以先给配备了特殊硬件的节点添加 taint
（例如 <code>kubectl taint nodes nodename special=true:NoSchedule</code> 或
<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>)，
然后给使用了这类特殊硬件的 Pod 添加一个相匹配的 toleration。
和专用节点的例子类似，添加这个容忍度的最简单的方法是使用自定义
<a href=/zh/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>。
比如，我们推荐使用<a href=/zh/docs/concepts/configuration/manage-resources-containers/#extended-resources>扩展资源</a>
来表示特殊硬件，给配置了特殊硬件的节点添加污点时包含扩展资源名称，
然后运行一个 <a href=/zh/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration>ExtendedResourceToleration</a>
准入控制器。此时，因为节点已经被设置污点了，没有对应容忍度的 Pod
不会被调度到这些节点。但当你创建一个使用了扩展资源的 Pod 时，
<code>ExtendedResourceToleration</code> 准入控制器会自动给 Pod 加上正确的容忍度，
这样 Pod 就会被自动调度到这些配置了特殊硬件件的节点上。
这样就能够确保这些配置了特殊硬件的节点专门用于运行需要使用这些硬件的 Pod，
并且您无需手动给这些 Pod 添加容忍度。</li>
</ul>
<ul>
<li><strong>基于污点的驱逐</strong>: 这是在每个 Pod 中配置的在节点出现问题时的驱逐行为，接下来的章节会描述这个特性。</li>
</ul>
<h2 id=taint-based-evictions>基于污点的驱逐 </h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>
<p>前文提到过污点的 effect 值 <code>NoExecute</code>会影响已经在节点上运行的 Pod</p>
<ul>
<li>如果 Pod 不能忍受 effect 值为 <code>NoExecute</code> 的污点，那么 Pod 将马上被驱逐</li>
<li>如果 Pod 能够忍受 effect 值为 <code>NoExecute</code> 的污点，但是在容忍度定义中没有指定
<code>tolerationSeconds</code>，则 Pod 还会一直在这个节点上运行。</li>
<li>如果 Pod 能够忍受 effect 值为 <code>NoExecute</code> 的污点，而且指定了 <code>tolerationSeconds</code>，
则 Pod 还能在这个节点上继续运行这个指定的时间长度。</li>
</ul>
<p>当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：</p>
<ul>
<li><code>node.kubernetes.io/not-ready</code>：节点未准备好。这相当于节点状态 <code>Ready</code> 的值为 "<code>False</code>"。</li>
<li><code>node.kubernetes.io/unreachable</code>：节点控制器访问不到节点. 这相当于节点状态 <code>Ready</code> 的值为 "<code>Unknown</code>"。</li>
<li><code>node.kubernetes.io/memory-pressure</code>：节点存在内存压力。</li>
<li><code>node.kubernetes.io/disk-pressure</code>：节点存在磁盘压力。</li>
<li><code>node.kubernetes.io/pid-pressure</code>: 节点的 PID 压力。</li>
<li><code>node.kubernetes.io/network-unavailable</code>：节点网络不可用。</li>
<li><code>node.kubernetes.io/unschedulable</code>: 节点不可调度。</li>
<li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 启动时指定了一个 "外部" 云平台驱动，
它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager
的一个控制器初始化这个节点后，kubelet 将删除这个污点。</li>
</ul>
<p>在节点被驱逐时，节点控制器或者 kubelet 会添加带有 <code>NoExecute</code> 效应的相关污点。
如果异常状态恢复正常，kubelet 或节点控制器能够移除相关的污点。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 为了保证由于节点问题引起的 Pod 驱逐
<a href=/zh/docs/concepts/architecture/nodes/>速率限制</a>行为正常，
系统实际上会以限定速率的方式添加污点。在像主控节点与工作节点间通信中断等场景下，
这样做可以避免 Pod 被大量驱逐。
</div>
<p>使用这个功能特性，结合 <code>tolerationSeconds</code>，Pod 就可以指定当节点出现一个
或全部上述问题时还将在这个节点上运行多长的时间。</p>
<p>比如，一个使用了很多本地状态的应用程序在网络断开时，仍然希望停留在当前节点上运行一段较长的时间，
愿意等待网络恢复以避免被驱逐。在这种情况下，Pod 的容忍度可能是下面这样的：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;node.kubernetes.io/unreachable&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>6000</span><span style=color:#bbb>
</span></code></pre></div>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>Kubernetes 会自动给 Pod 添加一个 key 为 <code>node.kubernetes.io/not-ready</code> 的容忍度
并配置 <code>tolerationSeconds=300</code>，除非用户提供的 Pod 配置中已经已存在了 key 为
<code>node.kubernetes.io/not-ready</code> 的容忍度。</p>
<p>同样，Kubernetes 会给 Pod 添加一个 key 为 <code>node.kubernetes.io/unreachable</code> 的容忍度
并配置 <code>tolerationSeconds=300</code>，除非用户提供的 Pod 配置中已经已存在了 key 为
<code>node.kubernetes.io/unreachable</code> 的容忍度。</p>
</div>
<p>这种自动添加的容忍度意味着在其中一种问题被检测到时 Pod
默认能够继续停留在当前节点运行 5 分钟。</p>
<p><a href=/zh/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> 中的 Pod 被创建时，
针对以下污点自动添加的 <code>NoExecute</code> 的容忍度将不会指定 <code>tolerationSeconds</code>：</p>
<ul>
<li><code>node.kubernetes.io/unreachable</code></li>
<li><code>node.kubernetes.io/not-ready</code></li>
</ul>
<p>这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐。</p>
<h2 id=基于节点状态添加污点>基于节点状态添加污点</h2>
<p>控制平面使用节点<a class=glossary-tooltip title="控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/architecture/controller/ target=_blank aria-label=控制器>控制器</a>自动创建
与<a href=/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions>节点状况</a>对应的带有 <code>NoSchedule</code> 效应的污点。</p>
<p>调度器在进行调度时检查污点，而不是检查节点状况。这确保节点状况不会直接影响调度。
例如，如果 <code>DiskPressure</code> 节点状况处于活跃状态，则控制平面
添加 <code>node.kubernetes.io/disk-pressure</code> 污点并且不会调度新的 pod
到受影响的节点。如果 <code>MemoryPressure</code> 节点状况处于活跃状态，则
控制平面添加 <code>node.kubernetes.io/memory-pressure</code> 污点。</p>
<p>对于新创建的 Pod，可以通过添加相应的 Pod 容忍度来忽略节点状况。
控制平面还在具有除 <code>BestEffort</code> 之外的 <a class=glossary-tooltip title="QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。" data-toggle=tooltip data-placement=top href="/zh/docs/reference/glossary/?all=true#term-qos-class" target=_blank aria-label="QoS 类">QoS 类</a>的 pod 上
添加 <code>node.kubernetes.io/memory-pressure</code> 容忍度。
这是因为 Kubernetes 将 <code>Guaranteed</code> 或 <code>Burstable</code> QoS 类中的 Pod（甚至没有设置内存请求的 Pod）
视为能够应对内存压力，而新创建的 <code>BestEffort</code> Pod 不会被调度到受影响的节点上。</p>
<p>DaemonSet 控制器自动为所有守护进程添加如下 <code>NoSchedule</code> 容忍度以防 DaemonSet 崩溃：</p>
<ul>
<li><code>node.kubernetes.io/memory-pressure</code></li>
<li><code>node.kubernetes.io/disk-pressure</code></li>
<li><code>node.kubernetes.io/pid-pressure</code> (1.14 或更高版本)</li>
<li><code>node.kubernetes.io/unschedulable</code> (1.10 或更高版本)</li>
<li><code>node.kubernetes.io/network-unavailable</code> (<em>只适合主机网络配置</em>)</li>
</ul>
<p>添加上述容忍度确保了向后兼容，您也可以选择自由向 DaemonSet 添加容忍度。</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>阅读<a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>节点压力驱逐</a>，以及如何配置其行为</li>
<li>阅读 <a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-60e5a2861609e0848d58ce8bf99c4a31>5 - Pod 优先级和抢占</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>
<p><a href=/zh/docs/concepts/workloads/pods/>Pod</a> 可以有 <em>优先级</em>。
优先级表示一个 Pod 相对于其他 Pod 的重要性。
如果一个 Pod 无法被调度，调度程序会尝试抢占（驱逐）较低优先级的 Pod，
以使悬决 Pod 可以被调度。</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Warning:</strong>
<p>在一个并非所有用户都是可信的集群中，恶意用户可能以最高优先级创建 Pod，
导致其他 Pod 被驱逐或者无法被调度。
管理员可以使用 ResourceQuota 来阻止用户创建高优先级的 Pod。
参见<a href=/zh/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>默认限制优先级消费</a>。
</div>
<h2 id=如何使用优先级和抢占>如何使用优先级和抢占</h2>
<p>要使用优先级和抢占：</p>
<ol>
<li>
<p>新增一个或多个 <a href=#priorityclass>PriorityClass</a>。</p>
</li>
<li>
<p>创建 Pod，并将其 <a href=#pod-priority><code>priorityClassName</code></a> 设置为新增的 PriorityClass。
当然你不需要直接创建 Pod；通常，你将会添加 <code>priorityClassName</code> 到集合对象（如 Deployment）
的 Pod 模板中。</p>
</li>
</ol>
<p>继续阅读以获取有关这些步骤的更多信息。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>Kubernetes 已经提供了 2 个 PriorityClass：
<code>system-cluster-critical</code> 和 <code>system-node-critical</code>。
这些是常见的类，用于<a href=/zh/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/>确保始终优先调度关键组件</a>。
</div>
<h2 id=priorityclass>PriorityClass</h2>
<p>PriorityClass 是一个无名称空间对象，它定义了从优先级类名称到优先级整数值的映射。
名称在 PriorityClass 对象元数据的 <code>name</code> 字段中指定。
值在必填的 <code>value</code> 字段中指定。值越大，优先级越高。
PriorityClass 对象的名称必须是有效的
<a href=/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS 子域名</a>，
并且它不能以 <code>system-</code> 为前缀。</p>
<p>PriorityClass 对象可以设置任何小于或等于 10 亿的 32 位整数值。
较大的数字是为通常不应被抢占或驱逐的关键的系统 Pod 所保留的。
集群管理员应该为这类映射分别创建独立的 PriorityClass 对象。</p>
<p>PriorityClass 还有两个可选字段：<code>globalDefault</code> 和 <code>description</code>。
<code>globalDefault</code> 字段表示这个 PriorityClass 的值应该用于没有 <code>priorityClassName</code> 的 Pod。
系统中只能存在一个 <code>globalDefault</code> 设置为 true 的 PriorityClass。
如果不存在设置了 <code>globalDefault</code> 的 PriorityClass，
则没有 <code>priorityClassName</code> 的 Pod 的优先级为零。</p>
<p><code>description</code> 字段是一个任意字符串。
它用来告诉集群用户何时应该使用此 PriorityClass。</p>
<h3 id=关于-podpriority-和现有集群的注意事项>关于 PodPriority 和现有集群的注意事项</h3>
<ul>
<li>
<p>如果你升级一个已经存在的但尚未使用此特性的集群，该集群中已经存在的 Pod 的优先级等效于零。</p>
</li>
<li>
<p>添加一个将 <code>globalDefault</code> 设置为 <code>true</code> 的 PriorityClass 不会改变现有 Pod 的优先级。
此类 PriorityClass 的值仅用于添加 PriorityClass 后创建的 Pod。</p>
</li>
<li>
<p>如果你删除了某个 PriorityClass 对象，则使用被删除的 PriorityClass 名称的现有 Pod 保持不变，
但是你不能再创建使用已删除的 PriorityClass 名称的 Pod。</p>
</li>
</ul>
<h3 id=priorityclass-示例>PriorityClass 示例</h3>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>scheduling.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#666>1000000</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>globalDefault</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>description</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;此优先级类应仅用于 XYZ 服务 Pod。&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<h2 id=non-preempting-priority-class>非抢占式 PriorityClass</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code>
</div>
<p>配置了 <code>preemptionPolicy: Never</code> 的 Pod 将被放置在调度队列中较低优先级 Pod 之前，
但它们不能抢占其他 Pod。等待调度的非抢占式 Pod 将留在调度队列中，直到有足够的可用资源，
它才可以被调度。非抢占式 Pod，像其他 Pod 一样，受调度程序回退的影响。
这意味着如果调度程序尝试这些 Pod 并且无法调度它们，它们将以更低的频率被重试，
从而允许其他优先级较低的 Pod 排在它们之前。</p>
<p>非抢占式 Pod 仍可能被其他高优先级 Pod 抢占。</p>
<p><code>preemptionPolicy</code> 默认为 <code>PreemptLowerPriority</code>，
这将允许该 PriorityClass 的 Pod 抢占较低优先级的 Pod（现有默认行为也是如此）。
如果 <code>preemptionPolicy</code> 设置为 <code>Never</code>，则该 PriorityClass 中的 Pod 将是非抢占式的。</p>
<p>数据科学工作负载是一个示例用例。用户可以提交他们希望优先于其他工作负载的作业，
但不希望因为抢占运行中的 Pod 而导致现有工作被丢弃。
设置为 <code>preemptionPolicy: Never</code> 的高优先级作业将在其他排队的 Pod 之前被调度，
只要足够的集群资源“自然地”变得可用。</p>
<h3 id=非抢占式-priorityclass-示例>非抢占式 PriorityClass 示例</h3>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>scheduling.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority-nonpreempting<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#666>1000000</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>preemptionPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>globalDefault</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>description</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;This priority class will not cause other pods to be preempted.&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<h2 id=pod-priority>Pod 优先级</h2>
<p>在你拥有一个或多个 PriorityClass 对象之后，
你可以创建在其规约中指定这些 PriorityClass 名称之一的 Pod。
优先级准入控制器使用 <code>priorityClassName</code> 字段并填充优先级的整数值。
如果未找到所指定的优先级类，则拒绝 Pod。</p>
<p>以下 YAML 是 Pod 配置的示例，它使用在前面的示例中创建的 PriorityClass。
优先级准入控制器检查 Pod 规约并将其优先级解析为 1000000。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityClassName</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></code></pre></div>
<h3 id=pod-优先级对调度顺序的影响>Pod 优先级对调度顺序的影响</h3>
<p>当启用 Pod 优先级时，调度程序会按优先级对悬决 Pod 进行排序，
并且每个悬决的 Pod 会被放置在调度队列中其他优先级较低的悬决 Pod 之前。
因此，如果满足调度要求，较高优先级的 Pod 可能会比具有较低优先级的 Pod 更早调度。
如果无法调度此类 Pod，调度程序将继续并尝试调度其他较低优先级的 Pod。</p>
<h2 id=preemption>抢占 </h2>
<p>Pod 被创建后会进入队列等待调度。
调度器从队列中挑选一个 Pod 并尝试将它调度到某个节点上。
如果没有找到满足 Pod 的所指定的所有要求的节点，则触发对悬决 Pod 的抢占逻辑。
让我们将悬决 Pod 称为 P。抢占逻辑试图找到一个节点，
在该节点中删除一个或多个优先级低于 P 的 Pod，则可以将 P 调度到该节点上。
如果找到这样的节点，一个或多个优先级较低的 Pod 会被从节点中驱逐。
被驱逐的 Pod 消失后，P 可以被调度到该节点上。</p>
<h3 id=用户暴露的信息>用户暴露的信息</h3>
<p>当 Pod P 抢占节点 N 上的一个或多个 Pod 时，
Pod P 状态的 <code>nominatedNodeName</code> 字段被设置为节点 N 的名称。
该字段帮助调度程序跟踪为 Pod P 保留的资源，并为用户提供有关其集群中抢占的信息。</p>
<p>请注意，Pod P 不一定会调度到“被提名的节点（Nominated Node）”。
在 Pod 因抢占而牺牲时，它们将获得体面终止期。
如果调度程序正在等待牺牲者 Pod 终止时另一个节点变得可用，
则调度程序将使用另一个节点来调度 Pod P。
因此，Pod 规约中的 <code>nominatedNodeName</code> 和 <code>nodeName</code> 并不总是相同。
此外，如果调度程序抢占节点 N 上的 Pod，但随后比 Pod P 更高优先级的 Pod 到达，
则调度程序可能会将节点 N 分配给新的更高优先级的 Pod。
在这种情况下，调度程序会清除 Pod P 的 <code>nominatedNodeName</code>。
通过这样做，调度程序使 Pod P 有资格抢占另一个节点上的 Pod。</p>
<h3 id=抢占的限制>抢占的限制</h3>
<h4 id=被抢占牺牲者的体面终止>被抢占牺牲者的体面终止</h4>
<p>当 Pod 被抢占时，牺牲者会得到他们的
<a href=/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>体面终止期</a>。
它们可以在体面终止期内完成工作并退出。如果它们不这样做就会被杀死。
这个体面终止期在调度程序抢占 Pod 的时间点和待处理的 Pod (P)
可以在节点 (N) 上调度的时间点之间划分出了一个时间跨度。
同时，调度器会继续调度其他待处理的 Pod。当牺牲者退出或被终止时，
调度程序会尝试在待处理队列中调度 Pod。
因此，调度器抢占牺牲者的时间点与 Pod P 被调度的时间点之间通常存在时间间隔。
为了最小化这个差距，可以将低优先级 Pod 的体面终止时间设置为零或一个小数字。</p>
<h4 id=支持-poddisruptionbudget-但不保证>支持 PodDisruptionBudget，但不保证</h4>
<p><a href=/zh/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a>
(PDB) 允许多副本应用程序的所有者限制因自愿性质的干扰而同时终止的 Pod 数量。
Kubernetes 在抢占 Pod 时支持 PDB，但对 PDB 的支持是基于尽力而为原则的。
调度器会尝试寻找不会因被抢占而违反 PDB 的牺牲者，但如果没有找到这样的牺牲者，
抢占仍然会发生，并且即使违反了 PDB 约束也会删除优先级较低的 Pod。</p>
<h4 id=与低优先级-pod-之间的-pod-间亲和性>与低优先级 Pod 之间的 Pod 间亲和性</h4>
<p>只有当这个问题的答案是肯定的时，才考虑在一个节点上执行抢占操作：
“如果从此节点上删除优先级低于悬决 Pod 的所有 Pod，悬决 Pod 是否可以在该节点上调度？”</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 抢占并不一定会删除所有较低优先级的 Pod。
如果悬决 Pod 可以通过删除少于所有较低优先级的 Pod 来调度，
那么只有一部分较低优先级的 Pod 会被删除。
即便如此，上述问题的答案必须是肯定的。
如果答案是否定的，则不考虑在该节点上执行抢占。
</div>
<p>如果悬决 Pod 与节点上的一个或多个较低优先级 Pod 具有 Pod 间<a class=glossary-tooltip title="调度程序用于确定在何处放置 Pods（亲和性）的规则" data-toggle=tooltip data-placement=top href=zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity target=_blank aria-label=亲和性>亲和性</a>，
则在没有这些较低优先级 Pod 的情况下，无法满足 Pod 间亲和性规则。
在这种情况下，调度程序不会抢占节点上的任何 Pod。
相反，它寻找另一个节点。调度程序可能会找到合适的节点，
也可能不会。无法保证悬决 Pod 可以被调度。</p>
<p>我们针对此问题推荐的解决方案是仅针对同等或更高优先级的 Pod 设置 Pod 间亲和性。</p>
<h4 id=跨节点抢占>跨节点抢占</h4>
<p>假设正在考虑在一个节点 N 上执行抢占，以便可以在 N 上调度待处理的 Pod P。
只有当另一个节点上的 Pod 被抢占时，P 才可能在 N 上变得可行。
下面是一个例子：</p>
<ul>
<li>正在考虑将 Pod P 调度到节点 N 上。</li>
<li>Pod Q 正在与节点 N 位于同一区域的另一个节点上运行。</li>
<li>Pod P 与 Pod Q 具有 Zone 维度的反亲和（<code>topologyKey:topology.kubernetes.io/zone</code>）。</li>
<li>Pod P 与 Zone 中的其他 Pod 之间没有其他反亲和性设置。</li>
<li>为了在节点 N 上调度 Pod P，可以抢占 Pod Q，但调度器不会进行跨节点抢占。
因此，Pod P 将被视为在节点 N 上不可调度。</li>
</ul>
<p>如果将 Pod Q 从所在节点中移除，则不会违反 Pod 间反亲和性约束，
并且 Pod P 可能会被调度到节点 N 上。</p>
<p>如果有足够的需求，并且如果我们找到性能合理的算法，
我们可能会考虑在未来版本中添加跨节点抢占。</p>
<h2 id=故障排除>故障排除</h2>
<p>Pod 优先级和抢占可能会产生不必要的副作用。以下是一些潜在问题的示例以及处理这些问题的方法。</p>
<h3 id=pod-被不必要地抢占>Pod 被不必要地抢占</h3>
<p>抢占在资源压​​力较大时从集群中删除现有 Pod，为更高优先级的悬决 Pod 腾出空间。
如果你错误地为某些 Pod 设置了高优先级，这些无意的高优先级 Pod 可能会导致集群中出现抢占行为。
Pod 优先级是通过设置 Pod 规约中的 <code>priorityClassName</code> 字段来指定的。
优先级的整数值然后被解析并填充到 <code>podSpec</code> 的 <code>priority</code> 字段。</p>
<p>为了解决这个问题，你可以将这些 Pod 的 <code>priorityClassName</code> 更改为使用较低优先级的类，
或者将该字段留空。默认情况下，空的 <code>priorityClassName</code> 解析为零。</p>
<p>当 Pod 被抢占时，集群会为被抢占的 Pod 记录事件。只有当集群没有足够的资源用于 Pod 时，
才会发生抢占。在这种情况下，只有当悬决 Pod（抢占者）的优先级高于受害 Pod 时才会发生抢占。
当没有悬决 Pod，或者悬决 Pod 的优先级等于或低于牺牲者时，不得发生抢占。
如果在这种情况下发生抢占，请提出问题。</p>
<h3 id=有-pod-被抢占-但抢占者并没有被调度>有 Pod 被抢占，但抢占者并没有被调度</h3>
<p>当 Pod 被抢占时，它们会收到请求的体面终止期，默认为 30 秒。
如果受害 Pod 在此期限内没有终止，它们将被强制终止。
一旦所有牺牲者都离开，就可以调度抢占者 Pod。</p>
<p>在抢占者 Pod 等待牺牲者离开的同时，可能某个适合同一个节点的更高优先级的 Pod 被创建。
在这种情况下，调度器将调度优先级更高的 Pod 而不是抢占者。</p>
<p>这是预期的行为：具有较高优先级的 Pod 应该取代具有较低优先级的 Pod。</p>
<h3 id=优先级较高的-pod-在优先级较低的-pod-之前被抢占>优先级较高的 Pod 在优先级较低的 Pod 之前被抢占</h3>
<p>调度程序尝试查找可以运行悬决 Pod 的节点。如果没有找到这样的节点，
调度程序会尝试从任意节点中删除优先级较低的 Pod，以便为悬决 Pod 腾出空间。
如果具有低优先级 Pod 的节点无法运行悬决 Pod，
调度器可能会选择另一个具有更高优先级 Pod 的节点（与其他节点上的 Pod 相比）进行抢占。
牺牲者的优先级必须仍然低于抢占者 Pod。</p>
<p>当有多个节点可供执行抢占操作时，调度器会尝试选择具有一组优先级最低的 Pod 的节点。
但是，如果此类 Pod 具有 PodDisruptionBudget，当它们被抢占时，
则会违反 PodDisruptionBudget，那么调度程序可能会选择另一个具有更高优先级 Pod 的节点。</p>
<p>当存在多个节点抢占且上述场景均不适用时，调度器会选择优先级最低的节点。</p>
<h2 id=interactions-of-pod-priority-and-qos>Pod 优先级和服务质量之间的相互作用</h2>
<p>Pod 优先级和 <a class=glossary-tooltip title="QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。" data-toggle=tooltip data-placement=top href="/zh/docs/reference/glossary/?all=true#term-qos-class" target=_blank aria-label="QoS 类">QoS 类</a>
是两个正交特征，交互很少，并且对基于 QoS 类设置 Pod 的优先级没有默认限制。
调度器的抢占逻辑在选择抢占目标时不考虑 QoS。
抢占会考虑 Pod 优先级并尝试选择一组优先级最低的目标。
仅当移除优先级最低的 Pod 不足以让调度程序调度抢占式 Pod，
或者最低优先级的 Pod 受 PodDisruptionBudget 保护时，才会考虑优先级较高的 Pod。</p>
<p>kubelet 使用优先级来确定
<a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>节点压力驱逐</a> Pod 的顺序。
你可以使用 QoS 类来估计 Pod 最有可能被驱逐的顺序。kubelet 根据以下因素对 Pod 进行驱逐排名：</p>
<ol>
<li>对紧俏资源的使用是否超过请求值</li>
<li>Pod 优先级</li>
<li>相对于请求的资源使用量</li>
</ol>
<p>有关更多详细信息，请参阅
<a href=/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction>kubelet 驱逐时 Pod 的选择</a>。</p>
<p>当某 Pod 的资源用量未超过其请求时，kubelet 节点压力驱逐不会驱逐该 Pod。
如果优先级较低的 Pod 没有超过其请求，则不会被驱逐。
另一个优先级高于其请求的 Pod 可能会被驱逐。</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>阅读有关将 ResourceQuota 与 PriorityClass 结合使用的信息：
<a href=/zh/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>默认限制优先级消费</a></li>
<li>了解 <a href=/zh/docs/concepts/workloads/pods/disruptions/>Pod 干扰</a></li>
<li>了解 <a href=/docs/reference/generated/kubernetes-api/v1.23/>API 发起的驱逐</a></li>
<li>了解<a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>节点压力驱逐</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-78e0431b4b7516092662a7c289cbb304>6 - 节点压力驱逐</h1>
<p>节点压力驱逐是 <a class=glossary-tooltip title="一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。" data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> 主动终止 Pod 以回收节点上资源的过程。</br></p>
<p><a class=glossary-tooltip title="一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。" data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
监控集群节点的 CPU、内存、磁盘空间和文件系统的 inode 等资源。
当这些资源中的一个或者多个达到特定的消耗水平，
kubelet 可以主动地使节点上一个或者多个 Pod 失效，以回收资源防止饥饿。</p>
<p>在节点压力驱逐期间，kubelet 将所选 Pod 的 <code>PodPhase</code> 设置为 <code>Failed</code>。这将终止 Pod。</p>
<p>节点压力驱逐不同于 <a href=/docs/reference/generated/kubernetes-api/v1.23/>API 发起的驱逐</a>。</p>
<p>kubelet 并不理会你配置的 <code>PodDisruptionBudget</code> 或者是 Pod 的 <code>terminationGracePeriodSeconds</code>。
如果你使用了<a href=#soft-eviction-thresholds>软驱逐条件</a>，kubelet 会考虑你所配置的
<code>eviction-max-pod-grace-period</code>。
如果你使用了<a href=#hard-eviction-thresholds>硬驱逐条件</a>，它使用 <code>0s</code> 宽限期来终止 Pod。</p>
<p>如果 Pod 是由替换失败 Pod 的<a class=glossary-tooltip title="工作负载是在 Kubernetes 上运行的应用程序。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/workloads/ target=_blank aria-label=工作负载>工作负载</a>资源
（例如 <a class=glossary-tooltip title="StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>
或者 <a class=glossary-tooltip title="Deployment 是管理应用副本的 API 对象。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>）管理，
则控制平面或 <code>kube-controller-manager</code> 会创建新的 Pod 来代替被驱逐的 Pod。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>kubelet 在终止最终用户 Pod 之前会尝试<a href=#reclaim-node-resources>回收节点级资源</a>。
例如，它会在磁盘资源不足时删除未使用的容器镜像。
</div>
<p>kubelet 使用各种参数来做出驱逐决定，如下所示：</p>
<ul>
<li>驱逐信号</li>
<li>驱逐条件</li>
<li>监控间隔</li>
</ul>
<h3 id=eviction-signals>驱逐信号</h3>
<p>驱逐信号是特定资源在特定时间点的当前状态。
kubelet 使用驱逐信号，通过将信号与驱逐条件进行比较来做出驱逐决定，
驱逐条件是节点上应该可用资源的最小量。</p>
<p>kubelet 使用以下驱逐信号：</p>
<table>
<thead>
<tr>
<th>驱逐信号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>memory.available</code></td>
<td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code></td>
</tr>
<tr>
<td><code>nodefs.available</code></td>
<td><code>nodefs.available</code> := <code>node.stats.fs.available</code></td>
</tr>
<tr>
<td><code>nodefs.inodesFree</code></td>
<td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td>
</tr>
<tr>
<td><code>imagefs.available</code></td>
<td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code></td>
</tr>
<tr>
<td><code>imagefs.inodesFree</code></td>
<td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td>
</tr>
<tr>
<td><code>pid.available</code></td>
<td><code>pid.available</code> := <code>node.stats.rlimit.maxpid</code> - <code>node.stats.rlimit.curproc</code></td>
</tr>
</tbody>
</table>
<p>在上表中，<code>描述</code>列显示了 kubelet 如何获取信号的值。每个信号支持百分比值或者是字面值。
kubelet 计算相对于与信号有关的总量的百分比值。</p>
<p><code>memory.available</code> 的值来自 cgroupfs，而不是像 <code>free -m</code> 这样的工具。
这很重要，因为 <code>free -m</code> 在容器中不起作用，如果用户使用
<a href=/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>节点可分配资源</a>
这一功能特性，资源不足的判定是基于 CGroup 层次结构中的用户 Pod 所处的局部及 CGroup 根节点作出的。
这个<a href=/zh/examples/admin/resource/memory-available.sh>脚本</a>
重现了 kubelet 为计算 <code>memory.available</code> 而执行的相同步骤。
kubelet 在其计算中排除了 inactive_file（即非活动 LRU 列表上基于文件来虚拟的内存的字节数），
因为它假定在压力下内存是可回收的。</p>
<p>kubelet 支持以下文件系统分区：</p>
<ol>
<li><code>nodefs</code>：节点的主要文件系统，用于本地磁盘卷、emptyDir、日志存储等。
例如，<code>nodefs</code> 包含 <code>/var/lib/kubelet/</code>。</li>
<li><code>imagefs</code>：可选文件系统，供容器运行时存储容器镜像和容器可写层。</li>
</ol>
<p>kubelet 会自动发现这些文件系统并忽略其他文件系统。kubelet 不支持其他配置。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>一些 kubelet 垃圾收集功能已被弃用，以支持驱逐。
有关已弃用功能的列表，请参阅
<a href=/zh/docs/concepts/cluster-administration/kubelet-garbage-collection/#deprecation>kubelet 垃圾收集弃用</a>。
</div>
<h3 id=eviction-thresholds>驱逐条件</h3>
<p>你可以为 kubelet 指定自定义驱逐条件，以便在作出驱逐决定时使用。</p>
<p>驱逐条件的形式为 <code>[eviction-signal][operator][quantity]</code>，其中：</p>
<ul>
<li><code>eviction-signal</code> 是要使用的<a href=#eviction-signals>驱逐信号</a>。</li>
<li><code>operator</code> 是你想要的<a href=https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators>关系运算符</a>，
比如 <code>&lt;</code>（小于）。</li>
<li><code>quantity</code> 是驱逐条件数量，例如 <code>1Gi</code>。
<code>quantity</code> 的值必须与 Kubernetes 使用的数量表示相匹配。
你可以使用文字值或百分比（<code>%</code>）。</li>
</ul>
<p>例如，如果一个节点的总内存为 10Gi 并且你希望在可用内存低于 1Gi 时触发驱逐，
则可以将驱逐条件定义为 <code>memory.available&lt;10%</code> 或 <code>memory.available&lt; 1G</code>。
你不能同时使用二者。</p>
<p>你可以配置软和硬驱逐条件。</p>
<h4 id=soft-eviction-thresholds>软驱逐条件</h4>
<p>软驱逐条件将驱逐条件与管理员所必须指定的宽限期配对。
在超过宽限期之前，kubelet 不会驱逐 Pod。
如果没有指定的宽限期，kubelet 会在启动时返回错误。</p>
<p>你可以既指定软驱逐条件宽限期，又指定 Pod 终止宽限期的上限，，给 kubelet 在驱逐期间使用。
如果你指定了宽限期的上限并且 Pod 满足软驱逐阈条件，则 kubelet 将使用两个宽限期中的较小者。
如果你没有指定宽限期上限，kubelet 会立即杀死被驱逐的 Pod，不允许其体面终止。</p>
<p>你可以使用以下标志来配置软驱逐条件：</p>
<ul>
<li><code>eviction-soft</code>：一组驱逐条件，如 <code>memory.available&lt;1.5Gi</code>，
如果驱逐条件持续时长超过指定的宽限期，可以触发 Pod 驱逐。</li>
<li><code>eviction-soft-grace-period</code>：一组驱逐宽限期，
如 <code>memory.available=1m30s</code>，定义软驱逐条件在触发 Pod 驱逐之前必须保持多长时间。</li>
<li><code>eviction-max-pod-grace-period</code>：在满足软驱逐条件而终止 Pod 时使用的最大允许宽限期（以秒为单位）。</li>
</ul>
<h4 id=hard-eviction-thresholds>硬驱逐条件</h4>
<p>硬驱逐条件没有宽限期。当达到硬驱逐条件时，
kubelet 会立即杀死 pod，而不会正常终止以回收紧缺的资源。</p>
<p>你可以使用 <code>eviction-hard</code> 标志来配置一组硬驱逐条件，
例如 <code>memory.available&lt;1Gi</code>。</p>
<p>kubelet 具有以下默认硬驱逐条件：</p>
<ul>
<li><code>memory.available&lt;100Mi</code></li>
<li><code>nodefs.available&lt;10%</code></li>
<li><code>imagefs.available&lt;15%</code></li>
<li><code>nodefs.inodesFree&lt;5%</code>（Linux 节点）</li>
</ul>
<h3 id=驱逐监测间隔>驱逐监测间隔</h3>
<p>kubelet 根据其配置的 <code>housekeeping-interval</code>（默认为 <code>10s</code>）评估驱逐条件。</p>
<h3 id=node-conditions>节点条件</h3>
<p>kubelet 报告节点状况以反映节点处于压力之下，因为满足硬或软驱逐条件，与配置的宽限期无关。</p>
<p>kubelet 根据下表将驱逐信号映射为节点状况：</p>
<table>
<thead>
<tr>
<th>节点条件</th>
<th>驱逐信号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MemoryPressure</code></td>
<td><code>memory.available</code></td>
<td>节点上的可用内存已满足驱逐条件</td>
</tr>
<tr>
<td><code>DiskPressure</code></td>
<td><code>nodefs.available</code>、<code>nodefs.inodesFree</code>、<code>imagefs.available</code> 或 <code>imagefs.inodesFree</code></td>
<td>节点的根文件系统或映像文件系统上的可用磁盘空间和 inode 已满足驱逐条件</td>
</tr>
<tr>
<td><code>PIDPressure</code></td>
<td><code>pid.available</code></td>
<td>(Linux) 节点上的可用进程标识符已低于驱逐条件</td>
</tr>
</tbody>
</table>
<p>kubelet 根据配置的 <code>--node-status-update-frequency</code> 更新节点条件，默认为 <code>10s</code>。</p>
<h4 id=节点条件振荡>节点条件振荡</h4>
<p>在某些情况下，节点在软驱逐条件上下振荡，而没有保持定义的宽限期。
这会导致报告的节点条件在 <code>true</code> 和 <code>false</code> 之间不断切换，从而导致错误的驱逐决策。</p>
<p>为了防止振荡，你可以使用 <code>eviction-pressure-transition-period</code> 标志，
该标志控制 kubelet 在将节点条件转换为不同状态之前必须等待的时间。
过渡期的默认值为 <code>5m</code>。</p>
<h3 id=reclaim-node-resources>回收节点级资源</h3>
<p>kubelet 在驱逐最终用户 Pod 之前会先尝试回收节点级资源。</p>
<p>当报告 <code>DiskPressure</code> 节点状况时，kubelet 会根据节点上的文件系统回收节点级资源。</p>
<h4 id=有-imagefs>有 <code>imagefs</code></h4>
<p>如果节点有一个专用的 <code>imagefs</code> 文件系统供容器运行时使用，kubelet 会执行以下操作：</p>
<ul>
<li>如果 <code>nodefs</code> 文件系统满足驱逐条件，kubelet 垃圾收集死亡 Pod 和容器。</li>
<li>如果 <code>imagefs</code> 文件系统满足驱逐条件，kubelet 将删除所有未使用的镜像。</li>
</ul>
<h4 id=没有-imagefs>没有 <code>imagefs</code></h4>
<p>如果节点只有一个满足驱逐条件的 <code>nodefs</code> 文件系统，
kubelet 按以下顺序释放磁盘空间：</p>
<ol>
<li>对死亡的 Pod 和容器进行垃圾收集</li>
<li>删除未使用的镜像</li>
</ol>
<h3 id=kubelet-驱逐时-pod-的选择>kubelet 驱逐时 Pod 的选择</h3>
<p>如果 kubelet 回收节点级资源的尝试没有使驱逐信号低于条件，
则 kubelet 开始驱逐最终用户 Pod。</p>
<p>kubelet 使用以下参数来确定 Pod 驱逐顺序：</p>
<ol>
<li>Pod 的资源使用是否超过其请求</li>
<li><a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级</a></li>
<li>Pod 相对于请求的资源使用情况</li>
</ol>
<p>因此，kubelet 按以下顺序排列和驱逐 Pod：</p>
<ol>
<li>首先考虑资源使用量超过其请求的 <code>BestEffort</code> 或 <code>Burstable</code> Pod。
这些 Pod 会根据它们的优先级以及它们的资源使用级别超过其请求的程度被逐出。</li>
<li>资源使用量少于请求量的 <code>Guaranteed</code> Pod 和 <code>Burstable</code> Pod 根据其优先级被最后驱逐。</li>
</ol>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>kubelet 不使用 Pod 的 QoS 类来确定驱逐顺序。
在回收内存等资源时，你可以使用 QoS 类来估计最可能的 Pod 驱逐顺序。
QoS 不适用于临时存储（EphemeralStorage）请求，
因此如果节点在 <code>DiskPressure</code> 下，则上述场景将不适用。
</div>
<p>仅当 <code>Guaranteed</code> Pod 中所有容器都被指定了请求和限制并且二者相等时，才保证 Pod 不被驱逐。
这些 Pod 永远不会因为另一个 Pod 的资源消耗而被驱逐。
如果系统守护进程（例如 <code>kubelet</code> 和 <code>journald</code>）
消耗的资源比通过 <code>system-reserved</code> 或 <code>kube-reserved</code> 分配保留的资源多，
并且该节点只有 <code>Guaranteed</code> 或 <code>Burstable</code> Pod 使用的资源少于其上剩余的请求，
那么 kubelet 必须选择驱逐这些 Pod 中的一个以保持节点稳定性并减少资源匮乏对其他 Pod 的影响。
在这种情况下，它会选择首先驱逐最低优先级的 Pod。</p>
<p>当 kubelet 因 inode 或 PID 不足而驱逐 pod 时，
它使用优先级来确定驱逐顺序，因为 inode 和 PID 没有请求。</p>
<p>kubelet 根据节点是否具有专用的 <code>imagefs</code> 文件系统对 Pod 进行不同的排序：</p>
<h4 id=有-imagefs-1>有 <code>imagefs</code></h4>
<p>如果 <code>nodefs</code> 触发驱逐，
kubelet 会根据 <code>nodefs</code> 使用情况（<code>本地卷 + 所有容器的日志</code>）对 Pod 进行排序。</p>
<p>如果 <code>imagefs</code> 触发驱逐，kubelet 会根据所有容器的可写层使用情况对 Pod 进行排序。</p>
<h4 id=没有-imagefs-1>没有 <code>imagefs</code></h4>
<p>如果 <code>nodefs</code> 触发驱逐，
kubelet 会根据磁盘总用量（<code>本地卷 + 日志和所有容器的可写层</code>）对 Pod 进行排序。</p>
<h3 id=minimum-eviction-reclaim>最小驱逐回收 </h3>
<p>在某些情况下，驱逐 Pod 只会回收少量的紧俏资源。
这可能导致 kubelet 反复达到配置的驱逐条件并触发多次驱逐。</p>
<p>你可以使用 <code>--eviction-minimum-reclaim</code> 标志或
<a href=/zh/docs/tasks/administer-cluster/kubelet-config-file/>kubelet 配置文件</a>
为每个资源配置最小回收量。
当 kubelet 注意到某个资源耗尽时，它会继续回收该资源，直到回收到你所指定的数量为止。</p>
<p>例如，以下配置设置最小回收量：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>evictionHard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>memory.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imagefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>evictionMinimumReclaim</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>memory.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imagefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span></code></pre></div>
<p>在这个例子中，如果 <code>nodefs.available</code> 信号满足驱逐条件，
kubelet 会回收资源，直到信号达到 <code>1Gi</code> 的条件，
然后继续回收至少 <code>500Mi</code> 直到信号达到 <code>1.5Gi</code>。</p>
<p>类似地，kubelet 会回收 <code>imagefs</code> 资源，直到 <code>imagefs.available</code> 信号达到 <code>102Gi</code>。</p>
<p>对于所有资源，默认的 <code>eviction-minimum-reclaim</code> 为 <code>0</code>。</p>
<h3 id=节点内存不足行为>节点内存不足行为</h3>
<p>如果节点在 kubelet 能够回收内存之前遇到内存不足（OOM）事件，
则节点依赖 <a href=https://lwn.net/Articles/391222/>oom_killer</a> 来响应。</p>
<p>kubelet 根据 Pod 的服务质量（QoS）为每个容器设置一个 <code>oom_score_adj</code> 值。</p>
<table>
<thead>
<tr>
<th>服务质量</th>
<th>oom_score_adj</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Guaranteed</code></td>
<td>-997</td>
</tr>
<tr>
<td><code>BestEffort</code></td>
<td>1000</td>
</tr>
<tr>
<td><code>Burstable</code></td>
<td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>kubelet 还将具有 <code>system-node-critical</code>
<a class=glossary-tooltip title="Pod 优先级表示一个 Pod 相对于其他 Pod 的重要性。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority target=_blank aria-label=优先级>优先级</a>
的 Pod 中的容器 <code>oom_score_adj</code> 值设为 <code>-997</code>。
</div>
<p>如果 kubelet 在节点遇到 OOM 之前无法回收内存，
则 <code>oom_killer</code> 根据它在节点上使用的内存百分比计算 <code>oom_score</code>，
然后加上 <code>oom_score_adj</code> 得到每个容器有效的 <code>oom_score</code>。
然后它会杀死得分最高的容器。</p>
<p>这意味着低 QoS Pod 中相对于其调度请求消耗内存较多的容器，将首先被杀死。</p>
<p>与 Pod 驱逐不同，如果容器被 OOM 杀死，
<code>kubelet</code> 可以根据其 <code>RestartPolicy</code> 重新启动它。</p>
<h3 id=node-pressure-eviction-good-practices>最佳实践</h3>
<p>以下部分描述了驱逐配置的最佳实践。</p>
<h4 id=可调度的资源和驱逐策略>可调度的资源和驱逐策略</h4>
<p>当你为 kubelet 配置驱逐策略时，
你应该确保调度程序不会在 Pod 触发驱逐时对其进行调度，因为这类 Pod 会立即引起内存压力。</p>
<p>考虑以下场景：</p>
<ul>
<li>节点内存容量：<code>10Gi</code></li>
<li>操作员希望为系统守护进程（内核、<code>kubelet</code> 等）保留 10% 的内存容量</li>
<li>操作员希望驱逐内存利用率为 95% 的Pod，以减少系统 OOM 的概率。</li>
</ul>
<p>为此，kubelet 启动设置如下：</p>
<pre><code>--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi
</code></pre>
<p>在此配置中，<code>--system-reserved</code> 标志为系统预留了 <code>1.5Gi</code> 的内存，
即 <code>总内存的 10% + 驱逐条件量</code>。</p>
<p>如果 Pod 使用的内存超过其请求值或者系统使用的内存超过 <code>1Gi</code>，
则节点可以达到驱逐条件，这使得 <code>memory.available</code> 信号低于 <code>500Mi</code> 并触发条件。</p>
<h3 id=daemonset>DaemonSet</h3>
<p>Pod 优先级是做出驱逐决定的主要因素。
如果你不希望 kubelet 驱逐属于 <code>DaemonSet</code> 的 Pod，
请在 Pod 规约中为这些 Pod 提供足够高的 <code>priorityClass</code>。
你还可以使用优先级较低的 <code>priorityClass</code> 或默认配置，
仅在有足够资源时才运行 <code>DaemonSet</code> Pod。</p>
<h3 id=已知问题>已知问题</h3>
<p>以下部分描述了与资源不足处理相关的已知问题。</p>
<h4 id=kubelet-可能不会立即观察到内存压力>kubelet 可能不会立即观察到内存压力</h4>
<p>默认情况下，kubelet 轮询 <code>cAdvisor</code> 以定期收集内存使用情况统计信息。
如果该轮询时间窗口内内存使用量迅速增加，kubelet 可能无法足够快地观察到 <code>MemoryPressure</code>，
但是 <code>OOMKiller</code> 仍将被调用。</p>
<p>你可以使用 <code>--kernel-memcg-notification</code>
标志在 kubelet 上启用 <code>memcg</code> 通知 API，以便在超过条件时立即收到通知。</p>
<p>如果你不是追求极端利用率，而是要采取合理的过量使用措施，
则解决此问题的可行方法是使用 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 标志为系统分配内存。</p>
<h4 id=active-file-内存未被视为可用内存>active_file 内存未被视为可用内存</h4>
<p>在 Linux 上，内核跟踪活动 LRU 列表上的基于文件所虚拟的内存字节数作为 <code>active_file</code> 统计信息。
kubelet 将 <code>active_file</code> 内存区域视为不可回收。
对于大量使用块设备形式的本地存储（包括临时本地存储）的工作负载，
文件和块数据的内核级缓存意味着许多最近访问的缓存页面可能被计为 <code>active_file</code>。
如果这些内核块缓冲区中在活动 LRU 列表上有足够多，
kubelet 很容易将其视为资源用量过量并为节点设置内存压力污点，从而触发 Pod 驱逐。</p>
<p>更多细节请参见 <a href=https://github.com/kubernetes/kubernetes/issues/43916>https://github.com/kubernetes/kubernetes/issues/43916</a></p>
<p>你可以通过为可能执行 I/O 密集型活动的容器设置相同的内存限制和内存请求来应对该行为。
你将需要估计或测量该容器的最佳内存限制值。</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>了解 <a href=/docs/reference/generated/kubernetes-api/v1.23/>API 发起的驱逐</a></li>
<li>了解 <a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级和驱逐</a></li>
<li>了解 <a href=/docs/tasks/run-application/configure-pdb/>PodDisruptionBudgets</a></li>
<li>了解<a href=/zh/docs/tasks/configure-pod-container/quality-service-pod/>服务质量</a>（QoS）</li>
<li>查看<a href=/docs/reference/generated/kubernetes-api/v1.23/#create-eviction-pod-v1-core>驱逐 API</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-b87723bf81b079042860f0ebd37b0a64>7 - API 发起的驱逐</h1>
<p>API 发起的驱逐是一个先调用
<a href=/docs/reference/generated/kubernetes-api/v1.23/#create-eviction-pod-v1-core>Eviction API</a>
创建 <code>Eviction</code> 对象，再由该对象体面地中止 Pod 的过程。 </br></p>
<p>你可以通过直接调用 Eviction API 发起驱逐，也可以通过编程的方式使用
<a class=glossary-tooltip title="提供 Kubernetes API 服务的控制面组件。" data-toggle=tooltip data-placement=top href=/zh/docs/reference/command-line-tools-reference/kube-apiserver/ target=_blank aria-label="API 服务器">API 服务器</a>的客户端来发起驱逐，
比如 <code>kubectl drain</code> 命令。
此操作创建一个 <code>Eviction</code> 对象，该对象再驱动 API 服务器终止选定的 Pod。</p>
<p>API 发起的驱逐将遵从你的
<a href=/zh/docs/tasks/run-application/configure-pdb/><code>PodDisruptionBudgets</code></a>
和 <a href=/zh/docs/concepts/workloads/pods/pod-lifecycle#pod-termination><code>terminationGracePeriodSeconds</code></a>
配置。</p>
<p>使用 API 创建 Eviction 对象，就像对 Pod 执行策略控制的
<a href=/zh/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod><code>DELETE</code> 操作</a></p>
<h2 id=调用-eviction-api>调用 Eviction API</h2>
<p>你可以使用 <a href=/zh/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api>Kubernetes 语言客户端</a>
来访问 Kubernetes API 并创建 <code>Eviction</code> 对象。
要执行此操作，你应该用 POST 发出要尝试的请求，类似于下面的示例：</p>
<ul class="nav nav-tabs" id=eviction-example role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#eviction-example-0 role=tab aria-controls=eviction-example-0 aria-selected=true>policy/v1</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#eviction-example-1 role=tab aria-controls=eviction-example-1>policy/v1beta1</a></li></ul>
<div class=tab-content id=eviction-example><div id=eviction-example-0 class="tab-pane show active" role=tabpanel aria-labelledby=eviction-example-0>
<p><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p><code>policy/v1</code> 版本的 Eviction 在 v1.22 以及更高的版本中可用，之前的发行版本使用 <code>policy/v1beta1</code> 版本。
</div>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;policy/v1&#34;</span>,
  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Eviction&#34;</span>,
  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;quux&#34;</span>,
    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
  }
}
</code></pre></div></div>
<div id=eviction-example-1 class=tab-pane role=tabpanel aria-labelledby=eviction-example-1>
<p><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong>
<p>在 v1.22 版本废弃以支持 <code>policy/v1</code>
</div>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;policy/v1beta1&#34;</span>,
  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Eviction&#34;</span>,
  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;quux&#34;</span>,
    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
  }
}
</code></pre></div></div></div>
<p>或者，你可以通过使用 <code>curl</code> 或者 <code>wget</code> 来访问 API 以尝试驱逐操作，类似于以下示例：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -v -H <span style=color:#b44>&#39;Content-type: application/json&#39;</span> https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
</code></pre></div>
<h2 id=api-发起驱逐的工作原理>API 发起驱逐的工作原理</h2>
<p>当你使用 API 来请求驱逐时，API 服务器将执行准入检查，并通过以下方式之一做出响应：</p>
<ul>
<li><code>200 OK</code>：允许驱逐，子资源 <code>Eviction</code> 被创建，并且 Pod 被删除，
类似于发送一个 <code>DELETE</code> 请求到 Pod 地址。</li>
<li><code>429 Too Many Requests</code>：当前不允许驱逐，因为配置了 <a class=glossary-tooltip title='Pod Disruption Budget 是这样一种对象：它保证在主动中断（ voluntary disruptions）时，多实例应用的 {{< glossary_tooltip text="Pod" term_id="pod" >}} 不会少于一定的数量。' data-toggle=tooltip data-placement=top href="/zh/docs/reference/glossary/?all=true#term-pod-disruption-budget" target=_blank aria-label=PodDisruptionBudget>PodDisruptionBudget</a>。
你可以稍后再尝试驱逐。你也可能因为 API 速率限制而看到这种响应。</li>
<li><code>500 Internal Server Error</code>：不允许驱逐，因为存在配置错误，
例如存在多个 PodDisruptionBudgets 引用同一个 Pod。</li>
</ul>
<p>如果你想驱逐的 Pod 不属于有 PodDisruptionBudget 的工作负载，
API 服务器总是返回 <code>200 OK</code> 并且允许驱逐。</p>
<p>如果 API 服务器允许驱逐，Pod 按照如下方式删除：</p>
<ol>
<li>API 服务器中的 <code>Pod</code> 资源会更新上删除时间戳，之后 API 服务器会认为此 <code>Pod</code> 资源将被终止。
此 <code>Pod</code> 资源还会标记上配置的宽限期。</li>
<li>本地运行状态的 Pod 所处的节点上的 <a class=glossary-tooltip title="一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。" data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
注意到 <code>Pod</code> 资源被标记为终止，并开始优雅停止本地 Pod。</li>
<li>当 kubelet 停止 Pod 时，控制面从 <a class=glossary-tooltip title="端点负责记录与服务（Service）的选择器相匹配的 Pods 的 IP 地址。" data-toggle=tooltip data-placement=top href="/zh/docs/reference/glossary/?all=true#term-endpoint" target=_blank aria-label=Endpoint>Endpoint</a>
和 <a class=glossary-tooltip title="一种将网络端点与 Kubernetes 资源组合在一起的方法。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/services-networking/endpoint-slices/ target=_blank aria-label=EndpointSlice>EndpointSlice</a>
对象中移除该 Pod。因此，控制器不再将此 Pod 视为有用对象。</li>
<li>Pod 的宽限期到期后，kubelet 强制终止本地 Pod。</li>
<li>kubelet 告诉 API 服务器删除 <code>Pod</code> 资源。</li>
<li>API 服务器删除 <code>Pod</code> 资源。</li>
</ol>
<h2 id=解决驱逐被卡住的问题>解决驱逐被卡住的问题</h2>
<p>在某些情况下，你的应用可能进入中断状态，
在你干预之前，驱逐 API 总是返回 <code>429</code> 或 <code>500</code>。
例如，如果 ReplicaSet 为你的应用程序创建了 Pod，
但新的 Pod 没有进入 <code>Ready</code> 状态，就会发生这种情况。
在最后一个被驱逐的 Pod 有很长的终止宽限期的情况下，你可能也会注意到这种行为。</p>
<p>如果你注意到驱逐被卡住，请尝试以下解决方案之一：</p>
<ul>
<li>终止或暂停导致问题的自动化操作，重新启动操作之前，请检查被卡住的应用程序。</li>
<li>等待一段时间后，直接从集群控制平面删除 Pod，而不是使用 Eviction API。</li>
</ul>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>了解如何使用 <a href=/zh/docs/tasks/run-application/configure-pdb/>Pod 干扰预算</a> 保护你的应用。</li>
<li>了解<a href=/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/>节点压力引发的驱逐</a>。</li>
<li>了解 <a href=/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级和抢占</a>。</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-961126cd43559012893979e568396a49>8 - 扩展资源的资源装箱</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes 1.16 [alpha]</code>
</div>
<p>使用 <code>RequestedToCapacityRatioResourceAllocation</code> 优先级函数，可以将 kube-scheduler
配置为支持包含扩展资源在内的资源装箱操作。
优先级函数可用于根据自定义需求微调 kube-scheduler 。</p>
<h2 id=使用-requestedtocapacityratioresourceallocation-启用装箱>使用 RequestedToCapacityRatioResourceAllocation 启用装箱</h2>
<p>Kubernetes 允许用户指定资源以及每类资源的权重，
以便根据请求数量与可用容量之比率为节点评分。
这就使得用户可以通过使用适当的参数来对扩展资源执行装箱操作，从而提高了大型集群中稀缺资源的利用率。
<code>RequestedToCapacityRatioResourceAllocation</code> 优先级函数的行为可以通过名为
<code>RequestedToCapacityRatioArgs</code> 的配置选项进行控制。
该标志由两个参数 <code>shape</code> 和 <code>resources</code> 组成。
<code>shape</code> 允许用户根据 <code>utilization</code> 和 <code>score</code> 值将函数调整为
最少请求（least requested）或最多请求（most requested）计算。
<code>resources</code> 包含由 <code>name</code> 和 <code>weight</code> 组成，<code>name</code> 指定评分时要考虑的资源，
<code>weight</code> 指定每种资源的权重。</p>
<p>以下是一个配置示例，该配置将 <code>requestedToCapacityRatioArguments</code> 设置为对扩展资源
<code>intel.com/foo</code> 和 <code>intel.com/bar</code> 的装箱行为</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># ...</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>RequestedToCapacityRatio<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> 
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/bar<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></code></pre></div>
<p>使用 kube-scheduler 标志 <code>--config=/path/to/config/file</code>
引用 <code>KubeSchedulerConfiguration</code> 文件将配置传递给调度器。</p>
<p><strong>默认情况下此功能处于被禁用状态</strong></p>
<h3 id=调整-requestedtocapacityratioresourceallocation-优先级函数>调整 RequestedToCapacityRatioResourceAllocation 优先级函数</h3>
<p><code>shape</code> 用于指定 <code>RequestedToCapacityRatioPriority</code> 函数的行为。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span><span style=color:#bbb> </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span><span style=color:#bbb> </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></code></pre></div>
<p>上面的参数在 <code>utilization</code> 为 0% 时给节点评分为 0，在 <code>utilization</code> 为
100% 时给节点评分为 10，因此启用了装箱行为。
要启用最少请求（least requested）模式，必须按如下方式反转得分值。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb> </span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></code></pre></div>
<p><code>resources</code> 是一个可选参数，默认情况下设置为：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></code></pre></div>
<p>它可以用来添加扩展资源，如下所示：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></code></pre></div>
<p>weight 参数是可选的，如果未指定，则设置为 1。
同时，weight 不能设置为负值。</p>
<h3 id=节点容量分配的评分>节点容量分配的评分</h3>
<p>本节适用于希望了解此功能的内部细节的人员。
以下是如何针对给定的一组值来计算节点得分的示例。</p>
<pre><code>请求的资源

intel.com/foo : 2
memory: 256MB
cpu: 2

资源权重

intel.com/foo : 5
memory: 1
cpu: 3

FunctionShapePoint {{0, 0}, {100, 10}}

节点 Node 1 配置

可用：
  intel.com/foo : 4
  memory : 1 GB
  cpu: 8

已用：
  intel.com/foo: 1
  memory: 256MB
  cpu: 1

节点得分：

intel.com/foo  = resourceScoringFunction((2+1),4)
               = (100 - ((4-3)*100/4)
               = (100 - 25)
               = 75
               = rawScoringFunction(75)
               = 7

memory         = resourceScoringFunction((256+256),1024)
               = (100 -((1024-512)*100/1024))
               = 50
               = rawScoringFunction(50)
               = 5

cpu            = resourceScoringFunction((2+1),8)
               = (100 -((8-3)*100/8))
               = 37.5
               = rawScoringFunction(37.5)
               = 3

NodeScore   =  (7 * 5) + (5 * 1) + (3 * 3) / (5 + 1 + 3)
            =  5


节点 Node 2 配置

可用：
  intel.com/foo: 8
  memory: 1GB
  cpu: 8

已用：
  intel.com/foo: 2
  memory: 512MB
  cpu: 6

节点得分：

intel.com/foo  = resourceScoringFunction((2+2),8)
               = (100 - ((8-4)*100/8)
               = (100 - 50)
               = 50
               = rawScoringFunction(50)
               = 5

memory         = resourceScoringFunction((256+512),1024)
               = (100 -((1024-768)*100/1024))
               = 75
               = rawScoringFunction(75)
               = 7

cpu            = resourceScoringFunction((2+6),8)
               = (100 -((8-8)*100/8))
               = 100
               = rawScoringFunction(100)
               = 10

NodeScore   =  (5 * 5) + (7 * 1) + (10 * 3) / (5 + 1 + 3)
            =  7
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-602208c95fe7b1f1170310ce993f5814>9 - 调度框架</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes 1.19 [stable]</code>
</div>
<p>调度框架是面向 Kubernetes 调度器的一种插件架构，
它为现有的调度器添加了一组新的“插件” API。插件会被编译到调度器之中。
这些 API 允许大多数调度功能以插件的形式实现，同时使调度“核心”保持简单且可维护。
请参考<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md>调度框架的设计提案</a>
获取框架设计的更多技术信息。</p>
<h1 id=框架工作流程>框架工作流程</h1>
<p>调度框架定义了一些扩展点。调度器插件注册后在一个或多个扩展点处被调用。
这些插件中的一些可以改变调度决策，而另一些仅用于提供信息。</p>
<p>每次调度一个 Pod 的尝试都分为两个阶段，即 <strong>调度周期</strong> 和 <strong>绑定周期</strong>。</p>
<h2 id=调度周期和绑定周期>调度周期和绑定周期</h2>
<p>调度周期为 Pod 选择一个节点，绑定周期将该决策应用于集群。
调度周期和绑定周期一起被称为“调度上下文”。</p>
<p>调度周期是串行运行的，而绑定周期可能是同时运行的。</p>
<p>如果确定 Pod 不可调度或者存在内部错误，则可以终止调度周期或绑定周期。
Pod 将返回队列并重试。</p>
<h2 id=扩展点>扩展点</h2>
<p>下图显示了一个 Pod 的调度上下文以及调度框架公开的扩展点。
在此图片中，“过滤器”等同于“断言”，“评分”相当于“优先级函数”。</p>
<p>一个插件可以在多个扩展点处注册，以执行更复杂或有状态的任务。</p>
<figure class=diagram-large>
<img src=/images/docs/scheduling-framework-extensions.png> <figcaption>
<h4>调度框架扩展点</h4>
</figcaption>
</figure>
<h3 id=queue-sort>队列排序</h3>
<p>这些插件用于对调度队列中的 Pod 进行排序。
队列排序插件本质上提供 <code>less(Pod1, Pod2)</code> 函数。
一次只能启动一个队列插件。</p>
<h3 id=pre-filter>PreFilter</h3>
<p>这些插件用于预处理 Pod 的相关信息，或者检查集群或 Pod 必须满足的某些条件。
如果 PreFilter 插件返回错误，则调度周期将终止。</p>
<h3 id=filter>Filter</h3>
<p>这些插件用于过滤出不能运行该 Pod 的节点。对于每个节点，
调度器将按照其配置顺序调用这些过滤插件。如果任何过滤插件将节点标记为不可行，
则不会为该节点调用剩下的过滤插件。节点可以被同时进行评估。</p>
<h3 id=post-filter>PostFilter </h3>
<p>这些插件在 Filter 阶段后调用，但仅在该 Pod 没有可行的节点时调用。
插件按其配置的顺序调用。如果任何 PostFilter 插件标记节点为“Schedulable”，
则其余的插件不会调用。典型的 PostFilter 实现是抢占，试图通过抢占其他 Pod
的资源使该 Pod 可以调度。</p>
<h3 id=pre-score>PreScore</h3>
<p>这些插件用于执行 “前置评分（pre-scoring）” 工作，即生成一个可共享状态供 Score 插件使用。
如果 PreScore 插件返回错误，则调度周期将终止。</p>
<h3 id=scoring>Score </h3>
<p>这些插件用于对通过过滤阶段的节点进行排序。调度器将为每个节点调用每个评分插件。
将有一个定义明确的整数范围，代表最小和最大分数。
在<a href=#normalize-scoring>标准化评分</a>阶段之后，调度器将根据配置的插件权重
合并所有插件的节点分数。</p>
<h3 id=normalize-scoring>NormalizeScore </h3>
<p>这些插件用于在调度器计算 Node 排名之前修改分数。
在此扩展点注册的插件被调用时会使用同一插件的 <a href=#scoring>Score</a> 结果。
每个插件在每个调度周期调用一次。</p>
<p>例如，假设一个 <code>BlinkingLightScorer</code> 插件基于具有的闪烁指示灯数量来对节点进行排名。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>ScoreNode</span>(_ <span style=color:#666>*</span>v1.pod, n <span style=color:#666>*</span>v1.Node) (<span style=color:#0b0;font-weight:700>int</span>, <span style=color:#0b0;font-weight:700>error</span>) {
   <span style=color:#a2f;font-weight:700>return</span> <span style=color:#00a000>getBlinkingLightCount</span>(n)
}
</code></pre></div>
<p>然而，最大的闪烁灯个数值可能比 <code>NodeScoreMax</code> 小。要解决这个问题，
<code>BlinkingLightScorer</code> 插件还应该注册该扩展点。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>NormalizeScores</span>(scores <span style=color:#a2f;font-weight:700>map</span>[<span style=color:#0b0;font-weight:700>string</span>]<span style=color:#0b0;font-weight:700>int</span>) {
   highest <span style=color:#666>:=</span> <span style=color:#666>0</span>
   <span style=color:#a2f;font-weight:700>for</span> _, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
      highest = <span style=color:#00a000>max</span>(highest, score)
   }
   <span style=color:#a2f;font-weight:700>for</span> node, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
      scores[node] = score<span style=color:#666>*</span>NodeScoreMax<span style=color:#666>/</span>highest
   }
}
</code></pre></div>
<p>如果任何 NormalizeScore 插件返回错误，则调度阶段将终止。</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 希望执行“预保留”工作的插件应该使用 NormalizeScore 扩展点。
</div>
<h3 id=reserve>Reserve</h3>
<p>Reserve 是一个信息性的扩展点。
管理运行时状态的插件（也成为“有状态插件”）应该使用此扩展点，以便
调度器在节点给指定 Pod 预留了资源时能够通知该插件。
这是在调度器真正将 Pod 绑定到节点之前发生的，并且它存在是为了防止
在调度器等待绑定成功时发生竞争情况。</p>
<p>这个是调度周期的最后一步。
一旦 Pod 处于保留状态，它将在绑定周期结束时触发 <a href=#unreserve>Unreserve</a> 插件
（失败时）或 <a href=#post-bind>PostBind</a> 插件（成功时）。</p>
<h3 id=permit>Permit</h3>
<p><em>Permit</em> 插件在每个 Pod 调度周期的最后调用，用于防止或延迟 Pod 的绑定。
一个允许插件可以做以下三件事之一：</p>
<ol>
<li><strong>批准</strong><br>
一旦所有 Permit 插件批准 Pod 后，该 Pod 将被发送以进行绑定。</li>
</ol>
<ol>
<li><strong>拒绝</strong><br>
如果任何 Permit 插件拒绝 Pod，则该 Pod 将被返回到调度队列。
这将触发<a href=#unreserve>Unreserve</a> 插件。</li>
</ol>
<ol>
<li><strong>等待</strong>（带有超时）<br>
如果一个 Permit 插件返回 “等待” 结果，则 Pod 将保持在一个内部的 “等待中”
的 Pod 列表，同时该 Pod 的绑定周期启动时即直接阻塞直到得到
<a href=#frameworkhandle>批准</a>。如果超时发生，<strong>等待</strong> 变成 <strong>拒绝</strong>，并且 Pod
将返回调度队列，从而触发 <a href=#unreserve>Unreserve</a> 插件。</li>
</ol>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> 尽管任何插件可以访问 “等待中” 状态的 Pod 列表并批准它们
(查看 <a href=https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle><code>FrameworkHandle</code></a>)。
我们期望只有允许插件可以批准处于 “等待中” 状态的预留 Pod 的绑定。
一旦 Pod 被批准了，它将发送到 <a href=#pre-bind>PreBind</a> 阶段。
</div>
<h3 id=pre-bind>PreBind </h3>
<p>这些插件用于执行 Pod 绑定前所需的所有工作。
例如，一个 PreBind 插件可能需要制备网络卷并且在允许 Pod 运行在该节点之前
将其挂载到目标节点上。</p>
<p>如果任何 PreBind 插件返回错误，则 Pod 将被 <a href=#unreserve>拒绝</a> 并且
退回到调度队列中。</p>
<h3 id=bind>Bind</h3>
<p>Bind 插件用于将 Pod 绑定到节点上。直到所有的 PreBind 插件都完成，Bind 插件才会被调用。
各 Bind 插件按照配置顺序被调用。Bind 插件可以选择是否处理指定的 Pod。
如果某 Bind 插件选择处理某 Pod，<strong>剩余的 Bind 插件将被跳过</strong>。</p>
<h3 id=post-bind>PostBind </h3>
<p>这是个信息性的扩展点。
PostBind 插件在 Pod 成功绑定后被调用。这是绑定周期的结尾，可用于清理相关的资源。</p>
<h3 id=unreserve>Unreserve</h3>
<p>这是个信息性的扩展点。
如果 Pod 被保留，然后在后面的阶段中被拒绝，则 Unreserve 插件将被通知。
Unreserve 插件应该清楚保留 Pod 的相关状态。</p>
<p>使用此扩展点的插件通常也使用 <a href=#reserve>Reserve</a>。</p>
<h2 id=插件-api>插件 API</h2>
<p>插件 API 分为两个步骤。首先，插件必须完成注册并配置，然后才能使用扩展点接口。
扩展点接口具有以下形式。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> Plugin <span style=color:#a2f;font-weight:700>interface</span> {
   <span style=color:#00a000>Name</span>() <span style=color:#0b0;font-weight:700>string</span>
}

<span style=color:#a2f;font-weight:700>type</span> QueueSortPlugin <span style=color:#a2f;font-weight:700>interface</span> {
   Plugin
   <span style=color:#00a000>Less</span>(<span style=color:#666>*</span>v1.pod, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>bool</span>
}

<span style=color:#a2f;font-weight:700>type</span> PreFilterPlugin <span style=color:#a2f;font-weight:700>interface</span> {
   Plugin
   <span style=color:#00a000>PreFilter</span>(context.Context, <span style=color:#666>*</span>framework.CycleState, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>error</span>
}

<span style=color:#080;font-style:italic>// ...
</span></code></pre></div>
<h1 id=插件配置>插件配置</h1>
<p>你可以在调度器配置中启用或禁用插件。
如果你在使用 Kubernetes v1.18 或更高版本，大部分调度
<a href=/zh/docs/reference/scheduling/config/#scheduling-plugins>插件</a>
都在使用中且默认启用。</p>
<p>除了默认的插件，你还可以实现自己的调度插件并且将它们与默认插件一起配置。
你可以访问 <a href=https://github.com/kubernetes-sigs/scheduler-plugins>scheduler-plugins</a>
了解更多信息。</p>
<p>如果你正在使用 Kubernetes v1.18 或更高版本，你可以将一组插件设置为
一个调度器配置文件，然后定义不同的配置文件来满足各类工作负载。
了解更多关于<a href=/zh/docs/reference/scheduling/config/#multiple-profiles>多配置文件</a>。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-d9574a30fcbc631b0d2a57850e161e89>10 - 调度器性能调优</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes 1.14 [beta]</code>
</div>
<p>作为 kubernetes 集群的默认调度器，
<a href=/zh/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler>kube-scheduler</a>
主要负责将 Pod 调度到集群的 Node 上。</p>
<p>在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 <em>可调度</em> Node。
调度器先在集群中找到一个 Pod 的可调度 Node，然后根据一系列函数对这些可调度 Node 打分，
之后选出其中得分最高的 Node 来运行 Pod。
最后，调度器将这个调度决定告知 kube-apiserver，这个过程叫做 <em>绑定（Binding）</em>。</p>
<p>这篇文章将会介绍一些在大规模 Kubernetes 集群下调度器性能优化的方式。</p>
<p>在大规模集群中，你可以调节调度器的表现来平衡调度的延迟（新 Pod 快速就位）
和精度（调度器很少做出糟糕的放置决策）。</p>
<p>你可以通过设置 kube-scheduler 的 <code>percentageOfNodesToScore</code> 来配置这个调优设置。
这个 KubeSchedulerConfiguration 设置决定了调度集群中节点的阈值。</p>
<h3 id=设置阈值>设置阈值</h3>
<p><code>percentageOfNodesToScore</code> 选项接受从 0 到 100 之间的整数值。
0 值比较特殊，表示 kube-scheduler 应该使用其编译后的默认值。
如果你设置 <code>percentageOfNodesToScore</code> 的值超过了 100，
kube-scheduler 的表现等价于设置值为 100。</p>
<p>要修改这个值，先编辑 <a href=/zh/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler 的配置文件</a>
然后重启调度器。
大多数情况下，这个配置文件是 <code>/etc/kubernetes/config/kube-scheduler.yaml</code>。</p>
<p>修改完成后，你可以执行</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pods -n kube-system | grep kube-scheduler
</code></pre></div>
<p>来检查该 kube-scheduler 组件是否健康。</p>
<h2 id=percentage-of-nodes-to-score>节点打分阈值</h2>
<p>要提升调度性能，kube-scheduler 可以在找到足够的可调度节点之后停止查找。
在大规模集群中，比起考虑每个节点的简单方法相比可以节省时间。</p>
<p>你可以使用整个集群节点总数的百分比作为阈值来指定需要多少节点就足够。
kube-scheduler 会将它转换为节点数的整数值。在调度期间，如果
kube-scheduler 已确认的可调度节点数足以超过了配置的百分比数量，
kube-scheduler 将停止继续查找可调度节点并继续进行
<a href=/zh/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation>打分阶段</a>。</p>
<p><a href=#how-the-scheduler-iterates-over-nodes>调度器如何遍历节点</a> 详细介绍了这个过程。</p>
<h3 id=默认阈值>默认阈值</h3>
<p>如果你不指定阈值，Kubernetes 使用线性公式计算出一个比例，在 100-节点集群
下取 50%，在 5000-节点的集群下取 10%。这个自动设置的参数的最低值是 5%。</p>
<p>这意味着，调度器至少会对集群中 5% 的节点进行打分，除非用户将该参数设置的低于 5。</p>
<p>如果你想让调度器对集群内所有节点进行打分，则将 <code>percentageOfNodesToScore</code> 设置为 100。</p>
<h2 id=示例>示例</h2>
<p>下面就是一个将 <code>percentageOfNodesToScore</code> 参数设置为 50% 的例子。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>algorithmSource</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>provider</span>:<span style=color:#bbb> </span>DefaultProvider<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>percentageOfNodesToScore</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></code></pre></div>
<h3 id=调节-percentageofnodestoscore-参数>调节 percentageOfNodesToScore 参数</h3>
<p><code>percentageOfNodesToScore</code> 的值必须在 1 到 100 之间，而且其默认值是通过集群的规模计算得来的。
另外，还有一个 50 个 Node 的最小值是硬编码在程序中。</p>
<p>值得注意的是，该参数设置后可能会导致只有集群中少数节点被选为可调度节点，
很多节点都没有进入到打分阶段。这样就会造成一种后果，
一个本来可以在打分阶段得分很高的节点甚至都不能进入打分阶段。</p>
<p>由于这个原因，这个参数不应该被设置成一个很低的值。
通常的做法是不会将这个参数的值设置的低于 10。
很低的参数值一般在调度器的吞吐量很高且对节点的打分不重要的情况下才使用。
换句话说，只有当你更倾向于在可调度节点中任意选择一个节点来运行这个 Pod 时，
才使用很低的参数设置。</p>
<h3 id=how-the-scheduler-iterates-over-nodes>调度器做调度选择的时候如何覆盖所有的 Node</h3>
<p>如果你想要理解这一个特性的内部细节，那么请仔细阅读这一章节。</p>
<p>在将 Pod 调度到节点上时，为了让集群中所有节点都有公平的机会去运行这些 Pod，
调度器将会以轮询的方式覆盖全部的 Node。
你可以将 Node 列表想象成一个数组。调度器从数组的头部开始筛选可调度节点，
依次向后直到可调度节点的数量达到 <code>percentageOfNodesToScore</code> 参数的要求。
在对下一个 Pod 进行调度的时候，前一个 Pod 调度筛选停止的 Node 列表的位置，
将会来作为这次调度筛选 Node 开始的位置。</p>
<p>如果集群中的 Node 在多个区域，那么调度器将从不同的区域中轮询 Node，
来确保不同区域的 Node 接受可调度性检查。如下例，考虑两个区域中的六个节点：</p>
<pre><code>Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
</code></pre>
<p>调度器将会按照如下的顺序去评估 Node 的可调度性：</p>
<pre><code>Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
</code></pre>
<p>在评估完所有 Node 后，将会返回到 Node 1，从头开始。</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>参见 <a href=/zh/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler 配置参考 (v1beta3)</a></li>
</ul>
</div>
</div>
</main>
</div>
</div>
<footer class=d-print-none>
<div class=footer__links>
<nav>
<a class=text-white href=/zh/docs/home/>主页</a>
<a class=text-white href=/zh/blog/>博客</a>
<a class=text-white href=/zh/training/>培训</a>
<a class=text-white href=/zh/partners/>合作伙伴</a>
<a class=text-white href=/zh/community/>社区</a>
<a class=text-white href=/zh/case-studies/>案例分析</a>
</nav>
</div>
<div class=container-fluid>
<div class=row>
<div class="col-6 col-sm-2 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list">
<a class=text-white target=_blank href=https://discuss.kubernetes.io>
<i class="fa fa-envelope"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter>
<a class=text-white target=_blank href=https://twitter.com/kubernetesio>
<i class="fab fa-twitter"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar>
<a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
<i class="fas fa-calendar-alt"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube>
<a class=text-white target=_blank href=https://youtube.com/kubernetescommunity>
<i class="fab fa-youtube"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank href=https://slack.k8s.io>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute>
<a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide>
<i class="fas fa-edit"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow">
<a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes>
<i class="fab fa-stack-overflow"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-8 text-center order-sm-2">
<small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small>
<br>
<small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small>
<br>
<small class=text-white>ICP license: 京ICP备17074266号-3</small>
</div>
</div>
</div>
</footer>
</div>
<script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script>
<script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script>
<script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script>
</body>
</html>