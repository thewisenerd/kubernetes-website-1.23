<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – 生产级别的容器编排系统</title><link>https://kubernetes.io/zh/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/zh/</link></image><atom:link href="https://kubernetes.io/zh/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Dockershim：历史背景</title><link>https://kubernetes.io/zh/blog/2022/05/03/dockershim-historical-context/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/05/03/dockershim-historical-context/</guid><description>
&lt;!--
layout: blog
title: "Dockershim: The Historical Context"
date: 2022-05-03
slug: dockershim-historical-context
-->
&lt;!--
**Author:** Kat Cosgrove
Dockershim has been removed as of Kubernetes v1.24, and this is a positive move for the project. However, context is important for fully understanding something, be it socially or in software development, and this deserves a more in-depth review. Alongside the dockershim removal in Kubernetes v1.24, we’ve seen some confusion (sometimes at a panic level) and dissatisfaction with this decision in the community, largely due to a lack of context around this removal. The decision to deprecate and eventually remove dockershim from Kubernetes was not made quickly or lightly. Still, it’s been in the works for so long that many of today’s users are newer than that decision, and certainly newer than the choices that led to the dockershim being necessary in the first place.
So what is the dockershim, and why is it going away?
-->
&lt;p>&lt;strong>作者：&lt;/strong> Kat Cosgrove&lt;/p>
&lt;p>自 Kubernetes v1.24 起，Dockershim 已被删除，这对项目来说是一个积极的举措。
然而，背景对于充分理解某事很重要，无论是社交还是软件开发，这值得更深入的审查。
除了 Kubernetes v1.24 中的 dockershim 移除之外，我们在社区中看到了一些
混乱（有时处于恐慌级别）和对这一决定的不满，主要是由于缺乏有关此删除背景的了解。
弃用并最终从 Kubernetes 中删除 dockershim 的决定并不是迅速或轻率地做出的。
尽管如此，它已经工作了很长时间，以至于今天的许多用户都比这个决定更新，
更不用提当初为何引入 dockershim 了。&lt;/p>
&lt;p>那么 dockershim 是什么，为什么它会消失呢？&lt;/p>
&lt;!--
In the early days of Kubernetes, we only supported one container runtime. That runtime was Docker Engine. Back then, there weren’t really a lot of other options out there and Docker was the dominant tool for working with containers, so this was not a controversial choice. Eventually, we started adding more container runtimes, like rkt and hypernetes, and it became clear that Kubernetes users want a choice of runtimes working best for them. So Kubernetes needed a way to allow cluster operators the flexibility to use whatever runtime they choose.
-->
&lt;p>在 Kubernetes 的早期，我们只支持一个容器运行时，那个运行时就是 Docker Engine。
那时，并没有太多其他选择，而 Docker 是使用容器的主要工具，所以这不是一个有争议的选择。
最终，我们开始添加更多的容器运行时，比如 rkt 和 hypernetes，很明显 Kubernetes 用户
希望选择最适合他们的运行时。 因此，Kubernetes 需要一种方法来允许集群操作员灵活地使用
他们选择的任何运行时。&lt;/p>
&lt;!--
The [Container Runtime Interface](/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) (CRI) was released to allow that flexibility. The introduction of CRI was great for the project and users alike, but it did introduce a problem: Docker Engine’s use as a container runtime predates CRI, and Docker Engine is not CRI-compatible. To solve this issue, a small software shim (dockershim) was introduced as part of the kubelet component specifically to fill in the gaps between Docker Engine and CRI, allowing cluster operators to continue using Docker Engine as their container runtime largely uninterrupted.
-->
&lt;p>&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">容器运行时接口&lt;/a> (CRI)
已发布以支持这种灵活性。 CRI 的引入对项目和用户来说都很棒，但它确实引入了一个问题：Docker Engine
作为容器运行时的使用早于 CRI，并且 Docker Engine 不兼容 CRI。 为了解决这个问题，在 kubelet 组件
中引入了一个小型软件 shim (dockershim)，专门用于填补 Docker Engine 和 CRI 之间的空白，
允许集群操作员继续使用 Docker Engine 作为他们的容器运行时基本上不间断。&lt;/p>
&lt;!--
However, this little software shim was never intended to be a permanent solution. Over the course of years, its existence has introduced a lot of unnecessary complexity to the kubelet itself. Some integrations are inconsistently implemented for Docker because of this shim, resulting in an increased burden on maintainers, and maintaining vendor-specific code is not in line with our open source philosophy. To reduce this maintenance burden and move towards a more collaborative community in support of open standards, [KEP-2221 was introduced](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim), proposing the removal of the dockershim. With the release of Kubernetes v1.20, the deprecation was official.
-->
&lt;p>然而，这个小软件 shim 从来没有打算成为一个永久的解决方案。 多年来，它的存在给 kubelet
本身带来了许多不必要的复杂性。 由于这个 shim，Docker 的一些集成实现不一致，导致维护人员
的负担增加，并且维护特定于供应商的代码不符合我们的开源理念。 为了减少这种维护负担并朝着支
持开放标准的更具协作性的社区迈进，[引入了 KEP-2221](&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-">https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-&lt;/a> remove-dockershim)，
建议移除 dockershim。 随着 Kubernetes v1.20 的发布，正式弃用。&lt;/p>
&lt;!--
We didn’t do a great job communicating this, and unfortunately, the deprecation announcement led to some panic within the community. Confusion around what this meant for Docker as a company, if container images built by Docker would still run, and what Docker Engine actually is led to a conflagration on social media. This was our fault; we should have more clearly communicated what was happening and why at the time. To combat this, we released [a blog](/blog/2020/12/02/dont-panic-kubernetes-and-docker/) and [accompanying FAQ](/blog/2020/12/02/dockershim-faq/) to allay the community’s fears and correct some misconceptions about what Docker is and how containers work within Kubernetes. As a result of the community’s concerns, Docker and Mirantis jointly agreed to continue supporting the dockershim code in the form of [cri-dockerd](https://www.mirantis.com/blog/the-future-of-dockershim-is-cri-dockerd/), allowing you to continue using Docker Engine as your container runtime if need be. For the interest of users who want to try other runtimes, like containerd or cri-o, [migration documentation was written](/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/).
-->
&lt;p>我们没有很好地传达这一点，不幸的是，弃用公告在社区内引起了一些恐慌。关于这对 Docker 作为
一家公司意味着什么，Docker 构建的容器镜像是否仍然可以运行，以及 Docker Engine 究竟是
什么导致了社交媒体上的一场大火，人们感到困惑。这是我们的错；我们应该更清楚地传达当时发生
的事情和原因。为了解决这个问题，我们发布了&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/">一篇博客&lt;/a>
和&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">相应的 FAQ&lt;/a> 以减轻社区的恐惧并纠正对
Docker 是什么以及容器如何在 Kubernetes 中工作的一些误解。由于社区的关注，Docker 和 Mirantis
共同决定继续以 [cri-dockerd] 的形式支持 dockershim 代码（https://www.mirantis.com/blog/the-future-of-dockershim-is -cri-dockerd/)，
允许你在需要时继续使用 Docker Engine 作为容器运行时。对于想要尝试其他运行时（如 containerd 或 cri-o）
的用户，&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">已编写迁移文档&lt;/a>。&lt;/p>
&lt;!--
We later [surveyed the community](https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/) and [discovered that there are still many users with questions and concerns](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim). In response, Kubernetes maintainers and the CNCF committed to addressing these concerns by extending documentation and other programs. In fact, this blog post is a part of this program. With so many end users successfully migrated to other runtimes, and improved documentation, we believe that everyone has a paved way to migration now.
-->
&lt;p>我们后来&lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">调查了社区&lt;/a>
&lt;a href="https://kubernetes.io/zh/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim">发现还有很多用户有疑问和顾虑&lt;/a>。
作为回应，Kubernetes 维护人员和 CNCF 承诺通过扩展文档和其他程序来解决这些问题。 事实上，这篇博文是
这个计划的一部分。 随着如此多的最终用户成功迁移到其他运行时，以及改进的文档，我们相信每个人现在都为迁移铺平了道路。&lt;/p>
&lt;!--
Docker is not going away, either as a tool or as a company. It’s an important part of the cloud native community and the history of the Kubernetes project. We wouldn’t be where we are without them. That said, removing dockershim from kubelet is ultimately good for the community, the ecosystem, the project, and open source at large. This is an opportunity for all of us to come together to support open standards, and we’re glad to be doing so with the help of Docker and the community.
-->
&lt;p>Docker 不会消失，无论是作为一种工具还是作为一家公司。 它是云原生社区的重要组成部分，
也是 Kubernetes 项目的历史。 没有他们，我们就不会是现在的样子。 也就是说，从 kubelet
中删除 dockershim 最终对社区、生态系统、项目和整个开源都有好处。 这是我们所有人齐心协力
支持开放标准的机会，我们很高兴在 Docker 和社区的帮助下这样做。&lt;/p></description></item><item><title>Blog: Kubernetes 1.24 的删除和弃用</title><link>https://kubernetes.io/zh/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes Removals and Deprecations In 1.24"
date: 2022-04-07
slug: upcoming-changes-in-kubernetes-1-24
-->
&lt;!--
**Author**: Mickey Boxell (Oracle)
As Kubernetes evolves, features and APIs are regularly revisited and removed. New features may offer
an alternative or improved approach to solving existing problems, motivating the team to remove the
old approach.
-->
&lt;p>&lt;strong>作者&lt;/strong>： Mickey Boxell (Oracle)&lt;/p>
&lt;p>随着 Kubernetes 的发展，特性和 API 会定期被重新访问和删除。
新特性可能会提供替代或改进的方法，来解决现有的问题，从而激励团队移除旧的方法。&lt;/p>
&lt;!--
We want to make sure you are aware of the changes coming in the Kubernetes 1.24 release. The release will
**deprecate** several (beta) APIs in favor of stable versions of the same APIs. The major change coming
in the Kubernetes 1.24 release is the
[removal of Dockershim](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim).
This is discussed below and will be explored in more depth at release time. For an early look at the
changes coming in Kubernetes 1.24, take a look at the in-progress
[CHANGELOG](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md).
-->
&lt;p>我们希望确保你了解 Kubernetes 1.24 版本的变化。 该版本将 &lt;strong>弃用&lt;/strong> 一些（测试版/beta）API，
转而支持相同 API 的稳定版本。 Kubernetes 1.24 版本的主要变化是
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">移除 Dockershim&lt;/a>。
这将在下面讨论，并将在发布时更深入地探讨。
要提前了解 Kubernetes 1.24 中的更改，请查看正在更新中的
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">CHANGELOG&lt;/a>。&lt;/p>
&lt;!--
## A note about Dockershim
It's safe to say that the removal receiving the most attention with the release of Kubernetes 1.24
is Dockershim. Dockershim was deprecated in v1.20. As noted in the [Kubernetes 1.20 changelog](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation):
"Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet
uses a module called "dockershim" which implements CRI support for Docker and it has seen maintenance
issues in the Kubernetes community." With the upcoming release of Kubernetes 1.24, the Dockershim will
finally be removed.
-->
&lt;h2 id="a-note-about-dockershim">关于 Dockershim &lt;/h2>
&lt;p>可以肯定地说，随着 Kubernetes 1.24 的发布，最受关注的删除是 Dockershim。
Dockershim 在 1.20 版本中已被弃用。 如
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">Kubernetes 1.20 变更日志&lt;/a>中所述：
&amp;quot;Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet
uses a module called &amp;quot;dockershim&amp;quot; which implements CRI support for Docker and it has seen maintenance
issues in the Kubernetes community.&amp;quot;
随着即将发布的 Kubernetes 1.24，Dockershim 将最终被删除。&lt;/p>
&lt;!--
In the article [Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/),
the authors succinctly captured the change's impact and encouraged users to remain calm:
> Docker as an underlying runtime is being deprecated in favor of runtimes that use the
> Container Runtime Interface (CRI) created for Kubernetes. Docker-produced images
> will continue to work in your cluster with all runtimes, as they always have.
-->
&lt;p>在文章&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/">别慌: Kubernetes 和 Docker&lt;/a> 中，
作者简洁地记述了变化的影响，并鼓励用户保持冷静：&lt;/p>
&lt;blockquote>
&lt;p>弃用 Docker 这个底层运行时，转而支持符合为 Kubernetes 创建的容器运行接口
Container Runtime Interface (CRI) 的运行时。
Docker 构建的镜像，将在你的集群的所有运行时中继续工作，一如既往。&lt;/p>
&lt;/blockquote>
&lt;!--
Several guides have been created with helpful information about migrating from dockershim
to container runtimes that are directly compatible with Kubernetes. You can find them on the
[Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/)
page in the Kubernetes documentation.
-->
&lt;p>已经有一些文档指南，提供了关于从 dockershim 迁移到与 Kubernetes 直接兼容的容器运行时的有用信息。
你可以在 Kubernetes 文档中的&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/">从 dockershim 迁移&lt;/a>
页面上找到它们。&lt;/p>
&lt;!--
For more information about why Kubernetes is moving away from dockershim, check out the aptly
named: [Kubernetes is Moving on From Dockershim](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/)
and the [updated dockershim removal FAQ](/blog/2022/02/17/dockershim-faq/).
Take a look at the [Is Your Cluster Ready for v1.24?](/blog/2022/03/31/ready-for-dockershim-removal/) post to learn about how to ensure your cluster continues to work after upgrading from v1.23 to v1.24.
-->
&lt;p>有关 Kubernetes 为何不再使用 dockershim 的更多信息，
请参见：&lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">Kubernetes 正在离开 Dockershim&lt;/a>
和&lt;a href="https://kubernetes.io/zh/blog/2022/02/17/dockershim-faq/">最新的弃用 Dockershim 的常见问题&lt;/a>。&lt;/p>
&lt;p>查看&lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">你的集群准备好使用 v1.24 了吗？&lt;/a> 一文，
了解如何确保你的集群在从 1.23 版本升级到 1.24 版本后继续工作。&lt;/p>
&lt;!--
## The Kubernetes API removal and deprecation process
Kubernetes contains a large number of components that evolve over time. In some cases, this
evolution results in APIs, flags, or entire features, being removed. To prevent users from facing
breaking changes, Kubernetes contributors adopted a feature [deprecation policy](/docs/reference/using-api/deprecation-policy/).
This policy ensures that stable APIs may only be deprecated when a newer stable version of that
same API is available and that APIs have a minimum lifetime as indicated by the following stability levels:
* Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.
* Beta or pre-release API versions must be supported for 3 releases after deprecation.
* Alpha or experimental API versions may be removed in any release without prior deprecation notice.
-->
&lt;h2 id="the-Kubernetes-api-removal-and-deprecation-process">Kubernetes API 删除和弃用流程 &lt;/h2>
&lt;p>Kubernetes 包含大量随时间演变的组件。在某些情况下，这种演变会导致 API、标志或整个特性被删除。
为了防止用户面对重大变化，Kubernetes 贡献者采用了一项特性&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/deprecation-policy/">弃用策略&lt;/a>。
此策略确保仅当同一 API 的较新稳定版本可用并且
API 具有以下稳定性级别所指示的最短生命周期时，才可能弃用稳定版本 API：&lt;/p>
&lt;ul>
&lt;li>正式发布 (GA) 或稳定的 API 版本可能被标记为已弃用，但不得在 Kubernetes 的主版本中删除。&lt;/li>
&lt;li>测试版（beta）或预发布 API 版本在弃用后必须支持 3 个版本。&lt;/li>
&lt;li>Alpha 或实验性 API 版本可能会在任何版本中被删除，恕不另行通知。&lt;/li>
&lt;/ul>
&lt;!--
Removals follow the same deprecation policy regardless of whether an API is removed due to a beta feature
graduating to stable or because that API was not proven to be successful. Kubernetes will continue to make
sure migration options are documented whenever APIs are removed.
-->
&lt;p>删除遵循相同的弃用政策，无论 API 是由于 测试版（beta）功能逐渐稳定还是因为该
API 未被证明是成功的而被删除。
Kubernetes 将继续确保在删除 API 时提供用来迁移的文档。&lt;/p>
&lt;!--
**Deprecated** APIs are those that have been marked for removal in a future Kubernetes release. **Removed**
APIs are those that are no longer available for use in current, supported Kubernetes versions after having
been deprecated. These removals have been superseded by newer, stable/generally available (GA) APIs.
-->
&lt;p>&lt;strong>弃用的&lt;/strong> API 是指那些已标记为在未来 Kubernetes 版本中被删除的 API。
&lt;strong>删除的&lt;/strong> API 是指那些在被弃用后不再可用于当前受支持的 Kubernetes 版本的 API。
这些删除已被更新的、稳定的/普遍可用的 (GA) API 所取代。&lt;/p>
&lt;!--
## API removals, deprecations, and other changes for Kubernetes 1.24
* [Dynamic kubelet configuration](https://github.com/kubernetes/enhancements/issues/281): `DynamicKubeletConfig` is used to enable the dynamic configuration of the kubelet. The `DynamicKubeletConfig` flag was deprecated in Kubernetes 1.22. In v1.24, this feature gate will be removed from the kubelet. See [Reconfigure kubelet](/docs/tasks/administer-cluster/reconfigure-kubelet/). Refer to the ["Dynamic kubelet config is removed" KEP](https://github.com/kubernetes/enhancements/issues/281) for more information.
-->
&lt;h2 id="api-removals-deprecations-and-other-changes-for-kubernetes-1.24">Kubernetes 1.24 的 API 删除、弃用和其他更改 &lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/281">动态 kubelet 配置&lt;/a>: &lt;code>DynamicKubeletConfig&lt;/code>
用于启用 kubelet 的动态配置。Kubernetes 1.22 中弃用 &lt;code>DynamicKubeletConfig&lt;/code> 标志。
在 1.24 版本中，此特性门控将从 kubelet 中移除。请参阅&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">重新配置 kubelet&lt;/a>。
更多详细信息，请参阅&lt;a href="https://github.com/kubernetes/enhancements/issues/281">“删除动态 kubelet 配置” 的 KEP&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [Dynamic log sanitization](https://github.com/kubernetes/kubernetes/pull/107207): The experimental dynamic log sanitization feature is deprecated and will be removed in v1.24. This feature introduced a logging filter that could be applied to all Kubernetes system components logs to prevent various types of sensitive information from leaking via logs. Refer to [KEP-1753: Kubernetes system components logs sanitization](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation) for more information and an [alternative approach](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#alternatives=).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107207">动态日志清洗&lt;/a>：实验性的动态日志清洗功能已被弃用，
将在 1.24 版本中被删除。该功能引入了一个日志过滤器，可以应用于所有 Kubernetes 系统组件的日志，
以防止各种类型的敏感信息通过日志泄漏。有关更多信息和替代方法，请参阅
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation">KEP-1753: Kubernetes 系统组件日志清洗&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* In-tree provisioner to CSI driver migration: This applies to a number of in-tree plugins, including [Portworx](https://github.com/kubernetes/enhancements/issues/2589). Refer to the [In-tree Storage Plugin to CSI Migration Design Doc](https://github.com/kubernetes/design-proposals-archive/blob/main/storage/csi-migration.md#background-and-motivations) for more information.
-->
&lt;ul>
&lt;li>树内驱动（In-tree provisioner）向 CSI 卷迁移：这适用于许多树内插件，
包括 &lt;a href="https://github.com/kubernetes/enhancements/issues/2589">Portworx&lt;/a>。
参见&lt;a href="https://github.com/kubernetes/design-proposals-archive/blob/main/storage/csi-migration.md#background-and-motivations">树内存储插件向 CSI 卷迁移的设计文档&lt;/a>
了解更多信息。&lt;/li>
&lt;/ul>
&lt;!--
* [Removing Dockershim from kubelet](https://github.com/kubernetes/enhancements/issues/2221): the Container Runtime Interface (CRI) for Docker (i.e. Dockershim) is currently a built-in container runtime in the kubelet code base. It was deprecated in v1.20. As of v1.24, the kubelet will no longer have dockershim. Check out this blog on [what you need to do be ready for v1.24](/blog/2022/03/31/ready-for-dockershim-removal/).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2221">从 kubelet 中移除 Dockershim&lt;/a>：Docker
的容器运行时接口(CRI)（即 Dockershim）目前是 kubelet 代码中内置的容器运行时。 它在 1.20 版本中已被弃用。
从 1.24 版本开始，kubelet 已经移除 dockershim。 查看这篇博客，
&lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">了解你需要为 1.24 版本做些什么&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [Storage capacity tracking for pod scheduling](https://github.com/kubernetes/enhancements/issues/1472): The CSIStorageCapacity API supports exposing currently available storage capacity via CSIStorageCapacity objects and enhances scheduling of pods that use CSI volumes with late binding. In v1.24, the CSIStorageCapacity API will be stable. The API graduating to stable initates the deprecation of the v1beta1 CSIStorageCapacity API. Refer to the [Storage Capacity Constraints for Pod Scheduling KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking) for more information.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">pod 调度的存储容量追踪&lt;/a>：CSIStorageCapacity API
支持通过 CSIStorageCapacity 对象暴露当前可用的存储容量，并增强了使用带有延迟绑定的 CSI 卷的 Pod 的调度。
CSIStorageCapacity API 自 1.24 版本起提供稳定版本。升级到稳定版的 API 将弃用 v1beta1 CSIStorageCapacity API。
更多信息请参见 &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking">Pod 调度存储容量约束 KEP&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [The `master` label is no longer present on kubeadm control plane nodes](https://github.com/kubernetes/kubernetes/pull/107533). For new clusters, the label 'node-role.kubernetes.io/master' will no longer be added to control plane nodes, only the label 'node-role.kubernetes.io/control-plane' will be added. For more information, refer to [KEP-2067: Rename the kubeadm "master" label and taint](https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107533">kubeadm 控制面节点上不再存在 &lt;code>master&lt;/code> 标签&lt;/a>。
对于新集群，控制平面节点将不再添加 'node-role.kubernetes.io/master' 标签，
只会添加 'node-role.kubernetes.io/control-plane' 标签。更多信息请参考
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067">KEP-2067：重命名 kubeadm “master” 标签和污点&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [VolumeSnapshot v1beta1 CRD will be removed](https://github.com/kubernetes/enhancements/issues/177). Volume snapshot and restore functionality for Kubernetes and the [Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md) (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, entered beta in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.21 and is now unsupported. Refer to [KEP-177: CSI Snapshot](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot) and [kubernetes-csi/external-snapshotter](https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v4.1.0) for more information.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD 在 1.24 版本中将被移除&lt;/a>。
Kubernetes 和 &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface&lt;/a> (CSI)
的卷快照和恢复功能，在 1.20 版本中进入测试版。该功能提供标准化 API 设计 (CRD ) 并为 CSI 卷驱动程序添加了 PV 快照/恢复支持，
VolumeSnapshot v1beta1 在 1.21 版本中已被弃用，现在不受支持。更多信息请参考
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177：CSI 快照&lt;/a>和
&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v4.1.0">kubernetes-csi/external-snapshotter&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
## What to do
### Dockershim removal
As stated earlier, there are several guides about
[Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/).
You can start with [Finding what container runtime are on your nodes](/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/).
If your nodes are using dockershim, there are other possible Docker Engine dependencies such as
Pods or third-party tools executing Docker commands or private registries in the Docker configuration file. You can follow the
[Check whether Dockershim deprecation affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/) guide to review possible
Docker Engine dependencies. Before upgrading to v1.24, you decide to either remain using Docker Engine and
[Migrate Docker Engine nodes from dockershim to cri-dockerd](/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/) or migrate to a CRI-compatible runtime. Here's a guide to
[change the container runtime on a node from Docker Engine to containerd](/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/).
-->
&lt;h2 id="what-to-do">需要做什么 &lt;/h2>
&lt;h3 id="dockershim-removal">删除 Dockershim &lt;/h3>
&lt;p>如前所述，有一些关于从 &lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/">dockershim 迁移&lt;/a>的指南。
你可以&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">从查明节点上所使用的容器运行时&lt;/a>开始。
如果你的节点使用 dockershim，则还有其他可能的 Docker Engine 依赖项，
例如 Pod 或执行 Docker 命令的第三方工具或 Docker 配置文件中的私有注册表。
你可以按照&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">检查弃用 Dockershim 对你的影响&lt;/a>
的指南来查看可能的 Docker 引擎依赖项。在升级到 1.24 版本之前， 你决定要么继续使用 Docker Engine 并
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">将 Docker Engine 节点从 dockershim 迁移到 cri-dockerd&lt;/a>，
要么迁移到与 CRI 兼容的运行时。这是&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">将节点上的容器运行时从 Docker Engine 更改为 containerd&lt;/a> 的指南。&lt;/p>
&lt;!--
### `kubectl convert`
The [`kubectl convert`](/docs/tasks/tools/included/kubectl-convert-overview/) plugin for `kubectl`
can be helpful to address migrating off deprecated APIs. The plugin facilitates the conversion of
manifests between different API versions, for example, from a deprecated to a non-deprecated API
version. More general information about the API migration process can be found in the [Deprecated API Migration Guide](/docs/reference/using-api/deprecation-guide/).
Follow the [install `kubectl convert` plugin](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin)
documentation to download and install the `kubectl-convert` binary.
-->
&lt;h3 id="kubectl-convert">&lt;code>kubectl convert&lt;/code> &lt;/h3>
&lt;p>kubectl 的 &lt;a href="https://kubernetes.io/zh/docs/tasks/tools/included/kubectl-convert-overview/">&lt;code>kubectl convert&lt;/code>&lt;/a>
插件有助于解决弃用 API 的迁移问题。该插件方便了不同 API 版本之间清单的转换，
例如，从弃用的 API 版本到非弃用的 API 版本。关于 API 迁移过程的更多信息可以在
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/">已弃用 API 的迁移指南&lt;/a>中找到。按照
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin">安装 &lt;code>kubectl convert&lt;/code> 插件&lt;/a>
文档下载并安装 &lt;code>kubectl-convert&lt;/code> 二进制文件。&lt;/p>
&lt;!--
### Looking ahead
The Kubernetes 1.25 and 1.26 releases planned for later this year will stop serving beta versions
of several currently stable Kubernetes APIs. The v1.25 release will also remove PodSecurityPolicy,
which was deprecated with Kubernetes 1.21 and will not graduate to stable. See [PodSecurityPolicy
Deprecation: Past, Present, and Future](/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/) for more information.
-->
&lt;h3 id="looking-ahead">展望未来 &lt;/h3>
&lt;p>计划在今年晚些时候发布的 Kubernetes 1.25 和 1.26 版本，将停止提供一些
Kubernetes API 的 beta 版本，这些 API 当前为稳定版。1.25 版本还将删除 PodSecurityPolicy，
它已在 Kubernetes 1.21 版本中被弃用，并且不会升级到稳定版。有关详细信息，请参阅
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy 弃用：过去、现在和未来&lt;/a>。&lt;/p>
&lt;!--
The official [list of API removals planned for Kubernetes 1.25](/docs/reference/using-api/deprecation-guide/#v1-25) is:
-->
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/deprecation-guide/#v1-25">Kubernetes 1.25 计划移除的 API 的官方列表&lt;/a>是：&lt;/p>
&lt;ul>
&lt;li>The beta CronJob API (batch/v1beta1)&lt;/li>
&lt;li>The beta EndpointSlice API (discovery.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta Event API (events.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta1)&lt;/li>
&lt;li>The beta PodDisruptionBudget API (policy/v1beta1)&lt;/li>
&lt;li>The beta PodSecurityPolicy API (policy/v1beta1)&lt;/li>
&lt;li>The beta RuntimeClass API (node.k8s.io/v1beta1)&lt;/li>
&lt;/ul>
&lt;!--
The official [list of API removals planned for Kubernetes 1.26](/docs/reference/using-api/deprecation-guide/#v1-26) is:
* The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)
* The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)
-->
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/deprecation-guide/#v1-25">Kubernetes 1.25 计划移除的 API 的官方列表&lt;/a>是：&lt;/p>
&lt;ul>
&lt;li>The beta FlowSchema 和 PriorityLevelConfiguration API (flowcontrol.apiserver.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)&lt;/li>
&lt;/ul>
&lt;!--
### Want to know more?
Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:
* [Kubernetes 1.21](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation)
* [Kubernetes 1.22](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation)
* [Kubernetes 1.23](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation)
* We will formally announce the deprecations that come with [Kubernetes 1.24](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation) as part of the CHANGELOG for that release.
For information on the process of deprecation and removal, check out the official Kubernetes [deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api) document.
-->
&lt;h3 id="want-to-know-more">了解更多&lt;/h3>
&lt;p>Kubernetes 发行说明中宣告了弃用信息。你可以在以下版本的发行说明中看到待弃用的公告：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>我们将正式宣布 &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a> 的弃用信息，
作为该版本 CHANGELOG 的一部分。&lt;/li>
&lt;/ul>
&lt;p>有关弃用和删除过程的信息，请查看 Kubernetes 官方&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">弃用策略&lt;/a> 文档。&lt;/p></description></item><item><title>Blog: 认识我们的贡献者 - 亚太地区（澳大利亚-新西兰地区）</title><link>https://kubernetes.io/zh/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</link><pubDate>Wed, 16 Mar 2022 12:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</guid><description>
&lt;!--
layout: blog
title: "Meet Our Contributors - APAC (Aus-NZ region)"
date: 2022-03-16T12:00:00+0000
slug: meet-our-contributors-au-nz-ep-02
canonicalUrl: https://www.kubernetes.dev/blog/2022/03/14/meet-our-contributors-au-nz-ep-02/
-->
&lt;!--
**Authors &amp; Interviewers:** [Anubhav Vardhan](https://github.com/anubha-v-ardhan), [Atharva Shinde](https://github.com/Atharva-Shinde), [Avinesh Tripathi](https://github.com/AvineshTripathi), [Brad McCoy](https://github.com/bradmccoydev), [Debabrata Panigrahi](https://github.com/Debanitrkl), [Jayesh Srivastava](https://github.com/jayesh-srivastava), [Kunal Verma](https://github.com/verma-kunal), [Pranshu Srivastava](https://github.com/PranshuSrivastava), [Priyanka Saggu](github.com/Priyankasaggu11929/), [Purneswar Prasad](https://github.com/PurneswarPrasad), [Vedant Kakde](https://github.com/vedant-kakde)
-->
&lt;p>&lt;strong>作者和采访者：&lt;/strong>
&lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a>,
&lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a>,
&lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>,
&lt;a href="https://github.com/bradmccoydev">Brad McCoy&lt;/a>,
&lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>,
&lt;a href="https://github.com/jayesh-srivastava">Jayesh Srivastava&lt;/a>,
&lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a>,
&lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a>,
&lt;a href="github.com/Priyankasaggu11929/">Priyanka Saggu&lt;/a>,
&lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>,
&lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;hr>
&lt;!--
Good day, everyone 👋
-->
&lt;p>大家好👋&lt;/p>
&lt;!--
Welcome back to the second episode of the "Meet Our Contributors" blog post series for APAC.
-->
&lt;p>欢迎来到亚太地区的”认识我们的贡献者”博文系列第二期。&lt;/p>
&lt;!--
This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project.
-->
&lt;p>这篇文章将介绍来自澳大利亚和新西兰地区的四位杰出贡献者，
他们在上游 Kubernetes 项目中承担着不同子项目的领导者和社区贡献者的角色。&lt;/p>
&lt;!--
So, without further ado, let's get straight to the blog.
-->
&lt;p>闲话少说，让我们直接进入主题。&lt;/p>
&lt;h2 id="caleb-woodbine-https-github-com-bobymcbobs">&lt;a href="https://github.com/BobyMCbobs">Caleb Woodbine&lt;/a>&lt;/h2>
&lt;!--
Caleb Woodbine is currently a member of the ii.nz organisation.
-->
&lt;p>Caleb Woodbine 目前是 ii.nz 组织的成员。&lt;/p>
&lt;!--
He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from [Hippie Hacker](https://github.com/hh), a fellow contributor from New Zealand.
-->
&lt;p>他于 2018 年作为 Kubernetes Conformance 工作组的成员开始为 Kubernetes 项目做贡献。
他积极向上，他从一位来自新西兰的贡献者 &lt;a href="https://github.com/hh">Hippie Hacker&lt;/a> 的早期指导中受益匪浅。&lt;/p>
&lt;!--
He has made major contributions to Kubernetes project since then through `SIG k8s-infra` and `k8s-conformance` working group.
-->
&lt;p>他在 &lt;code>SIG k8s-infra&lt;/code> 和 &lt;code>k8s-conformance&lt;/code> 工作组为 Kubernetes 项目做出了重大贡献。&lt;/p>
&lt;!--
Caleb is also a co-organizer of the [CloudNative NZ](https://www.meetup.com/cloudnative-nz/) community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities.
-->
&lt;p>Caleb 也是 &lt;a href="https://www.meetup.com/cloudnative-nz/">CloudNative NZ&lt;/a>
社区活动的联合组织者，该活动旨在扩大 Kubernetes 项目在整个新西兰的影响力，以鼓励科技教育和改善就业机会。&lt;/p>
&lt;!--
> _There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally._
-->
&lt;blockquote>
&lt;p>&lt;em>亚太地区需要更多的外联活动，教育工作者和大学必须学习 Kubernetes，因为他们非常缓慢，
而且已经落后了8年多。新西兰倾向于在海外付费，而不是教育当地人最新的云技术。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="dylan-graham-https-github-com-dylangraham">&lt;a href="https://github.com/DylanGraham">Dylan Graham&lt;/a>&lt;/h2>
&lt;!--
Dylan Graham is a cloud engineer from Adeliade, Australia. He has been contributing to the upstream Kubernetes project since 2018.
-->
&lt;p>Dylan Graham 是来自澳大利亚 Adeliade 的云计算工程师。自 2018 年以来，他一直在为上游 Kubernetes 项目做出贡献。&lt;/p>
&lt;!--
He stated that being a part of such a large-scale project was initially overwhelming, but that the community's friendliness and openness assisted him in getting through it.
-->
&lt;p>他表示，成为如此大项目的一份子，最初压力是比较大的，但社区的友好和开放帮助他度过了难关。&lt;/p>
&lt;!--
He began by contributing to the project documentation and is now mostly focused on the community support for the APAC region.
-->
&lt;p>开始在项目文档方面做贡献，现在主要致力于为亚太地区提供社区支持。&lt;/p>
&lt;!--
He believes that consistent attendance at community/project meetings, taking on project tasks, and seeking community guidance as needed can help new aspiring developers become effective contributors.
-->
&lt;p>他相信，持续参加社区/项目会议，承担项目任务，并在需要时寻求社区指导，可以帮助有抱负的新开发人员成为有效的贡献者。&lt;/p>
&lt;!--
> _The feeling of being a part of a large community is really special. I've met some amazing people, even some before the pandemic in real life :)_
-->
&lt;blockquote>
&lt;p>&lt;em>成为大社区的一份子感觉真的很特别。我遇到了一些了不起的人，甚至是在现实生活中疫情发生之前。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="hippie-hacker-https-github-com-hh">&lt;a href="https://github.com/hh">Hippie Hacker&lt;/a>&lt;/h2>
&lt;!--
Hippie has worked for the CNCF.io as a Strategic Initiatives contractor from New Zealand for almost 5+ years. He is an active contributor to k8s-infra, API conformance testing, Cloud provider conformance submissions, and apisnoop.cncf.io domains of the upstream Kubernetes &amp; CNCF projects.
-->
&lt;p>Hippie 来自新西兰，曾在 CNCF.io 作为战略计划承包商工作 5 年多。他是 k8s-infra、
API 一致性测试、云提供商一致性提交以及上游 Kubernetes 和 CNCF 项目 apisnoop.cncf.io 域的积极贡献者。&lt;/p>
&lt;!--
He recounts their early involvement with the Kubernetes project, which began roughly 5 years ago when their firm, ii.nz, demonstrated [network booting from a Raspberry Pi using PXE and running Gitlab in-cluster to install Kubernetes on servers](https://ii.nz/post/bringing-the-cloud-to-your-community/).
-->
&lt;p>他讲述了他们早期参与 Kubernetes 项目的情况，该项目始于大约 5 年前，当时他们的公司 ii.nz
演示了&lt;a href="https://ii.nz/post/bringing-the-cloud-to-your-community/">使用 PXE 从 Raspberry Pi 启动网络，并在集群中运行Gitlab，以便在服务器上安装 Kubernetes &lt;/a>&lt;/p>
&lt;!--
He describes their own contributing experience as someone who, at first, tried to do all of the hard lifting on their own, but eventually saw the benefit of group contributions which reduced burnout and task division which allowed folks to keep moving forward on their own momentum.
-->
&lt;p>他描述了自己的贡献经历：一开始，他试图独自完成所有艰巨的任务，但最终看到了团队协作贡献的好处，
分工合作减少了过度疲劳，这让人们能够凭借自己的动力继续前进。&lt;/p>
&lt;!--
He recommends that new contributors use pair programming.
-->
&lt;p>他建议新的贡献者结对编程。&lt;/p>
&lt;!--
> _The cross pollination of approaches and two pairs of eyes on the same work can often yield a much more amplified effect than a PR comment / approval alone can afford._
-->
&lt;blockquote>
&lt;p>&lt;em>针对一个项目，多人关注和交叉交流往往比单独的评审、批准 PR 能产生更大的效果。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="nick-young-https-github-com-youngnick">&lt;a href="https://github.com/youngnick">Nick Young&lt;/a>&lt;/h2>
&lt;!--
Nick Young works at VMware as a technical lead for Contour, a CNCF ingress controller. He was active with the upstream Kubernetes project from the beginning, and eventually became the chair of the LTS working group, where he advocated user concerns. He is currently the SIG Network Gateway API subproject's maintainer.
-->
&lt;p>Nick Young 在 VMware 工作，是 CNCF 入口控制器 Contour 的技术负责人。
他从一开始就积极参与上游 Kubernetes 项目，最终成为 LTS 工作组的主席，
他提倡关注用户。他目前是 SIG Network Gateway API 子项目的维护者。&lt;/p>
&lt;!--
His contribution path was notable in that he began working on major areas of the Kubernetes project early on, skewing his trajectory.
-->
&lt;p>他的贡献之路是引人注目的，因为他很早就在 Kubernetes 项目的主要领域工作，这改变了他的轨迹。&lt;/p>
&lt;!--
He asserts the best thing a new contributor can do is to "start contributing". Naturally, if it is relevant to their employment, that is excellent; however, investing non-work time in contributing can pay off in the long run in terms of work. He believes that new contributors, particularly those who are currently Kubernetes users, should be encouraged to participate in higher-level project discussions.
-->
&lt;p>他断言，一个新贡献者能做的最好的事情就是“开始贡献”。当然，如果与他的工作息息相关，那好极了;
然而，把非工作时间投入到贡献中去，从长远来看可以在工作上获得回报。
他认为，应该鼓励新的贡献者，特别是那些目前是 Kubernetes 用户的人，参与到更高层次的项目讨论中来。&lt;/p>
&lt;!--
> _Just being active and contributing will get you a long way. Once you've been active for a while, you'll find that you're able to answer questions, which will mean you're asked questions, and before you know it you are an expert._
-->
&lt;blockquote>
&lt;p>&lt;em>只要积极主动，做出贡献，你就可以走很远。一旦你活跃了一段时间，你会发现你能够解答别人的问题，
这意味着会有人请教你或和你讨论，在你意识到这一点之前，你就已经是专家了。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;!--
If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.
-->
&lt;p>如果你对我们接下来应该采访的人有任何意见/建议，请在 #sig-contribex 中告知我们。
非常感谢你的建议。我们很高兴有更多的人帮助我们接触到社区中更优秀的人。&lt;/p>
&lt;!--
We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋
-->
&lt;p>我们下期再见。祝你有个愉快的贡献之旅!👋&lt;/p></description></item><item><title>Blog: 更新：弃用 Dockershim 的常见问题</title><link>https://kubernetes.io/zh/blog/2022/02/17/dockershim-faq/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/02/17/dockershim-faq/</guid><description>
&lt;!--
layout: blog
title: "Updated: Dockershim Removal FAQ"
linkTitle: "Dockershim Removal FAQ"
date: 2022-02-17
slug: dockershim-faq
aliases: [ '/dockershim' ]
-->
&lt;!--
**This is an update to the original [Dockershim Deprecation FAQ](/blog/2020/12/02/dockershim-faq/) article,
published in late 2020.**
-->
&lt;p>&lt;strong>本文是针对2020年末发布的&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">弃用 Dockershim 的常见问题&lt;/a>的博客更新。&lt;/strong>&lt;/p>
&lt;!--
This document goes over some frequently asked questions regarding the
deprecation and removal of _dockershim_, that was
[announced](/blog/2020/12/08/kubernetes-1-20-release-announcement/)
as a part of the Kubernetes v1.20 release. For more detail
on what that means, check out the blog post
[Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/).
-->
&lt;p>本文回顾了自 Kubernetes v1.20 版本&lt;a href="https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/">宣布&lt;/a>弃用
Dockershim 以来所引发的一些常见问题。关于弃用细节以及这些细节背后的含义，请参考博文
&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/">别慌: Kubernetes 和 Docker&lt;/a>。&lt;/p>
&lt;!--
Also, you can read [check whether dockershim removal affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/)
to determine how much impact the removal of dockershim would have for you
or for your organization.
-->
&lt;p>你还可以查阅：&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">检查弃用 Dockershim 对你的影响&lt;/a>这篇文章，
以确定弃用 dockershim 会对你或你的组织带来多大的影响。&lt;/p>
&lt;!--
As the Kubernetes 1.24 release has become imminent, we've been working hard to try to make this a smooth transition.
-->
&lt;p>随着 Kubernetes 1.24 版本的发布迫在眉睫，我们一直在努力尝试使其能够平稳升级顺利过渡。&lt;/p>
&lt;!--
- We've written a blog post detailing our [commitment and next steps](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/).
- We believe there are no major blockers to migration to [other container runtimes](/docs/setup/production-environment/container-runtimes/#container-runtimes).
- There is also a [Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/) guide available.
- We've also created a page to list
[articles on dockershim removal and on using CRI-compatible runtimes](/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/).
That list includes some of the already mentioned docs, and also covers selected external sources
(including vendor guides).
-->
&lt;ul>
&lt;li>我们已经写了一篇博文，详细说明了我们的&lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">承诺和后续操作&lt;/a>。&lt;/li>
&lt;li>我们我们相信可以无障碍的迁移到其他&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#container-runtimes">容器运行时&lt;/a>。&lt;/li>
&lt;li>我们撰写了 &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">dockershim 迁移指南&lt;/a>供你参考。&lt;/li>
&lt;li>我们还创建了一个页面来列出&lt;a href="https://kubernetes.io/zh/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/">有关 dockershim 移除和使用 CRI 兼容运行时的文章&lt;/a>。
该列表包括一些已经提到的文档，还涵盖了选定的外部资源（包括供应商指南）。&lt;/li>
&lt;/ul>
&lt;!--
### Why is the dockershim being removed from Kubernetes?
-->
&lt;h3 id="为什么会从-kubernetes-中移除-dockershim">为什么会从 Kubernetes 中移除 dockershim ？&lt;/h3>
&lt;!--
Early versions of Kubernetes only worked with a specific container runtime:
Docker Engine. Later, Kubernetes added support for working with other container runtimes.
The CRI standard was [created](/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) to
enable interoperability between orchestrators (like Kubernetes) and many different container
runtimes.
Docker Engine doesn't implement that interface (CRI), so the Kubernetes project created
special code to help with the transition, and made that _dockershim_ code part of Kubernetes
itself.
-->
&lt;p>Kubernetes 的早期版本仅适用于特定的容器运行时：Docker Engine。
后来，Kubernetes 增加了对使用其他容器运行时的支持。&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">创建&lt;/a> CRI
标准是为了实现编排器（如 Kubernetes）和许多不同的容器运行时之间交互操作。
Docker Engine 没有实现（CRI）接口，因此 Kubernetes 项目创建了特殊代码来帮助过渡，
并使 dockershim 代码成为 Kubernetes 的一部分。&lt;/p>
&lt;!--
The dockershim code was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
In fact, maintaining dockershim had become a heavy burden on the Kubernetes maintainers.
-->
&lt;p>dockershim 代码一直是一个临时解决方案（因此得名：shim）。
你可以阅读 &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Kubernetes 移除 Dockershim 增强方案&lt;/a>
以了解相关的社区讨论和计划。
事实上，维护 dockershim 已经成为 Kubernetes 维护者的沉重负担。&lt;/p>
&lt;!--
Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.
-->
&lt;p>此外，在较新的 CRI 运行时中实现了与 dockershim 不兼容的功能，例如 cgroups v2 和用户命名空间。
取消对 dockershim 的支持将加速这些领域的发展。&lt;/p>
&lt;!--
### Can I still use Docker Engine in Kubernetes 1.23?
-->
&lt;h3 id="在-kubernetes-1-23-版本中还可以使用-docker-engine-吗">在 Kubernetes 1.23 版本中还可以使用 Docker Engine 吗？&lt;/h3>
&lt;!--
Yes, the only thing changed in 1.20 is a single warning log printed at [kubelet]
startup if using Docker Engine as the runtime. You'll see this warning in all versions up to 1.23. The dockershim removal occurs in Kubernetes 1.24.
-->
&lt;p>可以使用，在 1.20 版本中唯一的改动是，如果使用 Docker Engine，
在 &lt;a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
启动时会打印一个警告日志。
你将在 1.23 版本及以前版本看到此警告。dockershim 将在 Kubernetes 1.24 版本中移除 。&lt;/p>
&lt;!--
### When will dockershim be removed?
-->
&lt;h3 id="什么时候移除-dockershim">什么时候移除 dockershim ？&lt;/h3>
&lt;!--
Given the impact of this change, we are using an extended deprecation timeline.
Removal of dockershim is scheduled for Kubernetes v1.24, see [Dockershim Removal Kubernetes Enhancement Proposal][drkep].
The Kubernetes project will be working closely with vendors and other ecosystem groups to ensure
a smooth transition and will evaluate things as the situation evolves.
-->
&lt;p>考虑到此变更带来的影响，我们使用了一个加长的废弃时间表。
dockershim 计划在 Kubernetes v1.24 中进行移除，
参见 &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Kubernetes 移除 Dockershim 增强方案&lt;/a>。
Kubernetes 项目将与供应商和其他生态系统组织密切合作，以确保平稳过渡，并将依据事态的发展评估后续事项。&lt;/p>
&lt;!--
### Can I still use Docker Engine as my container runtime?
-->
&lt;h3 id="我还可以使用-docker-engine-作为我的容器运行时吗">我还可以使用 Docker Engine 作为我的容器运行时吗？&lt;/h3>
&lt;!--
First off, if you use Docker on your own PC to develop or test containers: nothing changes.
You can still use Docker locally no matter what container runtime(s) you use for your
Kubernetes clusters. Containers make this kind of interoperability possible.
-->
&lt;p>首先，如果你在自己的电脑上使用 Docker 用来做开发或测试容器：它将与之前没有任何变化。
无论你为 Kubernetes 集群使用什么容器运行时，你都可以在本地使用 Docker。容器使这种交互成为可能。&lt;/p>
&lt;!--
Mirantis and Docker have [committed][mirantis] to maintaining a replacement adapter for
Docker Engine, and to maintain that adapter even after the in-tree dockershim is removed
from Kubernetes. The replacement adapter is named [`cri-dockerd`](https://github.com/Mirantis/cri-dockerd).
-->
&lt;p>Mirantis 和 Docker 已&lt;a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/">承诺&lt;/a>
为 Docker Engine 维护一个替代适配器，
并在 dockershim 从 Kubernetes 移除后维护该适配器。
替代适配器名为 &lt;a href="https://github.com/Mirantis/cri-dockerd">&lt;code>cri-dockerd&lt;/code>&lt;/a>。&lt;/p>
&lt;!--
### Will my existing container images still work?
-->
&lt;h3 id="我现有的容器镜像还能正常工作吗">我现有的容器镜像还能正常工作吗？&lt;/h3>
&lt;!--
Yes, the images produced from `docker build` will work with all CRI implementations.
All your existing images will still work exactly the same.
-->
&lt;p>当然可以，&lt;code>docker build&lt;/code> 创建的镜像适用于任何 CRI 实现。
所有你的现有镜像将和往常一样工作。&lt;/p>
&lt;!--
#### What about private images?
-->
&lt;h3 id="私有镜像呢">私有镜像呢？&lt;/h3>
&lt;!--
Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.
-->
&lt;p>当然可以。所有 CRI 运行时均支持在 Kubernetes 中相同的拉取（pull）Secret 配置，
无论是通过 PodSpec 还是 ServiceAccount。&lt;/p>
&lt;!--
### Are Docker and containers the same thing?
-->
&lt;h3 id="docker-和容器是一回事吗">Docker 和容器是一回事吗？&lt;/h3>
&lt;!--
Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.
-->
&lt;p>Docker 普及了 Linux 容器模式，并在开发底层技术方面发挥了重要作用，
但是 Linux 中的容器已经存在了很长时间。容器的生态相比于 Docker 具有更宽广的领域。
OCI 和 CRI 等标准帮助许多工具在我们的生态系统中发展壮大，
其中一些替代了 Docker 的某些方面，而另一些则增强了现有功能。&lt;/p>
&lt;!--
### Are there examples of folks using other runtimes in production today?
-->
&lt;h3 id="现在是否有在生产系统中使用其他运行时的例子">现在是否有在生产系统中使用其他运行时的例子？&lt;/h3>
&lt;!--
All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.
-->
&lt;p>Kubernetes 所有项目在所有版本中出产的工件（Kubernetes 二进制文件）都经过了验证。&lt;/p>
&lt;!--
Additionally, the [kind] project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the [CRI-O] runtime in production since June 2019.
-->
&lt;p>此外，&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> 项目使用 containerd 已经有一段时间了，并且提高了其用例的稳定性。
Kind 和 containerd 每天都会被多次使用来验证对 Kubernetes 代码库的任何更改。
其他相关项目也遵循同样的模式，从而展示了其他容器运行时的稳定性和可用性。
例如，OpenShift 4.x 从 2019 年 6 月以来，就一直在生产环境中使用 &lt;a href="https://cri-o.io/">CRI-O&lt;/a> 运行时。&lt;/p>
&lt;!--
For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation ([CNCF]).
-->
&lt;p>至于其他示例和参考资料，你可以查看 containerd 和 CRI-O 的使用者列表，
这两个容器运行时是云原生基金会（&lt;a href="https://cncf.io">CNCF&lt;/a>）下的项目。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### People keep referencing OCI, what is that?
-->
&lt;h3 id="人们总在谈论-oci-它是什么">人们总在谈论 OCI，它是什么？&lt;/h3>
&lt;!--
OCI stands for the [Open Container Initiative], which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of [runc], which is the underlying default runtime for both
[containerd] and [CRI-O]. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.
-->
&lt;p>OCI 是 &lt;a href="https://opencontainers.org/about/overview/">Open Container Initiative&lt;/a> 的缩写，
它标准化了容器工具和底层实现之间的大量接口。
它们维护了打包容器镜像（OCI image）和运行时（OCI runtime）的标准规范。
它们还以 &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> 的形式维护了一个 runtime-spec 的真实实现，
这也是 &lt;a href="https://containerd.io/">containerd&lt;/a> 和 &lt;a href="https://cri-o.io/">CRI-O&lt;/a> 依赖的默认运行时。
CRI 建立在这些底层规范之上，为管理容器提供端到端的标准。&lt;/p>
&lt;!--
### Which CRI implementation should I use?
-->
&lt;h3 id="我应该用哪个-cri-实现">我应该用哪个 CRI 实现？&lt;/h3>
&lt;!--
That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the [CNCF landscape] in case another would be an
even better fit for your environment.
-->
&lt;p>这是一个复杂的问题，依赖于许多因素。
如果你正在使用 Docker，迁移到 containerd 应该是一个相对容易地转换，并将获得更好的性能和更少的开销。
然而，我们鼓励你探索 &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape&lt;/a>
提供的所有选项，做出更适合你的选择。&lt;/p>
&lt;!--
### What should I look out for when changing CRI implementations?
-->
&lt;h3 id="当切换-cri-实现时-应该注意什么">当切换 CRI 实现时，应该注意什么？&lt;/h3>
&lt;!--
While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:
-->
&lt;p>虽然 Docker 和大多数 CRI（包括 containerd）之间的底层容器化代码是相同的，
但其周边部分却存在差异。迁移时要考虑如下常见事项：&lt;/p>
&lt;!--
- Logging configuration
- Runtime resource limitations
- Node provisioning scripts that call docker or use docker via it's control socket
- Kubectl plugins that require docker CLI or the control socket
- Tools from the Kubernetes project that require direct access to Docker Engine
(for example: the deprecated `kube-imagepuller` tool)
- Configuration of functionality like `registry-mirrors` and insecure registries
- Other support scripts or daemons that expect Docker Engine to be available and are run
outside of Kubernetes (for example, monitoring or security agents)
- GPUs or special hardware and how they integrate with your runtime and Kubernetes
-->
&lt;ul>
&lt;li>日志配置&lt;/li>
&lt;li>运行时的资源限制&lt;/li>
&lt;li>调用 docker 或通过其控制套接字使用 docker 的节点配置脚本&lt;/li>
&lt;li>需要访问 docker 命令或控制套接字的 kubectl 插件&lt;/li>
&lt;li>需要直接访问 Docker Engine 的 Kubernetes 工具（例如：已弃用的 'kube-imagepuller' 工具）&lt;/li>
&lt;li>&lt;code>registry-mirrors&lt;/code> 和不安全注册表等功能的配置&lt;/li>
&lt;li>保障 Docker Engine 可用、且运行在 Kubernetes 之外的脚本或守护进程（例如：监视或安全代理）&lt;/li>
&lt;li>GPU 或特殊硬件，以及它们如何与你的运行时和 Kubernetes 集成&lt;/li>
&lt;/ul>
&lt;!--
If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your `dockerd` configuration, you’ll need to adapt that for your new container
runtime where possible.
-->
&lt;p>如果你只是用了 Kubernetes 资源请求/限制或基于文件的日志收集 DaemonSet，它们将继续稳定工作，
但是如果你用了自定义了 dockerd 配置，则可能需要为新的容器运行时做一些适配工作。&lt;/p>
&lt;!--
Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the [`crictl`][cr] tool as a drop-in replacement (see [mapping from docker cli to crictl](https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl)) and for the
latter you can use newer container build options like [img], [buildah],
[kaniko], or [buildkit-cli-for-kubectl] that don’t require Docker.
-->
&lt;p>另外还有一个需要关注的点，那就是当创建镜像时，系统维护或嵌入容器方面的任务将无法工作。
对于前者，可以用 &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> 工具作为临时替代方案
(参阅&lt;a href="https://kubernetes.io/zh/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl">从 docker cli 到 crictl 的映射&lt;/a>)。
对于后者，可以用新的容器创建选项，例如
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>、
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>、
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a> 或
&lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a>，
他们都不需要 Docker。&lt;/p>
&lt;!--
For containerd, you can start with their [documentation] to see what configuration
options are available as you migrate things over.
-->
&lt;p>对于 containerd，你可查阅有关它的&lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">文档&lt;/a>，
获取迁移时可用的配置选项。&lt;/p>
&lt;!--
For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on [Container Runtimes].
-->
&lt;p>有关如何在 Kubernetes 中使用 containerd 和 CRI-O 的说明，
请参阅 &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">Kubernetes 相关文档&lt;/a>。&lt;/p>
&lt;!--
### What if I have more questions?
-->
&lt;h3 id="我还有其他问题怎么办">我还有其他问题怎么办？&lt;/h3>
&lt;!--
If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: https://discuss.kubernetes.io/.
-->
&lt;p>如果你使用了供应商支持的 Kubernetes 发行版，你可以咨询供应商他们产品的升级计划。
对于最终用户的问题，请把问题发到我们的最终用户社区的论坛：https://discuss.kubernetes.io/。&lt;/p>
&lt;!--
You can discuss the decision to remove dockershim via a dedicated
[GitHub issue](https://github.com/kubernetes/kubernetes/issues/106917).
-->
&lt;p>你可以通过专用 &lt;a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub 问题&lt;/a>
讨论删除 dockershim 的决定。&lt;/p>
&lt;!--
You can also check out the excellent blog post
[Wait, Docker is deprecated in Kubernetes now?][dep] a more in-depth technical
discussion of the changes.
-->
&lt;p>你也可以看看这篇优秀的博客文章：&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">等等，Docker 被 Kubernetes 弃用了?&lt;/a>
对这些变化进行更深入的技术讨论。&lt;/p>
&lt;!--
### Is there any tooling that can help me find dockershim in use
-->
&lt;h3 id="是否有任何工具可以帮助我找到正在使用的-dockershim">是否有任何工具可以帮助我找到正在使用的 dockershim&lt;/h3>
&lt;!--
Yes! The [Detector for Docker Socket (DDS)][dds] is a kubectl plugin that you can
install and then use to check your cluster. DDS can detect if active Kubernetes workloads
are mounting the Docker Engine socket (`docker.sock`) as a volume.
Find more details and usage patterns in the DDS project's [README][dds].
-->
&lt;p>是的！ &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">Docker Socket 检测器 (DDS)&lt;/a> 是一个 kubectl 插件，
你可以安装它用于检查你的集群。 DDS 可以检测运行中的 Kubernetes
工作负载是否将 Docker 引擎套接字 (&lt;code>docker.sock&lt;/code>) 作为卷挂载。
在 DDS 项目的 &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">README&lt;/a> 中查找更多详细信息和使用方法。&lt;/p>
&lt;!--
### Can I have a hug?
-->
&lt;h3 id="我可以加入吗">我可以加入吗？&lt;/h3>
&lt;!--
Yes, we're still giving hugs as requested. 🤗🤗🤗
-->
&lt;p>当然，只要你愿意，随时随地欢迎。🤗🤗🤗&lt;/p></description></item><item><title>Blog: SIG Node CI 子项目庆祝测试改进两周年</title><link>https://kubernetes.io/zh/blog/2022/02/16/sig-node-ci-subproject-celebrates/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/02/16/sig-node-ci-subproject-celebrates/</guid><description>
&lt;!--
---
layout: blog
title: 'SIG Node CI Subproject Celebrates Two Years of Test Improvements'
date: 2022-02-16
slug: sig-node-ci-subproject-celebrates
canonicalUrl: https://www.kubernetes.dev/blog/2022/02/16/sig-node-ci-subproject-celebrates-two-years-of-test-improvements/
url: /zh/blog/2022/02/sig-node-ci-subproject-celebrates
---
-->
&lt;p>&lt;strong>作者：&lt;/strong> Sergey Kanzhelev (Google), Elana Hashman (Red Hat)&lt;/p>
&lt;!--**Authors:** Sergey Kanzhelev (Google), Elana Hashman (Red Hat)-->
&lt;!--Ensuring the reliability of SIG Node upstream code is a continuous effort
that takes a lot of behind-the-scenes effort from many contributors.
There are frequent releases of Kubernetes, base operating systems,
container runtimes, and test infrastructure that result in a complex matrix that
requires attention and steady investment to "keep the lights on."
In May 2020, the Kubernetes node special interest group ("SIG Node") organized a new
subproject for continuous integration (CI) for node-related code and tests. Since its
inauguration, the SIG Node CI subproject has run a weekly meeting, and even the full hour
is often not enough to complete triage of all bugs, test-related PRs and issues, and discuss all
related ongoing work within the subgroup.-->
&lt;p>保证 SIG 节点上游代码的可靠性是一项持续的工作，需要许多贡献者在幕后付出大量努力。
Kubernetes、基础操作系统、容器运行时和测试基础架构的频繁发布，导致了一个复杂的矩阵，
需要关注和稳定的投资来“保持灯火通明”。2020 年 5 月，Kubernetes Node 特殊兴趣小组
（“SIG Node”）为节点相关代码和测试组织了一个新的持续集成（CI）子项目。自成立以来，SIG Node CI
子项目每周举行一次会议，即使一整个小时通常也不足以完成对所有缺陷、测试相关的 PR 和问题的分类，
并讨论组内所有相关的正在进行的工作。&lt;/p>
&lt;!--Over the past two years, we've fixed merge-blocking and release-blocking tests, reducing time to merge Kubernetes contributors' pull requests thanks to reduced test flakes. When we started, Node test jobs only passed 42% of the time, and through our efforts, we now ensure a consistent >90% job pass rate. We've closed 144 test failure issues and merged 176 pull requests just in kubernetes/kubernetes. And we've helped subproject participants ascend the Kubernetes contributor ladder, with 3 new org members, 6 new reviewers, and 2 new approvers.-->
&lt;p>在过去两年中，我们修复了阻塞合并和阻塞发布的测试，由于减少了测试缺陷，缩短了合并 Kubernetes
贡献者的拉取请求的时间。通过我们的努力，任务通过率由开始时 42% 提高至稳定大于 90% 。我们已经解决了 144 个测试失败问题，
并在 kubernetes/kubernetes 中合并了 176 个拉取请求。
我们还帮助子项目参与者提升了 Kubernetes 贡献者的等级，新增了 3 名组织成员、6 名评审员和 2 名审批员。&lt;/p>
&lt;!--The Node CI subproject is an approachable first stop to help new contributors
get started with SIG Node. There is a low barrier to entry for new contributors
to address high-impact bugs and test fixes, although there is a long
road before contributors can climb the entire contributor ladder:
it took over a year to establish two new approvers for the group.
The complexity of all the different components that power Kubernetes nodes
and its test infrastructure requires a sustained investment over a long period
for developers to deeply understand the entire system,
both at high and low levels of detail.-->
&lt;p>Node CI 子项目是一个可入门的第一站，帮助新参与者开始使用 SIG Node。对于新贡献者来说，
解决影响较大的缺陷和测试修复的门槛很低，尽管贡献者要攀登整个贡献者阶梯还有很长的路要走：
为该团队培养了两个新的审批人花了一年多的时间。为 Kubernetes 节点及其测试基础设施提供动力的所有
不同组件的复杂性要求开发人员在很长一段时间内进行持续投资，
以深入了解整个系统，从宏观到微观。&lt;/p>
&lt;!--We have several regular contributors at our meetings, however; our reviewers
and approvers pool is still small. It is our goal to continue to grow
contributors to ensure a sustainable distribution of work
that does not just fall to a few key approvers.-->
&lt;p>虽然在我们的会议上有几个比较固定的贡献者；但是我们的评审员和审批员仍然很少。
我们的目标是继续增加贡献者，以确保工作的可持续分配，而不仅仅是少数关键批准者。&lt;/p>
&lt;!--It's not always obvious how subprojects within SIGs are formed, operate,
and work. Each is unique to its sponsoring SIG and tailored to the projects
that the group is intended to support. As a group that has welcomed many
first-time SIG Node contributors, we'd like to share some of the details and
accomplishments over the past two years,
helping to demystify our inner workings and celebrate the hard work
of all our dedicated contributors!-->
&lt;p>SIG 中的子项目如何形成、运行和工作并不总是显而易见的。每一个都是其背后的 SIG 所独有的，
并根据该小组打算支持的项目量身定制。作为一个欢迎了许多第一次 SIG Node 贡献者的团队，
我们想分享过去两年的一些细节和成就，帮助揭开我们内部工作的神秘面纱，并庆祝我们所有专注贡献者的辛勤工作！&lt;/p>
&lt;!--## Timeline-->
&lt;h2 id="时间线">时间线&lt;/h2>
&lt;!--***May 2020.*** SIG Node CI group was formed on May 11, 2020, with more than
[30 volunteers](https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib)
signed up, to improve SIG Node CI signal and overall observability.
Victor Pickard focused on getting
[testgrid jobs](https://testgrid.k8s.io/sig-node) passing
when Ning Liao suggested forming a group around this effort and came up with
the [original group charter document](https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf).
The SIG Node chairs sponsored group creation with Victor as a subproject lead.
Sergey Kanzhelev joined Victor shortly after as a co-lead.-->
&lt;p>&lt;em>&lt;strong>2020 年 5 月&lt;/strong>&lt;/em> SIG Node CI 组于 2020 年 5 月 11 日成立，超过
&lt;a href="https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib">30 名志愿者&lt;/a>
注册，以改进 SIG Node CI 信号和整体可观测性。
Victor Pickard 专注于让 &lt;a href="https://testgrid.k8s.io/sig-node">testgrid 可以运行&lt;/a> ，
当时 Ning Liao 建议围绕这项工作组建一个小组，并提出
&lt;a href="https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf">最初的小组章程文件&lt;/a> 。
SIG Node 赞助成立以 Victor 作为子项目负责人的小组。Sergey Kanzhelev 不久后就加入 Victor，担任联合领导人。&lt;/p>
&lt;!--At the kick-off meeting, we discussed which tests to concentrate on fixing first
and discussed merge-blocking and release-blocking tests, many of which were failing due
to infrastructure issues or buggy test code.-->
&lt;p>在启动会议上，我们讨论了应该首先集中精力修复哪些测试，并讨论了阻塞合并和阻塞发布的测试，
其中许多测试由于基础设施问题或错误的测试代码而失败。&lt;/p>
&lt;!--The subproject launched weekly hour-long meetings to discuss ongoing work
discussion and triage.-->
&lt;p>该子项目每周召开一小时的会议，讨论正在进行的工作会谈和分类。&lt;/p>
&lt;!--***June 2020.*** Morgan Bauer, Karan Goel, and Jorge Alarcon Ochoa were
recognized as reviewers for the SIG Node CI group for their contributions,
helping significantly with the early stages of the subproject.
David Porter and Roy Yang also joined the SIG test failures GitHub team.-->
&lt;p>&lt;em>&lt;strong>2020 年 6 月&lt;/strong>&lt;/em> Morgan Bauer 、 Karan Goel 和 Jorge Alarcon Ochoa
因其贡献而被公认为 SIG Node CI 小组的评审员，为该子项目的早期阶段提供了重要帮助。
David Porter 和 Roy Yang 也加入了 SIG 检测失败的 GitHub 测试团队。&lt;/p>
&lt;!--***August 2020.*** All merge-blocking and release-blocking tests were passing,
with some flakes. However, only 42% of all SIG Node test jobs were green, as there
were many flakes and failing tests.-->
&lt;p>&lt;em>&lt;strong>2020 年 8 月&lt;/strong>&lt;/em> 所有的阻塞合并和阻塞发布的测试都通过了，伴有一些逻辑问题。
然而，只有 42% 的 SIG Node 测试作业是绿色的，
因为有许多逻辑错误和失败的测试。&lt;/p>
&lt;!--***October 2020.*** Amim Knabben becomes a Kubernetes org member for his
contributions to the subproject.-->
&lt;p>&lt;em>&lt;strong>2020 年 10 月&lt;/strong>&lt;/em> Amim Knabben 因对子项目的贡献成为 Kubernetes 组织成员。&lt;/p>
&lt;!--***January 2021.*** With healthy presubmit and critical periodic jobs passing,
the subproject discussed its goal for cleaning up the rest of periodic tests
and ensuring they passed without flakes.-->
&lt;p>&lt;em>&lt;strong>2021 年 1 月&lt;/strong>&lt;/em> 随着健全的预提交和关键定期工作的通过，子项目讨论了清理其余定期测试并确保其顺利通过的目标。&lt;/p>
&lt;!--Elana Hashman joined the subproject, stepping up to help lead it after
Victor's departure.-->
&lt;p>Elana Hashman 加入了这个子项目，在 Victor 离开后帮助领导该项目。&lt;/p>
&lt;!--***February 2021.*** Artyom Lukianov becomes a Kubernetes org member for his
contributions to the subproject.-->
&lt;p>&lt;em>&lt;strong>2021 年 2 月&lt;/strong>&lt;/em> Artyom Lukianov 因其对子项目的贡献成为 Kubernetes 组织成员。&lt;/p>
&lt;!--***August 2021.*** After SIG Node successfully ran a [bug scrub](https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ)
to clean up its bug backlog, the scope of the meeting was extended to
include bug triage to increase overall reliability, anticipating issues
before they affect the CI signal.-->
&lt;p>&lt;em>&lt;strong>2021 年 8 月&lt;/strong>&lt;/em> 在 SIG Node 成功运行 &lt;a href="https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ">bug scrub&lt;/a>
以清理其累积的缺陷之后，会议的范围扩大到包括缺陷分类以提高整体可靠性，
在问题影响 CI 信号之前预测问题。&lt;/p>
&lt;!--Subproject leads Elana Hashman and Sergey Kanzhelev are both recognized as
approvers on all node test code, supported by SIG Node and SIG Testing.-->
&lt;p>子项目负责人 Elana Hashman 和 Sergey Kanzhelev 都被认为是所有节点测试代码的审批人，由 SIG node 和 SIG Testing 支持。&lt;/p>
&lt;!--***September 2021.*** After significant deflaking progress with serial tests in
the 1.22 release spearheaded by Francesco Romani, the subproject set a goal
for getting the serial job fully passing by the 1.23 release date.-->
&lt;p>&lt;em>&lt;strong>2021 年 9 月&lt;/strong>&lt;/em> 在 Francesco Romani 牵头的 1.22 版本系列测试取得重大进展后，
该子项目设定了一个目标，即在 1.23 发布日期之前让串行任务完全通过。&lt;/p>
&lt;!--Mike Miranda becomes a Kubernetes org member for his contributions
to the subproject.-->
&lt;p>Mike Miranda 因其对子项目的贡献成为 Kubernetes 组织成员。&lt;/p>
&lt;!--***November 2021.*** Throughout 2021, SIG Node had no merge or
release-blocking test failures. Many flaky tests from past releases are removed
from release-blocking dashboards as they had been fully cleaned up.-->
&lt;p>&lt;em>&lt;strong>2021 年 11 月&lt;/strong>&lt;/em> 在整个 2021 年， SIG Node 没有合并或发布的测试失败。
过去版本中的许多古怪测试都已从阻止发布的仪表板中删除，因为它们已被完全清理。&lt;/p>
&lt;!--Danielle Lancashire was recognized as a reviewer for SIG Node's subgroup, test code.-->
&lt;p>Danielle Lancashire 被公认为 SIG Node 子组测试代码的评审员。&lt;/p>
&lt;!--The final node serial tests were completely fixed. The serial tests consist of
many disruptive and slow tests which tend to be flakey and are hard
to troubleshoot. By the 1.23 release freeze, the last serial tests were
fixed and the job was passing without flakes.-->
&lt;p>最终节点系列测试已完全修复。系列测试由许多中断性和缓慢的测试组成，这些测试往往是碎片化的，很难排除故障。
到 1.23 版本冻结时，最后一次系列测试已修复，作业顺利通过。&lt;/p>
&lt;!--[![Slack announcement that Serial tests are green](serial-tests-green.png)](https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900)-->
&lt;p>&lt;a href="https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900">&lt;img src="serial-tests-green.png" alt="宣布系列测试为绿色">&lt;/a>&lt;/p>
&lt;!--The 1.23 release got a special shout out for the tests quality and CI signal.
The SIG Node CI subproject was proud to have helped contribute to such
a high-quality release, in part due to our efforts in identifying
and fixing flakes in Node and beyond.-->
&lt;p>1.23 版本在测试质量和 CI 信号方面得到了特别的关注。SIG Node CI 子项目很自豪能够为这样一个高质量的发布做出贡献，
部分原因是我们在识别和修复节点内外的碎片方面所做的努力。&lt;/p>
&lt;!--[![Slack shoutout that release was mostly green](release-mostly-green.png)](https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200)-->
&lt;p>&lt;a href="https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200">&lt;img src="release-mostly-green.png" alt="Slack 大声宣布发布的版本大多是绿色的">&lt;/a>&lt;/p>
&lt;!--***December 2021.*** An estimated 90% of test jobs were passing at the time of
the 1.23 release (up from 42% in August 2020).-->
&lt;p>&lt;em>&lt;strong>2021 年 12 月&lt;/strong>&lt;/em> 在 1.23 版本发布时，估计有 90% 的测试工作通过了测试（2020 年 8 月为 42%）。&lt;/p>
&lt;!--Dockershim code was removed from Kubernetes. This affected nearly half of SIG Node's
test jobs and the SIG Node CI subproject reacted quickly and retargeted all the
tests. SIG Node was the first SIG to complete test migrations off dockershim,
providing examples for other affected SIGs. The vast majority of new jobs passed
at the time of introduction without further fixes required. The [effort of
removing dockershim](https://k8s.io/dockershim)) from Kubernetes is ongoing.
There are still some wrinkles from the dockershim removal as we uncover more
dependencies on dockershim, but we plan to stabilize all test jobs
by the 1.24 release.-->
&lt;p>Dockershim 代码已从 Kubernetes 中删除。这影响了 SIG Node 近一半的测试作业，
SIG Node CI 子项目反应迅速，并重新确定了所有测试的目标。
SIG Node 是第一个完成 dockershim 外测试迁移的 SIG ，为其他受影响的 SIG 提供了示例。
绝大多数新工作在引入时都已通过，无需进一步修复。
从 Kubernetes 中&lt;a href="https://k8s.io/dockershim">将 dockershim 除名的工作&lt;/a> 正在进行中。
随着我们发现 dockershim 对 dockershim 的依赖性越来越大，dockershim 的删除仍然存在一些问题，
但我们计划在 1.24 版本之前确保所有测试任务稳定。&lt;/p>
&lt;!--## Statistics-->
&lt;h2 id="统计数据">统计数据&lt;/h2>
&lt;!--Our regular meeting attendees and subproject participants for the past few months:-->
&lt;p>我们过去几个月的定期会议与会者和子项目参与者：&lt;/p>
&lt;ul>
&lt;li>Aditi Sharma&lt;/li>
&lt;li>Artyom Lukianov&lt;/li>
&lt;li>Arnaud Meukam&lt;/li>
&lt;li>Danielle Lancashire&lt;/li>
&lt;li>David Porter&lt;/li>
&lt;li>Davanum Srinivas&lt;/li>
&lt;li>Elana Hashman&lt;/li>
&lt;li>Francesco Romani&lt;/li>
&lt;li>Matthias Bertschy&lt;/li>
&lt;li>Mike Miranda&lt;/li>
&lt;li>Paco Xu&lt;/li>
&lt;li>Peter Hunt&lt;/li>
&lt;li>Ruiwen Zhao&lt;/li>
&lt;li>Ryan Phillips&lt;/li>
&lt;li>Sergey Kanzhelev&lt;/li>
&lt;li>Skyler Clark&lt;/li>
&lt;li>Swati Sehgal&lt;/li>
&lt;li>Wenjun Wu&lt;/li>
&lt;/ul>
&lt;!--The [kubernetes/test-infra](https://github.com/kubernetes/test-infra/) source code repository contains test definitions. The number of
Node PRs just in that repository:
- 2020 PRs (since May): [183](https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+)
- 2021 PRs: [264](https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+)-->
&lt;p>&lt;a href="https://github.com/kubernetes/test-infra/">kubernetes/test-infra&lt;/a> 源代码存储库包含测试定义。该存储库中的节点 PR 数：&lt;/p>
&lt;ul>
&lt;li>2020 年 PR（自 5 月起）：&lt;a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+">183&lt;/a>&lt;/li>
&lt;li>2021 年 PR：&lt;a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+">264&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--Triaged issues and PRs on CI board (including triaging away from the subgroup scope):
- 2020 (since May)：[132](https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31)
- 2021: [532](https：//github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+)-->
&lt;p>CI 委员会上的问题和 PRs 分类（包括子组范围之外的分类）：&lt;/p>
&lt;ul>
&lt;li>2020 年（自 5 月起）：&lt;a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31">132&lt;/a>&lt;/li>
&lt;li>2021 年：&lt;a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+">532&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--## Future-->
&lt;h2 id="未来">未来&lt;/h2>
&lt;!--Just "keeping the lights on" is a bold task and we are committed to improving this experience.
We are working to simplify the triage and review processes for SIG Node.
Specifically, we are working on better test organization, naming,
and tracking:-->
&lt;p>只是“保持灯亮”是一项大胆的任务，我们致力于改善这种体验。
我们正在努力简化 SIG Node 的分类和审查流程。&lt;/p>
&lt;p>具体来说，我们正在致力于更好的测试组织、命名和跟踪：&lt;/p>
&lt;!-- - https://github.com/kubernetes/enhancements/pull/3042
- https://github.com/kubernetes/test-infra/issues/24641
- [Kubernetes SIG-Node CI Testgrid Tracker](https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0)-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/pull/3042">https://github.com/kubernetes/enhancements/pull/3042&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/issues/24641">https://github.com/kubernetes/test-infra/issues/24641&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0">Kubernetes SIG Node CI 测试网格跟踪器&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--We are also constantly making progress on improved tests debuggability and de-flaking.
If any of this interests you, we'd love for you to join us!
There's plenty to learn in debugging test failures, and it will help you gain
familiarity with the code that SIG Node maintains.-->
&lt;p>我们还在改进测试的可调试性和去剥落方面不断取得进展。&lt;/p>
&lt;p>如果你对此感兴趣，我们很乐意您能加入我们！
在调试测试失败中有很多东西需要学习，它将帮助你熟悉 SIG Node 维护的代码。&lt;/p>
&lt;!--You can always find information about the group on the
[SIG Node](https://github.com/kubernetes/community/tree/master/sig-node) page.
We give group updates at our maintainer track sessions, such as
[KubeCon + CloudNativeCon Europe 2021](https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google) 和
[KubeCon + CloudNative North America 2021](https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;w=100%&amp;sidebar=yes&amp;bg=no)。
Join us in our mission to keep the kubelet and other SIG Node components reliable and ensure smooth and uneventful releases!-->
&lt;p>你可以在 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> 页面上找到有关该组的信息。
我们在我们的维护者轨道会议上提供组更新，例如：
&lt;a href="https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google">KubeCon + CloudNativeCon Europe 2021&lt;/a> 和
&lt;a href="https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;amp;w=100%25&amp;amp;sidebar=yes&amp;amp;bg=no">KubeCon + CloudNative North America 2021&lt;/a>。
加入我们的使命，保持 kubelet 和其他 SIG Node 组件的可靠性，确保顺顺利利发布！&lt;/p></description></item><item><title>Blog: 确保准入控制器的安全</title><link>https://kubernetes.io/zh/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</link><pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</guid><description>
&lt;!--
layout: blog
title: "Securing Admission Controllers"
date: 2022-01-19
slug: secure-your-admission-controllers-and-webhooks
-->
&lt;!--
**Author:** Rory McCune (Aqua Security)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Rory McCune (Aqua Security)&lt;/p>
&lt;!--
[Admission control](/docs/reference/access-authn-authz/admission-controllers/) is a key part of Kubernetes security, alongside authentication and authorization.
Webhook admission controllers are extensively used to help improve the security of Kubernetes clusters in a variety of ways including restricting the privileges of workloads and ensuring that images deployed to the cluster meet organization’s security requirements.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制&lt;/a>和认证、授权都是 Kubernetes 安全性的关键部分。
Webhook 准入控制器被广泛用于以多种方式帮助提高 Kubernetes 集群的安全性，
包括限制工作负载权限和确保部署到集群的镜像满足组织安全要求。&lt;/p>
&lt;!--
However, as with any additional component added to a cluster, security risks can present themselves.
A security risk example is if the deployment and management of the admission controller are not handled correctly. To help admission controller users and designers manage these risks appropriately,
the [security documentation](https://github.com/kubernetes/community/tree/master/sig-security#security-docs) subgroup of SIG Security has spent some time developing a [threat model for admission controllers](https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control).
This threat model looks at likely risks which may arise from the incorrect use of admission controllers, which could allow security policies to be bypassed, or even allow an attacker to get unauthorised access to the cluster.
-->
&lt;p>然而，与添加到集群中的任何其他组件一样，安全风险也会随之出现。
一个安全风险示例是没有正确处理准入控制器的部署和管理。
为了帮助准入控制器用户和设计人员适当地管理这些风险，
SIG Security 的&lt;a href="https://github.com/kubernetes/community/tree/master/sig-security#security-docs">安全文档&lt;/a>小组
花费了一些时间来开发一个&lt;a href="https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control">准入控制器威胁模型&lt;/a>。
这种威胁模型着眼于由于不正确使用准入控制器而产生的可能的风险，可能允许绕过安全策略，甚至允许攻击者未经授权访问集群。&lt;/p>
&lt;!--
From the threat model, we developed a set of security best practices that should be adopted to ensure that cluster operators can get the security benefits of admission controllers whilst avoiding any risks from using them.
-->
&lt;p>基于这个威胁模型，我们开发了一套安全最佳实践。
你应该采用这些实践来确保集群操作员可以获得准入控制器带来的安全优势，同时避免使用它们带来的任何风险。&lt;/p>
&lt;!--
## Admission controllers and good practices for security
-->
&lt;h2 id="准入控制器和安全的良好做法">准入控制器和安全的良好做法&lt;/h2>
&lt;!--
From the threat model, a couple of themes emerged around how to ensure the security of admission controllers.
-->
&lt;p>基于这个威胁模型，围绕着如何确保准入控制器的安全性出现了几个主题。&lt;/p>
&lt;!--
### Secure webhook configuration
-->
&lt;h3 id="安全的-webhook-配置">安全的 webhook 配置&lt;/h3>
&lt;!--
It’s important to ensure that any security component in a cluster is well configured and admission controllers are no different here. There are a couple of security best practices to consider when using admission controllers
-->
&lt;p>确保集群中的任何安全组件都配置良好是很重要的，在这里准入控制器也并不例外。
使用准入控制器时需要考虑几个安全最佳实践：&lt;/p>
&lt;!--
* **Correctly configured TLS for all webhook traffic**. Communications between the API server and the admission controller webhook should be authenticated and encrypted to ensure that attackers who may be in a network position to view or modify this traffic cannot do so. To achieve this access the API server and webhook must be using certificates from a trusted certificate authority so that they can validate their mutual identities
-->
&lt;ul>
&lt;li>&lt;strong>为所有 webhook 流量正确配置了 TLS&lt;/strong>。
API 服务器和准入控制器 webhook 之间的通信应该经过身份验证和加密，以确保处于网络中查看或修改此流量的攻击者无法查看或修改。
要实现此访问，API 服务器和 webhook 必须使用来自受信任的证书颁发机构的证书，以便它们可以验证相互的身份。&lt;/li>
&lt;/ul>
&lt;!--
* **Only authenticated access allowed**. If an attacker can send an admission controller large numbers of requests, they may be able to overwhelm the service causing it to fail. Ensuring all access requires strong authentication should mitigate that risk.
-->
&lt;ul>
&lt;li>&lt;strong>只允许经过身份验证的访问&lt;/strong>。
如果攻击者可以向准入控制器发送大量请求，他们可能会压垮服务导致其失败。
确保所有访问都需要强身份验证可以降低这种风险。&lt;/li>
&lt;/ul>
&lt;!--
* **Admission controller fails closed**. This is a security practice that has a tradeoff, so whether a cluster operator wants to configure it will depend on the cluster’s threat model. If an admission controller fails closed, when the API server can’t get a response from it, all deployments will fail. This stops attackers bypassing the admission controller by disabling it, but, can disrupt the cluster’s operation. As clusters can have multiple webhooks, one approach to hit a middle ground might be to have critical controls on a fail closed setups and less critical controls allowed to fail open.
-->
&lt;ul>
&lt;li>&lt;strong>准入控制器关闭失败&lt;/strong>。
这是一种需要权衡的安全实践，集群操作员是否要对其进行配置取决于集群的威胁模型。
如果一个准入控制器关闭失败，当 API 服务器无法从它得到响应时，所有的部署都会失败。
这可以阻止攻击者通过禁用准入控制器绕过准入控制器，但可能会破坏集群的运行。
由于集群可以有多个 webhook，因此一种折中的方法是对关键控制允许故障关闭，
并允许不太关键的控制进行故障打开。&lt;/li>
&lt;/ul>
&lt;!--
* **Regular reviews of webhook configuration**. Configuration mistakes can lead to security issues, so it’s important that the admission controller webhook configuration is checked to make sure the settings are correct. This kind of review could be done automatically by an Infrastructure As Code scanner or manually by an administrator.
-->
&lt;ul>
&lt;li>&lt;strong>定期审查 webhook 配置&lt;/strong>。
配置错误可能导致安全问题，因此检查准入控制器 webhook 配置以确保设置正确非常重要。
这种审查可以由基础设施即代码扫描程序自动完成，也可以由管理员手动完成。&lt;/li>
&lt;/ul>
&lt;!--
### Secure cluster configuration for admission control
-->
&lt;h3 id="为准入控制保护集群配置">为准入控制保护集群配置&lt;/h3>
&lt;!--
In most cases, the admission controller webhook used by a cluster will be installed as a workload in the cluster. As a result, it’s important to ensure that Kubernetes' security features that could impact its operation are well configured.
-->
&lt;p>在大多数情况下，集群使用的准入控制器 webhook 将作为工作负载安装在集群中。
因此，确保正确配置了可能影响其操作的 Kubernetes 安全特性非常重要。&lt;/p>
&lt;!--
* **Restrict [RBAC](/docs/reference/access-authn-authz/rbac/) rights**. Any user who has rights which would allow them to modify the configuration of the webhook objects or the workload that the admission controller uses could disrupt its operation. So it’s important to make sure that only cluster administrators have those rights.
-->
&lt;ul>
&lt;li>&lt;strong>限制 &lt;a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/">RBAC&lt;/a> 权限&lt;/strong>。
任何有权修改 webhook 对象的配置或准入控制器使用的工作负载的用户都可以破坏其运行。
因此，确保只有集群管理员拥有这些权限非常重要。&lt;/li>
&lt;/ul>
&lt;!--
* **Prevent privileged workloads**. One of the realities of container systems is that if a workload is given certain privileges, it will be possible to break out to the underlying cluster node and impact other containers on that node. Where admission controller services run in the cluster they’re protecting, it’s important to ensure that any requirement for privileged workloads is carefully reviewed and restricted as much as possible.
-->
&lt;ul>
&lt;li>&lt;strong>防止特权工作负载&lt;/strong>。
容器系统的一个现实是，如果工作负载被赋予某些特权，
则有可能逃逸到下层的集群节点并影响该节点上的其他容器。
如果准入控制器服务在它们所保护的集群上运行，
一定要确保对特权工作负载的所有请求都要经过仔细审查并尽可能地加以限制。&lt;/li>
&lt;/ul>
&lt;!--
* **Strictly control external system access**. As a security service in a cluster admission controller systems will have access to sensitive information like credentials. To reduce the risk of this information being sent outside the cluster, [network policies](/docs/concepts/services-networking/network-policies/) should be used to restrict the admission controller services access to external networks.
-->
&lt;ul>
&lt;li>&lt;strong>严格控制外部系统访问&lt;/strong>。
作为集群中的安全服务，准入控制器系统将有权访问敏感信息，如凭证。
为了降低此信息被发送到集群外的风险，
应使用&lt;a href="https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/">网络策略&lt;/a>
来限制准入控制器服务对外部网络的访问。&lt;/li>
&lt;/ul>
&lt;!--
* **Each cluster has a dedicated webhook**. Whilst it may be possible to have admission controller webhooks that serve multiple clusters, there is a risk when using that model that an attack on the webhook service would have a larger impact where it’s shared. Also where multiple clusters use an admission controller there will be increased complexity and access requirements, making it harder to secure.
-->
&lt;ul>
&lt;li>&lt;strong>每个集群都有一个专用的 webhook&lt;/strong>。
虽然可能让准入控制器 webhook 服务于多个集群的，
但在使用该模型时存在对 webhook 服务的攻击会对共享它的地方产生更大影响的风险。
此外，在多个集群使用准入控制器的情况下，复杂性和访问要求也会增加，从而更难保护其安全。&lt;/li>
&lt;/ul>
&lt;!--
### Admission controller rules
-->
&lt;h3 id="准入控制器规则">准入控制器规则&lt;/h3>
&lt;!--
A key element of any admission controller used for Kubernetes security is the rulebase it uses. The rules need to be able to accurately meet their goals avoiding false positive and false negative results.
-->
&lt;p>对于用于 Kubernetes 安全的所有准入控制器而言，一个关键元素是它使用的规则库。
规则需要能够准确地满足其目标，避免假阳性和假阴性结果。&lt;/p>
&lt;!--
* **Regularly test and review rules**. Admission controller rules need to be tested to ensure their accuracy. They also need to be regularly reviewed as the Kubernetes API will change with each new version, and rules need to be assessed with each Kubernetes release to understand any changes that may be required to keep them up to date.
-->
&lt;ul>
&lt;li>&lt;strong>定期测试和审查规则&lt;/strong>。
需要测试准入控制器规则以确保其准确性。
还需要定期审查，因为 Kubernetes API 会随着每个新版本而改变，
并且需要在每个 Kubernetes 版本中评估规则，以了解使他们保持最新版本所需要做的任何改变。&lt;/li>
&lt;/ul></description></item><item><title>Blog: 认识我们的贡献者 - 亚太地区（印度地区）</title><link>https://kubernetes.io/zh/blog/2022/01/10/meet-our-contributors-india-ep-01/</link><pubDate>Mon, 10 Jan 2022 12:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/01/10/meet-our-contributors-india-ep-01/</guid><description>
&lt;!--
layout: blog
title: "Meet Our Contributors - APAC (India region)"
date: 2022-01-10T12:00:00+0000
slug: meet-our-contributors-india-ep-01
canonicalUrl: https://kubernetes.dev/blog/2022/01/10/meet-our-contributors-india-ep-01/
-->
&lt;!--
**Authors &amp; Interviewers:** [Anubhav Vardhan](https://github.com/anubha-v-ardhan), [Atharva Shinde](https://github.com/Atharva-Shinde), [Avinesh Tripathi](https://github.com/AvineshTripathi), [Debabrata Panigrahi](https://github.com/Debanitrkl), [Kunal Verma](https://github.com/verma-kunal), [Pranshu Srivastava](https://github.com/PranshuSrivastava), [Pritish Samal](https://github.com/CIPHERTron), [Purneswar Prasad](https://github.com/PurneswarPrasad), [Vedant Kakde](https://github.com/vedant-kakde)
-->
&lt;p>&lt;strong>作者和采访者：&lt;/strong> &lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a> ， &lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a> ， &lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a> ， &lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a> ， &lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a> ， &lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a> ， &lt;a href="https://github.com/CIPHERTron">Pritish Samal&lt;/a> ， &lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a> ， &lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;!--
**Editor:** [Priyanka Saggu](https://psaggu.com)
-->
&lt;p>&lt;strong>编辑：&lt;/strong> &lt;a href="https://psaggu.com">Priyanka Saggu&lt;/a>&lt;/p>
&lt;hr>
&lt;!--
Good day, everyone 👋
-->
&lt;p>大家好 👋&lt;/p>
&lt;!--
Welcome to the first episode of the APAC edition of the "Meet Our Contributors" blog post series.
-->
&lt;p>欢迎来到亚太地区的“认识我们的贡献者”博文系列第一期。&lt;/p>
&lt;!--
In this post, we'll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives.
-->
&lt;p>在这篇文章中，我们将向您介绍来自印度地区的五位优秀贡献者，他们一直在以各种方式积极地为上游 Kubernetes 项目做贡献，同时也是众多社区倡议的领导者和维护者。&lt;/p>
&lt;!--
💫 *Let's get started, so without further ado…*
-->
&lt;p>💫 &lt;em>闲话少说，我们开始吧。&lt;/em>&lt;/p>
&lt;h2 id="arsh-sharma-https-github-com-rinkiyakedad">&lt;a href="https://github.com/RinkiyaKeDad">Arsh Sharma&lt;/a>&lt;/h2>
&lt;!--
Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project.
-->
&lt;p>Arsh 目前在 Okteto 公司中担任开发者体验工程师职务。作为一名新的贡献者，他意识到一对一的指导机会让他在开始上游项目中受益匪浅。&lt;/p>
&lt;!--
He is presently a CI Signal shadow on the Kubernetes 1.23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the [cert-manager](https://github.com/cert-manager/infrastructure) tools development work that is being done under the aegis of SIG Architecture.
-->
&lt;p>他目前是 Kubernetes 1.23 版本团队的 CI Signal 经理。他还致力于为 SIG Testing 和 SIG Docs 项目提供贡献，并且在 SIG Architecture 项目中负责 &lt;a href="https://github.com/cert-manager/infrastructure">证书管理器&lt;/a> 工具的开发工作。&lt;/p>
&lt;!--
To the newcomers, Arsh helps plan their early contributions sustainably.
-->
&lt;p>对于新人来说，Arsh 帮助他们可持续地计划早期贡献。&lt;/p>
&lt;!--
> _I would encourage folks to contribute in a way that's sustainable. What I mean by that
> is that it's easy to be very enthusiastic early on and take up more stuff than one can
> actually handle. This can often lead to burnout in later stages. It's much more sustainable
> to work on things iteratively._
-->
&lt;blockquote>
&lt;p>&lt;em>我鼓励大家以可持续的方式为社区做贡献。我的意思是，一个人很容易在早期的时候非常有热情，并且承担了很多超出个人实际能力的事情。这通常会导致后期的倦怠。迭代地处理事情会让大家对社区的贡献变得可持续。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="kunal-kushwaha-https-github-com-kunal-kushwaha">&lt;a href="https://github.com/kunal-kushwaha">Kunal Kushwaha&lt;/a>&lt;/h2>
&lt;!--
Kunal Kushwaha is a core member of the Kubernetes marketing council. He is also a CNCF ambassador and one of the founders of the [CNCF Students Program](https://community.cncf.io/cloud-native-students/).. He also served as a Communications role shadow during the 1.22 release cycle.
-->
&lt;p>Kunal Kushwaha 是 Kubernetes 营销委员会的核心成员。他同时也是 &lt;a href="https://community.cncf.io/cloud-native-students/">CNCF 学生计划&lt;/a> 的创始人之一。他还在 1.22 版本周期中担任通信经理一职。&lt;/p>
&lt;!--
At the end of his first year, Kunal began contributing to the [fabric8io kubernetes-client](https://github.com/fabric8io/kubernetes-client) project. He was then selected to work on the same project as part of Google Summer of Code. Kunal mentored people on the same project, first through Google Summer of Code then through Google Code-in.
-->
&lt;p>在他的第一年结束时，Kunal 开始为 &lt;a href="https://github.com/fabric8io/kubernetes-client">fabric8io kubernetes-client&lt;/a> 项目做贡献。然后，他被推选从事同一项目，此项目是 Google Summer of Code 的一部分。Kunal 在 Google Summer of Code、Google Code-in 等项目中指导过很多人。&lt;/p>
&lt;!--
As an open-source enthusiast, he believes that diverse participation in the community is beneficial since it introduces new perspectives and opinions and respect for one's peers. He has worked on various open-source projects, and his participation in communities has considerably assisted his development as a developer.
-->
&lt;p>作为一名开源爱好者，他坚信，社区的多元化参与是非常有益的，因为他引入了新的观念和观点，并尊重自己的伙伴。它曾参与过各种开源项目，他在这些社区中的参与对他作为开发者的发展有很大帮助。&lt;/p>
&lt;!--
> _I believe if you find yourself in a place where you do not know much about the
> project, that's a good thing because now you can learn while contributing and the
> community is there to help you. It has helped me a lot in gaining skills, meeting
> people from around the world and also helping them. You can learn on the go,
> you don't have to be an expert. Make sure to also check out no code contributions
> because being a beginner is a skill and you can bring new perspectives to the
> organisation._
-->
&lt;blockquote>
&lt;p>&lt;em>我相信，如果你发现自己在一个了解不多的项目当中，那是件好事，因为现在你可以一边贡献一边学习，社区也会帮助你。它帮助我获得了很多技能，认识了来自世界各地的人，也帮助了他们。你可以在这个过程中学习，自己不一定必须是专家。请重视非代码贡献，因为作为初学者这是一项技能，你可以为组织带来新的视角。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="madhav-jivarajani-https-github-com-madhavjivrajani">&lt;a href="https://github.com/MadhavJivrajani">Madhav Jivarajani&lt;/a>&lt;/h2>
&lt;!--
Madhav Jivarajani works on the VMware Upstream Kubernetes stability team. He began contributing to the Kubernetes project in January 2021 and has since made significant contributions to several areas of work under SIG Architecture, SIG API Machinery, and SIG ContribEx (contributor experience).
-->
&lt;p>Madhav Jivarajani 在 VMware 上游 Kubernetes 稳定性团队工作。他于 2021 年 1 月开始为 Kubernetes 项目做贡献，此后在 SIG Architecture、SIG API Machinery 和 SIG ContribEx（贡献者经验）等项目的几个工作领域做出了重大贡献。&lt;/p>
&lt;!--
Among several significant contributions are his recent efforts toward the Archival of [design proposals](https://github.com/kubernetes/community/issues/6055), refactoring the ["groups" codebase](https://github.com/kubernetes/k8s.io/pull/2713) under k8s-infra repository to make it mockable and testable, and improving the functionality of the [GitHub k8s bot](https://github.com/kubernetes/test-infra/issues/23129).
-->
&lt;p>在这几个重要项目中，他最近致力于 &lt;a href="https://github.com/kubernetes/community/issues/6055">设计方案&lt;/a> 的存档工作，重构 k8s-infra 存储库下的 &lt;a href="https://github.com/kubernetes/k8s.io/pull/2713">&amp;quot;组&amp;quot;代码库&lt;/a> ，使其具有可模拟性和可测试性，以及改进 &lt;a href="https://github.com/kubernetes/test-infra/issues/23129">GitHub k8s 机器人&lt;/a> 的功能。&lt;/p>
&lt;!--
In addition to his technical efforts, Madhav oversees many projects aimed at assisting new contributors. He organises bi-weekly "KEP reading club" sessions to help newcomers understand the process of adding new features, deprecating old ones, and making other key changes to the upstream project. He has also worked on developing [Katacoda scenarios](https://github.com/kubernetes-sigs/contributor-katacoda) to assist new contributors to become acquainted with the process of contributing to k/k. In addition to his current efforts to meet with community members every week, he has organised several [new contributors workshops (NCW)](https://www.youtube.com/watch?v=FgsXbHBRYIc).
-->
&lt;p>除了在技术方面的贡献，Madhav 还监督许多旨在帮助新贡献者的项目。他每两周组织一次的“KEP 阅读俱乐部”会议，帮助新人了解添加新功能、摒弃旧功能以及对上游项目进行其他关键更改的过程。他还致力于开发 &lt;a href="https://github.com/kubernetes-sigs/contributor-katacoda">Katacoda 场景&lt;/a> ，以帮助新的贡献者在为 k/k 做贡献的过程更加熟练。目前除了每周与社区成员会面外，他还组织了几个 &lt;a href="https://www.youtube.com/watch?v=FgsXbHBRYIc">新贡献者讲习班（NCW）&lt;/a> 。&lt;/p>
&lt;!--
> _I initially did not know much about Kubernetes. I joined because the community was
> super friendly. But what made me stay was not just the people, but the project itself.
> My solution to not feeling overwhelmed in the community was to gain as much context
> and knowledge into the topics that I was interested in and were being discussed. And
> as a result I continued to dig deeper into Kubernetes and the design of it.
> I am a systems nut &amp; thus Kubernetes was an absolute goldmine for me._
-->
&lt;blockquote>
&lt;p>&lt;em>一开始我对 Kubernetes 了解并不多。我加入社区是因为社区超级友好。但让我留下来的不仅仅是人，还有项目本身。我在社区中不会感到不知所措，这是因为我能够在感兴趣的和正在讨论的主题中获得尽可能多的背景和知识。因此，我将继续深入探讨 Kubernetes 及其设计。我是一个系统迷，kubernetes 对我来说绝对是一个金矿。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="rajas-kakodkar-https-github-com-rajaskakodkar">&lt;a href="https://github.com/rajaskakodkar">Rajas Kakodkar&lt;/a>&lt;/h2>
&lt;!--
Rajas Kakodkar currently works at VMware as a Member of Technical Staff. He has been engaged in many aspects of the upstream Kubernetes project since 2019.
-->
&lt;p>Rajas Kakodkar 目前在 VMware 担任技术人员。自 2019 年以来，他一直多方面地从事上游 kubernetes 项目。&lt;/p>
&lt;!--
He is now a key contributor to the Testing special interest group. He is also active in the SIG Network community. Lately, Rajas has contributed significantly to the [NetworkPolicy++](https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/) and [`kpng`](https://github.com/kubernetes-sigs/kpng) sub-projects.
-->
&lt;p>他现在是 Testing 特别兴趣小组的关键贡献者。他还活跃在 SIG Network 社区。最近，Rajas 为 &lt;a href="https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/">NetworkPolicy++&lt;/a> 和 &lt;a href="https://github.com/kubernetes-sigs/kpng">&lt;code>kpng&lt;/code>&lt;/a> 子项目做出了重大贡献。&lt;/p>
&lt;!--
One of the first challenges he ran across was that he was in a different time zone than the upstream project's regular meeting hours. However, async interactions on community forums progressively corrected that problem.
-->
&lt;p>他遇到的第一个挑战是，他所处的时区与上游项目的日常会议时间不同。不过，社区论坛上的异步交互逐渐解决了这个问题。&lt;/p>
&lt;!--
> _I enjoy contributing to Kubernetes not just because I get to work on
> cutting edge tech but more importantly because I get to work with
> awesome people and help in solving real world problems._
-->
&lt;blockquote>
&lt;p>&lt;em>我喜欢为 kubernetes 做出贡献，不仅因为我可以从事尖端技术工作，更重要的是，我可以和优秀的人一起工作，并帮助解决现实问题。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="rajula-vineet-reddy-https-github-com-rajula96reddy">&lt;a href="https://github.com/rajula96reddy">Rajula Vineet Reddy&lt;/a>&lt;/h2>
&lt;!--
Rajula Vineet Reddy, a Junior Engineer at CERN, is a member of the Marketing Council team under SIG ContribEx . He also served as a release shadow for SIG Release during the 1.22 and 1.23 Kubernetes release cycles.
-->
&lt;p>Rajula Vineet Reddy，CERN 的初级工程师，是 SIG ContribEx 项目下营销委员会的成员。在 Kubernetes 1.22 和 1.23 版本周期中，他还担任 SIG Release 的版本经理。&lt;/p>
&lt;!--
He started looking at the Kubernetes project as part of a university project with the help of one of his professors. Over time, he spent a significant amount of time reading the project's documentation, Slack discussions, GitHub issues, and blogs, which helped him better grasp the Kubernetes project and piqued his interest in contributing upstream. One of his key contributions was his assistance with automation in the SIG ContribEx Upstream Marketing subproject.
-->
&lt;p>在他的一位教授的帮助下，他开始将 kubernetes 项目作为大学项目的一部分。慢慢地，他花费了大量的时间阅读项目的文档、Slack 讨论、GitHub issues 和博客，这有助于他更好地掌握 kubernetes 项目，并激发了他对上游项目做贡献的兴趣。他的主要贡献之一是他在SIG ContribEx上游营销子项目中协助实现了自动化。&lt;/p>
&lt;!--
According to Rajula, attending project meetings and shadowing various project roles are vital for learning about the community.
-->
&lt;p>Rajas 说，参与项目会议和跟踪各种项目角色对于了解社区至关重要。&lt;/p>
&lt;!--
> _I find the community very helpful and it's always_
> “you get back as much as you contribute”.
> _The more involved you are, the more you will understand, get to learn and
> contribute new things._
>
> _The first step to_ “come forward and start” _is hard. But it's all gonna be
> smooth after that. Just take that jump._
-->
&lt;blockquote>
&lt;p>&lt;em>我发现社区非常有帮助，而且总是“你得到的回报和你贡献的一样多”。你参与得越多，你就越会了解、学习和贡献新东西。&lt;/em>
&lt;em>“挺身而出”的第一步是艰难的。但在那之后一切都会顺利的。勇敢地参与进来吧。&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;!--
If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. We're thrilled to have other folks assisting us in reaching out to even more wonderful individuals of the community. Your suggestions would be much appreciated.
-->
&lt;p>如果您对我们下一步应该采访谁有任何意见/建议，请在 #sig-contribex 中告知我们。我们很高兴有其他人帮助我们接触社区中更优秀的人。我们将不胜感激。&lt;/p>
&lt;!--
We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋
-->
&lt;p>我们下期见。最后，祝大家都能快乐地为社区做贡献！👋&lt;/p></description></item><item><title>Blog: Kubernetes 即将移除 Dockershim：承诺和下一步</title><link>https://kubernetes.io/zh/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</link><pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes is Moving on From Dockershim: Commitments and Next Steps"
date: 2022-01-07
slug: kubernetes-is-moving-on-from-dockershim
-->
&lt;!--
**Authors:** Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)&lt;/p>
&lt;!--
Kubernetes is removing dockershim in the upcoming v1.24 release. We're excited
to reaffirm our community values by supporting open source container runtimes,
enabling a smaller kubelet, and increasing engineering velocity for teams using
Kubernetes. If you [use Docker Engine as a container runtime](/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/)
for your Kubernetes cluster, get ready to migrate in 1.24! To check if you're
affected, refer to [Check whether dockershim deprecation affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/).
-->
&lt;p>Kubernetes 将在即将发布的 1.24 版本中移除 dockershim。我们很高兴能够通过支持开源容器运行时、支持更小的
kubelet 以及为使用 Kubernetes 的团队提高工程速度来重申我们的社区价值。
如果你&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">使用 Docker Engine 作为 Kubernetes 集群的容器运行时&lt;/a>，
请准备好在 1.24 中迁移！要检查你是否受到影响，
请参考&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">检查弃用 Dockershim 对你的影响&lt;/a>。&lt;/p>
&lt;!--
## Why we’re moving away from dockershim
Docker was the first container runtime used by Kubernetes. This is one of the
reasons why Docker is so familiar to many Kubernetes users and enthusiasts.
Docker support was hardcoded into Kubernetes – a component the project refers to
as dockershim.
-->
&lt;h2 id="why-we-re-moving-away-from-dockershim">为什么我们要离开 dockershim &lt;/h2>
&lt;p>Docker 是 Kubernetes 使用的第一个容器运行时。
这也是许多 Kubernetes 用户和爱好者如此熟悉 Docker 的原因之一。
对 Docker 的支持被硬编码到 Kubernetes 中——一个被项目称为 dockershim 的组件。&lt;/p>
&lt;!--
As containerization became an industry standard, the Kubernetes project added support
for additional runtimes. This culminated in the implementation of the
container runtime interface (CRI), letting system components (like the kubelet)
talk to container runtimes in a standardized way. As a result, dockershim became
an anomaly in the Kubernetes project.
-->
&lt;p>随着容器化成为行业标准，Kubernetes 项目增加了对其他运行时的支持。
最终实现了容器运行时接口（CRI），让系统组件（如 kubelet）以标准化的方式与容器运行时通信。
因此，dockershim 成为了 Kubernetes 项目中的一个异常现象。&lt;/p>
&lt;!--
Dependencies on Docker and dockershim have crept into various tools
and projects in the CNCF ecosystem ecosystem, resulting in fragile code.
By removing the
dockershim CRI, we're embracing the first value of CNCF: "[Fast is better than
slow](https://github.com/cncf/foundation/blob/master/charter.md#3-values)".
Stay tuned for future communications on the topic!
-->
&lt;p>对 Docker 和 dockershim 的依赖已经渗透到 CNCF 生态系统中的各种工具和项目中，这导致了代码脆弱。&lt;/p>
&lt;p>通过删除 dockershim CRI，我们拥抱了 CNCF 的第一个价值：
“&lt;a href="https://github.com/cncf/foundation/blob/master/charter.md#3-values">快比慢好&lt;/a>”。
请继续关注未来关于这个话题的交流!&lt;/p>
&lt;!--
## Deprecation timeline
We [formally announced](/blog/2020/12/08/kubernetes-1-20-release-announcement/) the dockershim deprecation in December 2020. Full removal is targeted
in Kubernetes 1.24, in April 2022. This timeline
aligns with our [deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior),
which states that deprecated behaviors must function for at least 1 year
after their announced deprecation.
-->
&lt;h2 id="deprecation-timeline">弃用时间线 &lt;/h2>
&lt;p>我们&lt;a href="https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/">正式宣布&lt;/a>于
2020 年 12 月弃用 dockershim。目标是在 2022 年 4 月，
Kubernetes 1.24 中完全移除 dockershim。
此时间线与我们的[弃用策略](/zh/docs/reference/using api/deprecation-policy/#deprecating-a-feature-or-behavior)一致，
即规定已弃用的行为必须在其宣布弃用后至少运行 1 年。&lt;/p>
&lt;!--
We'll support Kubernetes version 1.23, which includes
dockershim, for another year in the Kubernetes project. For managed
Kubernetes providers, vendor support is likely to last even longer, but this is
dependent on the companies themselves. Regardless, we're confident all cluster operations will have
time to migrate. If you have more questions about the dockershim removal, refer
to the [Dockershim Deprecation FAQ](/dockershim).
-->
&lt;p>包括 dockershim 的 Kubernetes 1.23 版本，在 Kubernetes 项目中将再支持一年。
对于托管 Kubernetes 的供应商，供应商支持可能会持续更长时间，但这取决于公司本身。
无论如何，我们相信所有集群操作都有时间进行迁移。如果你有更多关于 dockershim 移除的问题，
请参考&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">弃用 Dockershim 的常见问题&lt;/a>。&lt;/p>
&lt;!--
We asked you whether you feel prepared for the migration from dockershim in this
survey: [Are you ready for Dockershim removal](/blog/2021/11/12/are-you-ready-for-dockershim-removal/).
We had over 600 responses. To everybody who took time filling out the survey,
thank you.
-->
&lt;p>在这个&lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">你是否为 dockershim 的删除做好了准备&lt;/a>的调查中，
我们询问你是否为 dockershim 的迁移做好了准备。我们收到了 600 多个回复。
感谢所有花时间填写调查问卷的人。&lt;/p>
&lt;!--
The results show that we still have a lot of ground to cover to help you to
migrate smoothly. Other container runtimes exist, and have been promoted
extensively. However, many users told us they still rely on dockershim,
and sometimes have dependencies that need to be re-worked. Some of these
dependencies are outside of your control. Based on your feedback, here are some
of the steps we are taking to help.
-->
&lt;p>结果表明，在帮助你顺利迁移方面，我们还有很多工作要做。
存在其他容器运行时，并且已被广泛推广。但是，许多用户告诉我们他们仍然依赖 dockershim，
并且有时需要重新处理依赖项。其中一些依赖项超出控制范围。
根据收集到的反馈，我们采取了一些措施提供帮助。&lt;/p>
&lt;!--
## Our next steps
Based on the feedback you provided:
- CNCF and the 1.24 release team are committed to delivering documentation in
time for the 1.24 release. This includes more informative blog posts like this
one, updating existing code samples, tutorials, and tasks, and producing a
migration guide for cluster operators.
- We are reaching out to the rest of the CNCF community to help prepare them for
this change.
-->
&lt;h2 id="our-next-steps">我们的下一个步骤&lt;/h2>
&lt;p>根据提供的反馈：&lt;/p>
&lt;ul>
&lt;li>CNCF 和 1.24 版本团队致力于及时交付 1.24 版本的文档。这包括像本文这样的包含更多信息的博客文章，
更新现有的代码示例、教程和任务，并为集群操作人员生成迁移指南。&lt;/li>
&lt;li>我们正在联系 CNCF 社区的其他成员，帮助他们为这一变化做好准备。&lt;/li>
&lt;/ul>
&lt;!--
If you're part of a project with dependencies on dockershim, or if you're
interested in helping with the migration effort, please join us! There's always
room for more contributors, whether to our transition tools or to our
documentation. To get started, say hello in the
[#sig-node](https://kubernetes.slack.com/archives/C0BP8PW9G)
channel on [Kubernetes Slack](https://slack.kubernetes.io/)!
-->
&lt;p>如果你是依赖 dockershim 的项目的一部分，或者如果你有兴趣帮助参与迁移工作，请加入我们！
无论是我们的迁移工具还是我们的文档，总是有更多贡献者的空间。
作为起步，请在 &lt;a href="https://slack.kubernetes.io/">Kubernetes Slack&lt;/a> 上的
&lt;a href="https://kubernetes.slack.com/archives/C0BP8PW9G">#sig-node&lt;/a> 频道打个招呼！&lt;/p>
&lt;!--
## Final thoughts
As a project, we've already seen cluster operators increasingly adopt other
container runtimes through 2021.
We believe there are no major blockers to migration. The steps we're taking to
improve the migration experience will light the path more clearly for you.
-->
&lt;h2 id="final-thoughts">最终想法 &lt;/h2>
&lt;p>作为一个项目，我们已经看到集群运营商在 2021 年之前越来越多地采用其他容器运行时。
我们相信迁移没有主要障碍。我们为改善迁移体验而采取的步骤将为你指明更清晰的道路。&lt;/p>
&lt;!--
We understand that migration from dockershim is yet another action you may need to
do to keep your Kubernetes infrastructure up to date. For most of you, this step
will be straightforward and transparent. In some cases, you will encounter
hiccups or issues. The community has discussed at length whether postponing the
dockershim removal would be helpful. For example, we recently talked about it in
the [SIG Node discussion on November 11th](https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid)
and in the [Kubernetes Steering committee meeting held on December 6th](https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx).
We already [postponed](https://github.com/kubernetes/enhancements/pull/2481/) it
once in 2021 because the adoption rate of other
runtimes was lower than we wanted, which also gave us more time to identify
potential blocking issues.
-->
&lt;p>我们知道，从 dockershim 迁移是你可能需要执行的另一项操作，以保证你的 Kubernetes 基础架构保持最新。
对于你们中的大多数人来说，这一步将是简单明了的。在某些情况下，你会遇到问题。
社区已经详细讨论了推迟 dockershim 删除是否会有所帮助。
例如，我们最近在 &lt;a href="https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid">11 月 11 日的 SIG Node 讨论&lt;/a>和
&lt;a href="https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx">12 月 6 日 Kubernetes Steering 举行的委员会会议&lt;/a>谈到了它。
我们已经在 2021 年&lt;a href="https://github.com/kubernetes/enhancements/pull/2481/">推迟&lt;/a>它一次，
因为其他运行时的采用率低于我们的预期，这也给了我们更多的时间来识别潜在的阻塞问题。&lt;/p>
&lt;!--
At this point, we believe that the value that you (and Kubernetes) gain from
dockershim removal makes up for the migration effort you'll have. Start planning
now to avoid surprises. We'll have more updates and guides before Kubernetes
1.24 is released.
-->
&lt;p>在这一点上，我们相信你（和 Kubernetes）从移除 dockershim 中获得的价值可以弥补你将要进行的迁移工作。
现在就开始计划以避免出现意外。在 Kubernetes 1.24 发布之前，我们将提供更多更新信息和指南。&lt;/p></description></item><item><title>Blog: Kubernetes 1.23: StatefulSet PVC 自动删除 (alpha)</title><link>https://kubernetes.io/zh/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.23: StatefulSet PVC Auto-Deletion (alpha)'
date: 2021-12-16
slug: kubernetes-1-23-statefulset-pvc-auto-deletion
-->
&lt;!--
**Author:** Matthew Cary (Google)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Matthew Cary (谷歌)&lt;/p>
&lt;!--
Kubernetes v1.23 introduced a new, alpha-level policy for
[StatefulSets](/docs/concepts/workloads/controllers/statefulset/) that controls the lifetime of
[PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/) (PVCs) generated from the
StatefulSet spec template for cases when they should be deleted automatically when the StatefulSet
is deleted or pods in the StatefulSet are scaled down.
-->
&lt;p>Kubernetes v1.23 为 &lt;a href="https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets&lt;/a>
引入了一个新的 alpha 级策略，用来控制由 StatefulSet 规约模板生成的
&lt;a href="https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaims&lt;/a> (PVCs) 的生命周期，
用于当删除 StatefulSet 或减少 StatefulSet 中的 Pods 数量时 PVCs 应该被自动删除的场景。&lt;/p>
&lt;!--
## What problem does this solve?
-->
&lt;h2 id="它解决了什么问题">它解决了什么问题？&lt;/h2>
&lt;!--
A StatefulSet spec can include Pod and PVC templates. When a replica is first created, the
Kubernetes control plane creates a PVC for that replica if one does not already exist. The behavior
before Kubernetes v1.23 was that the control plane never cleaned up the PVCs created for
StatefulSets - this was left up to the cluster administrator, or to some add-on automation that
you’d have to find, check suitability, and deploy. The common pattern for managing PVCs, either
manually or through tools such as Helm, is that the PVCs are tracked by the tool that manages them,
with explicit lifecycle. Workflows that use StatefulSets must determine on their own what PVCs are
created by a StatefulSet and what their lifecycle should be.
-->
&lt;p>StatefulSet 规约中可以包含 Pod 和 PVC 的模板。当副本先被创建时，如果 PVC 还不存在，
Kubernetes 控制面会为该副本自动创建一个 PVC。在 Kubernetes 1.23 版本之前，
控制面不会删除 StatefulSet 创建的 PVCs——这依赖集群管理员或你需要部署一些额外的适用的自动化工具来处理。
管理 PVC 的常见模式是通过手动或使用 Helm 等工具，PVC 的具体生命周期由管理它的工具跟踪。
使用 StatefulSet 时必须自行确定 StatefulSet 创建哪些 PVC，以及它们的生命周期应该是什么。&lt;/p>
&lt;!--
Before this new feature, when a StatefulSet-managed replica disappears, either because the
StatefulSet is reducing its replica count, or because its StatefulSet is deleted, the PVC and its
backing volume remains and must be manually deleted. While this behavior is appropriate when the
data is critical, in many cases the persistent data in these PVCs is either temporary, or can be
reconstructed from another source. In those cases, PVCs and their backing volumes remaining after
their StatefulSet or replicas have been deleted are not necessary, incur cost, and require manual
cleanup.
-->
&lt;p>在这个新特性之前，当一个 StatefulSet 管理的副本消失时，无论是因为 StatefulSet 减少了它的副本数，
还是因为它的 StatefulSet 被删除了，PVC 及其下层的卷仍然存在，需要手动删除。
当存储数据比较重要时，这样做是合理的，但在许多情况下，这些 PVC 中的持久化数据要么是临时的，
要么可以从另一个源端重建。在这些情况下，删除 StatefulSet 或减少副本后留下的 PVC 及其下层的卷是不必要的，
还会产生成本，需要手动清理。&lt;/p>
&lt;!--
## The new StatefulSet PVC retention policy
-->
&lt;h2 id="新的-statefulset-pvc-保留策略">新的 StatefulSet PVC 保留策略&lt;/h2>
&lt;!--
If you enable the alpha feature, a StatefulSet spec includes a PersistentVolumeClaim retention
policy. This is used to control if and when PVCs created from a StatefulSet’s `volumeClaimTemplate`
are deleted. This first iteration of the retention policy contains two situations where PVCs may be
deleted.
-->
&lt;p>如果你启用这个新 alpha 特性，StatefulSet 规约中就可以包含 PersistentVolumeClaim 的保留策略。
该策略用于控制是否以及何时删除基于 StatefulSet 的 &lt;code>volumeClaimTemplate&lt;/code> 属性所创建的 PVCs。
保留策略的首次迭代包含两种可能删除 PVC 的情况。&lt;/p>
&lt;!--
The first situation is when the StatefulSet resource is deleted (which implies that all replicas are
also deleted). This is controlled by the `whenDeleted` policy. The second situation, controlled by
`whenScaled` is when the StatefulSet is scaled down, which removes some but not all of the replicas
in a StatefulSet. In both cases the policy can either be `Retain`, where the corresponding PVCs are
not touched, or `Delete`, which means that PVCs are deleted. The deletion is done with a normal
[object deletion](/docs/concepts/architecture/garbage-collection/), so that, for example, all
retention policies for the underlying PV are respected.
-->
&lt;p>第一种情况是 StatefulSet 资源被删除时（这意味着所有副本也被删除），这由 &lt;code>whenDeleted&lt;/code> 策略控制的。
第二种情况是 StatefulSet 缩小时，即删除 StatefulSet 部分副本，这由 &lt;code>whenScaled&lt;/code> 策略控制。
在这两种情况下，策略即可以是 &lt;code>Retain&lt;/code> 不涉及相应 PVCs 的改变，也可以是 &lt;code>Delete&lt;/code> 即删除对应的 PVCs。
删除是通过普通的&lt;a href="https://kubernetes.io/zh/docs/concepts/architecture/garbage-collection/">对象删除&lt;/a>完成的，
因此，的所有保留策略都会被遵照执行。&lt;/p>
&lt;!--
This policy forms a matrix with four cases. I’ll walk through and give an example for each one.
-->
&lt;p>该策略形成包含四种情况的矩阵。我将逐一介绍，并为每一种情况给出一个例子。&lt;/p>
&lt;!--
* **`whenDeleted` and `whenScaled` are both `Retain`.** This matches the existing behavior for
StatefulSets, where no PVCs are deleted. This is also the default retention policy. It’s
appropriate to use when data on StatefulSet volumes may be irreplaceable and should only be
deleted manually.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> 和 &lt;code>whenScaled&lt;/code> 都是 &lt;code>Retain&lt;/code>。&lt;/strong> 这与 StatefulSets 的现有行为一致，
即不删除 PVCs。 这也是默认的保留策略。它适用于 StatefulSet
卷上的数据是不可替代的且只能手动删除的情况。&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` is `Delete` and `whenScaled` is `Retain`.** In this case, PVCs are deleted only when
the entire StatefulSet is deleted. If the StatefulSet is scaled down, PVCs are not touched,
meaning they are available to be reattached if a scale-up occurs with any data from the previous
replica. This might be used for a temporary StatefulSet, such as in a CI instance or ETL
pipeline, where the data on the StatefulSet is needed only during the lifetime of the
StatefulSet lifetime, but while the task is running the data is not easily reconstructible. Any
retained state is needed for any replicas that scale down and then up.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> 是 &lt;code>Delete&lt;/code> 但 &lt;code>whenScaled&lt;/code> 是 &lt;code>Retain&lt;/code>。&lt;/strong> 在这种情况下，
只有当整个 StatefulSet 被删除时，PVCs 才会被删除。
如果减少 StatefulSet 副本，PVCs 不会删除，这意味着如果增加副本时，可以从前一个副本重新连接所有数据。
这可能用于临时的 StatefulSet，例如在 CI 实例或 ETL 管道中，
StatefulSet 上的数据仅在 StatefulSet 生命周期内才需要，但在任务运行时数据不易重构。
任何保留状态对于所有先缩小后扩大的副本都是必需的。&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` and `whenScaled` are both `Delete`.** PVCs are deleted immediately when their
replica is no longer needed. Note this does not include when a Pod is deleted and a new version
rescheduled, for example when a node is drained and Pods need to migrate elsewhere. The PVC is
deleted only when the replica is no longer needed as signified by a scale-down or StatefulSet
deletion. This use case is for when data does not need to live beyond the life of its
replica. Perhaps the data is easily reconstructable and the cost savings of deleting unused PVCs
is more important than quick scale-up, or perhaps that when a new replica is created, any data
from a previous replica is not usable and must be reconstructed anyway.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> 和 &lt;code>whenScaled&lt;/code> 都是 &lt;code>Delete&lt;/code>。&lt;/strong> 当其副本不再被需要时，PVCs 会立即被删除。
注意，这并不包括 Pod 被删除且有新版本被调度的情况，例如当节点被腾空而 Pod 需要迁移到别处时。
只有当副本不再被需要时，如按比例缩小或删除 StatefulSet 时，才会删除 PVC。
此策略适用于数据生命周期短于副本生命周期的情况。即数据很容易重构，
且删除未使用的 PVC 所节省的成本比快速增加副本更重要，或者当创建一个新的副本时，
来自以前副本的任何数据都不可用，必须重新构建。&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` is `Retain` and `whenScaled` is `Delete`.** This is similar to the previous case,
when there is little benefit to keeping PVCs for fast reuse during scale-up. An example of a
situation where you might use this is an Elasticsearch cluster. Typically you would scale that
workload up and down to match demand, whilst ensuring a minimum number of replicas (for example:
3). When scaling down, data is migrated away from removed replicas and there is no benefit to
retaining those PVCs. However, it can be useful to bring the entire Elasticsearch cluster down
temporarily for maintenance. If you need to take the Elasticsearch system offline, you can do
this by temporarily deleting the StatefulSet, and then bringing the Elasticsearch cluster back
by recreating the StatefulSet. The PVCs holding the Elasticsearch data will still exist and the
new replicas will automatically use them.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> 是 &lt;code>Retain&lt;/code> 但 &lt;code>whenScaled&lt;/code> 是 &lt;code>Delete&lt;/code>。&lt;/strong> 这与前一种情况类似，
在增加副本时用保留的 PVCs 快速重构几乎没有什么益处。例如 Elasticsearch 集群就是使用的这种方式。
通常，你需要增大或缩小工作负载来满足业务诉求，同时确保最小数量的副本（例如：3）。
当减少副本时，数据将从已删除的副本迁移出去，保留这些 PVCs 没有任何用处。
但是，这对临时关闭整个 Elasticsearch 集群进行维护时是很有用的。
如果需要使 Elasticsearch 系统脱机，可以通过临时删除 StatefulSet 来实现，
然后通过重新创建 StatefulSet 来恢复 Elasticsearch 集群。
保存 Elasticsearch 数据的 PVCs 不会被删除，新的副本将自动使用它们。&lt;/li>
&lt;/ul>
&lt;!--
Visit the
[documentation](/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies) to
see all the details.
-->
&lt;p>查阅&lt;a href="https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies">文档&lt;/a>
获取更多详细信息。&lt;/p>
&lt;!--
## What’s next?
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;!--
Enable the feature and try it out! Enable the `StatefulSetAutoDeletePVC` feature gate on a cluster,
then create a StatefulSet using the new policy. Test it out and tell us what you think!
-->
&lt;p>启用该功能并尝试一下！在集群上启用 &lt;code>StatefulSetAutoDeletePVC&lt;/code> 功能，然后使用新策略创建 StatefulSet。
测试一下，告诉我们你的体验！&lt;/p>
&lt;!--
I'm very curious to see if this owner reference mechanism works well in practice. For example, we
realized there is no mechanism in Kubernetes for knowing who set a reference, so it’s possible that
the StatefulSet controller may fight with custom controllers that set their own
references. Fortunately, maintaining the existing retention behavior does not involve any new owner
references, so default behavior will be compatible.
-->
&lt;p>我很好奇这个属主引用机制在实践中是否有效。例如，我们意识到 Kubernetes 中没有可以知道谁设置了引用的机制，
因此 StatefulSet 控制器可能会与设置自己的引用的自定义控制器发生冲突。
幸运的是，维护现有的保留行为不涉及任何新属主引用，因此默认行为是兼容的。&lt;/p>
&lt;!--
Please tag any issues you report with the label `sig/apps` and assign them to Matthew Cary
([@mattcary](https://github.com/mattcary) at GitHub).
-->
&lt;p>请用标签 &lt;code>sig/apps&lt;/code> 标记你报告的任何问题，并将它们分配给 Matthew Cary
(在 GitHub上 &lt;a href="https://github.com/mattcary">@mattcary&lt;/a>)。&lt;/p>
&lt;!--
Enjoy!
-->
&lt;p>尽情体验吧！&lt;/p></description></item><item><title>Blog: Kubernetes 1.23：树内存储向 CSI 卷迁移工作的进展更新</title><link>https://kubernetes.io/zh/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/</guid><description>
&lt;!---
layout: blog
title: "Kubernetes 1.23: Kubernetes In-Tree to CSI Volume Migration Status Update"
date: 2021-12-10
slug: storage-in-tree-to-csi-migration-status-update
-->
&lt;!---
**Author:** Jiawei Wang (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Jiawei Wang（谷歌）&lt;/p>
&lt;!---
The Kubernetes in-tree storage plugin to [Container Storage Interface (CSI)](/blog/2019/01/15/container-storage-interface-ga/) migration infrastructure has already been [beta](/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/) since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
-->
&lt;p>自 Kubernetes v1.14 引入容器存储接口（&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface, CSI&lt;/a>）的工作达到 alpha 阶段后，自 v1.17 起，Kubernetes 树内存储插件（in-tree storage plugin）向 CSI 的迁移基础设施已步入 &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">beta 阶段&lt;/a>。&lt;/p>
&lt;!---
Since then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for GA.
This article is intended to give a status update to the feature as well as changes between Kubernetes 1.17 and 1.23. In addition, I will also cover the future roadmap for the CSI migration feature GA for each storage plugin.
-->
&lt;p>自那时起，Kubernetes 存储特别兴趣组（special interest groups, SIG）及其他 Kubernetes 特别兴趣组就在努力确保这一功能的稳定性和兼容性，为正式发布做准备。
本文旨在介绍该功能的最新开发进展，以及 Kubernetes v1.17 到 v1.23 之间的变化。此外，我还将介绍每个存储插件的 CSI 迁移功能达到正式发布阶段的未来路线图。&lt;/p>
&lt;!---
## Quick recap: What is CSI Migration, and why migrate?
The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins.
Kubernetes support for the [Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md#README) has been
[generally available](/blog/2019/01/15/container-storage-interface-ga/) since Kubernetes v1.13.
Support for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).
-->
&lt;h2 id="quick-recap-what-is-csi-migration-and-why-migrate">快速回顾：CSI 迁移功能是什么？为什么要迁移？ &lt;/h2>
&lt;p>容器存储接口旨在帮助 Kubernetes 取代其现有的树内存储驱动机制──特别是供应商的特定插件。自 v1.13 起，Kubernetes 对&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#README">容器存储接口&lt;/a>的支持工作已达到&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">正式发布阶段&lt;/a>。引入对 CSI 驱动的支持，将使得 Kubernetes 和存储后端技术之间的集成工作更易建立和维护。使用 CSI 驱动可以实现更好的可维护性（驱动作者可以决定自己的发布周期和支持生命周期）、减少出现漏洞的机会（得益于更少的树内代码，出现错误的风险会降低。另外，集群操作员可以只选择集群需要的存储驱动）。&lt;/p>
&lt;!---
As more CSI Drivers were created and became production ready, SIG Storage group wanted all Kubernetes users to benefit from the CSI model. However, we cannot break API compatibility with the existing storage API types. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.
-->
&lt;p>随着更多的 CSI 驱动诞生并进入生产就绪阶段，Kubernetes 存储特别兴趣组希望所有 Kubernetes 用户都能从 CSI 模型中受益──然而，我们不应破坏与现有存储 API 类型的 API 兼容性。对此，我们给出的解决方案是 CSI 迁移：该功能实现将树内存储 API 翻译成等效的 CSI API，并把操作委托给一个替换的 CSI 驱动来完成。&lt;/p>
&lt;!---
The CSI migration effort enables the replacement of existing in-tree storage plugins such as `kubernetes.io/gce-pd` or `kubernetes.io/aws-ebs` with a corresponding [CSI driver](https://kubernetes-csi.github.io/docs/introduction.html) from the storage backend.
If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. Existing `StorageClass`, `PersistentVolume` and `PersistentVolumeClaim` objects should continue to work.
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have.
However, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.
-->
&lt;p>CSI 迁移工作使存储后端现有的树内存储插件（如 &lt;code>kubernetes.io/gce-pd&lt;/code> 或 &lt;code>kubernetes.io/aws-ebs&lt;/code>）能够被相应的 &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI 驱动&lt;/a> 所取代。如果 CSI 迁移功能正确发挥作用，Kubernetes 终端用户应该不会注意到有什么变化。现有的 &lt;code>StorageClass&lt;/code>、&lt;code>PersistentVolume&lt;/code> 和 &lt;code>PersistentVolumeClaim&lt;/code> 对象应继续工作。当 Kubernetes 集群管理员更新集群以启用 CSI 迁移功能时，利用到 PVCs&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>（由树内存储插件支持）的现有工作负载将继续像以前一样运作──不过在幕后，Kubernetes 将所有存储管理操作（以前面向树内存储驱动的）交给 CSI 驱动控制。&lt;/p>
&lt;!---
For example, suppose you are a `kubernetes.io/gce-pd` user, after CSI migration, you can still use `kubernetes.io/gce-pd` to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing API/Interface will still function correctly. However, the underlying function calls are all going through the [GCE PD CSI driver](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver) instead of the in-tree Kubernetes function.
-->
&lt;p>举个例子。假设你是 &lt;code>kubernetes.io/gce-pd&lt;/code> 用户，在启用 CSI 迁移功能后，你仍然可以使用 &lt;code>kubernetes.io/gce-pd&lt;/code> 来配置新卷、挂载现有的 GCE-PD 卷或删除现有卷。所有现有的 API/接口 仍将正常工作──只不过，底层功能调用都将通向 &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI 驱动&lt;/a>，而不是 Kubernetes 的树内存储功能。&lt;/p>
&lt;!---
This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.
-->
&lt;p>这使得 Kubernetes 终端用户可以顺利过渡。另外，对于存储插件的开发者，我们可以减少他们维护树内存储插件的负担，并最终将这些插件从 Kubernetes 核心的二进制中移除。&lt;/p>
&lt;!---
## What has been changed, and what's new?
Building on the work done in Kubernetes v1.17 and earlier, the releases since then have
made a series of changes:
-->
&lt;h2 id="what-has-been-changed-and-what-s-new">改进与更新 &lt;/h2>
&lt;p>在 Kubernetes v1.17 及更早的工作基础上，此后的发布有了以下一系列改变：&lt;/p>
&lt;!---
### New feature gates
The Kubernetes v1.21 release deprecated the `CSIMigration{provider}Complete` feature flags, and stopped honoring them. In their place came new feature flags named `InTreePlugin{vendor}Unregister`, that replace the old feature flag and retain all the functionality that `CSIMigration{provider}Complete` provided.
-->
&lt;h3 id="new-feature-gates">新的特性门控（feature gate） &lt;/h3>
&lt;p>Kubernetes v1.21 弃用了 &lt;code>CSIMigration{provider}Complete&lt;/code> 特性参数（feature flag），它们不再生效。取而代之的是名为 &lt;code>InTreePlugin{vendor}Unregister&lt;/code> 的新特性参数，它们保留了 &lt;code>CSIMigration{provider}Complete&lt;/code> 提供的所有功能。&lt;/p>
&lt;!---
`CSIMigration{provider}Complete` was introduced before as a supplementary feature gate once CSI migration is enabled on all of the nodes. This flag unregisters the in-tree storage plugin you specify with the `{provider}` part of the flag name.
-->
&lt;p>&lt;code>CSIMigration{provider}Complete&lt;/code> 是作为 CSI 迁移功能在所有节点上启用后的补充特性门控于之前引入的。这个参数可注销参数名称中 &lt;code>{provider}&lt;/code> 部分所指定的树内存储插件。&lt;/p>
&lt;!---
When you enable that feature gate, then instead of using the in-tree driver code, your cluster directly selects and uses the relevant CSI driver. This happens without any check for whether CSI migration is enabled on the node, or whether you have in fact deployed that CSI driver.
-->
&lt;p>当你启用该特性门控时，你的集群不再使用树内驱动代码，而是直接选择并使用相应的 CSI 驱动。同时，集群并不检查节点上 CSI 迁移功能是否启用，以及 CSI 驱动是否实际部署。&lt;/p>
&lt;!---
While this feature gate is a great helper, SIG Storage (and, I'm sure, lots of cluster operators) also wanted a feature gate that lets you disable an in-tree storage plugin, even without also enabling CSI migration. For example, you might want to disable the EBS storage plugin on a GCE cluster, because EBS volumes are specific to a different vendor's cloud (AWS).
-->
&lt;p>虽然这一特性门控是一个很好的帮手，但 Kubernetes 存储特别兴趣组（以及，我相信还有很多集群操作员）同样希望有一个特性门控可以让你即使在不启用 CSI 迁移功能时，也能禁用树内存储插件。例如，你可能希望在一个 GCE 集群上禁用 EBS 存储插件，因为 EBS 卷是其他供应商的云（AWS）所专有的。&lt;/p>
&lt;!---
To make this possible, Kubernetes v1.21 introduced a new feature flag set: `InTreePlugin{vendor}Unregister`.
`InTreePlugin{vendor}Unregister` is a standalone feature gate that can be enabled and disabled independently from CSI Migration. When enabled, the component will not register the specific in-tree storage plugin to the supported list. If the cluster operator only enables this flag, end users will get an error from PVC saying it cannot find the plugin when the plugin is used. The cluster operator may want to enable this regardless of CSI Migration if they do not want to support the legacy in-tree APIs and only support CSI moving forward.
-->
&lt;p>为了使这成为可能，Kubernetes v1.21 引入了一个新的特性参数集合：&lt;code>InTreePlugin{vendor}Unregister&lt;/code>。&lt;/p>
&lt;p>&lt;code>InTreePlugin{vendor}Unregister&lt;/code> 是一种特性门控，可以独立于 CSI 迁移功能来启用或禁用。当启用此种特性门控时，组件将不会把相应的树内存储插件注册到支持的列表中。如果集群操作员只启用了这种参数，终端用户将在使用该插件的 PVC&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> 处遇到错误，提示其找不到插件。如果集群操作员不想支持过时的树内存储 API，只支持 CSI，那么他们可能希望启用这种特性门控而不考虑 CSI 迁移功能。&lt;/p>
&lt;!---
### Observability
Kubernetes v1.21 introduced [metrics](https://github.com/kubernetes/kubernetes/issues/98279) for tracking CSI migration.
You can use these metrics to observe how your cluster is using storage services and whether access to that storage is using the legacy in-tree driver or its CSI-based replacement.
-->
&lt;h3 id="observability">可观察性 &lt;/h3>
&lt;p>Kubernetes v1.21 引入了跟踪 CSI 迁移功能的&lt;a href="https://github.com/kubernetes/kubernetes/issues/98279">指标&lt;/a>。你可以使用这些指标来观察你的集群是如何使用存储服务的，以及对该存储的访问使用的是传统的树内驱动还是基于 CSI 的替代。&lt;/p>
&lt;!---
| Components | Metrics | Notes |
| -------------------------------------------- | ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Kube-Controller-Manager | storage_operation_duration_seconds | A new label `migrated` is added to the metric to indicate whether this storage operation is a CSI migration operation(string value `true` for enabled and `false` for not enabled). |
| Kubelet | csi_operations_seconds | The new metric exposes labels including `driver_name`, `method_name`, `grpc_status_code` and `migrated`. The meaning of these labels is identical to `csi_sidecar_operations_seconds`. |
| CSI Sidecars(provisioner, attacher, resizer) | csi_sidecar_operations_seconds | A new label `migrated` is added to the metric to indicate whether this storage operation is a CSI migration operation(string value `true` for enabled and `false` for not enabled). |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>指标&lt;/th>
&lt;th>注释&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kube-Controller-Manager&lt;/td>
&lt;td>storage_operation_duration_seconds&lt;/td>
&lt;td>一个新的标签 &lt;code>migrated&lt;/code> 被添加到指标中，以表明该存储操作是否由 CSI 迁移功能操作（字符串值为 &lt;code>true&lt;/code> 表示启用，&lt;code>false&lt;/code> 表示未启用）。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kubelet&lt;/td>
&lt;td>csi_operations_seconds&lt;/td>
&lt;td>新的指标提供的标签包括 &lt;code>driver_name&lt;/code>、&lt;code>method_name&lt;/code>、&lt;code>grpc_status_code&lt;/code> 和 &lt;code>migrated&lt;/code>。这些标签的含义与 &lt;code>csi_sidecar_operations_seconds&lt;/code> 相同。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSI Sidecars(provisioner, attacher, resizer)&lt;/td>
&lt;td>csi_sidecar_operations_seconds&lt;/td>
&lt;td>一个新的标签 &lt;code>migrated&lt;/code> 被添加到指标中，以表明该存储操作是否由 CSI 迁移功能操作（字符串值为 &lt;code>true&lt;/code> 表示启用，&lt;code>false&lt;/code> 表示未启用）。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
### Bug fixes and feature improvement
We have fixed numerous bugs like dangling attachment, garbage collection, incorrect topology label through the help of our beta testers.
-->
&lt;h3 id="bug-fixes-and-feature-improvement">错误修复和功能改进 &lt;/h3>
&lt;p>籍由 beta 测试人员的帮助，我们修复了许多错误──如悬空附件、垃圾收集、拓扑标签错误等。&lt;/p>
&lt;!---
### Cloud Provider &amp;&amp; Cluster Lifecycle Collaboration
SIG Storage has been working closely with SIG Cloud Provider and SIG Cluster Lifecycle on the rollout of CSI migration.
If you are a user of a managed Kubernetes service, check with your provider if anything needs to be done. In many cases, the provider will manage the migration and no additional work is required.
-->
&lt;h3 id="cloud-provider-cluster-lifecycle-collaboration">与 Kubernetes 云提供商特别兴趣组、集群生命周期特别兴趣组的合作 &lt;/h3>
&lt;p>Kubernetes 存储特别兴趣组与云提供商特别兴趣组和集群生命周期特别兴趣组，正为了 CSI 迁移功能上线而密切合作。&lt;/p>
&lt;p>如果你采用托管 Kubernetes 服务，请询问你的供应商是否有什么工作需要完成。在许多情况下，供应商将管理迁移，你不需要做额外的工作。&lt;/p>
&lt;!---
If you use a distribution of Kubernetes, check its official documentation for information about support for this feature. For the CSI Migration feature graduation to GA, SIG Storage and SIG Cluster Lifecycle are collaborating towards making the migration mechanisms available in tooling (such as kubeadm) as soon as they're available in Kubernetes itself.
-->
&lt;p>如果你使用的是 Kubernetes 的发行版，请查看其官方文档，了解对该功能的支持情况。对于已进入正式发布阶段的 CSI 迁移功能，Kubernetes 存储特别兴趣组正与Kubernetes 集群生命周期特别兴趣组合作，以便在这些功能于 Kubernetes 中可用时，使迁移机制也进入到周边工具（如 kubeadm）中。&lt;/p>
&lt;!---
## What is the timeline / status? {#timeline-and-status}
The current and targeted releases for each individual driver is shown in the table below:
-->
&lt;h2 id="timeline-and-status">时间计划及当前状态 &lt;/h2>
&lt;p>各驱动的当前发布及目标发布如下表所示：&lt;/p>
&lt;!---
| Driver | Alpha | Beta (in-tree deprecated) | Beta (on-by-default) | GA | Target "in-tree plugin" removal |
| ---------------- | ----- | ------------------------- | -------------------- | ------------- | ------------------------------- |
| AWS EBS | 1.14 | 1.17 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| GCE PD | 1.14 | 1.17 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| OpenStack Cinder | 1.14 | 1.18 | 1.21 | 1.24 (Target) | 1.26 (Target) |
| Azure Disk | 1.15 | 1.19 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| Azure File | 1.15 | 1.21 | 1.24 (Target) | 1.25 (Target) | 1.27 (Target) |
| vSphere | 1.18 | 1.19 | 1.24 (Target) | 1.25 (Target) | 1.27 (Target) |
| Ceph RBD | 1.23 |
| Portworx | 1.23 |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>驱动&lt;/th>
&lt;th>Alpha&lt;/th>
&lt;th>Beta（启用树内插件）&lt;/th>
&lt;th>Beta（默认启用）&lt;/th>
&lt;th>正式发布&lt;/th>
&lt;th>目标：移除“树内存储插件”&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AWS EBS&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GCE PD&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenStack Cinder&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure Disk&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure File&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vSphere&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ceph RBD&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Portworx&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
The following storage drivers will not have CSI migration support. The ScaleIO driver was already removed; the others are deprecated and will be removed from core Kubernetes.
-->
&lt;p>以下存储驱动将不会支持 CSI 迁移功能。其中 ScaleIO 驱动已经被移除；其他驱动都被弃用，并将从 Kubernetes 核心中删除。&lt;/p>
&lt;!---
| Driver | Deprecated | Code Removal |
| --------- | ---------- | ------------- |
| ScaleIO | 1.16 | 1.22 |
| Flocker | 1.22 | 1.25 (Target) |
| Quobyte | 1.22 | 1.25 (Target) |
| StorageOS | 1.22 | 1.25 (Target) |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>驱动&lt;/th>
&lt;th>被弃用&lt;/th>
&lt;th>代码移除&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ScaleIO&lt;/td>
&lt;td>1.16&lt;/td>
&lt;td>1.22&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Flocker&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Quobyte&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>StorageOS&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
## What's next?
With more CSI drivers graduating to GA, we hope to soon mark the overall CSI Migration feature as GA. We are expecting cloud provider in-tree storage plugins code removal to happen by Kubernetes v1.26 and v1.27.
-->
&lt;h2 id="what-s-next">下一步的计划 &lt;/h2>
&lt;p>随着更多的 CSI 驱动进入正式发布阶段，我们希望尽快将整个 CSI 迁移功能标记为正式发布状态。我们计划在 Kubernetes v1.26 和 v1.27 之前移除云提供商树内存储插件的代码。&lt;/p>
&lt;!---
## What should I do as a user?
Note that all new features for the Kubernetes storage system (such as volume snapshotting) will only be added to the CSI interface. Therefore, if you are starting up a new cluster, creating stateful applications for the first time, or require these new features we recommend using CSI drivers natively (instead of the in-tree volume plugin API). Follow the [updated user guides for CSI drivers](https://kubernetes-csi.github.io/docs/drivers.html) and use the new CSI APIs.
-->
&lt;h2 id="what-should-i-do-as-a-user">作为用户，我应该做什么？ &lt;/h2>
&lt;p>请注意，Kubernetes 存储系统的所有新功能（如卷快照）将只被添加到 CSI 接口。因此，如果你正在启动一个新的集群、首次创建有状态的应用程序，或者需要这些新功能，我们建议你在本地使用 CSI 驱动（而不是树内卷插件 API）。遵循&lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">最新的 CSI 驱动用户指南&lt;/a>并使用新的 CSI API。&lt;/p>
&lt;!---
However, if you choose to roll a cluster forward or continue using specifications with the legacy volume APIs, CSI Migration will ensure we continue to support those deployments with the new CSI drivers. However, if you want to leverage new features like snapshot, it will require a manual migration to re-import an existing intree PV as a CSI PV.
-->
&lt;p>然而，如果您选择沿用现有集群或继续使用传统卷 API 的规约，CSI 迁移功能将确保我们通过新 CSI 驱动继续支持这些部署。但是，如果您想利用快照等新功能，则需要进行手动迁移，将现有的树内持久卷重新导入为 CSI 持久卷。&lt;/p>
&lt;!---
## How do I get involved?
The Kubernetes Slack channel [#csi-migration](https://kubernetes.slack.com/messages/csi-migration) along with any of the standard [SIG Storage communication channels](https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact) are great mediums to reach out to the SIG Storage and migration working group teams.
-->
&lt;h2 id="how-do-i-get-involved">我如何参与其中？ &lt;/h2>
&lt;p>Kubernetes Slack 频道 &lt;a href="https://kubernetes.slack.com/messages/csi-migration">#csi-migration&lt;/a> 以及任何一个标准的 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage 通信频道&lt;/a>都是与 Kubernetes 存储特别兴趣组和迁移工作组团队联系的绝佳媒介。&lt;/p>
&lt;!---
This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:
* Michelle Au (msau42)
* Jan Šafránek (jsafrane)
* Hemant Kumar (gnufied)
-->
&lt;p>该项目，和其他所有 Kubernetes 项目一样，是许多来自不同背景的贡献者共同努力的结果。我们非常感谢在过去几个季度里挺身而出帮助推动项目发展的贡献者们：&lt;/p>
&lt;ul>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;li>Jan Šafránek (jsafrane)&lt;/li>
&lt;li>Hemant Kumar (gnufied)&lt;/li>
&lt;/ul>
&lt;!---
Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:
* Andy Zhang (andyzhangz)
* Divyen Patel (divyenpatel)
* Deep Debroy (ddebroy)
* Humble Devassy Chirammal (humblec)
* Jing Xu (jingxu97)
* Jordan Liggitt (liggitt)
* Matthew Cary (mattcary)
* Matthew Wong (wongma7)
* Neha Arora (nearora-msft)
* Oksana Naumov (trierra)
* Saad Ali (saad-ali)
* Tim Bannister (sftim)
* Xing Yang (xing-yang)
-->
&lt;p>特别感谢以下人士对 CSI 迁移功能的精辟评论、全面考虑和宝贵贡献：&lt;/p>
&lt;ul>
&lt;li>Andy Zhang (andyzhangz)&lt;/li>
&lt;li>Divyen Patel (divyenpatel)&lt;/li>
&lt;li>Deep Debroy (ddebroy)&lt;/li>
&lt;li>Humble Devassy Chirammal (humblec)&lt;/li>
&lt;li>Jing Xu (jingxu97)&lt;/li>
&lt;li>Jordan Liggitt (liggitt)&lt;/li>
&lt;li>Matthew Cary (mattcary)&lt;/li>
&lt;li>Matthew Wong (wongma7)&lt;/li>
&lt;li>Neha Arora (nearora-msft)&lt;/li>
&lt;li>Oksana Naumov (trierra)&lt;/li>
&lt;li>Saad Ali (saad-ali)&lt;/li>
&lt;li>Tim Bannister (sftim)&lt;/li>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;/ul>
&lt;!---
Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the [Kubernetes Storage Special Interest Group (SIG)](https://github.com/kubernetes/community/tree/master/sig-storage). We’re rapidly growing and always welcome new contributors.
-->
&lt;p>有兴趣参与 CSI 或 Kubernetes 存储系统任何部分的设计和开发的人，请加入 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes 存储特别兴趣组&lt;/a>。我们正在迅速成长，并一直欢迎新的贡献者。&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>持久卷申领（PersistentVolumeClaim，PVC）&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Blog: Kubernetes 1.23：IPv4/IPv6 双协议栈网络达到 GA</title><link>https://kubernetes.io/zh/blog/2021/12/08/dual-stack-networking-ga/</link><pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/12/08/dual-stack-networking-ga/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.23: Dual-stack IPv4/IPv6 Networking Reaches GA'
date: 2021-12-08
slug: dual-stack-networking-ga
-->
&lt;!--
**Author:** Bridget Kromhout (Microsoft)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Bridget Kromhout (微软)&lt;/p>
&lt;!--
"When will Kubernetes have IPv6?" This question has been asked with increasing frequency ever since alpha support for IPv6 was first added in k8s v1.9. While Kubernetes has supported IPv6-only clusters since v1.18, migration from IPv4 to IPv6 was not yet possible at that point. At long last, [dual-stack IPv4/IPv6 networking](https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack/) has reached general availability (GA) in Kubernetes v1.23.
What does dual-stack networking mean for you? Let’s take a look…
-->
&lt;p>“Kubernetes 何时支持 IPv6？” 自从 k8s v1.9 版本中首次添加对 IPv6 的 alpha 支持以来，这个问题的讨论越来越频繁。
虽然 Kubernetes 从 v1.18 版本开始就支持纯 IPv6 集群，但当时还无法支持 IPv4 迁移到 IPv6。
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack/">IPv4/IPv6 双协议栈网络&lt;/a>
在 Kubernetes v1.23 版本中进入正式发布（GA）阶段。&lt;/p>
&lt;p>让我们来看看双协议栈网络对你来说意味着什么？&lt;/p>
&lt;!--
## Service API updates
-->
&lt;h2 id="更新-service-api">更新 Service API&lt;/h2>
&lt;!--
[Services](/docs/concepts/services-networking/service/) were single-stack before 1.20, so using both IP families meant creating one Service per IP family. The user experience was simplified in 1.20, when Services were re-implemented to allow both IP families, meaning a single Service can handle both IPv4 and IPv6 workloads. Dual-stack load balancing is possible between services running any combination of IPv4 and IPv6.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/">Services&lt;/a> 在 1.20 版本之前是单协议栈的，
因此，使用两个 IP 协议族意味着需为每个 IP 协议族创建一个 Service。在 1.20 版本中对用户体验进行简化，
重新实现了 Service 以支持两个 IP 协议族，这意味着一个 Service 就可以处理 IPv4 和 IPv6 协议。
对于 Service 而言，任意的 IPv4 和 IPv6 协议组合都可以实现负载均衡。&lt;/p>
&lt;!--
The Service API now has new fields to support dual-stack, replacing the single ipFamily field.
* You can select your choice of IP family by setting `ipFamilyPolicy` to one of three options: SingleStack, PreferDualStack, or RequireDualStack. A service can be changed between single-stack and dual-stack (within some limits).
* Setting `ipFamilies` to a list of families assigned allows you to set the order of families used.
* `clusterIPs` is inclusive of the previous `clusterIP` but allows for multiple entries, so it’s no longer necessary to run duplicate services, one in each of the two IP families. Instead, you can assign cluster IP addresses in both IP families.
-->
&lt;p>Service API 现在有了支持双协议栈的新字段，取代了单一的 ipFamily 字段。&lt;/p>
&lt;ul>
&lt;li>你可以通过将 &lt;code>ipFamilyPolicy&lt;/code> 字段设置为 &lt;code>SingleStack&lt;/code>、&lt;code>PreferDualStack&lt;/code> 或
&lt;code>RequireDualStack&lt;/code> 来设置 IP 协议族。Service 可以在单协议栈和双协议栈之间进行转换(在某些限制内)。&lt;/li>
&lt;li>设置 &lt;code>ipFamilies&lt;/code> 为指定的协议族列表，可用来设置使用协议族的顺序。&lt;/li>
&lt;li>'clusterIPs' 的能力在涵盖了之前的 'clusterIP'的情况下，还允许设置多个 IP 地址。
所以不再需要运行重复的 Service，在两个 IP 协议族中各运行一个。你可以在两个 IP 协议族中分配集群 IP 地址。&lt;/li>
&lt;/ul>
&lt;!--
Note that Pods are also dual-stack. For a given pod, there is no possibility of setting multiple IP addresses in the same family.
-->
&lt;p>请注意，Pods 也是双协议栈的。对于一个给定的 Pod，不可能在同一协议族中设置多个 IP 地址。&lt;/p>
&lt;!--
## Default behavior remains single-stack
-->
&lt;h2 id="默认行为仍然是单协议栈">默认行为仍然是单协议栈&lt;/h2>
&lt;!--
Starting in 1.20 with the re-implementation of dual-stack services as alpha, the underlying networking for Kubernetes has included dual-stack whether or not a cluster was configured with the feature flag to enable dual-stack.
-->
&lt;p>从 1.20 版本开始，重新实现的双协议栈服务处于 Alpha 阶段，无论集群是否配置了启用双协议栈的特性标志，
Kubernetes 的底层网络都已经包括了双协议栈。&lt;/p>
&lt;!--
Kubernetes 1.23 removed that feature flag as part of graduating the feature to stable. Dual-stack networking is always available if you want to configure it. You can set your cluster network to operate as single-stack IPv4, as single-stack IPv6, or as dual-stack IPv4/IPv6.
-->
&lt;p>Kubernetes 1.23 删除了这个特性标志，说明该特性已经稳定。
如果你想要配置双协议栈网络，这一能力总是存在的。
你可以将集群网络设置为 IPv4 单协议栈 、IPv6 单协议栈或 IPV4/IPV6 双协议栈 。&lt;/p>
&lt;!--
While Services are set according to what you configure, Pods default to whatever the CNI plugin sets. If your CNI plugin assigns single-stack IPs, you will have single-stack unless `ipFamilyPolicy` specifies PreferDualStack or RequireDualStack. If your CNI plugin assigns dual-stack IPs, `pod.status.PodIPs` defaults to dual-stack.
-->
&lt;p>虽然 Service 是根据你的配置设置的，但 Pod 默认是由 CNI 插件设置的。
如果你的 CNI 插件分配单协议栈 IP，那么就是单协议栈，除非 &lt;code>ipFamilyPolicy&lt;/code> 设置为 &lt;code>PreferDualStack&lt;/code> 或 &lt;code>RequireDualStack&lt;/code>。
如果你的 CNI 插件分配双协议栈 IP，则 &lt;code>pod.status.PodIPs&lt;/code> 默认为双协议栈。&lt;/p>
&lt;!--
Even though dual-stack is possible, it is not mandatory to use it. Examples in the documentation show the variety possible in [dual-stack service configurations](/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios).
-->
&lt;p>尽管双协议栈是可用的，但并不强制你使用它。
在&lt;a href="https://kubernetes.io/zh/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios">双协议栈服务配置&lt;/a>
文档中的示例列出了可能出现的各种场景.&lt;/p>
&lt;!--
## Try dual-stack right now
-->
&lt;h2 id="现在尝试双协议栈">现在尝试双协议栈&lt;/h2>
&lt;!--
While upstream Kubernetes now supports [dual-stack networking](/docs/concepts/services-networking/dual-stack/) as a GA or stable feature, each provider’s support of dual-stack Kubernetes may vary. Nodes need to be provisioned with routable IPv4/IPv6 network interfaces. Pods need to be dual-stack. The [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) is what assigns the IP addresses to the Pods, so it's the network plugin being used for the cluster that needs to support dual-stack. Some Container Network Interface (CNI) plugins support dual-stack, as does kubenet.
-->
&lt;p>虽然现在上游 Kubernetes 支持&lt;a href="https://kubernetes.io/zh/docs/concepts/services-networking/dual-stack/">双协议栈网络&lt;/a>
作为 GA 或稳定特性，但每个提供商对双协议栈 Kubernetes 的支持可能会有所不同。节点需要提供可路由的 IPv4/IPv6 网络接口。
Pod 需要是双协议栈的。&lt;a href="https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件&lt;/a>
是用来为 Pod 分配 IP 地址的，所以集群需要支持双协议栈的网络插件。一些容器网络接口（CNI）插件支持双协议栈，例如 kubenet。&lt;/p>
&lt;!--
Ecosystem support of dual-stack is increasing; you can create [dual-stack clusters with kubeadm](/docs/setup/production-environment/tools/kubeadm/dual-stack-support/), try a [dual-stack cluster locally with KIND](https://kind.sigs.k8s.io/docs/user/configuration/#ip-family), and deploy dual-stack clusters in cloud providers (after checking docs for CNI or kubenet availability).
-->
&lt;p>支持双协议栈的生态系统在不断壮大；你可以使用
&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/dual-stack-support/">kubeadm 创建双协议栈集群&lt;/a>,
在本地尝试用 &lt;a href="https://kind.sigs.k8s.io/docs/user/configuration/#ip-family">KIND 创建双协议栈集群&lt;/a>，
还可以将双协议栈集群部署到云上（在查阅 CNI 或 kubenet 可用性的文档之后）&lt;/p>
&lt;!--
## Get involved with SIG Network
-->
&lt;h2 id="加入-network-sig">加入 Network SIG&lt;/h2>
&lt;!--
SIG-Network wants to learn from community experiences with dual-stack networking to find out more about evolving needs and your use cases. The [SIG-network update video from KubeCon NA 2021](https://www.youtube.com/watch?v=uZ0WLxpmBbY&amp;list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP&amp;index=4) summarizes the SIG’s recent updates, including dual-stack going to stable in 1.23.
-->
&lt;p>SIG-Network 希望从双协议栈网络的社区体验中学习，以了解更多不断变化的需求和你的用例信息。
&lt;a href="https://www.youtube.com/watch?v=uZ0WLxpmBbY&amp;amp;list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP&amp;amp;index=4">SIG-network 更新了来自 KubeCon 2021 北美大会的视频&lt;/a>
总结了 SIG 最近的更新，包括双协议栈将在 1.23 版本中稳定。&lt;/p>
&lt;!--
The current SIG-Network [KEPs](https://github.com/orgs/kubernetes/projects/10) and [issues](https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork) on GitHub illustrate the SIG’s areas of emphasis. The [dual-stack API server](https://github.com/kubernetes/enhancements/issues/2438) is one place to consider contributing.
-->
&lt;p>当前 SIG-Network 在 GitHub 上的 &lt;a href="https://github.com/orgs/kubernetes/projects/10">KEPs&lt;/a> 和
&lt;a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork">issues&lt;/a>
说明了该 SIG 的重点领域。&lt;a href="https://github.com/kubernetes/enhancements/issues/2438">双协议栈 API 服务器&lt;/a>
是一个考虑贡献的方向。&lt;/p>
&lt;!--
[SIG-Network meetings](https://github.com/kubernetes/community/tree/master/sig-network#meetings) are a friendly, welcoming venue for you to connect with the community and share your ideas. Looking forward to hearing from you!
-->
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network#meetings">SIG-Network 会议&lt;/a>
是一个友好、热情的场所，你可以与社区联系并分享你的想法。期待你的加入！&lt;/p>
&lt;!--
## Acknowledgments
-->
&lt;h2 id="致谢">致谢&lt;/h2>
&lt;!--
The dual-stack networking feature represents the work of many Kubernetes contributors. Thanks to all who contributed code, experience reports, documentation, code reviews, and everything in between. Bridget Kromhout details this community effort in [Dual-Stack Networking in Kubernetes](https://containerjournal.com/features/dual-stack-networking-in-kubernetes/). KubeCon keynotes by Tim Hockin &amp; Khaled (Kal) Henidak in 2019 ([The Long Road to IPv4/IPv6 Dual-stack Kubernetes](https://www.youtube.com/watch?v=o-oMegdZcg4)) and by Lachlan Evenson in 2021 ([And Here We Go: Dual-stack Networking in Kubernetes](https://www.youtube.com/watch?v=lVrt8F2B9CM)) talk about the dual-stack journey, spanning five years and a great many lines of code.
-->
&lt;p>许多 Kubernetes 贡献者为双协议栈网络做出了贡献。感谢所有贡献了代码、经验报告、文档、代码审查以及其他工作的人。
Bridget Kromhout 在 &lt;a href="https://containerjournal.com/features/dual-stack-networking-in-kubernetes/">Kubernetes的双协议栈网络&lt;/a>
中详细介绍了这项社区工作。Tim Hockin 和 Khaled (Kal) Henidak 在 2019 年的 KubeCon 大会演讲
（&lt;a href="https://www.youtube.com/watch?v=o-oMegdZcg4">Kubernetes 通往 IPv4/IPv6 双协议栈的漫漫长路&lt;/a>）
和 Lachlan Evenson 在 2021 年演讲（&lt;a href="https://www.youtube.com/watch?v=o-oMegdZcg4">我们来啦，Kubernetes 双协议栈网络&lt;/a>）
中讨论了双协议栈的发展旅程，耗时 5 年和海量代码。&lt;/p></description></item><item><title>Blog: 公布 2021 年指导委员会选举结果</title><link>https://kubernetes.io/zh/blog/2021/11/08/steering-committee-results-2021/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/11/08/steering-committee-results-2021/</guid><description>
&lt;!--
layout: blog
title: "Announcing the 2021 Steering Committee Election Results"
date: 2021-11-08
slug: steering-committee-results-2021
-->
&lt;!--
**Author**: Kaslin Fields
-->
&lt;p>&lt;strong>作者&lt;/strong>：Kaslin Fields&lt;/p>
&lt;!--
The [2021 Steering Committee Election](https://github.com/kubernetes/community/tree/master/events/elections/2021) is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2021. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.
-->
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/events/elections/2021">2021 年指导委员会选举&lt;/a>现已完成。
Kubernetes 指导委员会由 7 个席位组成，其中 4 个席位将在 2021 年进行选举。
新任委员会成员任期 2 年，所有成员均由 Kubernetes 社区选举产生。&lt;/p>
&lt;!--
This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their [charter](https://github.com/kubernetes/steering/blob/master/charter.md).
-->
&lt;p>这个社区机构非常重要，因为它监督整个 Kubernetes 项目的治理。
你可以在其&lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">章程&lt;/a>中了解更多关于指导委员会的角色。&lt;/p>
&lt;!--
## Results
-->
&lt;h2 id="选举结果">选举结果&lt;/h2>
&lt;!--
Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):
-->
&lt;p>祝贺当选的委员会成员，他们的两年任期即刻生效（按 GitHub handle 字母排序）:&lt;/p>
&lt;!--
* **Christoph Blecker ([@cblecker](https://github.com/cblecker)), Red Hat**
* **Stephen Augustus ([@justaugustus](https://github.com/justaugustus)), Cisco**
* **Paris Pittman ([@parispittman](https://github.com/parispittman)), Apple**
* **Tim Pepper ([@tpepper](https://github.com/tpepper)), VMware**
-->
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker（&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>）， 红帽&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Stephen Augustus（&lt;a href="https://github.com/justaugustus">@justaugustus&lt;/a>）， 思科&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman（&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>)， 苹果&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Tim Pepper（&lt;a href="https://github.com/tpepper">@tpepper&lt;/a>）， VMware&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
They join continuing members:
-->
&lt;p>他们加入永久成员：&lt;/p>
&lt;!--
* **Davanum Srinivas ([@dims](https://github.com/dims)), VMware**
* **Jordan Liggitt ([@liggitt](https://github.com/liggitt)), Google**
* **Bob Killen ([@mrbobbytables](https://github.com/mrbobbytables)), Google**
-->
&lt;ul>
&lt;li>&lt;strong>Davanum Srinivas（&lt;a href="https://github.com/dims">@dims&lt;/a>）， VMware&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Jordan Liggitt （&lt;a href="https://github.com/liggitt">@liggitt&lt;/a>）， 谷歌&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Bob Killen （&lt;a href="https://github.com/mrbobbytables">@mrbobbytables&lt;/a>）， 谷歌&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
Paris Pittman and Christoph Blecker are returning Steering Committee Members.
-->
&lt;p>Paris Pittman 和 Christoph Blecker 将回到指导委员会。&lt;/p>
&lt;!--
## Big Thanks
-->
&lt;h2 id="非常感谢">非常感谢&lt;/h2>
&lt;!--
Thank you and congratulations on a successful election to this round’s election officers:
-->
&lt;p>感谢并祝贺完成本轮成功选举的选举官们:&lt;/p>
&lt;ul>
&lt;li>Alison Dowdney, (&lt;a href="https://github.com/alisondy">@alisondy&lt;/a>)&lt;/li>
&lt;li>Noah Kantrowitz (&lt;a href="https://github.com/coderanger">@coderanger&lt;/a>)&lt;/li>
&lt;li>Josh Berkus (&lt;a href="https://github.com/jberkus">@jberkus&lt;/a>)&lt;/li>
&lt;/ul>
&lt;!--
Special thanks to Arnaud Meukam ([@ameukam](https://github.com/ameukam)), k8s-infra liaison, who enabled our voting software on community-owned infrastructure.
-->
&lt;p>特别感谢 k8s-infra 联络员 Arnaud Meukam（&lt;a href="https://github.com/ameukam">@ameukam&lt;/a>），
他在社区的基础设施上启动了我们的投票软件。&lt;/p>
&lt;!--
Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:
-->
&lt;p>感谢荣誉退休的指导委员会成员。对你们之前对社区的贡献表示感谢:&lt;/p>
&lt;ul>
&lt;li>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>)&lt;/li>
&lt;li>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>)&lt;/li>
&lt;/ul>
&lt;!--
And thank you to all the candidates who came forward to run for election.
-->
&lt;p>感谢所有前来参加竞选的候选人。&lt;/p>
&lt;!--
## Get Involved with the Steering Committee
-->
&lt;h2 id="参与指导委员会">参与指导委员会&lt;/h2>
&lt;!--
This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee [backlog items](https://github.com/kubernetes/steering/projects/1) and weigh in by filing an issue or creating a PR against their [repo](https://github.com/kubernetes/steering). They have an open meeting on [the first Monday at 9:30am PT of every month](https://github.com/kubernetes/steering) and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list steering@kubernetes.io.
-->
&lt;p>与所有 Kubernetes 一样，这个管理机构对所有人开放。
你可以查看指导委员会的&lt;a href="https://github.com/kubernetes/steering/projects/1">待办事项&lt;/a>，
通过在他们的 &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>
中提交一个 issue 或创建一个 PR 来参与讨论。
他们在&lt;a href="https://github.com/kubernetes/steering">每月的第一个星期一上午 9:30&lt;/a> 举行公开会议，
并定期参加会见我们的贡献者活动。也可以通过他们的公共邮件列表 &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a> 联系他们。&lt;/p>
&lt;!--
You can see what the Steering Committee meetings are all about by watching past meetings on the [YouTube Playlist](https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM).
-->
&lt;p>你可以在 &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube 播放列表&lt;/a>
上观看之前的会议视频，了解指导委员会的会议讨论内容。&lt;/p>
&lt;hr>
&lt;!--
_This post was written by the [Upstream Marketing Working Group](https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing). If you want to write stories about the Kubernetes community, learn more about us._
-->
&lt;p>&lt;em>本文是由&lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing">上游营销工作组&lt;/a>撰写的。
如果你想撰写有关 Kubernetes 社区的故事，请了解更多关于我们的信息。&lt;/em>&lt;/p></description></item><item><title>Blog: 关注 SIG Node</title><link>https://kubernetes.io/zh/blog/2021/09/27/sig-node-spotlight-2021/</link><pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2021/09/27/sig-node-spotlight-2021/</guid><description>
&lt;!--
---
layout: blog
title: "Spotlight on SIG Node"
date: 2021-09-27
slug: sig-node-spotlight-2021
---
-->
&lt;p>&lt;strong>Author:&lt;/strong> Dewan Ahmed, Red Hat&lt;/p>
&lt;!--
**Author:** Dewan Ahmed, Red Hat
-->
&lt;!--
## Introduction
In Kubernetes, a _Node_ is a representation of a single machine in your cluster. [SIG Node](https://github.com/kubernetes/community/tree/master/sig-node) owns that very important Node component and supports various subprojects such as Kubelet, Container Runtime Interface (CRI) and more to support how the pods and host resources interact. In this blog, we have summarized our conversation with [Elana Hashman (EH)](https://twitter.com/ehashdn) &amp; [Sergey Kanzhelev (SK)](https://twitter.com/SergeyKanzhelev), who walk us through the various aspects of being a part of the SIG and share some insights about how others can get involved.
-->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>在 Kubernetes 中，一个 &lt;em>Node&lt;/em> 是你集群中的某台机器。
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> 负责这一非常重要的 Node 组件并支持各种子项目，
如 Kubelet, Container Runtime Interface (CRI) 以及其他支持 Pod 和主机资源间交互的子项目。
在这篇文章中，我们总结了和 &lt;a href="https://twitter.com/ehashdn">Elana Hashman (EH)&lt;/a> &amp;amp; &lt;a href="https://twitter.com/SergeyKanzhelev">Sergey Kanzhelev (SK)&lt;/a> 的对话，是他们带领我们了解作为此 SIG 一份子的各个方面，并分享一些关于其他人如何参与的见解。&lt;/p>
&lt;!--
## A summary of our conversation
### Could you tell us a little about what SIG Node does?
SK: SIG Node is a vertical SIG responsible for the components that support the controlled interactions between the pods and host resources. We manage the lifecycle of pods that are scheduled to a node. This SIG's focus is to enable a broad set of workload types, including workloads with hardware specific or performance sensitive requirements. All while maintaining isolation boundaries between pods on a node, as well as the pod and the host. This SIG maintains quite a few components and has many external dependencies (like container runtimes or operating system features), which makes the complexity we deal with huge. We tame the complexity and aim to continuously improve node reliability.
-->
&lt;h2 id="我们的对话总结">我们的对话总结&lt;/h2>
&lt;h3 id="你能告诉我们一些关于-sig-node-的工作吗">你能告诉我们一些关于 SIG Node 的工作吗？&lt;/h3>
&lt;p>SK：SIG Node 是一个垂直 SIG，负责支持 Pod 和主机资源之间受控互动的组件。我们管理被调度到节点上的 Pod 的生命周期。
这个 SIG 的重点是支持广泛的工作负载类型，包括具有硬件特性或性能敏感要求的工作负载。同时保持节点上 Pod 之间的隔离边界，以及 Pod 和主机的隔离边界。
这个 SIG 维护了相当多的组件，并有许多外部依赖（如容器运行时间或操作系统功能），这使得我们处理起来十分复杂。但我们战胜了这种复杂度，旨在不断提高节点的可靠性。&lt;/p>
&lt;!--
### "SIG Node is a vertical SIG" could you explain a bit more?
EH: There are two kinds of SIGs: horizontal and vertical. Horizontal SIGs are concerned with a particular function of every component in Kubernetes: for example, SIG Security considers security aspects of every component in Kubernetes, or SIG Instrumentation looks at the logs, metrics, traces and events of every component in Kubernetes. Such SIGs don't tend to own a lot of code.
Vertical SIGs, on the other hand, own a single component, and are responsible for approving and merging patches to that code base. SIG Node owns the "Node" vertical, pertaining to the kubelet and its lifecycle. This includes the code for the kubelet itself, as well as the node controller, the container runtime interface, and related subprojects like the node problem detector.
-->
&lt;h3 id="你能再解释一下-sig-node-是一种垂直-sig-的含义吗">你能再解释一下 “SIG Node 是一种垂直 SIG” 的含义吗？&lt;/h3>
&lt;p>EH：有两种 SIG：横向和垂直。横向 SIG 关注 Kubernetes 中每个组件的特定功能：例如，SIG Security 考虑 Kubernetes 中每个组件的安全方面，或者 SIG Instrumentation 关注 Kubernetes 中每个组件的日志、度量、跟踪和事件。
这样的 SIG 并不太会拥有大量的代码。&lt;/p>
&lt;p>相反，垂直 SIG 拥有一个单一的组件，并负责批准和合并该代码库的补丁。
SIG Node 拥有 &amp;quot;Node&amp;quot; 的垂直性，与 kubelet 和它的生命周期有关。这包括 kubelet 本身的代码，以及节点控制器、容器运行时接口和相关的子项目，比如节点问题检测器。&lt;/p>
&lt;!--
### How did the CI subproject start? Is this specific to SIG Node and how does it help the SIG?
SK: The subproject started as a follow up after one of the releases was blocked by numerous test failures of critical tests. These tests haven’t started falling all at once, rather continuous lack of attention led to slow degradation of tests quality. SIG Node was always prioritizing quality and reliability, and forming of the subproject was a way to highlight this priority.
-->
&lt;h3 id="ci-子项目是如何开始的-这是专门针对-sig-node-的吗-它对-sig-有什么帮助">CI 子项目是如何开始的？这是专门针对 SIG Node 的吗？它对 SIG 有什么帮助？&lt;/h3>
&lt;p>SK：该子项目是在其中一个版本因关键测试的大量测试失败而受阻后开始跟进的。
这些测试并不是一下子就开始下降的，而是持续的缺乏关注导致了测试质量的缓慢下降。
SIG Node 一直将质量和可靠性放在首位，组建这个子项目是强调这一优先事项的一种方式。&lt;/p>
&lt;!--
### As the 3rd largest SIG in terms of number of issues and PRs, how does your SIG juggle so much work?
EH: It helps to be organized. When I increased my contributions to the SIG in January of 2021, I found myself overwhelmed by the volume of pull requests and issues and wasn't sure where to start. We were already tracking test-related issues and pull requests on the CI subproject board, but that was missing a lot of our bugfixes and feature work. So I began putting together a triage board for the rest of our pull requests, which allowed me to sort each one by status and what actions to take, and documented its use for other contributors. We closed or merged over 500 issues and pull requests tracked by our two boards in each of the past two releases. The Kubernetes devstats showed that we have significantly increased our velocity as a result.
In June, we ran our first bug scrub event to work through the backlog of issues filed against SIG Node, ensuring they were properly categorized. We closed over 130 issues over the course of this 48 hour global event, but as of writing we still have 333 open issues.
-->
&lt;h3 id="作为-issue-和-pr-数量第三大的-sig-你们-sig-是如何兼顾这么多工作的">作为 issue 和 PR 数量第三大的 SIG，你们 SIG 是如何兼顾这么多工作的？&lt;/h3>
&lt;p>EH：这归功于有组织性。当我在 2021 年 1 月增加对 SIG 的贡献时，我发现自己被大量的 PR 和 issue 淹没了，不知道该从哪里开始。
我们已经在 CI 子项目板上跟踪与测试有关的 issue 和 PR 请求，但这缺少了很多 bug 修复和功能工作。
因此，我开始为我们剩余的 PR 建立一个分流板，这使我能够根据状态和采取的行动对其进行分类，并为其他贡献者记录它的用途。
在过去的两个版本中，我们关闭或合并了超过 500 个 issue 和 PR。Kubernetes devstats 显示，我们的速度因此而大大提升。&lt;/p>
&lt;p>6月，我们进行了第一次 bug 清除活动，以解决针对 SIG Node 的积压问题，确保它们被正确归类。
在这次 48 小时的全球活动中，我们关闭了 130 多个问题，但截至发稿时，我们仍有 333 个问题没有解决。&lt;/p>
&lt;!--
### Why should new and existing contributors consider joining SIG Node?
SK: Being a SIG Node contributor gives you skills and recognition that are rewarding and useful. Understanding under the hood of a kubelet helps architecting better apps, tune and optimize those apps, and gives leg up in issues troubleshooting. If you are a new contributor, SIG Node gives you the foundational knowledge that is key to understanding why other Kubernetes components are designed the way they are. Existing contributors may benefit as many features will require SIG Node changes one way or another. So being a SIG Node contributor helps building features in other SIGs faster.
SIG Node maintains numerous components, many of which have dependency on external projects or OS features. This makes the onboarding process quite lengthy and demanding. But if you are up for a challenge, there is always a place for you, and a group of people to support.
-->
&lt;h3 id="为什么新的和现有的贡献者应该考虑加入-node-兴趣小组呢">为什么新的和现有的贡献者应该考虑加入 Node 兴趣小组呢？&lt;/h3>
&lt;p>SK：作为 SIG Node 的贡献者会带给你有意义且有用的技能和认可度。
了解 Kubelet 的内部结构有助于构建更好的应用程序，调整和优化这些应用程序，并在 issue 排查上获得优势。
如果你是一个新手贡献者，SIG Node 为你提供了基础知识，这是理解其他 Kubernetes 组件的设计方式的关键。
现在的贡献者可能会受益于许多功能都需要 SIG Node 的这种或那种变化。所以成为 SIG Node 的贡献者有助于更快地建立其他 SIG 的功能。&lt;/p>
&lt;p>SIG Node 维护着许多组件，其中许多组件都依赖于外部项目或操作系统功能。这使得入职过程相当冗长和苛刻。
但如果你愿意接受挑战，总有一个地方适合你，也有一群人支持你。&lt;/p>
&lt;!--
### What do you do to help new contributors get started?
EH: Getting started in SIG Node can be intimidating, since there is so much work to be done, our SIG meetings are very large, and it can be hard to find a place to start.
I always encourage new contributors to work on things that they have some investment in already. In SIG Node, that might mean volunteering to help fix a bug that you have personally been affected by, or helping to triage bugs you care about by priority.
To come up to speed on any open source code base, there are two strategies you can take: start by exploring a particular issue deeply, and follow that to expand the edges of your knowledge as needed, or briefly review as many issues and change requests as you possibly can to get a higher level picture of how the component works. Ultimately, you will need to do both if you want to become a Node reviewer or approver.
[Davanum Srinivas](https://twitter.com/dims) and I each ran a cohort of group mentoring to help teach new contributors the skills to become Node reviewers, and if there's interest we can work to find a mentor to run another session. I also encourage new contributors to attend our Node CI Subproject meeting: it's a smaller audience and we don't record the triage sessions, so it can be a less intimidating way to get started with the SIG.
-->
&lt;h3 id="你是如何帮助新手贡献者开始工作的">你是如何帮助新手贡献者开始工作的？&lt;/h3>
&lt;p>EH：在 SIG Node 的起步工作可能是令人生畏的，因为有太多的工作要做，我们的 SIG 会议非常大，而且很难找到一个开始的地方。&lt;/p>
&lt;p>我总是鼓励新手贡献者在他们已经有一些投入的方向上更进一步。
在 SIG Node 中，这可能意味着自愿帮助修复一个只影响到你个人的 bug，或者按优先级去分流你关心的 bug。&lt;/p>
&lt;p>为了尽快了解任何开源代码库，你可以采取两种策略：从深入探索一个特定的问题开始，然后根据需要扩展你的知识边缘，或者单纯地尽可能多的审查 issues 和变更请求，以了解更高层次的组件工作方式。
最终，如果你想成为一名 Node reviewer 或 approver，两件事是不可避免的。&lt;/p>
&lt;p>&lt;a href="https://twitter.com/dims">Davanum Srinivas&lt;/a> 和我各自举办了一次小组辅导，以帮助教导新手贡献者成为 Node reviewer 的技能，如果有兴趣，我们可以努力寻找一个导师来举办另一次会议。
我也鼓励新手贡献者参加我们的 Node CI 子项目会议：它的听众较少，而且我们不记录分流会议，所以它可以是一个比较温和的方式来开始 SIG 之旅。&lt;/p>
&lt;!--
### Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?
SK: SIG Node works on many workstreams in very different areas. All of these areas are on system level. For the typical code contributions you need to have a passion for building and utilizing low level APIs and writing performant and reliable components. Being a contributor you will learn how to debug and troubleshoot, profile, and monitor these components, as well as user workload that is run by these components. Often, with the limited to no access to Nodes, as they are running production workloads.
The other way of contribution is to help document SIG node features. This type of contribution requires a deep understanding of features, and ability to explain them in simple terms.
Finally, we are always looking for feedback on how best to run your workload. Come and explain specifics of it, and what features in SIG Node components may help to run it better.
-->
&lt;h3 id="有什么特别的技能者是你想招募的吗-对-sig-可用性的贡献者可能会学到什么技能">有什么特别的技能者是你想招募的吗？对 SIG 可用性的贡献者可能会学到什么技能？&lt;/h3>
&lt;p>SK：SIG Node 在大相径庭的领域从事许多工作流。所有这些领域都是系统级的。
对于典型的代码贡献，你需要对建立和善用低级别的 API 以及编写高性能和可靠的组件有热情。
作为一个贡献者，你将学习如何调试和排除故障，剖析和监控这些组件，以及由这些组件运行的用户工作负载。
通常情况下，由于节点正在运行生产工作负载，所以对节点的访问是有限的，甚至是没有的。&lt;/p>
&lt;p>另一种贡献方式是帮助记录 SIG Node 的功能。这种类型的贡献需要对功能有深刻的理解，并有能力用简单的术语解释它们。&lt;/p>
&lt;p>最后，我们一直在寻找关于如何最好地运行你的工作负载的反馈。来解释一下它的具体情况，以及 SIG Node 组件中的哪些功能可能有助于更好地运行它。&lt;/p>
&lt;!--
### What are you getting positive feedback on, and what’s coming up next for SIG Node?
EH: Over the past year SIG Node has adopted some new processes to help manage our feature development and Kubernetes enhancement proposals, and other SIGs have looked to us for inspiration in managing large workloads. I hope that this is an area we can continue to provide leadership in and further iterate on.
We have a great balance of new features and deprecations in flight right now. Deprecations of unused or difficult to maintain features help us keep technical debt and maintenance load under control, and examples include the dockershim and DynamicKubeletConfiguration deprecations. New features will unlock additional functionality in end users' clusters, and include exciting features like support for cgroups v2, swap memory, graceful node shutdowns, and device management policies.
-->
&lt;h3 id="你在哪些方面得到了积极的反馈-以及-sig-node-的下一步计划是什么">你在哪些方面得到了积极的反馈，以及 SIG Node 的下一步计划是什么？&lt;/h3>
&lt;p>EH：在过去的一年里，SIG Node 采用了一些新的流程来帮助管理我们的功能开发和 Kubernetes 增强提议，其他 SIG 也向我们寻求在管理大型工作负载方面的灵感。
我希望这是一个我们可以继续领导并进一步迭代的领域。&lt;/p>
&lt;p>现在，我们在新功能和废弃功能之间保持了很好的平衡。
废弃未使用或难以维护的功能有助于我们控制技术债务和维护负荷，例子包括 dockershim 和 DynamicKubeletConfiguration 的废弃。
新功能将在终端用户的集群中释放更多的功能，包括令人兴奋的功能，如支持 cgroups v2、交换内存、优雅的节点关闭和设备管理策略。&lt;/p>
&lt;!--
### Any closing thoughts/resources you’d like to share?
SK/EH: It takes time and effort to get to any open source community. SIG Node may overwhelm you at first with the number of participants, volume of work, and project scope. But it is totally worth it. Join our welcoming community! [SIG Node GitHub Repo](https://github.com/kubernetes/community/tree/master/sig-node) contains many useful resources including Slack, mailing list and other contact info.
-->
&lt;h3 id="最后你有什么想法-资源要分享吗">最后你有什么想法/资源要分享吗？&lt;/h3>
&lt;p>SK/EH：进入任何开源社区都需要时间和努力。一开始 SIG Node 可能会因为参与者的数量、工作量和项目范围而让你不知所措。但这是完全值得的。
请加入我们这个热情的社区! [SIG Node GitHub Repo]（https://github.com/kubernetes/community/tree/master/sig-node）包含许多有用的资源，包括 Slack、邮件列表和其他联系信息。&lt;/p>
&lt;!--
## Wrap Up
SIG Node hosted a [KubeCon + CloudNativeCon Europe 2021 talk](https://www.youtube.com/watch?v=z5aY4e2RENA) with an intro and deep dive to their awesome SIG. Join the SIG's meetings to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream Node team as a contributor!
-->
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>SIG Node 举办了一场 &lt;a href="https://www.youtube.com/watch?v=z5aY4e2RENA">KubeCon + CloudNativeCon Europe 2021 talk&lt;/a>，对他们强大的 SIG 进行了介绍和深入探讨。
加入 SIG 的会议，了解最新的研究成果，未来一年的计划是什么，以及如何作为贡献者参与到上游的 Node 团队中!&lt;/p></description></item><item><title>Blog: Kubernetes 1.20：CSI 驱动程序中的 Pod 身份假扮和短时卷</title><link>https://kubernetes.io/zh/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers'
date: 2020-12-18
slug: kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi
-->
&lt;!--
**Author**: Shihang Zhang (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Shihang Zhang（谷歌）&lt;/p>
&lt;!--
Typically when a [CSI](https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md) driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods' identities rather than the CSI driver's identity. CSI drivers, therefore, need some way to retrieve pod's service account token.
-->
&lt;p>通常，当 &lt;a href="https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md">CSI&lt;/a> 驱动程序挂载
诸如 Secret 和证书之类的凭据时，它必须通过存储提供者的身份认证才能访问这些凭据。
然而，对这些凭据的访问是根据 Pod 的身份而不是 CSI 驱动程序的身份来控制的。
因此，CSI 驱动程序需要某种方法来取得 Pod 的服务帐户令牌。&lt;/p>
&lt;!--
Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.
-->
&lt;p>当前，有两种不是那么理想的方法来实现这一目的，要么通过授予 CSI 驱动程序使用 TokenRequest API 的权限，要么直接从主机文件系统中读取令牌。&lt;/p>
&lt;!--
Both of them exhibit the following drawbacks:
-->
&lt;p>两者都存在以下缺点：&lt;/p>
&lt;!--
- Violating the principle of least privilege
- Every CSI driver needs to re-implement the logic of getting the pod’s service account token
-->
&lt;ul>
&lt;li>违反最少特权原则&lt;/li>
&lt;li>每个 CSI 驱动程序都需要重新实现获取 Pod 的服务帐户令牌的逻辑&lt;/li>
&lt;/ul>
&lt;!--
The second approach is more problematic due to:
-->
&lt;p>第二种方式问题更多，因为：&lt;/p>
&lt;!--
- The audience of the token defaults to the kube-apiserver
- The token is not guaranteed to be available (e.g. `AutomountServiceAccountToken=false`)
- The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See [file permission section for service account token](https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission)
- The token might be legacy Kubernetes service account token which doesn’t expire if `BoundServiceAccountTokenVolume=false`
-->
&lt;ul>
&lt;li>令牌的受众默认为 kube-apiserver&lt;/li>
&lt;li>该令牌不能保证可用（例如，&lt;code>AutomountServiceAccountToken=false&lt;/code>）&lt;/li>
&lt;li>该方法不适用于以与 Pod 不同的（非 root 用户）用户身份运行的 CSI 驱动程序。请参见
&lt;a href="https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission">服务帐户令牌的文件许可权部分&lt;/a>&lt;/li>
&lt;li>该令牌可能是旧的 Kubernetes 服务帐户令牌，如果 &lt;code>BoundServiceAccountTokenVolume=false&lt;/code>，该令牌不会过期。&lt;/li>
&lt;/ul>
&lt;!--
Kubernetes 1.20 introduces an alpha feature, `CSIServiceAccountToken`, to improve the security posture. The new feature allows CSI drivers to receive pods' [bound service account tokens](https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md).
-->
&lt;p>Kubernetes 1.20 引入了一个内测功能 &lt;code>CSIServiceAccountToken&lt;/code> 以改善安全状况。这项新功能允许 CSI 驱动程序接收 Pod 的&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md">绑定服务帐户令牌&lt;/a>。&lt;/p>
&lt;!--
This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.
-->
&lt;p>此功能还提供了一个重新发布卷的能力，以便可以刷新短时卷。&lt;/p>
&lt;!--
## Pod Impersonation
### Using GCP APIs
-->
&lt;h2 id="pod-身份假扮">Pod 身份假扮&lt;/h2>
&lt;h3 id="使用-gcp-apis">使用 GCP APIs&lt;/h3>
&lt;!--
Using [Workload Identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity), a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod's service account token to [exchange for GCP tokens](https://cloud.google.com/iam/docs/reference/sts/rest). The pod's service account token is plumbed through the volume context in `NodePublishVolume` RPC calls when the feature `CSIServiceAccountToken` is enabled. For example: accessing [Google Secret Manager](https://cloud.google.com/secret-manager/) via a [secret store CSI driver](https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp).
-->
&lt;p>使用 &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity&lt;/a>，Kubernetes 服务帐户可以在访问 Google Cloud API 时验证为 Google 服务帐户。
如果 CSI 驱动程序要代表其为挂载卷的 Pod 访问 GCP API，则可以使用 Pod 的服务帐户令牌来
&lt;a href="https://cloud.google.com/iam/docs/reference/sts/rest">交换 GCP 令牌&lt;/a>。启用功能 &lt;code>CSIServiceAccountToken&lt;/code> 后，
可通过 &lt;code>NodePublishVolume&lt;/code> RPC 调用中的卷上下文来访问 Pod 的服务帐户令牌。例如：通过 &lt;a href="https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp">Secret 存储 CSI 驱动&lt;/a>
访问 &lt;a href="https://cloud.google.com/secret-manager/">Google Secret Manager&lt;/a>。&lt;/p>
&lt;!--
### Using Vault
If users configure [Kubernetes as an auth method](https://www.vaultproject.io/docs/auth/kubernetes), Vault uses the `TokenReview` API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod's service account to Vault. For example, [secrets store CSI driver](https://github.com/hashicorp/secrets-store-csi-driver-provider-vault) and [cert manager CSI driver](https://github.com/jetstack/cert-manager-csi).
-->
&lt;h3 id="使用vault">使用Vault&lt;/h3>
&lt;p>如果用户将 &lt;a href="https://www.vaultproject.io/docs/auth/kubernetes">Kubernetes 作为身份验证方法&lt;/a>配置，
则 Vault 使用 &lt;code>TokenReview&lt;/code> API 来验证 Kubernetes 服务帐户令牌。
对于使用 Vault 作为资源提供者的 CSI 驱动程序，它们需要将 Pod 的服务帐户提供给 Vault。
例如，&lt;a href="https://github.com/hashicorp/secrets-store-csi-driver-provider-vault">Secret 存储 CSI 驱动&lt;/a>和
&lt;a href="https://github.com/jetstack/cert-manager-csi">证书管理器 CSI 驱动&lt;/a>。&lt;/p>
&lt;!--
## Short-lived Volumes
To keep short-lived volumes such as certificates effective, CSI drivers can specify `RequiresRepublish=true` in their`CSIDriver` object to have the kubelet periodically call `NodePublishVolume` on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.
-->
&lt;h2 id="短时卷">短时卷&lt;/h2>
&lt;p>为了使诸如证书之类的短时卷保持有效，CSI 驱动程序可以在其 &lt;code>CSIDriver&lt;/code> 对象中指定 &lt;code>RequiresRepublish=true&lt;/code>，
以使 kubelet 定期针对已挂载的卷调用 &lt;code>NodePublishVolume&lt;/code>。
这些重新发布操作使 CSI 驱动程序可以确保卷内容是最新的。&lt;/p>
&lt;!--
## Next steps
This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:
-->
&lt;h2 id="下一步">下一步&lt;/h2>
&lt;p>此功能是 Alpha 版，预计将在 1.21 版中移至 Beta 版。 请参阅以下 KEP 和 CSI 文档中的更多内容：&lt;/p>
&lt;!--
- [KEP-1855: Service Account Token for CSI Driver](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md)
- [Token Requests](https://kubernetes-csi.github.io/docs/token-requests.html)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md">KEP-1855: CSI 驱动程序的服务帐户令牌&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/token-requests.html">令牌请求&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Your feedback is always welcome!
- SIG-Auth [meets regularly](https://github.com/kubernetes/community/tree/master/sig-auth#meetings) and can be reached via [Slack and the mailing list](https://github.com/kubernetes/community/tree/master/sig-auth#contact)
- SIG-Storage [meets regularly](https://github.com/kubernetes/community/tree/master/sig-storage#meetings) and can be reached via [Slack and the mailing list](https://github.com/kubernetes/community/tree/master/sig-storage#contact).
-->
&lt;p>随时欢迎您提供反馈!&lt;/p>
&lt;ul>
&lt;li>SIG-Auth &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#meetings">定期开会&lt;/a>，可以通过 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#contact">Slack 和邮件列表&lt;/a>加入&lt;/li>
&lt;li>SIG-Storage &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">定期开会&lt;/a>，可以通过 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack 和邮件列表&lt;/a>加入&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.20: 最新版本</title><link>https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/</guid><description>
&lt;!-- ---
layout: blog
title: 'Kubernetes 1.20: The Raddest Release'
date: 2020-12-08
slug: kubernetes-1-20-release-announcement
evergreen: true
--- -->
&lt;p>&lt;strong>作者:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md">Kubernetes 1.20 发布团队&lt;/a>&lt;/p>
&lt;!-- **Authors:** [Kubernetes 1.20 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md) -->
&lt;p>我们很高兴地宣布 Kubernetes 1.20 的发布，这是我们 2020 年的第三个也是最后一个版本！此版本包含 42 项增强功能：11 项增强功能已升级到稳定版，15 项增强功能正在进入测试版，16 项增强功能正在进入 Alpha 版。&lt;/p>
&lt;!-- We’re pleased to announce the release of Kubernetes 1.20, our third and final release of 2020! This release consists of 42 enhancements: 11 enhancements have graduated to stable, 15 enhancements are moving to beta, and 16 enhancements are entering alpha. -->
&lt;p>1.20 发布周期在上一个延长的发布周期之后恢复到 11 周的正常节奏。这是一段时间以来功能最密集的版本之一：Kubernetes 创新周期仍呈上升趋势。此版本具有更多的 Alpha 而非稳定的增强功能，表明云原生生态系统仍有许多需要探索的地方。&lt;/p>
&lt;!-- The 1.20 release cycle returned to its normal cadence of 11 weeks following the previous extended release cycle. This is one of the most feature dense releases in a while: the Kubernetes innovation cycle is still trending upward. This release has more alpha than stable enhancements, showing that there is still much to explore in the cloud native ecosystem. -->
&lt;h2 id="major-themes">主题&lt;/h2>
&lt;!-- ## Major Themes -->
&lt;h3 id="volume-snapshot-operations-goes-stable">Volume 快照操作变得稳定&lt;/h3>
&lt;!-- This feature provides a standard way to trigger volume snapshot operations and allows users to incorporate snapshot operations in a portable manner on any Kubernetes environment and supported storage providers. -->
&lt;p>此功能提供了触发卷快照操作的标准方法，并允许用户以可移植的方式在任何 Kubernetes 环境和支持的存储提供程序上合并快照操作。&lt;/p>
&lt;!-- Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise-grade, storage administration features for Kubernetes, including application or cluster level backup solutions. -->
&lt;p>此外，这些 Kubernetes 快照原语充当基本构建块，解锁为 Kubernetes 开发高级企业级存储管理功能的能力，包括应用程序或集群级备份解决方案。&lt;/p>
&lt;!-- Note that snapshot support requires Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. A CSI driver supporting the snapshot functionality must also be deployed on the cluster. -->
&lt;p>请注意，快照支持要求 Kubernetes 分销商捆绑 Snapshot 控制器、Snapshot CRD 和验证 webhook。还必须在集群上部署支持快照功能的 CSI 驱动程序。&lt;/p>
&lt;!-- ### Kubectl Debug Graduates to Beta -->
&lt;h3 id="kubectl-debug-graduates-to-beta">Kubectl Debug 功能升级到 Beta&lt;/h3>
&lt;!-- The `kubectl alpha debug` features graduates to beta in 1.20, becoming `kubectl debug`. The feature provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of kubectl include: -->
&lt;p>&lt;code>kubectl alpha debug&lt;/code> 功能在 1.20 中升级到测试版，成为 &lt;code>kubectl debug&lt;/code>. 该功能直接从 kubectl 提供对常见调试工作流的支持。此版本的 kubectl 支持的故障排除场景包括：&lt;/p>
&lt;!-- * Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.
* Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)
* Troubleshoot on a node by creating a container running in the host namespaces and with access to the host’s filesystem. -->
&lt;ul>
&lt;li>通过创建使用不同容器映像或命令的 pod 副本，对在启动时崩溃的工作负载进行故障排除。&lt;/li>
&lt;li>通过在 pod 的新副本或使用临时容器中添加带有调试工具的新容器来对 distroless 容器进行故障排除。（临时容器是默认未启用的 alpha 功能。）&lt;/li>
&lt;li>通过创建在主机命名空间中运行并可以访问主机文件系统的容器来对节点进行故障排除。&lt;/li>
&lt;/ul>
&lt;!-- Note that as a new built-in command, `kubectl debug` takes priority over any kubectl plugin named “debug”. You must rename the affected plugin. -->
&lt;p>请注意，作为新的内置命令，&lt;code>kubectl debug&lt;/code> 优先于任何名为 “debug” 的 kubectl 插件。你必须重命名受影响的插件。&lt;/p>
&lt;!-- Invocations using `kubectl alpha debug` are now deprecated and will be removed in a subsequent release. Update your scripts to use `kubectl debug`. For more information about `kubectl debug`, see [Debugging Running Pods](https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/). -->
&lt;p>&lt;code>kubectl alpha debug&lt;/code> 现在不推荐使用，并将在后续版本中删除。更新你的脚本以使用 &lt;code>kubectl debug&lt;/code>。 有关更多信息 &lt;code>kubectl debug&lt;/code>，请参阅[调试正在运行的 Pod]((&lt;a href="https://kubernetes.io/zh/docs/tasks/debug/debug-application/debug-running-pod/">https://kubernetes.io/zh/docs/tasks/debug/debug-application/debug-running-pod/&lt;/a>)。&lt;/p>
&lt;!-- ### Beta: API Priority and Fairness -->
&lt;h3 id="测试版-api-优先级和公平性-beta-api-priority-and-fairness">测试版：API 优先级和公平性 {#beta-api-priority-and-fairness)&lt;/h3>
&lt;!-- Introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows `kube-apiserver` to categorize incoming requests by priority levels. -->
&lt;p>Kubernetes 1.20 由 1.18 引入，现在默认启用 API 优先级和公平性 (APF)。这允许 &lt;code>kube-apiserver&lt;/code> 按优先级对传入请求进行分类。&lt;/p>
&lt;!-- ### Alpha with updates: IPV4/IPV6 -->
&lt;h3 id="alpha-with-updates-ipv4-ipv6">Alpha 更新：IPV4/IPV6&lt;/h3>
&lt;!-- The IPv4/IPv6 dual stack has been reimplemented to support dual stack services based on user and community feedback. This allows both IPv4 and IPv6 service cluster IP addresses to be assigned to a single service, and also enables a service to be transitioned from single to dual IP stack and vice versa. -->
&lt;p>基于用户和社区反馈，重新实现了 IPv4/IPv6 双栈以支持双栈服务。
这允许将 IPv4 和 IPv6 服务集群 IP 地址分配给单个服务，还允许服务从单 IP 堆栈转换为双 IP 堆栈，反之亦然。&lt;/p>
&lt;!-- ### GA: Process PID Limiting for Stability -->
&lt;h3 id="ga-process-pid-limiting-for-stability">GA：进程 PID 稳定性限制&lt;/h3>
&lt;!-- Process IDs (pids) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine. -->
&lt;p>进程 ID (pid) 是 Linux 主机上的基本资源。达到任务限制而不达到任何其他资源限制并导致主机不稳定是很可能发生的。&lt;/p>
&lt;!-- Administrators require mechanisms to ensure that user pods cannot induce pid exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that pids are limited among pods in order to ensure they have limited impact to other workloads on the node. -->
&lt;!-- After being enabled-by-default for a year, SIG Node graduates PID Limits to GA on both `SupportNodePidsLimit` (node-to-pod PID isolation) and `SupportPodPidsLimit` (ability to limit PIDs per pod). -->
&lt;p>管理员需要机制来确保用户 pod 不会导致 pid 耗尽，从而阻止主机守护程序（运行时、kubelet 等）运行。此外，重要的是要确保 pod 之间的 pid 受到限制，以确保它们对节点上的其他工作负载的影响有限。
默认启用一年后，SIG Node 在 &lt;code>SupportNodePidsLimit&lt;/code>（节点到 Pod PID 隔离）和 &lt;code>SupportPodPidsLimit&lt;/code>（限制每个 Pod 的 PID 的能力）上都将 PID 限制升级为 GA。&lt;/p>
&lt;!-- ### Alpha: Graceful node shutdown -->
&lt;h3 id="alpha-graceful-node-shutdown">Alpha：节点体面地关闭&lt;/h3>
&lt;!-- Users and cluster administrators expect that pods will adhere to expected pod lifecycle including pod termination. Currently, when a node shuts down, pods do not follow the expected pod termination lifecycle and are not terminated gracefully which can cause issues for some workloads.
The `GracefulNodeShutdown` feature is now in Alpha. `GracefulNodeShutdown` makes the kubelet aware of node system shutdowns, enabling graceful termination of pods during a system shutdown. -->
&lt;p>用户和集群管理员希望 Pod 遵守预期的 Pod 生命周期，包括 Pod 终止。目前，当一个节点关闭时，Pod 不会遵循预期的 Pod 终止生命周期，也不会正常终止，这可能会导致某些工作负载出现问题。
该 &lt;code>GracefulNodeShutdown&lt;/code> 功能现在处于 Alpha 阶段。&lt;code>GracefulNodeShutdown&lt;/code> 使 kubelet 知道节点系统关闭，从而在系统关闭期间正常终止 pod。&lt;/p>
&lt;!-- ## Major Changes -->
&lt;h2 id="major-changes">主要变化&lt;/h2>
&lt;!-- ### Dockershim Deprecation -->
&lt;h3 id="dockershim-deprecation">Dockershim 弃用&lt;/h3>
&lt;!-- Dockershim, the container runtime interface (CRI) shim for Docker is being deprecated. Support for Docker is deprecated and will be removed in a future release. Docker-produced images will continue to work in your cluster with all CRI compliant runtimes as Docker images follow the Open Container Initiative (OCI) image specification.
The Kubernetes community has written a [detailed blog post about deprecation](https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/) with [a dedicated FAQ page for it](https://blog.k8s.io/2020/12/02/dockershim-faq/). -->
&lt;p>Dockershim，Docker 的容器运行时接口 (CRI) shim 已被弃用。不推荐使用对 Docker 的支持，并将在未来版本中删除。由于 Docker 映像遵循开放容器计划 (OCI) 映像规范，因此 Docker 生成的映像将继续在具有所有 CRI 兼容运行时的集群中工作。
Kubernetes 社区写了一篇关于弃用的详细&lt;a href="https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/">博客文章&lt;/a>，并为其提供了一个专门的常见问题&lt;a href="https://blog.k8s.io/2020/12/02/dockershim-faq/">解答页面&lt;/a>。&lt;/p>
&lt;!-- ### Exec Probe Timeout Handling -->
&lt;h3 id="exec-probe-timeout-handling">Exec 探测超时处理&lt;/h3>
&lt;!-- A longstanding bug regarding exec probe timeouts that may impact existing pod definitions has been fixed. Prior to this fix, the field `timeoutSeconds` was not respected for exec probes. Instead, probes would run indefinitely, even past their configured deadline, until a result was returned. With this change, the default value of `1 second` will be applied if a value is not specified and existing pod definitions may no longer be sufficient if a probe takes longer than one second. A feature gate, called `ExecProbeTimeout`, has been added with this fix that enables cluster operators to revert to the previous behavior, but this will be locked and removed in subsequent releases. In order to revert to the previous behavior, cluster operators should set this feature gate to `false`. -->
&lt;p>一个关于 exec 探测超时的长期错误可能会影响现有的 pod 定义，已得到修复。在此修复之前，exec 探测器不考虑 &lt;code>timeoutSeconds&lt;/code> 字段。相反，探测将无限期运行，甚至超过其配置的截止日期，直到返回结果。
通过此更改，如果未指定值，将应用默认值 &lt;code>1 second&lt;/code>，并且如果探测时间超过一秒，现有 pod 定义可能不再足够。
新引入的 &lt;code>ExecProbeTimeout&lt;/code> 特性门控所提供的修复使集群操作员能够恢复到以前的行为，但这种行为将在后续版本中锁定并删除。为了恢复到以前的行为，集群运营商应该将此特性门控设置为 &lt;code>false&lt;/code>。&lt;/p>
&lt;!-- Please review the updated documentation regarding [configuring probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes) for more details. -->
&lt;p>有关更多详细信息，请查看有关配置探针的&lt;a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">更新文档&lt;/a>。&lt;/p>
&lt;!-- ## Other Updates -->
&lt;h2 id="other-updates">其他更新&lt;/h2>
&lt;!-- ### Graduated to Stable -->
&lt;h3 id="graduated-to-stable">稳定版&lt;/h3>
&lt;!-- * [RuntimeClass](https://github.com/kubernetes/enhancements/issues/585)
* [Built-in API Types Defaults](https://github.com/kubernetes/enhancements/issues/1929)
* [Add Pod-Startup Liveness-Probe Holdoff](https://github.com/kubernetes/enhancements/issues/950)
* [Support CRI-ContainerD On Windows](https://github.com/kubernetes/enhancements/issues/1001)
* [SCTP Support for Services](https://github.com/kubernetes/enhancements/issues/614)
* [Adding AppProtocol To Services And Endpoints](https://github.com/kubernetes/enhancements/issues/1507) -->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/585">RuntimeClass&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1929">内置 API 类型默认值&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/950">添加了对 Pod 层面启动探针和活跃性探针的扼制&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1001">在 Windows 上支持 CRI-ContainerD&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/614">SCTP 对 Services 的支持&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">将 AppProtocol 添加到 Services 和 Endpoints 上&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- ### Notable Feature Updates -->
&lt;h3 id="notable-feature-updates">值得注意的功能更新&lt;/h3>
&lt;!-- * [CronJobs](https://github.com/kubernetes/enhancements/issues/19) -->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- # Release notes -->
&lt;h1 id="release-notes">发行说明&lt;/h1>
&lt;!-- You can check out the full details of the 1.20 release in the [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md). -->
&lt;p>你可以在&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">发行说明&lt;/a>中查看 1.20 发行版的完整详细信息。&lt;/p>
&lt;!-- # Availability of release -->
&lt;h1 id="availability-of-release">可用的发布&lt;/h1>
&lt;!-- Kubernetes 1.20 is available for [download on GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0). There are some great resources out there for getting started with Kubernetes. You can check out some [interactive tutorials](https://kubernetes.io/docs/tutorials/) on the main Kubernetes site, or run a local cluster on your machine using Docker containers with [kind](https://kind.sigs.k8s.io). If you’d like to try building a cluster from scratch, check out the [Kubernetes the Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way) tutorial by Kelsey Hightower. -->
&lt;p>Kubernetes 1.20 可在 &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0">GitHub&lt;/a> 上下载。有一些很棒的资源可以帮助你开始使用 Kubernetes。你可以在 Kubernetes 主站点上查看一些&lt;a href="https://kubernetes.io/docs/tutorials/">交互式教程&lt;/a>，或者使用 &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a> 的 Docker 容器在你的机器上运行本地集群。如果你想尝试从头开始构建集群，请查看 Kelsey Hightower 的 &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> 教程。&lt;/p>
&lt;!-- # Release Team -->
&lt;h1 id="release-team">发布团队&lt;/h1>
&lt;!-- This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Jeremy Rickard, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.20 release for the community. -->
&lt;p>这个版本是由一群非常敬业的人促成的，他们在世界上发生的许多事情的时段作为一个团队走到了一起。
非常感谢发布负责人 Jeremy Rickard 以及发布团队中的其他所有人，感谢他们相互支持，并努力为社区发布 1.20 版本。&lt;/p>
&lt;!-- # Release Logo -->
&lt;h1 id="release-logo">发布 Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-08-kubernetes-1.20-release-announcement/laser.png" alt="Kubernetes 1.20 Release Logo">&lt;/p>
&lt;p>&lt;a href="https://www.dictionary.com/browse/rad">raddest&lt;/a>: &lt;em>adjective&lt;/em>, Slang. excellent; wonderful; cool:&lt;/p>
&lt;!-- > The Kubernetes 1.20 Release has been the raddest release yet. -->
&lt;blockquote>
&lt;p>Kubernetes 1.20 版本是迄今为止最激动人心的版本。&lt;/p>
&lt;/blockquote>
&lt;!-- 2020 has been a challenging year for many of us, but Kubernetes contributors have delivered a record-breaking number of enhancements in this release. That is a great accomplishment, so the release lead wanted to end the year with a little bit of levity and pay homage to [Kubernetes 1.14 - Caturnetes](https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14) with a "rad" cat named Humphrey. -->
&lt;p>2020 年对我们中的许多人来说都是充满挑战的一年，但 Kubernetes 贡献者在此版本中提供了创纪录的增强功能。这是一项了不起的成就，因此发布负责人希望以一点轻松的方式结束这一年，并向 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14">Kubernetes 1.14 - Caturnetes&lt;/a> 和一只名叫 Humphrey 的 “rad” 猫致敬。&lt;/p>
&lt;!-- Humphrey is the release lead's cat and has a permanent [`blep`](https://www.inverse.com/article/42316-why-do-cats-blep-science-explains). *Rad* was pretty common slang in the 1990s in the United States, and so were laser backgrounds. Humphrey in a 1990s style school picture felt like a fun way to end the year. Hopefully, Humphrey and his *blep* bring you a little joy at the end of 2020! -->
&lt;p>Humphrey是发布负责人的猫，有一个永久的 &lt;code>blep&lt;/code>. 在 1990 年代，&lt;em>Rad&lt;/em> 是美国非常普遍的俚语，激光背景也是如此。Humphrey 在 1990 年代风格的学校照片中感觉像是结束这一年的有趣方式。希望 Humphrey 和它的 &lt;em>blep&lt;/em> 在 2020 年底给你带来一点快乐！&lt;/p>
&lt;!-- The release logo was created by [Henry Hsu - @robotdancebattle](https://www.instagram.com/robotdancebattle/). -->
&lt;p>发布标志由 &lt;a href="https://www.instagram.com/robotdancebattle/">Henry Hsu - @robotdancebattle&lt;/a> 创建。&lt;/p>
&lt;!-- # User Highlights -->
&lt;h1 id="user-highlights">用户亮点&lt;/h1>
&lt;!-- - Apple is operating multi-thousand node Kubernetes clusters in data centers all over the world. Watch [Alena Prokharchyk's KubeCon NA Keynote](https://youtu.be/Tx8qXC-U3KM) to learn more about their cloud native journey. -->
&lt;ul>
&lt;li>Apple 正在世界各地的数据中心运行数千个节点的 Kubernetes 集群。观看 &lt;a href="https://youtu.be/Tx8qXC-U3KM">Alena Prokarchyk&lt;/a> 的 KubeCon NA 主题演讲，了解有关他们的云原生之旅的更多信息。&lt;/li>
&lt;/ul>
&lt;!-- # Project Velocity -->
&lt;h1 id="project-velocity">项目速度&lt;/h1>
&lt;!-- The [CNCF K8s DevStats project](https://k8s.devstats.cncf.io/) aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem. -->
&lt;p>&lt;a href="https://k8s.devstats.cncf.io/">CNCF K8S DevStats 项目&lt;/a>聚集了许多有关Kubernetes和各分项目的速度有趣的数据点。这包括从个人贡献到做出贡献的公司数量的所有内容，并且清楚地说明了为发展这个生态系统所做的努力的深度和广度。&lt;/p>
&lt;!-- In the v1.20 release cycle, which ran for 11 weeks (September 25 to December 9), we saw contributions from [967 companies](https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.19.0%20-%20now&amp;var-metric=contributions) and [1335 individuals](https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.19.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All) ([44 of whom](https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;from=1601006400000&amp;to=1607576399000&amp;var-repogroup_name=Kubernetes) made their first Kubernetes contribution) from [26 countries](https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;from=1601006400000&amp;to=1607576399000&amp;var-period_name=Quarter&amp;var-countries=All&amp;var-repogroup_name=Kubernetes&amp;var-metric=rcommitters&amp;var-cum=countries). -->
&lt;p>在持续 11 周（9 月 25 日至 12 月 9 日）的 v1.20 发布周期中，我们看到了来自 &lt;a href="https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-period_name=Quarter&amp;amp;var-countries=All&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-metric=rcommitters&amp;amp;var-cum=countries">26 个国家/地区&lt;/a> 的 &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions">967 家公司&lt;/a> 和 &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">1335 名个人&lt;/a>（其中 &lt;a href="https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-repogroup_name=Kubernetes">44 人&lt;/a>首次为 Kubernetes 做出贡献）的贡献。&lt;/p>
&lt;!-- # Ecosystem Updates -->
&lt;h1 id="ecosystem-updates">生态系统更新&lt;/h1>
&lt;!-- - KubeCon North America just wrapped up three weeks ago, the second such event to be virtual! All talks are [now available to all on-demand](https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut) for anyone still needing to catch up!
- In June, the Kubernetes community formed a new working group as a direct response to the Black Lives Matter protests occurring across America. WG Naming's goal is to remove harmful and unclear language in the Kubernetes project as completely as possible and to do so in a way that is portable to other CNCF projects. A great introductory talk on this important work and how it is conducted was given [at KubeCon 2020 North America](https://sched.co/eukp), and the initial impact of this labor [can actually be seen in the v1.20 release](https://github.com/kubernetes/enhancements/issues/2067).
- Previously announced this summer, [The Certified Kubernetes Security Specialist (CKS) Certification](https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/) was released during Kubecon NA for immediate scheduling! Following the model of CKA and CKAD, the CKS is a performance-based exam, focused on security-themed competencies and domains. This exam is targeted at current CKA holders, particularly those who want to round out their baseline knowledge in securing cloud workloads (which is all of us, right?). -->
&lt;ul>
&lt;li>KubeCon North America 三周前刚刚结束，这是第二个虚拟的此类活动！现在所有演讲都可以&lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut">点播&lt;/a>，供任何需要赶上的人使用！&lt;/li>
&lt;li>6 月，Kubernetes 社区成立了一个新的工作组，作为对美国各地发生的 Black Lives Matter 抗议活动的直接回应。WG Naming 的目标是尽可能彻底地删除 Kubernetes 项目中有害和不清楚的语言，并以可移植到其他 CNCF 项目的方式进行。在 &lt;a href="https://sched.co/eukp">KubeCon 2020 North America&lt;/a> 上就这项重要工作及其如何进行进行了精彩的介绍性演讲，这项工作的初步影响&lt;a href="https://github.com/kubernetes/enhancements/issues/2067">实际上可以在 v1.20 版本中看到&lt;/a>。&lt;/li>
&lt;li>此前于今年夏天宣布，在 Kubecon NA 期间发布了经认证的 &lt;a href="https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/">Kubernetes 安全专家 (CKS) 认证&lt;/a> ，以便立即安排！遵循 CKA 和 CKAD 的模型，CKS 是一项基于性能的考试，侧重于以安全为主题的能力和领域。该考试面向当前的 CKA 持有者，尤其是那些想要完善其在保护云工作负载方面的基础知识的人（这是我们所有人，对吧？）。&lt;/li>
&lt;/ul>
&lt;!-- # Event Updates -->
&lt;h1 id="event-updates">活动更新&lt;/h1>
&lt;!-- KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! Registration will open on January 11. You can find more information about the conference [here](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/). Remember that [the CFP](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/) closes on Sunday, December 13, 11:59pm PST! -->
&lt;p>KubeCon + CloudNativeCon Europe 2021 将于 2021 年 5 月 4 日至 7 日举行！注册将于 1 月 11 日开放。你可以在&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">此处&lt;/a>找到有关会议的更多信息。
请记住，&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/">CFP&lt;/a> 将于太平洋标准时间 12 月 13 日星期日晚上 11:59 关闭！&lt;/p>
&lt;!-- # Upcoming release webinar -->
&lt;h1 id="upcoming-release-webinar">即将发布的网络研讨会&lt;/h1>
&lt;!-- Stay tuned for the upcoming release webinar happening this January. -->
&lt;p>请继续关注今年 1 月即将举行的发布网络研讨会。&lt;/p>
&lt;!-- # Get Involved -->
&lt;h1 id="get-involved">参与其中&lt;/h1>
&lt;!-- If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels: -->
&lt;p>如果你有兴趣为 Kubernetes 社区做出贡献，那么特别兴趣小组 (SIG) 是一个很好的起点。其中许多可能符合你的兴趣！如果你有什么想与社区分享的内容，你可以参加每周的社区会议，或使用以下任一渠道：&lt;/p>
&lt;!-- * Find out more about contributing to Kubernetes at the new [Kubernetes Contributor website](https://www.kubernetes.dev/)
* Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
* Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
* Join the community on [Slack](http://slack.k8s.io/)
* Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
* Read more about what’s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
* Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team) -->
&lt;ul>
&lt;li>在新的 &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor 网站&lt;/a>上了解更多关于为Kubernetes 做出贡献的信息&lt;/li>
&lt;li>在 Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> 上关注我们以获取最新更新&lt;/li>
&lt;li>加入关于讨论的&lt;a href="https://discuss.kubernetes.io/">社区&lt;/a>讨论&lt;/li>
&lt;li>加入 &lt;a href="http://slack.k8s.io/">Slack 社区&lt;/a>&lt;/li>
&lt;li>分享你的 &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">Kubernetes 故事&lt;/a>&lt;/li>
&lt;li>在&lt;a href="https://kubernetes.io/blog/">博客&lt;/a>上阅读更多关于 Kubernetes 发生的事情&lt;/li>
&lt;li>了解有关 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes 发布团队&lt;/a>的更多信息&lt;/li>
&lt;/ul></description></item><item><title>Blog: 别慌: Kubernetes 和 Docker</title><link>https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/</guid><description>
&lt;!--
layout: blog
title: "Don't Panic: Kubernetes and Docker"
date: 2020-12-02
slug: dont-panic-kubernetes-and-docker
-->
&lt;p>&lt;strong>作者：&lt;/strong> Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan “POP” Papandrea, Jeffrey Sica, Davanum “Dims” Srinivas&lt;/p>
&lt;!--
_Update: Kubernetes support for Docker via `dockershim` is now deprecated.
For more information, read the [deprecation notice](/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation).
You can also discuss the deprecation via a dedicated [GitHub issue](https://github.com/kubernetes/kubernetes/issues/106917)._
-->
&lt;p>&lt;em>更新：Kubernetes 通过 &lt;code>dockershim&lt;/code> 对 Docker 的支持现已弃用。
有关更多信息，请阅读&lt;a href="https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">弃用通知&lt;/a>。
你还可以通过专门的 &lt;a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub issue&lt;/a> 讨论弃用。&lt;/em>&lt;/p>
&lt;!--
Kubernetes is [deprecating
Docker](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation)
as a container runtime after v1.20.
-->
&lt;p>Kubernetes 从版本 v1.20 之后，&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">弃用 Docker&lt;/a>
这个容器运行时。&lt;/p>
&lt;!--
**You do not need to panic. It’s not as dramatic as it sounds.**
-->
&lt;p>&lt;strong>不必慌张，这件事并没有听起来那么吓人。&lt;/strong>&lt;/p>
&lt;!--
TL;DR Docker as an underlying runtime is being deprecated in favor of runtimes
that use the [Container Runtime Interface (CRI)](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)
created for Kubernetes. Docker-produced images will continue to work in your
cluster with all runtimes, as they always have.
-->
&lt;p>弃用 Docker 这个底层运行时，转而支持符合为 Kubernetes 创建的容器运行接口
&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface (CRI)&lt;/a>
的运行时。
Docker 构建的镜像，将在你的集群的所有运行时中继续工作，一如既往。&lt;/p>
&lt;!--
If you’re an end-user of Kubernetes, not a whole lot will be changing for you.
This doesn’t mean the death of Docker, and it doesn’t mean you can’t, or
shouldn’t, use Docker as a development tool anymore. Docker is still a useful
tool for building containers, and the images that result from running `docker
build` can still run in your Kubernetes cluster.
-->
&lt;p>如果你是 Kubernetes 的终端用户，这对你不会有太大影响。
这事并不意味着 Docker 已死、也不意味着你不能或不该继续把 Docker 用作开发工具。
Docker 仍然是构建容器的利器，使用命令 &lt;code>docker build&lt;/code> 构建的镜像在 Kubernetes 集群中仍然可以运行。&lt;/p>
&lt;!--
If you’re using a managed Kubernetes service like GKE, EKS, or AKS (which [defaults to containerd](https://github.com/Azure/AKS/releases/tag/2020-11-16)) you will need to
make sure your worker nodes are using a supported container runtime before
Docker support is removed in a future version of Kubernetes. If you have node
customizations you may need to update them based on your environment and runtime
requirements. Please work with your service provider to ensure proper upgrade
testing and planning.
-->
&lt;p>如果你正在使用 GKE、EKS、或 AKS
(&lt;a href="https://github.com/Azure/AKS/releases/tag/2020-11-16">默认使用 containerd&lt;/a>)&lt;br>
这类托管 Kubernetes 服务，你需要在 Kubernetes 后续版本移除对 Docker 支持之前，
确认工作节点使用了被支持的容器运行时。
如果你的节点被定制过，你可能需要根据你自己的环境和运行时需求更新它们。
请与你的服务供应商协作，确保做出适当的升级测试和计划。&lt;/p>
&lt;!--
If you’re rolling your own clusters, you will also need to make changes to avoid
your clusters breaking. At v1.20, you will get a deprecation warning for Docker.
When Docker runtime support is removed in a future release (currently planned
for the 1.22 release in late 2021) of Kubernetes it will no longer be supported
and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports
the docker daemon configurations you currently use (e.g. logging).
-->
&lt;p>如果你正在运营你自己的集群，那还应该做些工作，以避免集群中断。
在 v1.20 版中，你仅会得到一个 Docker 的弃用警告。
当对 Docker 运行时的支持在 Kubernetes 某个后续发行版（目前的计划是 2021 年晚些时候的 1.22 版）中被移除时，
你需要切换到 containerd 或 CRI-O 等兼容的容器运行时。
只要确保你选择的运行时支持你当前使用的 Docker 守护进程配置（例如 logging）。&lt;/p>
&lt;!--
## So why the confusion and what is everyone freaking out about?
-->
&lt;h2 id="so-why-the-confusion-and-what-is-everyone-freaking-out-about">那为什么会有这样的困惑，为什么每个人要害怕呢？&lt;/h2>
&lt;!--
We’re talking about two different environments here, and that’s creating
confusion. Inside of your Kubernetes cluster, there’s a thing called a container
runtime that’s responsible for pulling and running your container images. Docker
is a popular choice for that runtime (other common options include containerd
and CRI-O), but Docker was not designed to be embedded inside Kubernetes, and
that causes a problem.
-->
&lt;p>我们在这里讨论的是两套不同的环境，这就是造成困惑的根源。
在你的 Kubernetes 集群中，有一个叫做容器运行时的东西，它负责拉取并运行容器镜像。
Docker 对于运行时来说是一个流行的选择（其他常见的选择包括 containerd 和 CRI-O），
但 Docker 并非设计用来嵌入到 Kubernetes，这就是问题所在。&lt;/p>
&lt;!--
You see, the thing we call “Docker” isn’t actually one thing&amp;mdash;it’s an entire
tech stack, and one part of it is a thing called “containerd,” which is a
high-level container runtime by itself. Docker is cool and useful because it has
a lot of UX enhancements that make it really easy for humans to interact with
while we’re doing development work, but those UX enhancements aren’t necessary
for Kubernetes, because it isn’t a human.
-->
&lt;p>你看，我们称之为 “Docker” 的物件实际上并不是一个物件——它是一个完整的技术堆栈，
它其中一个叫做 “containerd” 的部件本身，才是一个高级容器运行时。
Docker 既酷炫又实用，因为它提供了很多用户体验增强功能，而这简化了我们做开发工作时的操作，
Kubernetes 用不到这些增强的用户体验，毕竟它并非人类。&lt;/p>
&lt;!--
As a result of this human-friendly abstraction layer, your Kubernetes cluster
has to use another tool called Dockershim to get at what it really needs, which
is containerd. That’s not great, because it gives us another thing that has to
be maintained and can possibly break. What’s actually happening here is that
Dockershim is being removed from Kubelet as early as v1.23 release, which
removes support for Docker as a container runtime as a result. You might be
thinking to yourself, but if containerd is included in the Docker stack, why
does Kubernetes need the Dockershim?
-->
&lt;p>因为这个用户友好的抽象层，Kubernetes 集群不得不引入一个叫做 Dockershim 的工具来访问它真正需要的 containerd。
这不是一件好事，因为这引入了额外的运维工作量，而且还可能出错。
实际上正在发生的事情就是：Dockershim 将在不早于 v1.23 版中从 kubelet 中被移除，也就取消对 Docker 容器运行时的支持。
你心里可能会想，如果 containerd 已经包含在 Docker 堆栈中，为什么 Kubernetes 需要 Dockershim。&lt;/p>
&lt;!--
Docker isn’t compliant with CRI, the [Container Runtime Interface](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/).
If it were, we wouldn’t need the shim, and this wouldn’t be a thing. But it’s
not the end of the world, and you don’t need to panic&amp;mdash;you just need to change
your container runtime from Docker to another supported container runtime.
-->
&lt;p>Docker 不兼容 CRI，
&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">容器运行时接口&lt;/a>。
如果支持，我们就不需要这个 shim 了，也就没问题了。
但这也不是世界末日，你也不需要恐慌——你唯一要做的就是把你的容器运行时从 Docker 切换到其他受支持的容器运行时。&lt;/p>
&lt;!--
One thing to note: If you are relying on the underlying docker socket
(`/var/run/docker.sock`) as part of a workflow within your cluster today, moving
to a different runtime will break your ability to use it. This pattern is often
called Docker in Docker. There are lots of options out there for this specific
use case including things like
[kaniko](https://github.com/GoogleContainerTools/kaniko),
[img](https://github.com/genuinetools/img), and
[buildah](https://github.com/containers/buildah).
-->
&lt;p>要注意一点：如果你依赖底层的 Docker 套接字(&lt;code>/var/run/docker.sock&lt;/code>)，作为你集群中工作流的一部分，
切换到不同的运行时会导致你无法使用它。
这种模式经常被称之为嵌套 Docker（Docker in Docker）。
对于这种特殊的场景，有很多选项，比如：
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>、
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>、和
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>。&lt;/p>
&lt;!--
## What does this change mean for developers, though? Do we still write Dockerfiles? Do we still build things with Docker?
-->
&lt;h2 id="what-does-this-change-mean-for-developers">那么，这一改变对开发人员意味着什么？我们还要写 Dockerfile 吗？还能用 Docker 构建镜像吗？&lt;/h2>
&lt;!--
This change addresses a different environment than most folks use to interact
with Docker. The Docker installation you’re using in development is unrelated to
the Docker runtime inside your Kubernetes cluster. It’s confusing, we understand.
As a developer, Docker is still useful to you in all the ways it was before this
change was announced. The image that Docker produces isn’t really a
Docker-specific image&amp;mdash;it’s an OCI ([Open Container Initiative](https://opencontainers.org/)) image.
Any OCI-compliant image, regardless of the tool you use to build it, will look
the same to Kubernetes. Both [containerd](https://containerd.io/) and
[CRI-O](https://cri-o.io/) know how to pull those images and run them. This is
why we have a standard for what containers should look like.
-->
&lt;p>此次改变带来了一个不同的环境，这不同于我们常用的 Docker 交互方式。
你在开发环境中用的 Docker 和你 Kubernetes 集群中的 Docker 运行时无关。
我们知道这听起来让人困惑。
对于开发人员，Docker 从所有角度来看仍然有用，就跟这次改变之前一样。
Docker 构建的镜像并不是 Docker 特有的镜像——它是一个
OCI（&lt;a href="https://opencontainers.org/">开放容器标准&lt;/a>）镜像。
任一 OCI 兼容的镜像，不管它是用什么工具构建的，在 Kubernetes 的角度来看都是一样的。
&lt;a href="https://containerd.io/">containerd&lt;/a> 和
&lt;a href="https://cri-o.io/">CRI-O&lt;/a>
两者都知道怎么拉取并运行这些镜像。
这就是我们制定容器标准的原因。&lt;/p>
&lt;!--
So, this change is coming. It’s going to cause issues for some, but it isn’t
catastrophic, and generally it’s a good thing. Depending on how you interact
with Kubernetes, this could mean nothing to you, or it could mean a bit of work.
In the long run, it’s going to make things easier. If this is still confusing
for you, that’s okay&amp;mdash;there’s a lot going on here; Kubernetes has a lot of
moving parts, and nobody is an expert in 100% of it. We encourage any and all
questions regardless of experience level or complexity! Our goal is to make sure
everyone is educated as much as possible on the upcoming changes. We hope
this has answered most of your questions and soothed some anxieties! ❤️
-->
&lt;p>所以，改变已经发生。
它确实带来了一些问题，但这不是一个灾难，总的说来，这还是一件好事。
根据你操作 Kubernetes 的方式的不同，这可能对你不构成任何问题，或者也只是意味着一点点的工作量。
从一个长远的角度看，它使得事情更简单。
如果你还在困惑，也没问题——这里还有很多事情；
Kubernetes 有很多变化中的功能，没有人是100%的专家。
我们鼓励你提出任何问题，无论水平高低、问题难易。
我们的目标是确保所有人都能在即将到来的改变中获得足够的了解。
我们希望这已经回答了你的大部分问题，并缓解了一些焦虑！❤️&lt;/p>
&lt;!--
Looking for more answers? Check out our accompanying [Dockershim Removal FAQ](/blog/2022/02/17/dockershim-faq/) _(updated February 2022)_.
-->
&lt;p>还在寻求更多答案吗？请参考我们附带的
&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">移除 Dockershim 的常见问题&lt;/a> &lt;em>(2022年2月更新)&lt;/em>。&lt;/p></description></item><item><title>Blog: 弃用 Dockershim 的常见问题</title><link>https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/</guid><description>
&lt;!--
layout: blog
title: "Dockershim Deprecation FAQ"
date: 2020-12-02
slug: dockershim-faq
aliases: [ '/dockershim' ]
-->
&lt;!--
_**Update**: There is a [newer version](/blog/2022/02/17/dockershim-faq/) of this article available._
-->
&lt;p>&lt;em>&lt;strong>更新&lt;/strong>：本文有&lt;a href="https://kubernetes.io/zh/blog/2022/02/17/dockershim-faq/">较新版本&lt;/a>。&lt;/em>&lt;/p>
&lt;!--
This document goes over some frequently asked questions regarding the Dockershim
deprecation announced as a part of the Kubernetes v1.20 release. For more detail
on the deprecation of Docker as a container runtime for Kubernetes kubelets, and
what that means, check out the blog post
[Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/).
Also, you can read [check whether Dockershim deprecation affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/) to check whether it does.
-->
&lt;p>本文回顾了自 Kubernetes v1.20 版宣布弃用 Dockershim 以来所引发的一些常见问题。
关于 Kubernetes kubelets 从容器运行时的角度弃用 Docker 的细节以及这些细节背后的含义，请参考博文
&lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">别慌: Kubernetes 和 Docker&lt;/a>。&lt;/p>
&lt;p>此外，你可以阅读 &lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/">检查 Dockershim 弃用是否影响你&lt;/a>
以检查它是否会影响你。&lt;/p>
&lt;!--
### Why is dockershim being deprecated?
-->
&lt;h3 id="why-is-dockershim-being-deprecated">为什么弃用 dockershim&lt;/h3>
&lt;!--
Maintaining dockershim has become a heavy burden on the Kubernetes maintainers.
The CRI standard was created to reduce this burden and allow smooth interoperability
of different container runtimes. Docker itself doesn't currently implement CRI,
thus the problem.
-->
&lt;p>维护 dockershim 已经成为 Kubernetes 维护者肩头一个沉重的负担。
创建 CRI 标准就是为了减轻这个负担，同时也可以增加不同容器运行时之间平滑的互操作性。
但反观 Docker 却至今也没有实现 CRI，所以麻烦就来了。&lt;/p>
&lt;!--
Dockershim was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
-->
&lt;p>Dockershim 向来都是一个临时解决方案（因此得名：shim）。
你可以进一步阅读
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">移除 Kubernetes 增强方案 Dockershim&lt;/a>
以了解相关的社区讨论和计划。&lt;/p>
&lt;!--
Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.
-->
&lt;p>此外，与 dockershim 不兼容的一些特性，例如：控制组（cgoups）v2 和用户名字空间（user namespace），已经在新的 CRI 运行时中被实现。
移除对 dockershim 的支持将加速这些领域的发展。&lt;/p>
&lt;!--
### Can I still use Docker in Kubernetes 1.20?
-->
&lt;h3 id="can-I-still-use-docker-in-kubernetes-1.20">在 Kubernetes 1.20 版本中，我还可以用 Docker 吗？&lt;/h3>
&lt;!--
Yes, the only thing changing in 1.20 is a single warning log printed at [kubelet]
startup if using Docker as the runtime.
-->
&lt;p>当然可以，在 1.20 版本中仅有的改变就是：如果使用 Docker 运行时，启动
&lt;a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
的过程中将打印一条警告日志。&lt;/p>
&lt;!--
### When will dockershim be removed?
-->
&lt;h3 id="when-will-dockershim-be-removed">什么时候移除 dockershim&lt;/h3>
&lt;!--
Given the impact of this change, we are using an extended deprecation timeline.
It will not be removed before Kubernetes 1.22, meaning the earliest release without
dockershim would be 1.23 in late 2021.
_Update_: removal of dockershim is scheduled for Kubernetes v1.24, see
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
We will be working closely with vendors and other ecosystem groups to ensure a smooth transition and will evaluate
things as the situation evolves.
-->
&lt;p>考虑到此改变带来的影响，我们使用了一个加长的废弃时间表。
在 Kubernetes 1.22 版之前，它不会被彻底移除；换句话说，dockershim 被移除的最早版本会是 2021 年底发布的 1.23 版。
&lt;em>更新&lt;/em>：dockershim 计划在 Kubernetes 1.24 版被移除，
请参阅&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">移除 Kubernetes 增强方案 Dockershim&lt;/a>。
我们将与供应商以及其他生态团队紧密合作，确保顺利过渡，并将依据事态的发展评估后续事项。&lt;/p>
&lt;!--
### Will my existing Docker images still work?
-->
&lt;h3 id="will-my-existing-docker-image-still-work">我现有的 Docker 镜像还能正常工作吗？&lt;/h3>
&lt;!--
Yes, the images produced from `docker build` will work with all CRI implementations.
All your existing images will still work exactly the same.
-->
&lt;p>当然可以，&lt;code>docker build&lt;/code> 创建的镜像适用于任何 CRI 实现。
所有你的现有镜像将和往常一样工作。&lt;/p>
&lt;!--
### What about private images?
-->
&lt;h3 id="what-about-private-images">私有镜像呢？&lt;/h3>
&lt;!--
Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.
-->
&lt;p>当然可以。所有 CRI 运行时均支持 Kubernetes 中相同的拉取（pull）Secret 配置，
不管是通过 PodSpec 还是通过 ServiceAccount 均可。&lt;/p>
&lt;!--
### Are Docker and containers the same thing?
-->
&lt;h3 id="are-docker-and-containers-the-same-thing">Docker 和容器是一回事吗？&lt;/h3>
&lt;!--
Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.
-->
&lt;p>虽然 Linux 的容器技术已经存在了很久，
但 Docker 普及了 Linux 容器这种技术模式，并在开发底层技术方面发挥了重要作用。
容器的生态相比于单纯的 Docker，已经进化到了一个更宽广的领域。
像 OCI 和 CRI 这类标准帮助许多工具在我们的生态中成长和繁荣，
其中一些工具替代了 Docker 的某些部分，另一些增强了现有功能。&lt;/p>
&lt;!--
### Are there examples of folks using other runtimes in production today?
-->
&lt;h3 id="are-there-example-of-folks-using-other-runtimes-in-production-today">现在是否有在生产系统中使用其他运行时的例子？&lt;/h3>
&lt;!--
All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.
-->
&lt;p>Kubernetes 所有项目在所有版本中出产的工件（Kubernetes 二进制文件）都经过了验证。&lt;/p>
&lt;!--
Additionally, the [kind] project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the [CRI-O] runtime in production since June 2019.
-->
&lt;p>此外，&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> 项目使用 containerd 已经有年头了，
并且在这个场景中，稳定性还明显得到提升。
Kind 和 containerd 每天都会做多次协调，以验证对 Kubernetes 代码库的所有更改。
其他相关项目也遵循同样的模式，从而展示了其他容器运行时的稳定性和可用性。
例如，OpenShift 4.x 从 2019 年 6 月以来，就一直在生产环境中使用 &lt;a href="https://cri-o.io/">CRI-O&lt;/a> 运行时。&lt;/p>
&lt;!--
For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation ([CNCF]).
- [containerd](https://github.com/containerd/containerd/blob/master/ADOPTERS.md)
- [CRI-O](https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md)
-->
&lt;p>至于其他示例和参考资料，你可以查看 containerd 和 CRI-O 的使用者列表，
这两个容器运行时是云原生基金会（[CNCF]）下的项目。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### People keep referencing OCI, what is that?
-->
&lt;h3 id="people-keep-referenceing-oci-what-is-that">人们总在谈论 OCI，那是什么？&lt;/h3>
&lt;!--
OCI stands for the [Open Container Initiative], which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of [runc], which is the underlying default runtime for both
[containerd] and [CRI-O]. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.
-->
&lt;p>OCI 代表&lt;a href="https://opencontainers.org/about/overview/">开放容器标准&lt;/a>，
它标准化了容器工具和底层实现（technologies）之间的大量接口。
他们维护了打包容器镜像（OCI image-spec）和运行容器（OCI runtime-spec）的标准规范。
他们还以 &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>
的形式维护了一个 runtime-spec 的真实实现，
这也是 &lt;a href="https://containerd.io/">containerd&lt;/a> 和 &lt;a href="https://cri-o.io/">CRI-O&lt;/a> 依赖的默认运行时。
CRI 建立在这些底层规范之上，为管理容器提供端到端的标准。&lt;/p>
&lt;!--
### Which CRI implementation should I use?
-->
&lt;h3 id="which-cri-implementation-should-I-use">我应该用哪个 CRI 实现？&lt;/h3>
&lt;!--
That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the [CNCF landscape] in case another would be an
even better fit for your environment.
-->
&lt;p>这是一个复杂的问题，依赖于许多因素。
在 Docker 工作良好的情况下，迁移到 containerd 是一个相对容易的转换，并将获得更好的性能和更少的开销。
然而，我们建议你先探索 &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF 全景图&lt;/a>
提供的所有选项，以做出更适合你的环境的选择。&lt;/p>
&lt;!--
### What should I look out for when changing CRI implementations?
-->
&lt;h3 id="what-should-I-look-out-for-when-changing-CRI-implementation">当切换 CRI 底层实现时，我应该注意什么？&lt;/h3>
&lt;!--
While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:
-->
&lt;p>Docker 和大多数 CRI（包括 containerd）的底层容器化代码是相同的，但其周边部分却存在一些不同。
迁移时一些常见的关注点是：&lt;/p>
&lt;!--
- Logging configuration
- Runtime resource limitations
- Node provisioning scripts that call docker or use docker via it's control socket
- Kubectl plugins that require docker CLI or the control socket
- Kubernetes tools that require direct access to Docker (e.g. kube-imagepuller)
- Configuration of functionality like `registry-mirrors` and insecure registries
- Other support scripts or daemons that expect Docker to be available and are run
outside of Kubernetes (e.g. monitoring or security agents)
- GPUs or special hardware and how they integrate with your runtime and Kubernetes
-->
&lt;ul>
&lt;li>日志配置&lt;/li>
&lt;li>运行时的资源限制&lt;/li>
&lt;li>直接访问 docker 命令或通过控制套接字调用 Docker 的节点供应脚本&lt;/li>
&lt;li>需要访问 docker 命令或控制套接字的 kubectl 插件&lt;/li>
&lt;li>需要直接访问 Docker 的 Kubernetes 工具（例如：kube-imagepuller）&lt;/li>
&lt;li>像 &lt;code>registry-mirrors&lt;/code> 和不安全的注册表这类功能的配置&lt;/li>
&lt;li>需要 Docker 保持可用、且运行在 Kubernetes 之外的，其他支持脚本或守护进程（例如：监视或安全代理）&lt;/li>
&lt;li>GPU 或特殊硬件，以及它们如何与你的运行时和 Kubernetes 集成&lt;/li>
&lt;/ul>
&lt;!--
If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your dockerd configuration, you’ll need to adapt that for your new container
runtime where possible.
-->
&lt;p>如果你只是用了 Kubernetes 资源请求/限制或基于文件的日志收集 DaemonSet，它们将继续稳定工作，
但是如果你用了自定义了 dockerd 配置，则可能需要为新容器运行时做一些适配工作。&lt;/p>
&lt;!--
Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the [`crictl`][cr] tool as a drop-in replacement (see [mapping from docker cli to crictl](https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl)) and for the
latter you can use newer container build options like [img], [buildah],
[kaniko], or [buildkit-cli-for-kubectl] that don’t require Docker.
-->
&lt;p>另外还有一个需要关注的点，那就是当创建镜像时，系统维护或嵌入容器方面的任务将无法工作。
对于前者，可以用 &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> 工具作为临时替代方案
(参见 &lt;a href="https://kubernetes.io/zh/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl">从 docker 命令映射到 crictl&lt;/a>)；
对于后者，可以用新的容器创建选项，比如
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>、
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>、
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>、或
&lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a>，
他们均不需要访问 Docker。&lt;/p>
&lt;!--
For containerd, you can start with their [documentation] to see what configuration
options are available as you migrate things over.
-->
&lt;p>对于 containerd，你可以从它们的
&lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">文档&lt;/a>
开始，看看在迁移过程中有哪些配置选项可用。&lt;/p>
&lt;!--
For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on [Container Runtimes]
-->
&lt;p>对于如何协同 Kubernetes 使用 containerd 和 CRI-O 的说明，参见 Kubernetes 文档中这部分：
&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes">容器运行时&lt;/a>。&lt;/p>
&lt;!--
### What if I have more questions?
-->
&lt;h3 id="what-if-I-have-more-question">我还有问题怎么办？&lt;/h3>
&lt;!--
If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: https://discuss.kubernetes.io/.
-->
&lt;p>如果你使用了一个有供应商支持的 Kubernetes 发行版，你可以咨询供应商他们产品的升级计划。
对于最终用户的问题，请把问题发到我们的最终用户社区的论坛：https://discuss.kubernetes.io/。&lt;/p>
&lt;!--
You can also check out the excellent blog post
[Wait, Docker is deprecated in Kubernetes now?][dep] a more in-depth technical
discussion of the changes.
-->
&lt;p>你也可以看看这篇优秀的博文：
&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">等等，Docker 刚刚被 Kubernetes 废掉了？&lt;/a>
一个对此变化更深入的技术讨论。&lt;/p>
&lt;!--
### Can I have a hug?
-->
&lt;h3 id="can-I-have-a-hug">我可以加入吗？&lt;/h3>
&lt;!--
Always and whenever you want! 🤗🤗
-->
&lt;p>只要你愿意，随时随地欢迎加入！&lt;/p></description></item><item><title>Blog: 为开发指南做贡献</title><link>https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/</guid><description>
&lt;!--
---
title: "Contributing to the Development Guide"
linkTitle: "Contributing to the Development Guide"
Author: Erik L. Arneson
Description: "A new contributor describes the experience of writing and submitting changes to the Kubernetes Development Guide."
date: 2020-10-01
canonicalUrl: https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/
resources:
- src: "jorge-castro-code-of-conduct.jpg"
title: "Jorge Castro announcing the Kubernetes Code of Conduct during a weekly SIG ContribEx meeting."
---
-->
&lt;!--
When most people think of contributing to an open source project, I suspect they probably think of
contributing code changes, new features, and bug fixes. As a software engineer and a long-time open
source user and contributor, that's certainly what I thought. Although I have written a good quantity
of documentation in different workflows, the massive size of the Kubernetes community was a new kind
of "client." I just didn't know what to expect when Google asked my compatriots and me at
[Lion's Way](https://lionswaycontent.com/) to make much-needed updates to the Kubernetes Development Guide.
*This article originally appeared on the [Kubernetes Contributor Community blog](https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/).*
-->
&lt;p>当大多数人想到为一个开源项目做贡献时，我猜想他们可能想到的是贡献代码修改、新功能和错误修复。作为一个软件工程师和一个长期的开源用户和贡献者，这也正是我的想法。
虽然我已经在不同的工作流中写了不少文档，但规模庞大的 Kubernetes 社区是一种新型 &amp;quot;客户&amp;quot;。我只是不知道当 Google 要求我和 &lt;a href="https://lionswaycontent.com/">Lion's Way&lt;/a> 的同胞们对 Kubernetes 开发指南进行必要更新时会发生什么。&lt;/p>
&lt;p>&lt;em>本文最初出现在 &lt;a href="https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/">Kubernetes Contributor Community blog&lt;/a>。&lt;/em>&lt;/p>
&lt;!--
## The Delights of Working With a Community
As professional writers, we are used to being hired to write very specific pieces. We specialize in
marketing, training, and documentation for technical services and products, which can range anywhere from relatively fluffy marketing emails to deeply technical white papers targeted at IT and developers. With
this kind of professional service, every deliverable tends to have a measurable return on investment.
I knew this metric wouldn't be present when working on open source documentation, but I couldn't
predict how it would change my relationship with the project.
-->
&lt;h2 id="与社区合作的乐趣">与社区合作的乐趣&lt;/h2>
&lt;p>作为专业的写手，我们习惯了受雇于他人去书写非常具体的项目。我们专注于技术服务，产品营销，技术培训以及文档编制，范围从相对宽松的营销邮件到针对 IT 和开发人员的深层技术白皮书。
在这种专业服务下，每一个可交付的项目往往都有可衡量的投资回报。我知道在从事开源文档工作时不会出现这个指标，但我不确定它将如何改变我与项目的关系。&lt;/p>
&lt;!--
One of the primary traits of the relationship between our writing and our traditional clients is that we
always have one or two primary points of contact inside a company. These contacts are responsible
for reviewing our writing and making sure it matches the voice of the company and targets the
audience they're looking for. It can be stressful -- which is why I'm so glad that my writing
partner, eagle-eyed reviewer, and bloodthirsty editor [Joel](https://twitter.com/JoelByronBarker)
handles most of the client contact.
-->
&lt;p>我们的写作和传统客户之间的关系有一个主要的特点，就是我们在一个公司里面总是有一两个主要的对接人。他们负责审查我们的文稿，并确保文稿内容符合公司的声明且对标于他们正在寻找的受众。
这随之而来的压力--正好解释了为什么我很高兴我的写作伙伴、鹰眼审稿人同时也是嗜血编辑的 &lt;a href="https://twitter.com/JoelByronBarker">Joel&lt;/a> 处理了大部分的客户联系。&lt;/p>
&lt;!--
I was surprised and delighted that all of the stress of client contact went out the window when
working with the Kubernetes community.
-->
&lt;p>在与 Kubernetes 社区合作时，所有与客户接触的压力都消失了，这让我感到惊讶和高兴。&lt;/p>
&lt;!--
"How delicate do I have to be? What if I screw up? What if I make a developer angry? What if I make
enemies?" These were all questions that raced through my mind and made me feel like I was
approaching a field of eggshells when I first joined the `#sig-contribex` channel on the Kubernetes
Slack and announced that I would be working on the
[Development Guide](https://github.com/kubernetes/community/blob/master/contributors/devel/development.md).
-->
&lt;p>&amp;quot;我必须得多仔细？如果我搞砸了怎么办？如果我让开发商生气了怎么办？如果我树敌了怎么办？&amp;quot;。
当我第一次加入 Kubernetes Slack 上的 &amp;quot;#sig-contribex &amp;quot; 频道并宣布我将编写 &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/development.md">开发指南&lt;/a> 时，这些问题都在我脑海中奔腾，让我感觉如履薄冰。&lt;/p>
&lt;!--
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"The Kubernetes Code of Conduct is in effect, so please be excellent to each other." &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
-->
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"Kubernetes 编码准则已经生效，让我们共同勉励。" &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
&lt;!--
My fears were unfounded. Immediately, I felt welcome. I like to think this isn't just because I was
working on a much needed task, but rather because the Kubernetes community is filled
with friendly, welcoming people. During the weekly SIG ContribEx meetings, our reports on progress
with the Development Guide were included immediately. In addition, the leader of the meeting would
always stress that the [Kubernetes Code of Conduct](https://www.kubernetes.dev/resources/code-of-conduct/) was in
effect, and that we should, like Bill and Ted, be excellent to each other.
-->
&lt;p>事实上我的担心是多虑的。很快，我就感觉到自己是被欢迎的。我倾向于认为这不仅仅是因为我正在从事一项急需的任务，而是因为 Kubernetes 社区充满了友好、热情的人们。
在每周的 SIG ContribEx 会议上，我们关于开发指南进展情况的报告会被立即纳入其中。此外，会议的领导会一直强调 &lt;a href="https://www.kubernetes.dev/resources/code-of-conduct/">Kubernetes&lt;/a> 编码准则，我们应该像 Bill 和 Ted 一样，相互进步。&lt;/p>
&lt;!--
## This Doesn't Mean It's All Easy
The Development Guide needed a pretty serious overhaul. When we got our hands on it, it was already
packed with information and lots of steps for new developers to go through, but it was getting dusty
with age and neglect. Documentation can really require a global look, not just point fixes.
As a result, I ended up submitting a gargantuan pull request to the
[Community repo](https://github.com/kubernetes/community): 267 additions and 88 deletions.
-->
&lt;h2 id="这并不意味着这一切都很简单">这并不意味着这一切都很简单&lt;/h2>
&lt;p>开发指南需要一次全面检查。当我们拿到它的时候，它已经捆绑了大量的信息和很多新开发者需要经历的步骤，但随着时间的推移和被忽视，它变得相当陈旧。
文档的确需要全局观，而不仅仅是点与点的修复。结果，最终我向这个项目提交了一个巨大的 pull 请求。&lt;a href="https://github.com/kubernetes/community">社区仓库&lt;/a>：新增 267 行，删除 88 行。&lt;/p>
&lt;!--
The life cycle of a pull request requires a certain number of Kubernetes organization members to review and approve changes
before they can be merged. This is a great practice, as it keeps both documentation and code in
pretty good shape, but it can be tough to cajole the right people into taking the time for such a hefty
review. As a result, that massive PR took 26 days from my first submission to final merge. But in
the end, [it was successful](https://github.com/kubernetes/community/pull/5003).
-->
&lt;p>pull 请求的周期需要一定数量的 Kubernetes 组织成员审查和批准更改后才能合并。这是一个很好的做法，因为它使文档和代码都保持在相当不错的状态，
但要哄骗合适的人花时间来做这样一个赫赫有名的审查是很难的。
因此，那次大规模的 PR 从我第一次提交到最后合并，用了 26 天。 但最终，&lt;a href="https://github.com/kubernetes/community/pull/5003">它是成功的&lt;/a>.&lt;/p>
&lt;!--
Since Kubernetes is a pretty fast-moving project, and since developers typically aren't really
excited about writing documentation, I also ran into the problem that sometimes, the secret jewels
that describe the workings of a Kubernetes subsystem are buried deep within the [labyrinthine mind of
a brilliant engineer](https://github.com/amwat), and not in plain English in a Markdown file. I ran headlong into this issue
when it came time to update the getting started documentation for end-to-end (e2e) testing.
-->
&lt;p>由于 Kubernetes 是一个发展相当迅速的项目，而且开发人员通常对编写文档并不十分感兴趣，所以我也遇到了一个问题，那就是有时候，
描述 Kubernetes 子系统工作原理的秘密珍宝被深埋在 &lt;a href="https://github.com/amwat">天才工程师的迷宫式思维&lt;/a> 中，而不是用单纯的英文写在 Markdown 文件中。
当我要更新端到端（e2e）测试的入门文档时，就一头撞上了这个问题。&lt;/p>
&lt;!--
This portion of my journey took me out of documentation-writing territory and into the role of a
brand new user of some unfinished software. I ended up working with one of the developers of the new
[`kubetest2` framework](https://github.com/kubernetes-sigs/kubetest2) to document the latest process of
getting up-and-running for e2e testing, but it required a lot of head scratching on my part. You can
judge the results for yourself by checking out my
[completed pull request](https://github.com/kubernetes/community/pull/5045).
-->
&lt;p>这段旅程将我带出了编写文档的领域，进入到一些未完成软件的全新用户角色。最终我花了很多心思与新的 &lt;a href="https://github.com/kubernetes-sigs/kubetest2">kubetest2`框架&lt;/a> 的开发者之一合作，
记录了最新 e2e 测试的启动和运行过程。
你可以通过查看我的 &lt;a href="https://github.com/kubernetes/community/pull/5045">已完成的 pull request&lt;/a> 来自己判断结果。&lt;/p>
&lt;!--
## Nobody Is the Boss, and Everybody Gives Feedback
But while I secretly expected chaos, the process of contributing to the Kubernetes Development Guide
and interacting with the amazing Kubernetes community went incredibly smoothly. There was no
contention. I made no enemies. Everybody was incredibly friendly and welcoming. It was *enjoyable*.
-->
&lt;h2 id="没有人是老板-每个人都给出反馈">没有人是老板，每个人都给出反馈。&lt;/h2>
&lt;p>但当我暗自期待混乱的时候，为 Kubernetes 开发指南做贡献以及与神奇的 Kubernetes 社区互动的过程却非常顺利。
没有争执，我也没有树敌。每个人都非常友好和热情。这是令人&lt;em>愉快的&lt;/em>。&lt;/p>
&lt;!--
With an open source project, there is no one boss. The Kubernetes project, which approaches being
gargantuan, is split into many different special interest groups (SIGs), working groups, and
communities. Each has its own regularly scheduled meetings, assigned duties, and elected
chairpersons. My work intersected with the efforts of both SIG ContribEx (who watch over and seek to
improve the contributor experience) and SIG Testing (who are in charge of testing). Both of these
SIGs proved easy to work with, eager for contributions, and populated with incredibly friendly and
welcoming people.
-->
&lt;p>对于一个开源项目，没人是老板。Kubernetes 项目，一个近乎巨大的项目，被分割成许多不同的特殊兴趣小组（SIG）、工作组和社区。
每个小组都有自己的定期会议、职责分配和主席推选。我的工作与 SIG ContribEx（负责监督并寻求改善贡献者体验）和 SIG Testing（负责测试）的工作有交集。
事实证明，这两个 SIG 都很容易合作，他们渴望贡献，而且都是非常友好和热情的人。&lt;/p>
&lt;!--
In an active, living project like Kubernetes, documentation continues to need maintenance, revision,
and testing alongside the code base. The Development Guide will continue to be crucial to onboarding
new contributors to the Kubernetes code base, and as our efforts have shown, it is important that
this guide keeps pace with the evolution of the Kubernetes project.
-->
&lt;p>在 Kubernetes 这样一个活跃的、有生命力的项目中，文档仍然需要与代码库一起进行维护、修订和测试。
开发指南将继续对 Kubernetes 代码库的新贡献者起到至关重要的作用，正如我们的努力所显示的那样，该指南必须与 Kubernetes 项目的发展保持同步。&lt;/p>
&lt;!--
Joel and I really enjoy interacting with the Kubernetes community and contributing to
the Development Guide. I really look forward to continuing to not only contributing more, but to
continuing to build the new friendships I've made in this vast open source community over the past
few months.
-->
&lt;p>Joel 和我非常喜欢与 Kubernetes 社区互动并为开发指南做出贡献。我真的很期待，不仅能继续做出更多贡献，还能继续与过去几个月在这个庞大的开源社区中结识的新朋友进行合作。&lt;/p></description></item><item><title>Blog: 结构化日志介绍</title><link>https://kubernetes.io/zh/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</guid><description>
&lt;!--
layout: blog
title: 'Introducing Structured Logs'
date: 2020-09-04
slug: kubernetes-1-19-Introducing-Structured-Logs
-->
&lt;!--
**Authors:** Marek Siarkowicz (Google), Nathan Beach (Google)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Marek Siarkowicz（谷歌），Nathan Beach（谷歌）&lt;/p>
&lt;!--
Logs are an essential aspect of observability and a critical tool for debugging. But Kubernetes logs have traditionally been unstructured strings, making any automated parsing difficult and any downstream processing, analysis, or querying challenging to do reliably.
-->
&lt;p>日志是可观察性的一个重要方面，也是调试的重要工具。 但是Kubernetes日志传统上是非结构化的字符串，因此很难进行自动解析，以及任何可靠的后续处理、分析或查询。&lt;/p>
&lt;!--
In Kubernetes 1.19, we are adding support for structured logs, which natively support (key, value) pairs and object references. We have also updated many logging calls such that over 99% of logging volume in a typical deployment are now migrated to the structured format.
-->
&lt;p>在Kubernetes 1.19中，我们添加结构化日志的支持，该日志本身支持（键，值）对和对象引用。 我们还更新了许多日志记录调用，以便现在将典型部署中超过99％的日志记录量迁移为结构化格式。&lt;/p>
&lt;!--
To maintain backwards compatibility, structured logs will still be outputted as a string where the string contains representations of those "key"="value" pairs. Starting in alpha in 1.19, logs can also be outputted in JSON format using the `--logging-format=json` flag.
-->
&lt;p>为了保持向后兼容性，结构化日志仍将作为字符串输出，其中该字符串包含这些“键” =“值”对的表示。 从1.19的Alpha版本开始，日志也可以使用&lt;code>--logging-format = json&lt;/code>标志以JSON格式输出。&lt;/p>
&lt;h2 id="使用结构化日志">使用结构化日志&lt;/h2>
&lt;!--
We've added two new methods to the klog library: InfoS and ErrorS. For example, this invocation of InfoS:
-->
&lt;p>我们在klog库中添加了两个新方法：InfoS和ErrorS。 例如，InfoS的此调用：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">klog.&lt;span style="color:#00a000">InfoS&lt;/span>(&lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, klog.&lt;span style="color:#00a000">KObj&lt;/span>(pod), &lt;span style="color:#b44">&amp;#34;status&amp;#34;&lt;/span>, status)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
will result in this log:
-->
&lt;p>将得到下面的日志输出：&lt;/p>
&lt;pre>&lt;code>I1025 00:15:15.525108 1 controller_utils.go:116] &amp;quot;Pod status updated&amp;quot; pod=&amp;quot;kube-system/kubedns&amp;quot; status=&amp;quot;ready&amp;quot;
&lt;/code>&lt;/pre>&lt;!--
Or, if the --logging-format=json flag is set, it will result in this output:
-->
&lt;p>或者, 如果 --logging-format=json 模式被设置, 将会产生如下结果:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;ts&amp;#34;&lt;/span>: &lt;span style="color:#666">1580306777.04728&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;msg&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;pod&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;coredns&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kube-system&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ready&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
This means downstream logging tools can easily ingest structured logging data and instead of using regular expressions to parse unstructured strings. This also makes processing logs easier, querying logs more robust, and analyzing logs much faster.
-->
&lt;p>这意味着下游日志记录工具可以轻松地获取结构化日志数据，而无需使用正则表达式来解析非结构化字符串。这也使处理日志更容易，查询日志更健壮，并且分析日志更快。&lt;/p>
&lt;!--
With structured logs, all references to Kubernetes objects are structured the same way, so you can filter the output and only log entries referencing the particular pod. You can also find logs indicating how the scheduler was scheduling the pod, how the pod was created, the health probes of the pod, and all other changes in the lifecycle of the pod.
-->
&lt;p>使用结构化日志，所有对Kubernetes对象的引用都以相同的方式进行结构化，因此您可以过滤输出并且仅引用特定Pod的日志条目。您还可以发现指示调度程序如何调度Pod，如何创建Pod，监测Pod的运行状况以及Pod生命周期中的所有其他更改的日志。&lt;/p>
&lt;!--
Suppose you are debugging an issue with a pod. With structured logs, you can filter to only those log entries referencing the pod of interest, rather than needing to scan through potentially thousands of log lines to find the relevant ones.
-->
&lt;p>假设您正在调试Pod的问题。使用结构化日志，您可以只过滤查看感兴趣的Pod的日志条目，而无需扫描可能成千上万条日志行以找到相关的日志行。&lt;/p>
&lt;!--
Not only are structured logs more useful when manual debugging of issues, they also enable richer features like automated pattern recognition within logs or tighter correlation of log and trace data.
-->
&lt;p>结构化日志不仅在手动调试问题时更有用，而且还启用了更丰富的功能，例如日志的自动模式识别或日志和所跟踪数据的更紧密关联性（分析）。&lt;/p>
&lt;!--
Finally, structured logs can help reduce storage costs for logs because most storage systems are more efficiently able to compress structured key=value data than unstructured strings.
-->
&lt;p>最后，结构化日志可以帮助降低日志的存储成本，因为大多数存储系统比非结构化字符串更有效地压缩结构化键值数据。&lt;/p>
&lt;h2 id="参与其中">参与其中&lt;/h2>
&lt;!--
While we have updated over 99% of the log entries by log volume in a typical deployment, there are still thousands of logs to be updated. Pick a file or directory that you would like to improve and [migrate existing log calls to use structured logs](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md). It's a great and easy way to make your first contribution to Kubernetes!
-->
&lt;p>虽然在典型部署中，我们已按日志量更新了99％以上的日志条目，但仍有数千个日志需要更新。 选择一个您要改进的文件或目录，然后[迁移现有的日志调用以使用结构化日志]（https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md）。这是对Kubernetes做出第一笔贡献的好方法!&lt;/p></description></item><item><title>Blog: Kubernetes 1.18: Fit &amp; Finish</title><link>https://kubernetes.io/zh/blog/2020/03/25/kubernetes-1-18-release-announcement/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/03/25/kubernetes-1-18-release-announcement/</guid><description>
&lt;!--
**Authors:** [Kubernetes 1.18 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md)
-->
&lt;p>&lt;strong>作者:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">Kubernetes 1.18 发布团队&lt;/a>&lt;/p>
&lt;!--
We're pleased to announce the delivery of Kubernetes 1.18, our first release of 2020! Kubernetes 1.18 consists of 38 enhancements: 15 enhancements are moving to stable, 11 enhancements in beta, and 12 enhancements in alpha.
-->
&lt;p>我们很高兴宣布 Kubernetes 1.18 版本的交付，这是我们 2020 年的第一版！ Kubernetes 1.18 包含 38 个增强功能：15 项增强功能已转为稳定版，11 项增强功能处于 beta 阶段，12 项增强功能处于 alpha 阶段。&lt;/p>
&lt;!--
Kubernetes 1.18 is a "fit and finish" release. Significant work has gone into improving beta and stable features to ensure users have a better experience. An equal effort has gone into adding new developments and exciting new features that promise to enhance the user experience even more.
-->
&lt;p>Kubernetes 1.18 是一个近乎 “完美” 的版本。 为了改善 beta 和稳定的特性，已进行了大量工作，以确保用户获得更好的体验。 我们在增强现有功能的同时也增加了令人兴奋的新特性，这些有望进一步增强用户体验。&lt;/p>
&lt;!--
Having almost as many enhancements in alpha, beta, and stable is a great achievement. It shows the tremendous effort made by the community on improving the reliability of Kubernetes as well as continuing to expand its existing functionality.
-->
&lt;p>对 alpha，beta 和稳定版进行几乎同等程度的增强是一项伟大的成就。 它展现了社区在提高 Kubernetes 的可靠性以及继续扩展其现有功能方面所做的巨大努力。&lt;/p>
&lt;!--
## Major Themes
-->
&lt;h2 id="主要内容">主要内容&lt;/h2>
&lt;!--
### Kubernetes Topology Manager Moves to Beta - Align Up!
-->
&lt;h3 id="kubernetes-拓扑管理器-topology-manager-进入-beta-阶段-对齐">Kubernetes 拓扑管理器（Topology Manager）进入 Beta 阶段 - 对齐！&lt;/h3>
&lt;!--
A beta feature of Kubernetes in release 1.18, the [Topology Manager feature](https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md) enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.
-->
&lt;p>Kubernetes 在 1.18 版中的 Beta 阶段功能 &lt;a href="https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md">拓扑管理器特性&lt;/a> 启用 CPU 和设备（例如 SR-IOV VF）的 NUMA 对齐，这将使您的工作负载在针对低延迟而优化的环境中运行。在引入拓扑管理器之前，CPU 和设备管理器将做出彼此独立的资源分配决策。 这可能会导致在多处理器系统上非预期的资源分配结果，从而导致对延迟敏感的应用程序的性能下降。&lt;/p>
&lt;!--
### Serverside Apply Introduces Beta 2
-->
&lt;h3 id="serverside-apply-推出beta-2">Serverside Apply 推出Beta 2&lt;/h3>
&lt;!--
Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.
-->
&lt;p>Serverside Apply 在1.16 中进入 Beta 阶段，但现在在 1.18 中进入了第二个 Beta 阶段。 这个新版本将跟踪和管理所有新 Kubernetes 对象的字段更改，从而使您知道什么更改了资源以及何时发生了更改。&lt;/p>
&lt;!--
### Extending Ingress with and replacing a deprecated annotation with IngressClass
-->
&lt;h3 id="使用-ingressclass-扩展-ingress-并用-ingressclass-替换已弃用的注释">使用 IngressClass 扩展 Ingress 并用 IngressClass 替换已弃用的注释&lt;/h3>
&lt;!--
In Kubernetes 1.18, there are two significant additions to Ingress: A new `pathType` field and a new `IngressClass` resource. The `pathType` field allows specifying how paths should be matched. In addition to the default `ImplementationSpecific` type, there are new `Exact` and `Prefix` path types.
-->
&lt;p>在 Kubernetes 1.18 中，Ingress 有两个重要的补充：一个新的 &lt;code>pathType&lt;/code> 字段和一个新的 &lt;code>IngressClass&lt;/code> 资源。&lt;code>pathType&lt;/code> 字段允许指定路径的匹配方式。 除了默认的&lt;code>ImplementationSpecific&lt;/code>类型外，还有新的 &lt;code>Exact&lt;/code>和&lt;code>Prefix&lt;/code> 路径类型。&lt;/p>
&lt;!--
The `IngressClass` resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new `ingressClassName` field on Ingresses. This new resource and field replace the deprecated `kubernetes.io/ingress.class` annotation.
-->
&lt;p>&lt;code>IngressClass&lt;/code> 资源用于描述 Kubernetes 集群中 Ingress 的类型。 Ingress 对象可以通过在Ingress 资源类型上使用新的&lt;code>ingressClassName&lt;/code> 字段来指定与它们关联的类。 这个新的资源和字段替换了不再建议使用的 &lt;code>kubernetes.io/ingress.class&lt;/code> 注解。&lt;/p>
&lt;!--
### SIG-CLI introduces kubectl alpha debug
-->
&lt;h3 id="sig-cli-引入了-kubectl-alpha-debug">SIG-CLI 引入了 kubectl alpha debug&lt;/h3>
&lt;!--
SIG-CLI was debating the need for a debug utility for quite some time already. With the development of [ephemeral containers](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/), it became more obvious how we can support developers with tooling built on top of `kubectl exec`. The addition of the [`kubectl alpha debug` command](https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md) (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.
-->
&lt;p>SIG-CLI 一直在争论着调试工具的必要性。随着 &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">临时容器&lt;/a> 的发展，我们如何使用基于 &lt;code>kubectl exec&lt;/code> 的工具来支持开发人员的必要性变得越来越明显。 &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">&lt;code>kubectl alpha debug&lt;/code> 命令&lt;/a> 的增加，（由于是 alpha 阶段，非常欢迎您反馈意见），使开发人员可以轻松地在集群中调试 Pod。我们认为这个功能的价值非常高。 此命令允许创建一个临时容器，该容器在要尝试检查的 Pod 旁边运行，并且还附加到控制台以进行交互式故障排除。&lt;/p>
&lt;!--
### Introducing Windows CSI support alpha for Kubernetes
-->
&lt;h3 id="为-kubernetes-引入-windows-csi-支持-alpha">为 Kubernetes 引入 Windows CSI 支持（Alpha）&lt;/h3>
&lt;!--
The alpha version of CSI Proxy for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.
-->
&lt;p>用于 Windows 的 CSI 代理的 Alpha 版本随 Kubernetes 1.18 一起发布。 CSI 代理通过允许Windows 中的容器执行特权存储操作来启用 Windows 上的 CSI 驱动程序。&lt;/p>
&lt;!--
## Other Updates
-->
&lt;h2 id="其它更新">其它更新&lt;/h2>
&lt;!--
### Graduated to Stable 💯
-->
&lt;h3 id="毕业转为稳定版">毕业转为稳定版&lt;/h3>
&lt;!--
- [Taint Based Eviction](https://github.com/kubernetes/enhancements/issues/166)
- [`kubectl diff`](https://github.com/kubernetes/enhancements/issues/491)
- [CSI Block storage support](https://github.com/kubernetes/enhancements/issues/565)
- [API Server dry run](https://github.com/kubernetes/enhancements/issues/576)
- [Pass Pod information in CSI calls](https://github.com/kubernetes/enhancements/issues/603)
- [Support Out-of-Tree vSphere Cloud Provider](https://github.com/kubernetes/enhancements/issues/670)
- [Support GMSA for Windows workloads](https://github.com/kubernetes/enhancements/issues/689)
- [Skip attach for non-attachable CSI volumes](https://github.com/kubernetes/enhancements/issues/770)
- [PVC cloning](https://github.com/kubernetes/enhancements/issues/989)
- [Moving kubectl package code to staging](https://github.com/kubernetes/enhancements/issues/1020)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
- [AppProtocol for Services and Endpoints](https://github.com/kubernetes/enhancements/issues/1507)
- [Extending Hugepage Feature](https://github.com/kubernetes/enhancements/issues/1539)
- [client-go signature refactor to standardize options and context handling](https://github.com/kubernetes/enhancements/issues/1601)
- [Node-local DNS cache](https://github.com/kubernetes/enhancements/issues/1024)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/166">基于污点的逐出操作&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/491">&lt;code>kubectl diff&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/565">CSI 块存储支持&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/576">API 服务器 dry run&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/603">在 CSI 调用中传递 Pod 信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/670">支持树外 vSphere 云驱动&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/689">对 Windows 负载支持 GMSA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/770">对不可挂载的CSI卷跳过挂载&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/989">PVC 克隆&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">移动 kubectl 包代码到 staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">Windows 的 RunAsUserName&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">服务和端点的 AppProtocol&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">扩展 Hugepage 特性&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1024">Node-local DNS cache&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Major Changes
-->
&lt;h3 id="主要变化">主要变化&lt;/h3>
&lt;!--
- [EndpointSlice API](https://github.com/kubernetes/enhancements/issues/752)
- [Moving kubectl package code to staging](https://github.com/kubernetes/enhancements/issues/1020)
- [CertificateSigningRequest API](https://github.com/kubernetes/enhancements/issues/1513)
- [Extending Hugepage Feature](https://github.com/kubernetes/enhancements/issues/1539)
- [client-go signature refactor to standardize options and context handling](https://github.com/kubernetes/enhancements/issues/1601)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go 的调用规范重构来标准化选项和管理上下文&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Release Notes
-->
&lt;h3 id="发布说明">发布说明&lt;/h3>
&lt;!--
Check out the full details of the Kubernetes 1.18 release in our [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md).
-->
&lt;p>在我们的 &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md">发布文档&lt;/a>中查看 Kubernetes 1.18 发行版的完整详细信息。&lt;/p>
&lt;!--
### Availability
-->
&lt;h3 id="下载安装">下载安装&lt;/h3>
&lt;!--
Kubernetes 1.18 is available for download on [GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/) or run local Kubernetes clusters using Docker container “nodes” with [kind](https://kind.sigs.k8s.io/). You can also easily install 1.18 using [kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;p>Kubernetes 1.18 可以在 &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0">GitHub&lt;/a> 上下载。 要开始使用Kubernetes，请查看这些 &lt;a href="https://kubernetes.io/docs/tutorials/">交互教程&lt;/a> 或通过&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> 使用 Docker 容器运行本地 kubernetes 集群。您还可以使用&lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>轻松安装 1.18。&lt;/p>
&lt;!--
### Release Team
-->
&lt;h3 id="发布团队">发布团队&lt;/h3>
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md) led by Jorge Alarcon Ochoa, Site Reliability Engineer at Searchable AI. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
-->
&lt;p>通过数百位贡献了技术和非技术内容的个人的努力，使本次发行成为可能。 特别感谢由 Searchable AI 的网站可靠性工程师 Jorge Alarcon Ochoa 领导的&lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">发布团队&lt;/a>。 34 位发布团队成员协调了发布的各个方面，从文档到测试、验证和功能完整性。&lt;/p>
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [40,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 3,000 people.
-->
&lt;p>随着 Kubernetes 社区的发展壮大，我们的发布过程很好地展示了开源软件开发中的协作。 Kubernetes 继续快速获取新用户。 这种增长创造了一个积极的反馈回路，其中有更多的贡献者提交了代码，从而创建了更加活跃的生态系统。 迄今为止，Kubernetes 已有 &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">40,000 独立贡献者&lt;/a> 和一个超过3000人的活跃社区。&lt;/p>
&lt;!--
### Release Logo
-->
&lt;h3 id="发布-logo">发布 logo&lt;/h3>
&lt;!--
![Kubernetes 1.18 Release Logo](/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png)
-->
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png" alt="Kubernetes 1.18 发布图标">&lt;/p>
&lt;!--
#### Why the LHC?
-->
&lt;h4 id="为什么是-lhc">为什么是 LHC&lt;/h4>
&lt;!--
The LHC is the world’s largest and most powerful particle accelerator. It is the result of the collaboration of thousands of scientists from around the world, all for the advancement of science. In a similar manner, Kubernetes has been a project that has united thousands of contributors from hundreds of organizations – all to work towards the same goal of improving cloud computing in all aspects! "A Bit Quarky" as the release name is meant to remind us that unconventional ideas can bring about great change and keeping an open mind to diversity will lead help us innovate.
-->
&lt;p>LHC 是世界上最大，功能最强大的粒子加速器。它是由来自世界各地成千上万科学家合作的结果，所有这些合作都是为了促进科学的发展。以类似的方式，Kubernetes 已经成为一个聚集了来自数百个组织的数千名贡献者–所有人都朝着在各个方面改善云计算的相同目标努力的项目！ 发布名称“ A Bit Quarky” 的意思是提醒我们，非常规的想法可以带来巨大的变化，对开放性保持开放态度将有助于我们进行创新。&lt;/p>
&lt;!--
#### About the designer
-->
&lt;h4 id="关于设计者">关于设计者&lt;/h4>
&lt;!--
Maru Lango is a designer currently based in Mexico City. While her area of expertise is Product Design, she also enjoys branding, illustration and visual experiments using CSS + JS and contributing to diversity efforts within the tech and design communities. You may find her in most social media as @marulango or check her website: https://marulango.com
-->
&lt;p>Maru Lango 是目前居住在墨西哥城的设计师。她的专长是产品设计，她还喜欢使用 CSS + JS 进行品牌、插图和视觉实验，为技术和设计社区的多样性做贡献。您可能会在大多数社交媒体上以 @marulango 的身份找到她，或查看她的网站： &lt;a href="https://marulango.com">https://marulango.com&lt;/a>&lt;/p>
&lt;!--
### User Highlights
-->
&lt;h3 id="高光用户">高光用户&lt;/h3>
&lt;!--
- Ericsson is using Kubernetes and other cloud native technology to deliver a [highly demanding 5G network](https://www.cncf.io/case-study/ericsson/) that resulted in up to 90 percent CI/CD savings.
- Zendesk is using Kubernetes to [run around 70% of its existing applications](https://www.cncf.io/case-study/zendesk/). It’s also building all new applications to also run on Kubernetes, which has brought time savings, greater flexibility, and increased velocity to its application development.
- LifeMiles has [reduced infrastructure spending by 50%](https://www.cncf.io/case-study/lifemiles/) because of its move to Kubernetes. It has also allowed them to double its available resource capacity.
-->
&lt;ul>
&lt;li>爱立信正在使用 Kubernetes 和其他云原生技术来交付&lt;a href="https://www.cncf.io/case-study/ericsson/">高标准的 5G 网络&lt;/a>，这可以在 CI/CD 上节省多达 90％ 的支出。&lt;/li>
&lt;li>Zendesk 正在使用 Kubernetes &lt;a href="https://www.cncf.io/case-study/zendesk/">运行其现有应用程序的约 70％&lt;/a>。它还正在使所构建的所有新应用都可以在 Kubernetes 上运行，从而节省时间、提高灵活性并加快其应用程序开发的速度。&lt;/li>
&lt;li>LifeMiles 因迁移到 Kubernetes 而&lt;a href="https://www.cncf.io/case-study/lifemiles/">降低了 50% 的基础设施开支&lt;/a>。Kubernetes 还使他们可以将其可用资源容量增加一倍。&lt;/li>
&lt;/ul>
&lt;!--
### Ecosystem Updates
-->
&lt;h3 id="生态系统更新">生态系统更新&lt;/h3>
&lt;!--
- The CNCF published the results of its [annual survey](https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/) showing that Kubernetes usage in production is skyrocketing. The survey found that 78% of respondents are using Kubernetes in production compared to 58% last year.
- The “Introduction to Kubernetes” course hosted by the CNCF [surpassed 100,000 registrations](https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/).
-->
&lt;ul>
&lt;li>CNCF发布了&lt;a href="https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/">年度调查&lt;/a> 的结果，表明 Kubernetes 在生产中的使用正在飞速增长。调查发现，有78％的受访者在生产中使用Kubernetes，而去年这一比例为 58％。&lt;/li>
&lt;li>CNCF 举办的 “Kubernetes入门” 课程有&lt;a href="https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/">超过 100,000 人注册&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
### Project Velocity
-->
&lt;h3 id="项目速度">项目速度&lt;/h3>
&lt;!--
The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. [K8s DevStats](https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1) illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times.
-->
&lt;p>CNCF 继续完善 DevStats。这是一个雄心勃勃的项目，旨在对项目中的无数贡献数据进行可视化展示。&lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">K8s DevStats&lt;/a> 展示了主要公司贡献者的贡献细目，以及一系列令人印象深刻的预定义的报告，涉及从贡献者个人的各方面到 PR 生命周期的各个方面。&lt;/p>
&lt;!--
This past quarter, 641 different companies and over 6,409 individuals contributed to Kubernetes. [Check out DevStats](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;var-period=m&amp;var-repogroup_name=All) to learn more about the overall velocity of the Kubernetes project and community.
-->
&lt;p>在过去的一个季度中，641 家不同的公司和超过 6,409 个个人为 Kubernetes 作出贡献。 &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All">查看 DevStats&lt;/a> 以了解有关 Kubernetes 项目和社区发展速度的信息。&lt;/p>
&lt;!--
### Event Update
-->
&lt;h3 id="活动信息">活动信息&lt;/h3>
&lt;!--
Kubecon + CloudNativeCon EU 2020 is being pushed back – for the more most up-to-date information, please check the [Novel Coronavirus Update page](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/).
-->
&lt;p>Kubecon + CloudNativeCon EU 2020 已经推迟 - 有关最新信息，请查看&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">新型肺炎发布页面&lt;/a>。&lt;/p>
&lt;!--
### Upcoming Release Webinar
-->
&lt;h3 id="即将到来的发布的线上会议">即将到来的发布的线上会议&lt;/h3>
&lt;!--
Join members of the Kubernetes 1.18 release team on April 23rd, 2020 to learn about the major features in this release including kubectl debug, Topography Manager, Ingress to V1 graduation, and client-go. Register here: https://www.cncf.io/webinars/kubernetes-1-18/.
-->
&lt;p>在 2020 年 4 月 23 日，和 Kubernetes 1.18 版本团队一起了解此版本的主要功能，包括 kubectl debug、拓扑管理器、Ingress 毕业为 V1 版本以及 client-go。 在此处注册：https://www.cncf.io/webinars/kubernetes-1-18/ 。&lt;/p>
&lt;!--
### Get Involved
-->
&lt;h3 id="如何参与">如何参与&lt;/h3>
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
-->
&lt;p>参与 Kubernetes 的最简单方法是加入众多与您的兴趣相关的 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">特别兴趣小组&lt;/a> (SIGs) 之一。 您有什么想向 Kubernetes 社区发布的内容吗？ 参与我们的每周 &lt;a href="https://github.com/kubernetes/community/tree/master/communication">社区会议&lt;/a>，并通过以下渠道分享您的声音。 感谢您一直以来的反馈和支持。&lt;/p>
&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
- Read more about what’s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)
-->
&lt;ul>
&lt;li>在 Twitter 上关注我们 &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>，了解最新动态&lt;/li>
&lt;li>在 &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a> 上参与社区讨论&lt;/li>
&lt;li>加入 &lt;a href="http://slack.k8s.io/">Slack&lt;/a> 上的社区&lt;/li>
&lt;li>在&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>提问（或回答）&lt;/li>
&lt;li>分享您的 Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">故事&lt;/a>&lt;/li>
&lt;li>通过 &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>了解更多关于 Kubernetes 的新鲜事&lt;/li>
&lt;li>了解更多关于 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes 发布团队&lt;/a> 的信息&lt;/li>
&lt;/ul></description></item><item><title>Blog: 基于 MIPS 架构的 Kubernetes 方案</title><link>https://kubernetes.io/zh/blog/2020/01/15/kubernetes-on-mips/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/01/15/kubernetes-on-mips/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes on MIPS"
date: 2020-01-15
slug: Kubernetes-on-MIPS
-->
&lt;!--
**Authors:** TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)
-->
&lt;p>&lt;strong>作者:&lt;/strong> 石光银，尹东超，展望，江燕，蔡卫卫，高传集，孙思清（浪潮）&lt;/p>
&lt;!--
## Background
-->
&lt;h2 id="背景">背景&lt;/h2>
&lt;!--
[MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.
-->
&lt;p>&lt;a href="https://zh.wikipedia.org/wiki/MIPS%E6%9E%B6%E6%A7%8B">MIPS&lt;/a> (Microprocessor without Interlocked Pipelined Stages) 是一种采取精简指令集（RISC）的处理器架构 (ISA)，出现于 1981 年，由 MIPS 科技公司开发。如今 MIPS 架构被广泛应用于许多电子产品上。&lt;/p>
&lt;!--
[Kubernetes](https://kubernetes.io) has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it's a pity that Kubernetes doesn't support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.
-->
&lt;p>&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> 官方目前支持众多 CPU 架构诸如 x86, arm/arm64, ppc64le, s390x 等。然而目前还不支持 MIPS 架构，始终是一个遗憾。随着云原生技术的广泛应用，MIPS 架构下的用户始终对 Kubernetes on MIPS 有着迫切的需求。&lt;/p>
&lt;!--
## Achievements
-->
&lt;h2 id="成果">成果&lt;/h2>
&lt;!--
For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.
-->
&lt;p>多年来，为了丰富开源社区的生态，我们一直致力于在 MIPS 架构下适配 Kubernetes。随着 MIPS CPU 的不断迭代优化和性能的提升，我们在 mips64el 平台上取得了一些突破性的进展。&lt;/p>
&lt;!--
Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.
-->
&lt;p>多年来，我们一直积极投入 Kubernetes 社区，在 Kubernetes 技术应用和优化方面具备了丰富的经验。最近，我们在研发过程中尝试将 Kubernetes 适配到 MIPS 架构平台，并取得了阶段性成果。成功完成了 Kubernetes 以及相关组件的迁移适配，不仅搭建出稳定高可用的 MIPS 集群，同时完成了 Kubernetes v1.16.2 版本的一致性测试。&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png" alt="Kubernetes on MIPS">&lt;/p>
&lt;!--
_Figure 1 Kubernetes on MIPS_
-->
&lt;p>&lt;em>图一 Kubernetes on MIPS&lt;/em>&lt;/p>
&lt;!--
## K8S-MIPS component build
-->
&lt;h2 id="k8s-mips-组件构建">K8S-MIPS 组件构建&lt;/h2>
&lt;!--
Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:
-->
&lt;p>几乎所有的 Kubernetes 相关的云原生组件都没有提供 MIPS 版本的安装包或镜像，在 MIPS 平台上部署 Kubernetes 的前提是自行编译构建出全部所需组件。这些组件主要包括：&lt;/p>
&lt;ul>
&lt;li>golang&lt;/li>
&lt;li>docker-ce&lt;/li>
&lt;li>hyperkube&lt;/li>
&lt;li>pause&lt;/li>
&lt;li>etcd&lt;/li>
&lt;li>calico&lt;/li>
&lt;li>coredns&lt;/li>
&lt;li>metrics-server&lt;/li>
&lt;/ul>
&lt;!--
Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.
-->
&lt;p>得益于 Golang 优秀的设计以及对于 MIPS 平台的良好支持，极大地简化了上述云原生组件的编译过程。首先，我们在 mips64el 平台编译出了最新稳定的 golang, 然后通过源码构建的方式编译完成了上述大部分组件。&lt;/p>
&lt;!--
During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.
-->
&lt;p>在编译过程中，我们不可避免地遇到了很多平台兼容性的问题，比如关于 golang 系统调用 (syscall) 的兼容性问题, syscall.Stat_t 32 位 与 64 位类型转换，EpollEvent 修正位缺失等等。&lt;/p>
&lt;!--
To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.
-->
&lt;p>构建 K8S-MIPS 组件主要使用了交叉编译技术。构建过程包括集成 QEMU 工具来实现 MIPS CPU 指令的转换。同时修改 Kubernetes 和 E2E 镜像的构建脚本，构建了 Hyperkube 和 MIPS 架构的 E2E 测试镜像。&lt;/p>
&lt;!--
After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.
-->
&lt;p>成功构建出以上组件后，我们使用工具完成 Kubernetes 集群的搭建，比如 kubespray、kubeadm 等。&lt;/p>
&lt;!--
| Name | Version | MIPS Repository |
|--------------------------------|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| golang on MIPS | 1.12.5 | - |
| docker-ce on MIPS | 18.09.8 | - |
| metrics-server for CKE on MIPS | 0.3.2 | `registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2` |
| etcd for CKE on MIPS | 3.2.26 | `registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26` |
| pause for CKE on MIPS | 3.1 | `registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1` |
| hyperkube for CKE on MIPS | 1.14.3 | `registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3` |
| coredns for CKE on MIPS | 1.6.5 | `registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5` |
| calico for CKE on MIPS | 3.8.0 | `registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0` |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>名称&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>MIPS 镜像仓库&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MIPS 版本 golang&lt;/td>
&lt;td>1.12.5&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 docker-ce&lt;/td>
&lt;td>18.09.8&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 metrics-server&lt;/td>
&lt;td>0.3.2&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 etcd&lt;/td>
&lt;td>3.2.26&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 pause&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 hyperkube&lt;/td>
&lt;td>1.14.3&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 coredns&lt;/td>
&lt;td>1.6.5&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 calico&lt;/td>
&lt;td>3.8.0&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!--
**Note**: CKE is a Kubernetes-based cloud container engine launched by Inspur
-->
&lt;p>&lt;strong>注&lt;/strong>: CKE 是浪潮推出的一款基于 Kubernetes 的容器云服务引擎&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png" alt="K8S-MIPS Cluster Components">&lt;/p>
&lt;!--
_Figure 2 K8S-MIPS Cluster Components_
-->
&lt;p>&lt;em>图二 K8S-MIPS 集群组件&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png" alt="CPU Architecture">&lt;/p>
&lt;!--
_Figure 3 CPU Architecture_
-->
&lt;p>&lt;em>图三 CPU 架构&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png" alt="Cluster Node Information">&lt;/p>
&lt;!--
_Figure 4 Cluster Node Information_
-->
&lt;p>&lt;em>图四 集群节点信息&lt;/em>&lt;/p>
&lt;!--
## Run K8S Conformance Test
-->
&lt;h2 id="运行-k8s-一致性测试">运行 K8S 一致性测试&lt;/h2>
&lt;!--
The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes [conformance test](https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md).
-->
&lt;p>验证 K8S-MIP 集群稳定性和可用性最简单直接的方式是运行 Kubernetes 的 &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md">一致性测试&lt;/a>。&lt;/p>
&lt;!--
Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.
-->
&lt;p>一致性测试是一个独立的容器，用于启动 Kubernetes 端到端的一致性测试。&lt;/p>
&lt;!--
Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from `kubernetes/test/images`, and the built images are at `gcr.io/kubernetes-e2e-test-images`. Since there are no MIPS images in the repository, we must first build all needed images to run the test.
-->
&lt;p>当执行一致性测试时，测试程序会启动许多 Pod 进行各种端到端的行为测试，这些 Pod 使用的镜像源码大部分来自于 &lt;code>kubernetes/test/images&lt;/code> 目录下，构建的镜像位于 &lt;code>gcr.io/kubernetes-e2e-test-images/&lt;/code>。由于镜像仓库中目前并不存在 MIPS 架构的镜像，我们要想运行 E2E 测试，必须首先构建出测试所需的全部镜像。&lt;/p>
&lt;!--
### Build needed images for test
-->
&lt;h3 id="构建测试所需镜像">构建测试所需镜像&lt;/h3>
&lt;!--
The first step is to find all needed images for the test. We can run `sonobuoy images-p e2e` command to list all images, or we can find those images in [/test/utils/image/manifest.go](https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go). Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.
-->
&lt;p>第一步是找到测试所需的所有镜像。我们可以执行 &lt;code>sonobuoy images-p e2e&lt;/code> 命令来列出所有镜像，或者我们可以在 &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go">/test/utils/image/manifest.go&lt;/a> 中找到这些镜像。尽管 Kubernetes 官方提供了完整的 Makefile 和 shell 脚本，为构建测试映像提供了命令，但是仍然有许多与体系结构相关的问题未能解决，比如基础映像和依赖包的不兼容问题。因此，我们无法通过直接执行这些构建命令来制作 mips64el 架构镜像。&lt;/p>
&lt;!--
Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of [alpine](https://www.alpinelinux.org/), so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.
-->
&lt;p>多数测试镜像都是使用 golang 编写，然后编译出二进制文件，并基于相应的 Dockerfile 制作出镜像。这些镜像对我们来说可以轻松地制作出来。但是需要注意一点：测试镜像默认使用的基础镜像大多是 alpine, 目前 &lt;a href="https://www.alpinelinux.org/">Alpine&lt;/a> 官方并不支持 mips64el 架构，我们暂时未能自己制作出 mips64el 版本的 alpine 础镜像，只能将基础镜像替换为我们目前已有的 mips64el 基础镜像，比如 debian-stretch,fedora, ubuntu 等。替换基础镜像的同时也需要替换安装依赖包的命令，甚至依赖包的版本等。&lt;/p>
&lt;!--
Some images are not in `kubernetes/test/images`, such as `gcr.io/google-samples/gb-frontend:v6`. There is no clear documentation explaining where these images are locaated, though we found the source code in repository [github.com/GoogleCloudPlatform/kubernetes-engine-samples](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples). We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as `php:5-apache`, `redis`, and `perl`.
-->
&lt;p>有些测试所需镜像的源码并不在 &lt;code>kubernetes/test/images&lt;/code> 下,比如 &lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code> 等，没有明确的文档说明这类镜像来自于何方，最终还是在 &lt;a href="github.com/GoogleCloudPlatform/kubernetes-engine-samples">github.com/GoogleCloudPlatform/kubernetes-engine-samples&lt;/a> 这个仓库找到了原始的镜像源代码。但是很快我们遇到了新的问题，为了制作这些镜像，还要制作它依赖的基础镜像，甚至基础镜像的基础镜像，比如 &lt;code>php:5-apache&lt;/code>、&lt;code>redis&lt;/code>、&lt;code>perl&lt;/code> 等等。&lt;/p>
&lt;!--
After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is `imagePullPolicy: ifNotPresent`.
-->
&lt;p>经过漫长庞杂的的镜像重制工作，我们完成了总计约 40 个镜像的制作 ，包括测试镜像以及直接和间接依赖的基础镜像。
最终我们将所有镜像在集群内准备妥当，并确保测试用例内所有 Pod 的镜像拉取策略设置为 &lt;code>imagePullPolicy: ifNotPresent&lt;/code>。&lt;/p>
&lt;!--
Here are some of the images we built
-->
&lt;p>这是我们构建出的部分镜像列表：&lt;/p>
&lt;ul>
&lt;li>&lt;code>docker.io/library/busybox:1.29&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.14-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.15-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/perl:5.26&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/httpd:2.4.38-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/redis:5.0.5-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/conformance:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/hyperkube:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/agnhost:2.6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/dnsutils:1.1&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/echoserver:2.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/kitten:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nautilus:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonroot:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/test-webserver:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/etcd:3.3.15&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/pause:3.1&lt;/code>&lt;/li>
&lt;/ul>
&lt;!--
Finally, we ran the tests and got the test result, include `e2e.log`, which showed that all test cases passed. Additionally, we submitted our test result to [k8s-conformance](https://github.com/cncf/k8s-conformance) as a [pull request](https://github.com/cncf/k8s-conformance/pull/779).
-->
&lt;p>最终我们执行一致性测试并且得到了测试报告，包括 &lt;code>e2e.log&lt;/code>，显示我们通过了全部的测试用例。此外，我们将测试结果以 &lt;a href="https://github.com/cncf/k8s-conformance/pull/779">pull request&lt;/a> 的形式提交给了 &lt;a href="https://github.com/cncf/k8s-conformance">k8s-conformance&lt;/a> 。&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png" alt="Pull request for conformance test results">&lt;/p>
&lt;!--
_Figure 5 Pull request for conformance test results_
-->
&lt;p>&lt;em>图五 一致性测试结果的 PR&lt;/em>&lt;/p>
&lt;!--
## What's next
-->
&lt;h2 id="后续计划">后续计划&lt;/h2>
&lt;!--
We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.
-->
&lt;p>我们手动构建了 K8S-MIPS 组件以及执行了 E2E 测试，验证了 Kubernetes on MIPS 的可行性，极大的增强了我们对于推进 Kubernetes 支持 MIPS 架构的信心。&lt;/p>
&lt;!--
In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.
-->
&lt;p>后续，我们将积极地向社区贡献我们的工作经验以及成果，提交 PR 以及 Patch For MIPS 等， 希望能够有更多的来自社区的力量加入进来，共同推进 Kubernetes for MIPS 的进程。&lt;/p>
&lt;!--
Contribution plan：
-->
&lt;p>后续开源贡献计划：&lt;/p>
&lt;!--
- contribute the source of e2e test images for MIPS
- contribute the source of hyperkube for MIPS
- contribute the source of deploy tools like kubeadm for MIPS
-->
&lt;ul>
&lt;li>贡献构建 E2E 测试镜像代码&lt;/li>
&lt;li>贡献构建 MIPS 版本 hyperkube 代码&lt;/li>
&lt;li>贡献构建 MIPS 版本 kubeadm 等集群部署工具&lt;/li>
&lt;/ul>
&lt;hr></description></item><item><title>Blog: Kubernetes 1.17：稳定</title><link>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</link><pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</guid><description>
&lt;!-- ---
layout: blog
title: "Kubernetes 1.17: Stability"
date: 2019-12-09T13:00:00-08:00
slug: kubernetes-1-17-release-announcement
evergreen: true
--- -->
&lt;p>&lt;strong>作者:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">Kubernetes 1.17发布团队&lt;/a>&lt;/p>
&lt;!--
**Authors:** [Kubernetes 1.17 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md)
-->
&lt;p>我们高兴的宣布Kubernetes 1.17版本的交付，它是我们2019年的第四个也是最后一个发布版本。Kubernetes v1.17包含22个增强功能：有14个增强已经逐步稳定(stable)，4个增强功能已经进入公开测试版(beta)，4个增强功能刚刚进入内部测试版(alpha)。&lt;/p>
&lt;!--
We’re pleased to announce the delivery of Kubernetes 1.17, our fourth and final release of 2019! Kubernetes v1.17 consists of 22 enhancements: 14 enhancements have graduated to stable, 4 enhancements are moving to beta, and 4 enhancements are entering alpha.
-->
&lt;h2 id="主要的主题">主要的主题&lt;/h2>
&lt;!--
## Major Themes
-->
&lt;h3 id="云服务提供商标签基本可用">云服务提供商标签基本可用&lt;/h3>
&lt;!--
### Cloud Provider Labels reach General Availability
-->
&lt;p>作为公开测试版特性添加到 v1.2 ，v1.17 中可以看到云提供商标签达到基本可用。&lt;/p>
&lt;!--
Added as a beta feature way back in v1.2, v1.17 sees the general availability of cloud provider labels.
-->
&lt;h3 id="卷快照进入公开测试版">卷快照进入公开测试版&lt;/h3>
&lt;!--
### Volume Snapshot Moves to Beta
-->
&lt;p>在 v1.17 中，Kubernetes卷快照特性是公开测试版。这个特性是在 v1.12 中以内部测试版引入的，第二个有重大变化的内部测试版是 v1.13 。&lt;/p>
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.
-->
&lt;h2 id="容器存储接口迁移公开测试版">容器存储接口迁移公开测试版&lt;/h2>
&lt;!--
### CSI Migration Beta
-->
&lt;p>在 v1.17 中，Kubernetes树内存储插件到容器存储接口(CSI)的迁移基础架构是公开测试版。容器存储接口迁移最初是在Kubernetes v1.14 中以内部测试版引入的。&lt;/p>
&lt;!--
The Kubernetes in-tree storage plugin to Container Storage Interface (CSI) migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
-->
&lt;h2 id="云服务提供商标签基本可用-1">云服务提供商标签基本可用&lt;/h2>
&lt;!--
## Cloud Provider Labels reach General Availability
-->
&lt;p>当节点和卷被创建，会基于基础云提供商的Kubernetes集群打上一系列标准标签。节点会获得一个实例类型标签。节点和卷都会得到两个描述资源在云提供商拓扑的位置标签,通常是以区域和地区的方式组织。&lt;/p>
&lt;!--
When nodes and volumes are created, a set of standard labels are applied based on the underlying cloud provider of the Kubernetes cluster. Nodes get a label for the instance type. Both nodes and volumes get two labels describing the location of the resource in the cloud provider topology, usually organized in zones and regions.
-->
&lt;p>Kubernetes组件使用标准标签来支持一些特性。例如，调度者会保证pods和它们所声明的卷放置在相同的区域；当调度部署的pods时，调度器会优先将它们分布在不同的区域。你还可以在自己的pods标准中利用标签来配置，如节点亲和性，之类的事。标准标签使得你写的pod规范在不同的云提供商之间是可移植的。&lt;/p>
&lt;!--
Standard labels are used by Kubernetes components to support some features. For example, the scheduler would ensure that pods are placed on the same zone as the volumes they claim; and when scheduling pods belonging to a deployment, the scheduler would prioritize spreading them across zones. You can also use the labels in your pod specs to configure things as such node affinity. Standard labels allow you to write pod specs that are portable among different cloud providers.
-->
&lt;p>在这个版本中，标签已经达到基本可用。Kubernetes组件都已经更新，可以填充基本可用和公开测试版标签，并对两者做出反应。然而，如果你的pod规范或自定义的控制器正在使用公开测试版标签，如节点亲和性，我们建议你可以将它们迁移到新的基本可用标签中。你可以从如下地方找到新标签的文档：&lt;/p>
&lt;!--
The labels are reaching general availability in this release. Kubernetes components have been updated to populate the GA and beta labels and to react to both. However, if you are using the beta labels in your pod specs for features such as node affinity, or in your custom controllers, we recommend that you start migrating them to the new GA labels. You can find the documentation for the new labels here:
-->
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/zh/docs/reference/labels-annotations-taints/#nodekubernetesioinstance-type">实例类型&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/zh/docs/reference/labels-annotations-taints/#topologykubernetesioregion">地区&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/zh/docs/reference/labels-annotations-taints/#topologykubernetesiozone">区域&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [node.kubernetes.io/instance-type](/docs/reference/labels-annotations-taints/#nodekubernetesioinstance-type)
- [topology.kubernetes.io/region](/docs/reference/labels-annotations-taints/#topologykubernetesioregion)
- [topology.kubernetes.io/zone](/docs/reference/labels-annotations-taints/#topologykubernetesiozone)
-->
&lt;h2 id="卷快照进入公开测试版-1">卷快照进入公开测试版&lt;/h2>
&lt;!--
## Volume Snapshot Moves to Beta
-->
&lt;p>在 v1.17 中，Kubernetes卷快照是是公开测试版。最初是在 v1.12 中以内部测试版引入的，第二个有重大变化的内部测试版是 v1.13 。这篇文章总结它在公开版本中的变化。&lt;/p>
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13. This post summarizes the changes in the beta release.
-->
&lt;h3 id="卷快照是什么">卷快照是什么？&lt;/h3>
&lt;!-- ### What is a Volume Snapshot? -->
&lt;p>许多的存储系统(如谷歌云持久化磁盘，亚马逊弹性块存储和许多的内部存储系统)支持为持久卷创建快照。快照代表卷在一个时间点的复制。它可用于配置新卷(使用快照数据提前填充)或恢复卷到一个之前的状态(用快照表示)。&lt;/p>
&lt;!--
Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).
-->
&lt;h3 id="为什么给kubernetes加入卷快照">为什么给Kubernetes加入卷快照？&lt;/h3>
&lt;!--
### Why add Volume Snapshots to Kubernetes?
-->
&lt;p>Kubernetes卷插件系统已经提供了功能强大的抽象用于自动配置、附加和挂载块文件系统。&lt;/p>
&lt;!--
The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.
-->
&lt;p>支持所有这些特性是Kubernets负载可移植的目标：Kubernetes旨在分布式系统应用和底层集群之间创建一个抽象层,使得应用可以不感知其运行集群的具体信息并且部署也不需特定集群的知识。&lt;/p>
&lt;!--
Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.
-->
&lt;p>Kubernetes存储特别兴趣组(SIG)将快照操作确定为对很多有状态负载的关键功能。如数据库管理员希望在操作数据库前保存数据库卷快照。&lt;/p>
&lt;!--
The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.
-->
&lt;p>在Kubernetes接口中提供一种标准的方式触发快照操作，Kubernetes用户可以处理这种用户场景，而不必使用Kubernetes API(并手动执行存储系统的具体操作)。&lt;/p>
&lt;!--
By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).
-->
&lt;p>取而代之的是，Kubernetes用户现在被授权以与集群无关的方式将快照操作放进他们的工具和策略中，并且确信它将对任意的Kubernetes集群有效，而与底层存储无关。&lt;/p>
&lt;!--
Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.
-->
&lt;p>此外，Kubernetes 快照原语作为基础构建能力解锁了为Kubernetes开发高级、企业级、存储管理特性的能力:包括应用或集群级别的备份方案。&lt;/p>
&lt;!--
Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.
-->
&lt;p>你可以阅读更多关于&lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/">发布容器存储接口卷快照公开测试版&lt;/a>&lt;/p>
&lt;!--
You can read more in the blog entry about [releasing CSI volume snapshots to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/).
-->
&lt;h2 id="容器存储接口迁移公测版">容器存储接口迁移公测版&lt;/h2>
&lt;!--
## CSI Migration Beta
-->
&lt;h3 id="为什么我们迁移内建树插件到容器存储接口">为什么我们迁移内建树插件到容器存储接口？&lt;/h3>
&lt;!--
### Why are we migrating in-tree plugins to CSI?
-->
&lt;p>在容器存储接口之前，Kubernetes提供功能强大的卷插件系统。这些卷插件是树内的意味着它们的代码是核心Kubernetes代码的一部分并附带在核心Kubernetes二进制中。然而，为Kubernetes添加插件支持新卷是非常有挑战的。希望在Kubernetes上为自己存储系统添加支持(或修复现有卷插件的bug)的供应商被迫与Kubernetes发行进程对齐。此外，第三方存储代码在核心Kubernetes二进制中会造成可靠性和安全问题，并且这些代码对于Kubernetes的维护者来说是难以(一些场景是不可能)测试和维护的。在Kubernetes上采用容器存储接口可以解决大部分问题。&lt;/p>
&lt;!--
Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.
-->
&lt;p>随着更多容器存储接口驱动变成生产环境可用，我们希望所有的Kubernetes用户从容器存储接口模型中获益。然而，我们不希望强制用户以破坏现有基本可用的存储接口的方式去改变负载和配置。道路很明确，我们将不得不用CSI替换树内插件接口。什么是容器存储接口迁移？&lt;/p>
&lt;!--
As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.What is CSI migration?
-->
&lt;p>在容器存储接口迁移上所做的努力使得替换现有的树内存储插件，如&lt;code>kubernetes.io/gce-pd&lt;/code>或&lt;code>kubernetes.io/aws-ebs&lt;/code>，为相应的容器存储接口驱动成为可能。如果容器存储接口迁移正常工作，Kubernetes终端用户不会注意到任何差别。迁移过后，Kubernetes用户可以继续使用现有接口来依赖树内存储插件的功能。&lt;/p>
&lt;!--
The CSI migration effort enables the replacement of existing in-tree storage plugins such as `kubernetes.io/gce-pd` or `kubernetes.io/aws-ebs` with a corresponding CSI driver. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.
-->
&lt;p>当Kubernetes集群管理者更新集群使得CSI迁移可用，现有的有状态部署和工作负载照常工作；然而，在幕后Kubernetes将存储管理操作交给了(以前是交给树内驱动)CSI驱动。&lt;/p>
&lt;!--
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.
-->
&lt;p>Kubernetes组非常努力地保证存储接口的稳定性和平滑升级体验的承诺。这需要细致的考虑现有特性和行为来确保后向兼容和接口稳定性。你可以想像成在加速行驶的直线上给赛车换轮胎。&lt;/p>
&lt;!--
The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.
-->
&lt;p>你可以在这篇博客中阅读更多关于&lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">容器存储接口迁移成为公开测试版&lt;/a>.&lt;/p>
&lt;!--
You can read more in the blog entry about [CSI migration going to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/). -->
&lt;h2 id="其它更新">其它更新&lt;/h2>
&lt;!--
## Other Updates
-->
&lt;h3 id="稳定">稳定💯&lt;/h3>
&lt;!--
### Graduated to Stable 💯
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/382">按条件污染节点&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/495">可配置的Pod进程共享命名空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/548">采用kube-scheduler调度DaemonSet Pods&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/554">动态卷最大值&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/557">Kubernetes容器存储接口支持拓扑&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/559">在SubPath挂载提供环境变量扩展&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/575">为Custom Resources提供默认值&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/589">从频繁的Kublet心跳到租约接口&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/714">拆分Kubernetes测试Tarball&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/956">添加Watch书签支持&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/960">行为驱动一致性测试&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/980">服务负载均衡终结保护&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1152">避免每一个Watcher独立序列化相同的对象&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Taint Node by Condition](https://github.com/kubernetes/enhancements/issues/382)
- [Configurable Pod Process Namespace Sharing](https://github.com/kubernetes/enhancements/issues/495)
- [Schedule DaemonSet Pods by kube-scheduler](https://github.com/kubernetes/enhancements/issues/548)
- [Dynamic Maximum Volume Count](https://github.com/kubernetes/enhancements/issues/554)
- [Kubernetes CSI Topology Support](https://github.com/kubernetes/enhancements/issues/557)
- [Provide Environment Variables Expansion in SubPath Mount](https://github.com/kubernetes/enhancements/issues/559)
- [Defaulting of Custom Resources](https://github.com/kubernetes/enhancements/issues/575)
- [Move Frequent Kubelet Heartbeats To Lease Api](https://github.com/kubernetes/enhancements/issues/589)
- [Break Apart The Kubernetes Test Tarball](https://github.com/kubernetes/enhancements/issues/714)
- [Add Watch Bookmarks Support](https://github.com/kubernetes/enhancements/issues/956)
- [Behavior-Driven Conformance Testing](https://github.com/kubernetes/enhancements/issues/960)
- [Finalizer Protection For Service Loadbalancers](https://github.com/kubernetes/enhancements/issues/980)
- [Avoid Serializing The Same Object Independently For Every Watcher](https://github.com/kubernetes/enhancements/issues/1152)
-->
&lt;h3 id="主要变化">主要变化&lt;/h3>
&lt;!--
### Major Changes
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/563">添加IPv4/IPv6双栈支持&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Add IPv4/IPv6 Dual Stack Support](https://github.com/kubernetes/enhancements/issues/563)
-->
&lt;h3 id="其它显著特性">其它显著特性&lt;/h3>
&lt;!--
### Other Notable Features
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/536">拓扑感知路由服务(内部测试版)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">为Windows添加RunAsUserName&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Topology Aware Routing of Services (Alpha)](https://github.com/kubernetes/enhancements/issues/536)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
-->
&lt;h3 id="可用性">可用性&lt;/h3>
&lt;!--
### Availability
-->
&lt;p>Kubernetes 1.17 可以&lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0">在GitHub下载&lt;/a>。开始使用Kubernetes，看看这些&lt;a href="https://kubernetes.io/docs/tutorials/">交互教学&lt;/a>。你可以非常容易使用&lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>安装1.17。&lt;/p>
&lt;!--
Kubernetes 1.17 is available for [download on GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/). You can also easily install 1.17 using
[kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;h3 id="发布团队">发布团队&lt;/h3>
&lt;!--
### Release Team
-->
&lt;p>正是因为有上千人参与技术或非技术内容的贡献才使这个版本成为可能。特别感谢由Guinevere Saenger领导的&lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">发布团队&lt;/a>。发布团队的35名成员在发布版本的多方面进行了协调，从文档到测试，校验和特性的完善。&lt;/p>
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md) led by Guinevere Saenger. The 35 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
-->
&lt;p>随着Kubernetes社区的成长，我们的发布流程是在开源软件协作方面惊人的示例。Kubernetes快速并持续获得新用户。这一成长产生了良性的反馈循环，更多的贡献者贡献代码创造了更加活跃的生态。Kubernetes已经有超过&lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">39000位贡献者&lt;/a>和一个超过66000人的活跃社区。&lt;/p>
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [39,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 66,000 people.
-->
&lt;h3 id="网络研讨会">网络研讨会&lt;/h3>
&lt;!--
### Webinar
-->
&lt;p>2020年1月7号，加入Kubernetes 1.17发布团队，学习关于这次发布的主要特性。&lt;a href="https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A">这里&lt;/a>注册。&lt;/p>
&lt;!--
Join members of the Kubernetes 1.17 release team on Jan 7th, 2020 to learn about the major features in this release. Register [here](https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A).
-->
&lt;h3 id="参与其中">参与其中&lt;/h3>
&lt;!--
### Get Involved
-->
&lt;p>最简单的参与Kubernetes的方式是加入其中一个与你兴趣相同的&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">特别兴趣组&lt;/a>（SIGs)。有什么想要广播到Kubernetes社区吗？通过如下的频道，在每周的&lt;a href="https://github.com/kubernetes/community/tree/master/communication">社区会议&lt;/a>分享你的声音。感谢你的贡献和支持。&lt;/p>
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
-->
&lt;ul>
&lt;li>在Twitter上关注我们&lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>获取最新的更新&lt;/li>
&lt;li>在&lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>参与社区的讨论&lt;/li>
&lt;li>在&lt;a href="http://slack.k8s.io/">Slack&lt;/a>加入社区&lt;/li>
&lt;li>在&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>发布问题(或回答问题)&lt;/li>
&lt;li>分享你的Kubernetes&lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">故事&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
--></description></item><item><title>Blog: 使用 Java 开发一个 Kubernetes controller</title><link>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</guid><description>
&lt;!--
---
layout: blog
title: "Develop a Kubernetes controller in Java"
date: 2019-11-26
slug: Develop-A-Kubernetes-Controller-in-Java
---
-->
&lt;!--
**Authors:** Min Kim (Ant Financial), Tony Ado (Ant Financial)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Min Kim (蚂蚁金服), Tony Ado (蚂蚁金服)&lt;/p>
&lt;!--
The official [Kubernetes Java SDK](https://github.com/kubernetes-client/java) project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.
-->
&lt;p>&lt;a href="https://github.com/kubernetes-client/java">Kubernetes Java SDK&lt;/a> 官方项目最近发布了他们的最新工作，为 Java Kubernetes 开发人员提供一个便捷的 Kubernetes 控制器-构建器 SDK，它有助于轻松开发高级工作负载或系统。&lt;/p>
&lt;!--
## Overall
Java is no doubt one of the most popular programming languages in the world but
it's been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there're already some excellent controller
frameworks, for example, [controller runtime](https://github.com/kubernetes-sigs/controller-runtime),
[operator SDK](https://github.com/operator-framework/operator-sdk). These
existing Golang frameworks are relying on the various utilities from the
[Kubernetes Golang SDK](https://github.com/kubernetes/client-go) proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.
-->
&lt;h2 id="综述">综述&lt;/h2>
&lt;p>Java 无疑是世界上最流行的编程语言之一，但由于社区中缺少库资源，一段时间以来，那些非 Golang 开发人员很难构建他们定制的 controller/operator。在 Golang 的世界里，已经有一些很好的 controller 框架了，例如，&lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller runtime&lt;/a>，&lt;a href="https://github.com/operator-framework/operator-sdk">operator SDK&lt;/a>。这些现有的 Golang 框架依赖于 &lt;a href="https://github.com/kubernetes/client-go">Kubernetes Golang SDK&lt;/a> 提供的各种实用工具，这些工具经过多年证明是稳定的。受进一步集成到 Kubernetes 平台的需求驱动，我们不仅将 Golang SDK 中的许多基本工具移植到 kubernetes Java SDK 中，包括 informers、work-queues、leader-elections 等，也开发了一个控制器构建 SDK，它可以将所有东西连接到一个可运行的控制器中，而不会产生任何问题。&lt;/p>
&lt;!--
## Backgrounds
Why use Java to implement Kubernetes tooling? You might pick Java for:
- __Integrating legacy enterprise Java systems__: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.
- __More open-source community resources__: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.
-->
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>为什么要使用 Java 实现 kubernetes 工具？选择 Java 的原因可能是：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>集成遗留的企业级 Java 系统&lt;/strong>：许多公司的遗留系统或框架都是用 Java 编写的，用以支持稳定性。我们不能轻易把所有东西搬到 Golang。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>更多开源社区的资源&lt;/strong>：Java 是成熟的，并且在过去几十年中累计了丰富的开源库，尽管 Golang 对于开发人员来说越来越具有吸引力，越来越流行。此外，现在开发人员能够在 SQL 存储上开发他们的聚合-apiserver，而 Java 在 SQL 上有更好的支持。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
## How to use?
Take maven project as example, adding the following dependencies into your dependencies:
-->
&lt;h2 id="如何去使用">如何去使用&lt;/h2>
&lt;p>以 maven 项目为例，将以下依赖项添加到您的依赖中：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-xml" data-lang="xml">&lt;span style="color:#008000;font-weight:bold">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;groupId&amp;gt;&lt;/span>io.kubernetes&lt;span style="color:#008000;font-weight:bold">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;artifactId&amp;gt;&lt;/span>client-java-extended&lt;span style="color:#008000;font-weight:bold">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;version&amp;gt;&lt;/span>6.0.1&lt;span style="color:#008000;font-weight:bold">&amp;lt;/version&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example [here](https://github.com/kubernetes-client/java/blob/master/examples/examples-release-13/src/main/java/io/kubernetes/client/examples/ControllerExample.java):
-->
&lt;p>然后我们可以使用提供的生成器库来编写自己的控制器。例如，下面是一个简单的控制，它打印出关于监视通知的节点信息，
在&lt;a href="https://github.com/kubernetes-client/java/blob/master/examples/examples-release-13/src/main/java/io/kubernetes/client/examples/ControllerExample.java">此处&lt;/a>
查看完整的例子：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-java" data-lang="java">&lt;span style="color:#666">...&lt;/span>
Reconciler reconciler &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Reconciler&lt;span style="color:#666">()&lt;/span> &lt;span style="color:#666">{&lt;/span>
&lt;span style="color:#a2f">@Override&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">public&lt;/span> Result &lt;span style="color:#00a000">reconcile&lt;/span>&lt;span style="color:#666">(&lt;/span>Request request&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">{&lt;/span>
V1Node node &lt;span style="color:#666">=&lt;/span> nodeLister&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">get&lt;/span>&lt;span style="color:#666">(&lt;/span>request&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
System&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">out&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">println&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;triggered reconciling &amp;#34;&lt;/span> &lt;span style="color:#666">+&lt;/span> node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getMetadata&lt;/span>&lt;span style="color:#666">().&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Result&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#666">);&lt;/span>
&lt;span style="color:#666">}&lt;/span>
&lt;span style="color:#666">};&lt;/span>
Controller controller &lt;span style="color:#666">=&lt;/span>
ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">defaultBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>informerFactory&lt;span style="color:#666">)&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">watch&lt;/span>&lt;span style="color:#666">(&lt;/span>
&lt;span style="color:#666">(&lt;/span>workQueue&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">-&amp;gt;&lt;/span> ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">controllerWatchBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>V1Node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">class&lt;/span>&lt;span style="color:#666">,&lt;/span> workQueue&lt;span style="color:#666">).&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">())&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReconciler&lt;/span>&lt;span style="color:#666">(&lt;/span>nodeReconciler&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// required, set the actual reconciler
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withName&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;node-printing-controller&amp;#34;&lt;/span>&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set name for controller for logging, thread-tracing
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withWorkerCount&lt;/span>&lt;span style="color:#666">(&lt;/span>4&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set worker thread count
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReadyFunc&lt;/span>&lt;span style="color:#666">(&lt;/span> nodeInformer&lt;span style="color:#666">::&lt;/span>hasSynced&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, only starts controller when the cache has synced up
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">();&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
If you notice, the new Java controller framework learnt a lot from the design of
[controller-runtime](https://github.com/kubernetes-sigs/controller-runtime) which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.
-->
&lt;p>如果您留意，新的 Java 控制器框架很多地方借鉴于 &lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime&lt;/a> 的设计，它成功地将控制器内部的复杂组件封装到几个干净的接口中。在 Java 泛型的帮助下，我们甚至更进一步，以更好的方式简化了封装。&lt;/p>
&lt;!--
As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.
-->
&lt;p>我们可以将多个控制器封装到一个 controller-manager 或 leader-electing controller 中，这有助于在 HA 设置中进行部署。&lt;/p>
&lt;!--
## Future steps
The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo [kubernetes-client/java](https://github.com/kubernetes-client/java).
Feel free to share also your feedback with us, through Issues or [Slack](http://kubernetes.slack.com/messages/kubernetes-client/).
-->
&lt;h2 id="未来计划">未来计划&lt;/h2>
&lt;p>Kubernetes Java SDK 项目背后的社区将专注于为希望编写云原生 Java 应用程序来扩展 Kubernetes 的开发人员提供更有用的实用程序。如果您对更详细的信息感兴趣，请查看我们的仓库 &lt;a href="https://github.com/kubernetes-client/java">kubernetes-client/java&lt;/a>。请通过问题或 &lt;a href="http://kubernetes.slack.com/messages/kubernetes-client/">Slack&lt;/a> 与我们分享您的反馈。&lt;/p></description></item><item><title>Blog: 使用 Microk8s 在 Linux 上本地运行 Kubernetes</title><link>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</guid><description>
&lt;!--
---
title: 'Running Kubernetes locally on Linux with Microk8s'
date: 2019-11-26
---
-->
&lt;!--
**Authors**: [Ihor Dvoretskyi](https://twitter.com/idvoretskyi), Developer Advocate, Cloud Native Computing Foundation; [Carmine Rimi](https://twitter.com/carminerimi)
-->
&lt;p>&lt;strong>作者&lt;/strong>: &lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>，开发支持者，云原生计算基金会；&lt;a href="https://twitter.com/carminerimi">Carmine Rimi&lt;/a>&lt;/p>
&lt;!--
This article, the second in a [series](/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/) about local deployment options on Linux, and covers [MicroK8s](https://microk8s.io/). Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.
-->
&lt;p>本文是关于 Linux 上的本地部署选项&lt;a href="https://twitter.com/idvoretskyi">系列&lt;/a>的第二篇，涵盖了 &lt;a href="https://microk8s.io/">MicroK8s&lt;/a>。Microk8s 是本地部署 Kubernetes 集群的 'click-and-run' 方案，最初由 Ubuntu 的发布者 Canonical 开发。&lt;/p>
&lt;!--
While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesn’t require a VM. It uses [snap](https://snapcraft.io/) packages, an application packaging and isolation technology.
-->
&lt;p>虽然 Minikube 通常为 Kubernetes 集群创建一个本地虚拟机（VM），但是 MicroK8s 不需要 VM。它使用&lt;a href="https://snapcraft.io/">snap&lt;/a> 包，这是一种应用程序打包和隔离技术。&lt;/p>
&lt;!--
This difference has its pros and cons. Here we’ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions [that support snaps](https://snapcraft.io/docs/installing-snapd). Most popular Linux distributions are supported.
-->
&lt;p>这种差异有其优点和缺点。在这里，我们将讨论一些有趣的区别，并且基于 VM 的方法和非 VM 方法的好处。第一个因素是跨平台的移植性。虽然 Minikube VM 可以跨操作系统移植——它不仅支持 Linux，还支持 Windows、macOS、甚至 FreeBSD，但 Microk8s 需要 Linux，而且只在&lt;a href="https://snapcraft.io/docs/installing-snapd">那些支持 snaps&lt;/a> 的发行版上。支持大多数流行的 Linux 发行版。&lt;/p>
&lt;!--
Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean you’ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. You’ll consume more disk space when the VM is dormant. You’ll consume more RAM and CPU while it is running. Since Microk8s doesn’t require spinning up a virtual machine you’ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!
-->
&lt;p>另一个考虑到的因素是资源消耗。虽然 VM 设备为您提供了更好的可移植性，但它确实意味着您将消耗更多资源来运行 VM，这主要是因为 VM 提供了一个完整的操作系统，并且运行在管理程序之上。当 VM 处于休眠时你将消耗更多的磁盘空间。当它运行时，你将会消耗更多的 RAM 和 CPU。因为 Microk8s 不需要创建虚拟机，你将会有更多的资源去运行你的工作负载和其他设备。考虑到所占用的空间更小，MicroK8s 是物联网设备的理想选择-你甚至可以在 Paspberry Pi 和设备上使用它！&lt;/p>
&lt;!--
Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide [channels](https://snapcraft.io/docs/channels) that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.
-->
&lt;p>最后，项目似乎遵循了不同的发布节奏和策略。Microk8s 和 snaps 通常提供&lt;a href="https://snapcraft.io/docs/channels">渠道&lt;/a>允许你使用测试版和发布 KUbernetes 新版本的候选版本，同样也提供先前稳定版本。Microk8s 通常几乎立刻发布 Kubernetes 上游的稳定版本。&lt;/p>
&lt;!--
But wait, there’s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - there’s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. We’ll write more about this kind of architecture in a future article.
-->
&lt;p>但是等等，还有更多！Minikube 和 Microk8s 都是作为单节点集群启动的。本质上来说，它们允许你用单个工作节点创建 Kubernetes 集群。这种情况即将改变 - MicroK8s 早期的 alpha 版本包括集群。有了这个能力，你可以创建正如你希望多的工作节点的 KUbernetes 集群。对于创建集群来说，这是一个没有主见的选项 - 开发者在节点之间创建网络连接和集成了其他所需要的基础设施，比如一个外部的负载均衡。总的来说，MicroK8s 提供了一种快速简易的方法，使得少量的计算机和虚拟机变成一个多节点的 Kubernetes 集群。以后我们将撰写更多这种体系结构的文章。&lt;/p>
&lt;!--
## Disclaimer
This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it's official [webpage](https://microk8s.io/docs/), where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.
-->
&lt;h2 id="免责声明">免责声明&lt;/h2>
&lt;p>这不是 MicroK8s 官方介绍文档。你可以在它的官方&lt;a href="https://microk8s.io/docs/">网页&lt;/a>查询运行和使用 MicroK8s 的详情信息，其中覆盖了不同的用例，操作系统，环境等。相反，这篇文章的意图是提供在 Linux 上运行 MicroK8s 清晰易懂的指南。&lt;/p>
&lt;!--
## Prerequisites
A Linux distribution that [supports snaps](https://snapcraft.io/docs/installing-snapd), is required. In this guide, we’ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out [Multipass](https://multipass.run) to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.
-->
&lt;h2 id="前提条件">前提条件&lt;/h2>
&lt;p>一个&lt;a href="https://snapcraft.io/docs/installing-snapd">支持 snaps&lt;/a> 的 Linux 发行版是被需要的。这篇指南，我们将会用支持 snaps 且即开即用的 Ubuntu 18.04 LTS。如果你对运行在 Windows 或者 Mac 上的 MicroK8s 感兴趣，你应该检查&lt;a href="https://multipass.run">多通道&lt;/a>，安装一个快速的 Ubuntu VM，作为在你的系统上运行虚拟机 Ubuntu 的官方方式。&lt;/p>
&lt;!--
## MicroK8s installation
MicroK8s installation is straightforward:
-->
&lt;h2 id="microk8s-安装">MicroK8s 安装&lt;/h2>
&lt;p>简洁的 MicroK8s 安装：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap install microk8s --classic
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.
&lt;p>You may verify the MicroK8s status with the following command:
--&amp;gt;
以上的命令将会在几秒内安装一个本地单节点的 Kubernetes 集群。一旦命令执行结束，你的 Kubernetes 集群将会启动并运行。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo microk8s.status
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Using microk8s
Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a `kubectl` binary, which can be accessed by running the `microk8s.kubectl` command. As an example:
-->
&lt;h2 id="使用-microk8s">使用 microk8s&lt;/h2>
&lt;p>使用 MicrosK8s 就像和安装它一样便捷。MicroK8s 本身包括一个 &lt;code>kubectl&lt;/code> 库，该库可以通过执行 &lt;code>microk8s.kubectl&lt;/code> 命令去访问。例如：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">microk8s.kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
While using the prefix `microk8s.kubectl` allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the `snap alias` command:
-->
&lt;p>当使用前缀 &lt;code>microk8s.kubectl&lt;/code> 时，允许在没有影响的情况下并行地安装另一个系统级的 kubectl，你可以便捷地使用 &lt;code>snap alias&lt;/code> 命令摆脱它：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap &lt;span style="color:#a2f">alias&lt;/span> microk8s.kubectl kubectl
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
This will allow you to simply use `kubectl` after. You can revert this change using the `snap unalias` command.
-->
&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;p>这将允许你以后便捷地使用 &lt;code>kubectl&lt;/code>，你可以用 &lt;code>snap unalias&lt;/code>命令恢复这个改变。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## MicroK8s addons
One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.
The full list of extensions can be checked by running the `microk8s.status` command:
-->
&lt;h2 id="microk8s-插件">MicroK8s 插件&lt;/h2>
&lt;p>使用 MicroK8s 其中最大的好处之一事实上是也支持各种各样的插件和扩展。更重要的是它们是开箱即用的，用户仅仅需要启动它们。通过运行 &lt;code>microk8s.status&lt;/code> 命令检查出扩展的完整列表。&lt;/p>
&lt;pre>&lt;code>sudo microk8s.status
&lt;/code>&lt;/pre>&lt;!--
As of the time of writing this article, the following add-ons are supported:
-->
&lt;p>截至到写这篇文章为止，MicroK8s 已支持以下插件：&lt;/p>
&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
More add-ons are being created and contributed by the community all the time, it definitely helps to check often!
-->
&lt;p>社区创建和贡献了越来越多的插件，经常检查他们是十分有帮助的。&lt;/p>
&lt;!--
## Release channels
-->
&lt;h2 id="发布渠道">发布渠道&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap info microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Installing the sample application
In this tutorial we’ll use NGINX as a sample application ([the official Docker Hub image](https://hub.docker.com/_/nginx)).
It will be installed as a Kubernetes deployment:
-->
&lt;h2 id="安装简单的应用">安装简单的应用&lt;/h2>
&lt;p>在这篇指南中我将会用 NGINX 作为一个示例应用程序（&lt;a href="https://hub.docker.com/_/nginx">官方 Docker Hub 镜像&lt;/a>）。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create deployment nginx --image&lt;span style="color:#666">=&lt;/span>nginx
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
To verify the installation, let’s run the following:
-->
&lt;p>为了检查安装，让我们运行以下命令：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get deployments
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get pods
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
Also, we can retrieve the full output of all available objects within our Kubernetes cluster:
-->
&lt;p>我们也可以检索出 Kubernetes 集群中所有可用对象的完整输出。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Uninstalling MicroK8s
Uninstalling your microk8s cluster is so easy as uninstalling the snap:
-->
&lt;h2 id="卸载-mircrok8s">卸载 MircroK8s&lt;/h2>
&lt;p>卸载您的 microk8s 集群与卸载 Snap 同样便捷。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap remove microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Screencast
-->
&lt;h2 id="截屏视频">截屏视频&lt;/h2>
&lt;p>&lt;a href="https://asciinema.org/a/263394">&lt;img src="https://asciinema.org/a/263394.svg" alt="asciicast">&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes 文档最终用户调研</title><link>https://kubernetes.io/zh/blog/2019/10/29/kubernetes-documentation-end-user-survey/</link><pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/29/kubernetes-documentation-end-user-survey/</guid><description>
&lt;!--
---
layout: blog
title: "Kubernetes Documentation Survey"
date: 2019-10-29
slug: kubernetes-documentation-end-user-survey
---
-->
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://www.linkedin.com/in/aimee-ukasick/">Aimee Ukasick&lt;/a> and SIG Docs&lt;/p>
&lt;!--
In September, SIG Docs conducted its first survey about the [Kubernetes
documentation](https://kubernetes.io/docs/). We'd like to thank the CNCF's Kim
McMahon for helping us create the survey and access the results.
-->
&lt;p>9月，SIG Docs 进行了第一次关于 &lt;a href="https://kubernetes.io/docs/">Kubernetes 文档&lt;/a>的用户调研。我们要感谢 CNCF
的 Kim McMahon 帮助我们创建调查并获取结果。&lt;/p>
&lt;!--
# Key takeaways
-->
&lt;h1 id="主要收获">主要收获&lt;/h1>
&lt;!--
Respondents would like more example code, more detailed content, and more
diagrams in the Concepts, Tasks, and Reference sections.
-->
&lt;p>受访者希望能在概念、任务和参考部分得到更多示例代码、更详细的内容和更多图表。&lt;/p>
&lt;!--
74% of respondents would like the Tutorials section to contain advanced content.
-->
&lt;p>74% 的受访者希望教程部分包含高级内容。&lt;/p>
&lt;!--
69.70% said the Kubernetes documentation is the first place they look for
information about Kubernetes.
-->
&lt;p>69.70% 的受访者认为 Kubernetes 文档是他们首要寻找关于 Kubernetes 资料的地方。&lt;/p>
&lt;!--
# Survey methodology and respondents
-->
&lt;h1 id="调查方法和受访者">调查方法和受访者&lt;/h1>
&lt;!--
We conducted the survey in English. The survey was only available for 4 days due
to time constraints. We announced the survey on Kubernetes mailing lists, in
Kubernetes Slack channels, on Twitter, and in Kube Weekly. There were 23
questions, and respondents took an average of 4 minutes to complete the survey.
-->
&lt;p>我们用英语进行了调查。由于时间限制，调查的有效期只有 4 天。
我们在 Kubernetes 邮件列表、Kubernetes Slack 频道、Twitter、Kube Weekly 上发布了我们的调查问卷。
这份调查有 23 个问题， 受访者平均用 4 分钟完成这个调查。&lt;/p>
&lt;!--
## Quick facts about respondents:
-->
&lt;h2 id="关于受访者的简要情况">关于受访者的简要情况&lt;/h2>
&lt;!--
- 48.48% are experienced Kubernetes users, 26.26% expert, and 25.25% beginner
- 57.58% use Kubernetes in both administrator and developer roles
- 64.65% have been using the Kubernetes documentation for more than 12 months
- 95.96% read the documentation in English
-->
&lt;ul>
&lt;li>48.48% 是经验丰富的 Kubernetes 用户，26.26% 是专家，25.25% 是初学者&lt;/li>
&lt;li>57.58% 的人同时使用 Kubernetes 作为管理员和开发人员&lt;/li>
&lt;li>64.65% 的人使用 Kubernetes 文档超过 12 个月&lt;/li>
&lt;li>95.96% 的人阅读英文文档&lt;/li>
&lt;/ul>
&lt;!--
# Question and response highlights
-->
&lt;h1 id="问题和回答要点">问题和回答要点&lt;/h1>
&lt;!--
## Why people access the Kubernetes documentation
-->
&lt;h2 id="人们为什么访问-kubernetes-文档">人们为什么访问 Kubernetes 文档&lt;/h2>
&lt;!--
The majority of respondents stated that they access the documentation for the Concepts.
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png"
alt="Why respondents access the Kubernetes documentation"/>
&lt;/figure>
-->
&lt;p>大多数受访者表示，他们访问文档是为了了解概念。&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png"
alt="受访者为什么访问 Kubernetes 文档"/>
&lt;/figure>
&lt;!--
This deviates only slightly from what we see in Google Analytics: of the top 10
most viewed pages this year, #1 is the kubectl cheatsheet in the Reference section,
followed overwhelmingly by pages in the Concepts section.
-->
&lt;p>这与我们在 Google Analytics 上看到的略有不同：在今年浏览量最多的10个页面中，第一是在参考部分的 kubectl
的备忘单，其次是概念部分的页面。&lt;/p>
&lt;!--
## Satisfaction with the documentation
-->
&lt;h2 id="对文档的满意程度">对文档的满意程度&lt;/h2>
&lt;!--
We asked respondents to record their level of satisfaction with the detail in
the Concepts, Tasks, Reference, and Tutorials sections:
-->
&lt;p>我们要求受访者从概念、任务、参考和教程部分记录他们对细节的满意度：&lt;/p>
&lt;!--
- Concepts: 47.96% Moderately Satisfied
- Tasks: 50.54% Moderately Satisfied
- Reference: 40.86% Very Satisfied
- Tutorial: 47.25% Moderately Satisfied
-->
&lt;ul>
&lt;li>概念：47.96% 中等满意&lt;/li>
&lt;li>任务：50.54% 中等满意&lt;/li>
&lt;li>参考：40.86% 非常满意&lt;/li>
&lt;li>教程：47.25% 中等满意&lt;/li>
&lt;/ul>
&lt;!--
## How SIG Docs can improve each documentation section
-->
&lt;h2 id="sig-docs-如何改进文档的各个部分">SIG Docs 如何改进文档的各个部分&lt;/h2>
&lt;!--
We asked how we could improve each section, providing respondents with
selectable answers as well as a text field. The clear majority would like more
example code, more detailed content, more diagrams, and advanced tutorials:
-->
&lt;p>我们询问如何改进每个部分，为受访者提供可选答案以及文本输入框。绝大多数人想要更多
示例代码、更详细的内容、更多图表和更高级的教程：&lt;/p>
&lt;!--
```text
- Personally, would like to see more analogies to help further understanding.
- Would be great if corresponding sections of code were explained too
- Expand on the concepts to bring them together - they're a bucket of separate eels moving in different directions right now
- More diagrams, and more example code
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- 就个人而言，希望看到更多的类比，以帮助进一步理解。
- 如果代码的相应部分也能解释一下就好了
- 通过扩展概念把它们融合在一起 - 它们现在宛如在一桶水内朝各个方向游动的一条条鳗鱼。
- 更多的图表，更多的示例代码
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Respondents used the "Other" text box to record areas causing frustration:
-->
&lt;p>受访者使用“其他”文本框记录引发阻碍的区域：&lt;/p>
&lt;!--
```text
- Keep concepts up to date and accurate
- Keep task topics up to date and accurate. Human testing.
- Overhaul the examples. Many times the output of commands shown is not actual.
- I've never understood how to navigate or interpret the reference section
- Keep the tutorials up to date, or remove them
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- 使概念保持最新和准确
- 保持任务主题的最新性和准确性。亲身试验。
- 彻底检查示例。很多时候显示的命令输出不是实际情况。
- 我从来都不知道如何导航或解释参考部分
- 使教程保持最新，或将其删除
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## How SIG Docs can improve the documentation overall
-->
&lt;h2 id="sig-docs-如何全面改进文档">SIG Docs 如何全面改进文档&lt;/h2>
&lt;!--
We asked respondents how we can improve the Kubernetes documentation
overall. Some took the opportunity to tell us we are doing a good job:
-->
&lt;p>我们询问受访者如何从整体上改进 Kubernetes 文档。一些人抓住这次机会告诉我们我们正在做一个很棒的
工作：&lt;/p>
&lt;!--
```text
- For me, it is the best documented open source project.
- Keep going!
- I find the documentation to be excellent.
- You [are] doing a great job. For real.
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- 对我而言，这是我见过的文档最好的开源项目。
- 继续努力！
- 我觉得文档很好。
- 你们做得真好。真的。
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Other respondents provided feedback on the content:
-->
&lt;p>其它受访者提供关于内容的反馈：&lt;/p>
&lt;!--
```text
- ...But since we're talking about docs, more is always better. More
advanced configuration examples would be, to me, the way to go. Like a Use Case page for each configuration topic with beginner to advanced example scenarios. Something like that would be
awesome....
-->
&lt;!--
- More in-depth examples and use cases would be great. I often feel that the Kubernetes documentation scratches the surface of a topic, which might be great for new users, but it leaves more experienced users without much "official" guidance on how to implement certain things.
-->
&lt;!--
- More production like examples in the resource sections (notably secrets) or links to production like examples
-->
&lt;!--
- It would be great to see a very clear "Quick Start" A->Z up and running like many other tech projects. There are a handful of almost-quick-starts, but no single guidance. The result is information overkill.
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- ...但既然我们谈论的是文档，多多益善。更多的高级配置示例对我来说将是最好的选择。比如每个配置主题的用例页面，
从初学者到高级示例场景。像这样的东西真的是令人惊叹......
- 更深入的例子和用例将是很好的。我经常感觉 Kubernetes 文档只是触及了一个主题的表面，这可能对新用户很好，
但是它没有让更有经验的用户获取多少关于如何实现某些东西的“官方”指导。
- 资源节（特别是 secrets）希望有更多类似于产品的示例或指向类似产品的示例的链接
- 如果能像很多其它技术项目那样有非常清晰的“快速启动” 逐步教学完成搭建就更好了。现有的快速入门内容屈指可数，
也没有统一的指南。结果是信息泛滥。
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
A few respondents provided technical suggestions:
```text
- Make table columns sortable and filterable using a ReactJS or Angular component.
-->
&lt;!--
- For most, I think creating documentation with Hugo - a system for static site generation - is not appropriate. There are better systems for documenting large software project.
-->
&lt;!--
Specifically, I would like to see k8s switch to Sphinx for documentation. It has an excellent built-in search, it is easy tolearn if you know markdown, it is widely adopted by other projects (e.g. every software project in readthedocs.io, linux kernel, docs.python.org etc).
```
-->
&lt;p>少数受访者提供的技术建议：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- 使用 ReactJS 或者 Angular component 使表的列可排序和可筛选。
- 对于大多数人来说，我认为用 Hugo - 一个静态站点生成系统 - 创建文档是不合适的。有更好的系统来记录大型软件项目。
具体来说，我希望看到 k8s 切换到 Sphinx 来获取文档。Sphinx 有一个很好的内置搜索。如果你了解 markdown，学习起来也很容易。
Sphinx 被其他项目广泛采用（例如，在 readthedocs.io、linux kernel、docs.python.org 等等）。
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Overall, respondents provided constructive criticism focusing on the need for
advanced use cases as well as more in-depth examples, guides, and walkthroughs.
-->
&lt;p>总体而言，受访者提供了建设性的批评，其关注点是高级用例以及更深入的示例、指南和演练。&lt;/p>
&lt;!--
# Where to see more
-->
&lt;h1 id="哪里可以看到更多">哪里可以看到更多&lt;/h1>
&lt;!--
Survey results summary, charts, and raw data are available in `kubernetes/community` sig-docs [survey](https://github.com/kubernetes/community/tree/master/sig-docs/survey) directory.
-->
&lt;p>调查结果摘要、图表和原始数据可在 &lt;code>kubernetes/community&lt;/code> sig-docs
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs/survey">survey&lt;/a>
目录下。&lt;/p></description></item><item><title>Blog: 圣迭戈贡献者峰会日程公布！</title><link>https://kubernetes.io/zh/blog/2019/10/10/contributor-summit-san-diego-schedule/</link><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/10/contributor-summit-san-diego-schedule/</guid><description>
&lt;!--
layout: blog
title: "Contributor Summit San Diego Schedule Announced!"
date: 2019-10-10
slug: contributor-summit-san-diego-schedule
-->
&lt;!--
Authors: Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)
-->
&lt;p>作者：Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)&lt;/p>
&lt;!--
tl;dr A week ago we announced that [registration is open][reg] for the contributor
summit , and we're now live with [the full Contributor Summit schedule!][schedule]
Grab your spot while tickets are still available. There is currently a waitlist
for new contributor workshop. ([Register here!][reg])
-->
&lt;p>一周前，我们宣布贡献者峰会&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">开放注册&lt;/a>，现在我们已经完成了整个&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/program/schedule/">贡献者峰会的日程安排&lt;/a>！趁现在还有票，马上抢占你的位置。这里有一个新贡献者研讨会的等待名单。 (&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">点击这里注册!&lt;/a>)&lt;/p>
&lt;!--
There are many great sessions planned for the Contributor Summit, spread across
five rooms of current contributor content in addition to the new contributor
workshops. Since this is an upstream contributor summit and we don't often meet,
being a globally distributed team, most of these sessions are discussions or
hands-on labs, not just presentations. We want folks to learn and have a
good time meeting their OSS teammates.
-->
&lt;p>除了新贡献者研讨会之外，贡献者峰会还安排了许多精彩的会议，这些会议分布在当前五个贡献者内容的会议室中。由于这是一个上游贡献者峰会，并且我们不经常见面，所以作为一个全球分布的团队，这些会议大多是讨论或动手实践，而不仅仅是演示。我们希望大家互相学习，并于他们的开源代码队友玩的开心。&lt;/p>
&lt;!--
Unconference tracks are returning from last year with sessions to be chosen
Monday morning. These are ideal for the latest hot topics and specific
discussions that contributors want to have. In previous years, we've covered
flaky tests, cluster lifecycle, KEPs (Kubernetes Enhancement Proposals), mentoring,
security, and more.
-->
&lt;p>像去年一样，非组织会议将重新开始，会议将在周一上午进行选择。对于最新的热门话题和贡献者想要进行的特定讨论，这是理想的选择。在过去的几年中，我们涵盖了不稳定的测试，集群生命周期，KEP（Kubernetes增强建议），指导，安全性等等。&lt;/p>
&lt;!--
![Unconference](/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg)
-->
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg" alt="非组织会议">&lt;/p>
&lt;!--
While the schedule contains difficult decisions in every timeslot, we've picked
a few below to give you a taste of what you'll hear, see, and participate in, at
the summit:
-->
&lt;p>尽管在每个时间间隙日程安排都包含困难的决定，但我们选择了以下几点，让您体验一下您将在峰会上听到、看到和参与的内容：&lt;/p>
&lt;!--
* **[Vision]**: SIG-Architecture will be sharing their vision of where we're going
with Kubernetes development for the next year and beyond.
* **[Security]**: Tim Allclair and CJ Cullen will present on the current state of
Kubernetes security. In another security talk, Vallery Lancey will lead a
discussion about making our platform secure by default.
* **[Prow]**: Interested in working with Prow and contributing to Test-Infra, but
not sure where to start? Rob Keilty will help you get a Prow test environment
running on your laptop.
* **[Git]**: Staff from GitHub will be collaborating with Christoph Blecker to share
practical Git tips for Kubernetes contributors.
* **[Reviewing]**: Tim Hockin will share the secrets of becoming a great code
reviewer, and Jordan Liggitt will conduct a live API review so that you can do
one, or at least pass one.
* **[End Users]**: Several end users from the CNCF partner ecosystem, invited by
Cheryl Hung, will hold a Q&amp;A with contributors to strengthen our feedback loop.
* **[Docs]**: As always, SIG-Docs will run a three-hour contributing-to-documentation
workshop.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMc">预见&lt;/a>&lt;/strong>: SIG组织将分享他们对于明年和以后Kubernetes开发发展方向的认识。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMj">安全&lt;/a>&lt;/strong>: Tim Allclair和CJ Cullen将介绍Kubernetes安全的当前情况。在另一个安全性演讲中，Vallery Lancey将主持有关使我们的平台默认情况下安全的讨论。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vv6Z">Prow&lt;/a>&lt;/strong>: 有兴趣与Prow合作并为Test-Infra做贡献，但不确定从哪里开始？ Rob Keilty将帮助您在笔记本电脑上运行Prow测试环境&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNa">Git&lt;/a>&lt;/strong>: GitHub的员工将与Christoph Blecker合作，为Kubernetes贡献者分享实用的Git技巧。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VutA">审阅&lt;/a>&lt;/strong>: 蒂姆·霍金（Tim Hockin）将分享成为一名出色的代码审阅者的秘密，而乔丹·利吉特（Jordan Liggitt）将进行实时API审阅，以便您可以进行一次或至少了解一次审阅。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNJ">终端用户&lt;/a>&lt;/strong>: 应Cheryl Hung邀请，来自CNCF合作伙伴生态的数个终端用户，将回答贡献者的问题，以加强我们的反馈循环。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vux2">文档&lt;/a>&lt;/strong>: 与往常一样，SIG-Docs将举办一个为时三个小时的文档撰写研讨会。&lt;/li>
&lt;/ul>
&lt;!--
We're also giving out awards to contributors who distinguished themselves in 2019,
and there will be a huge Meet &amp; Greet for new contributors to find their SIG
(and for existing contributors to ask about their PRs) at the end of the day on
Monday.
-->
&lt;p>我们还将向在2019年杰出的贡献者颁发奖项，周一星期一结束时将有一个巨大的见面会，供新的贡献者找到他们的SIG（以及现有的贡献者询问他们的PR）。&lt;/p>
&lt;!--
Hope to see you all there, and [make sure you register!][reg]
-->
&lt;p>希望能够在峰会上见到您，并且确保您已经提前&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">注册&lt;/a>！&lt;/p>
&lt;!--
[San Diego team][team]
-->
&lt;p>&lt;a href="http://git.k8s.io/community/events/events-team">圣迭戈团队&lt;/a>&lt;/p></description></item><item><title>Blog: 2019 指导委员会选举结果</title><link>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</guid><description>
&lt;!--
---
layout: blog
title: "2019 Steering Committee Election Results"
date: 2019-10-03
slug: 2019-steering-committee-election-results
---
-->
&lt;!--
**Authors**: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)&lt;/p>
&lt;!--
The [2019 Steering Committee Election] is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.
-->
&lt;p>&lt;a href="https://git.k8s.io/community/events/elections/2021">2019 指导委员会选举&lt;/a> 是 Kubernetes 项目的重要里程碑。最初的自助委员会正逐步退休，现在该委员会已缩减到最后分配的 7 个席位。指导委员会的所有成员现在都由 Kubernetes 社区选举产生。&lt;/p>
&lt;!--
Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.
-->
&lt;p>接下来的选举将选出 3 到 4 名委员，任期两年。&lt;/p>
&lt;!--
## **Results**
The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):
-->
&lt;h2 id="选举结果">选举结果&lt;/h2>
&lt;p>Kubernetes 指导委员会选举现已完成，以下候选人提前获得立即开始的两年任期 (按 GitHub handle 的字母顺序排列) ：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>), Loodse&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>)&lt;/strong>, &lt;strong>Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
They join Aaron Crickenberger ([@spiffxp]), Google; Davanum Srinivas ([@dims]),
VMware; and Timothy St. Clair ([@timothysc]), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.
-->
&lt;p>他们加入了 Aaron Crickenberger (&lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>)， Google；Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>)，VMware; and Timothy St. Clair (&lt;a href="https://github.com/timothysc">@timothysc&lt;/a>), VMware，使得委员会更圆满。Aaron、Davanum 和 Timothy 占据的这些席位将会在明年的这个时候进行选举。&lt;/p>
&lt;!--
## Big Thanks!
* Thanks to the initial bootstrap committee for establishing the initial
project governance and overseeing a multi-year transition period:
-->
&lt;h2 id="诚挚的感谢">诚挚的感谢！&lt;/h2>
&lt;ul>
&lt;li>感谢最初的引导委员会创立了最初项目的管理并监督了多年的过渡期：
&lt;ul>
&lt;li>Joe Beda (&lt;a href="https://github.com/jbeda">@jbeda&lt;/a>), VMware&lt;/li>
&lt;li>Brendan Burns (&lt;a href="https://github.com/brendandburns">@brendandburns&lt;/a>), Microsoft&lt;/li>
&lt;li>Clayton Coleman (&lt;a href="https://github.com/smarterclayton">@smarterclayton&lt;/a>), Red Hat&lt;/li>
&lt;li>Brian Grant (&lt;a href="https://github.com/bgrant0607">@bgrant0607&lt;/a>), Google&lt;/li>
&lt;li>Tim Hockin (&lt;a href="https://github.com/thockin">@thockin&lt;/a>), Google&lt;/li>
&lt;li>Sarah Novotny (&lt;a href="https://github.com/sarahnovotny">@sarahnovotny&lt;/a>), Microsoft&lt;/li>
&lt;li>Brandon Philips (&lt;a href="https://github.com/philips">@philips&lt;/a>), Red Hat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:
-->
&lt;ul>
&lt;li>同样感谢其他的已退休指导委员会成员。社区对你们先前的服务表示赞赏：
&lt;ul>
&lt;li>Quinton Hoole (&lt;a href="https://github.com/quinton-hoole">@quinton-hoole&lt;/a>), Huawei&lt;/li>
&lt;li>Michelle Noorali (&lt;a href="https://github.com/michelleN">@michelleN&lt;/a>), Microsoft&lt;/li>
&lt;li>Phillip Wittrock (&lt;a href="https://github.com/pwittrock">@pwittrock&lt;/a>), Google&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.
* Thanks to all 377 voters who cast a ballot.
* And last but not least…Thanks to Cornell University for hosting [CIVS]!
-->
&lt;ul>
&lt;li>感谢参选的候选人。 愿在每次选举中，我们都能拥有一群像您一样推动社区向前发展的人。&lt;/li>
&lt;li>感谢所有投票的377位选民。&lt;/li>
&lt;li>最后，感谢康奈尔大学举办的 &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>!&lt;/li>
&lt;/ul>
&lt;!--
## Get Involved with the Steering Committee
You can follow along with Steering Committee [backlog items] and weigh in by
filing an issue or creating a PR against their [repo]. They meet bi-weekly on
[Wednesdays at 8pm UTC] and regularly attend Meet Our Contributors. They can
also be contacted at their public mailing list [steering@kubernetes.io].
Steering Committee Meetings:
-->
&lt;h2 id="参与指导委员会">参与指导委员会&lt;/h2>
&lt;p>你可以跟进指导委员会的 &lt;a href="https://github.com/kubernetes/steering/projects/1">代办事项&lt;/a>，通过提出问题或者向 &lt;a href="https://github.com/kubernetes/steering">仓库&lt;/a> 提交一个 pr 。他们每两周一次，在 &lt;a href="https://github.com/kubernetes/steering">UTC 时间周三晚上 8 点&lt;/a> 会面，并定期与我们的贡献者见面。也可以通过他们的公共邮件列表 &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a> 联系他们。&lt;/p>
&lt;p>指导委员会会议：&lt;/p>
&lt;!--
* [YouTube Playlist]
-->
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube 播放列表&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: San Diego 贡献者峰会开放注册！</title><link>https://kubernetes.io/zh/blog/2019/09/24/san-diego-contributor-summit/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/09/24/san-diego-contributor-summit/</guid><description>
&lt;!--
---
layout: blog
title: "Contributor Summit San Diego Registration Open!"
date: 2019-09-24
slug: san-diego-contributor-summit
---
--->
&lt;!--
**Authors: Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)**
--->
&lt;p>&lt;strong>作者：Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)&lt;/strong>&lt;/p>
&lt;!--
[Contributor Summit San Diego 2019 Event Page]
Registration is now open and in record time, we’ve hit capacity for the
*new contributor workshop* session of the event! Waitlist is now available.
--->
&lt;p>&lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">2019 San Diego 贡献者峰会活动页面&lt;/a>
注册已经开放，并且在创纪录的时间内，&lt;em>新贡献者研讨会&lt;/em> 活动已满员！候补名单已经开放。&lt;/p>
&lt;!--
**Sunday, November 17**
Evening Contributor Celebration:
[QuartYard]*
Address: 1301 Market Street, San Diego, CA 92101
Time: 6:00PM - 9:00PM
--->
&lt;p>&lt;strong>11月17日，星期日&lt;/strong>&lt;br>
晚间贡献者庆典：&lt;br>
&lt;a href="https://quartyardsd.com/">QuartYard&lt;/a>*&lt;br>
地址: 1301 Market Street, San Diego, CA 92101&lt;br>
时间: 下午6:00 - 下午9:00&lt;/p>
&lt;!--
**Monday, November 18**
All Day Contributor Summit:
[Marriott Marquis San Diego Marina]
Address: 333 W Harbor Dr, San Diego, CA 92101
Time: 9:00AM - 5:00PM
--->
&lt;p>&lt;strong>11月18日，星期一&lt;/strong>&lt;br>
全天贡献者峰会：&lt;br>
&lt;a href="https://www.marriott.com/hotels/travel/sandt-marriott-marquis-san-diego-marina/?scid=bb1a189a-fec3-4d19-a255-54ba596febe2">Marriott Marquis San Diego Marina&lt;/a>&lt;br>
地址: 333 W Harbor Dr, San Diego, CA 92101&lt;br>
时间: 上午9:00 - 下午5:00&lt;/p>
&lt;!--
While the Kubernetes project is only five years old, we’re already going into our
9th Contributor Summit this November in San Diego before KubeCon + CloudNativeCon.
The rapid increase is thanks to adding European and Asian Contributor Summits to
the North American events we’ve done previously. We will continue to run Contributor
Summits across the globe, as it is important that our contributor base grows in
all forms of diversity.
--->
&lt;p>虽然 Kubernetes 项目只有五年的历史，但是在 KubeCon + CloudNativeCon 之前，今年11月在圣迭戈时我们举办的第九届贡献者峰会了。快速增长的原因是在我们之前所做的北美活动中增加了欧洲和亚洲贡献者峰会。我们将继续在全球举办贡献者峰会，因为重要的是，我们的贡献者要以各种形式的多样性地成长。&lt;/p>
&lt;!--
Kubernetes has a large distributed remote contributing team, from [individuals and
organizations] all over the world. The Contributor Summits give the community three
chances a year to get together, work on community topics, and have hallway track
time. The upcoming San Diego summit is expected to bring over 450 attendees, and
will contain multiple tracks with something for everyone. The focus will be around
contributor growth and sustainability. We're going to stop here with capacity for
future summits; we want this event to offer value to individuals and the project.
We've heard from past summit attendee feedback that getting work done, learning,
and meeting folks face to face is a priority. By capping attendance and offering
the contributor gatherings in more locations, it will help us achieve those goals.
--->
&lt;p>Kubernetes 拥有一个庞大的分布式远程贡献团队，由来自世界各地的 &lt;a href="https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&amp;amp;var-period=y&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&amp;amp;var-companies=All">个人和组织&lt;/a> 组成。贡献者峰会每年为社区提供三次聚会的机会，围绕社区主题开展工作，并有互相了解的时间。即将举行的 San Diego 峰会预计将吸引 450 多名与会者，并将包含多个方向，适合所有人。重点将围绕贡献者的增长和可持续性。我们将在这里停留，为举行未来峰会做准备；我们希望这次活动为个人和项目提供价值。我们已经从峰会与会者的反馈中得知，完成工作、学习和与人们面对面交流是当务之急。通过限制参加人数并在更多地方提供贡献者聚会，将有助于我们实现这些目标。&lt;/p>
&lt;!--
This summit is unique as we’ve taken big moves on sustaining ourselves, the
contributor experience events team. Taking a page from the release team’s playbook,
we have added additional core team and shadow roles making it a natural mentoring
(watching+doing) relationship. The shadows are expected to fill another role at
one of the three events in 2020, and core team members to take the lead.
In preparation for this team, we’ve open sourced our [rolebooks, guidelines,
best practices] and opened up our [meetings] and [project board]. Our team makes up
many parts of the Kubernetes project and takes care of making sure all voices
are represented.
--->
&lt;p>这次峰会是独一无二的，因为我们在贡献者体验活动团队的坚持自我方面已采取了重大举措。从发行团队的手册中摘录，我们添加了其他核心团队和跟随学习角色，使其成为天然的辅导关系（包括监督和实施）。预计跟随学习者将在 2020 年的三项活动中扮演另一个角色，核心团队成员将带头完成。为这个团队做准备，我们开源了 &lt;a href="https://github.com/kubernetes/community/tree/master/events/events-team">技术手册，指南，最佳做法&lt;/a>，并开放了 &lt;a href="https://docs.google.com/document/d/1oLXv5_rM4f645jlXym_Vd7AUq7x6DV-O87E6tcW1sjU/edit?usp=sharing">会议&lt;/a> 和 &lt;a href="https://github.com/orgs/kubernetes/projects/21">项目委员会&lt;/a>。我们的团队组成了 Kubernetes 项目的许多部分，并确保所有声音都得到体现。&lt;/p>
&lt;!--
Are you at KubeCon + CloudNativeCon but can’t make it to the summit? Check out
the [SIG Intro and Deep Dive sessions] during KubeCon + CloudNativeCon to
participate in Q&amp;A and hear what’s up with each Special interest Group (SIG).
We’ll also record all of Contributor Summit’s presentation sessions, take notes
in discussions, and share it back with you, after the event is complete.
--->
&lt;p>您是否已经在 KubeCon + CloudNativeCon 上，但无法参与会议？ 在 KubeCon + CloudNativeCon 期间查看 &lt;a href="https://kccncna19.sched.com/overview/type/Maintainer+Track+Sessions?iframe=yes">SIG入门和深潜课程&lt;/a> 参与问答，并听取每个特殊兴趣小组（SIG）的最新消息。活动结束后，我们还将记录所有贡献者峰会的课题，在讨论中做笔记，并与您分享。&lt;/p>
&lt;!--
We hope to see you all at Kubernetes Contributor Summit San Diego, make sure you
head over and [register right now]! This event will sell out - here’s your warning.
:smiley:
--->
&lt;p>我们希望能在 San Diego 的 Kubernetes 贡献者峰会上与大家见面，确保您直接进入并点击 &lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">立即注册&lt;/a>！ 此活动将关闭 - 特此提醒。 ：笑脸：&lt;/p>
&lt;!--
Check out past blogs on [persona building around our events] and the [Barcelona summit story].
![Group Picture in 2018](/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG)
--->
&lt;p>查看往期博客有关 &lt;a href="https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/">围绕我们的活动构建角色&lt;/a> 和 &lt;a href="https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/">巴塞罗那峰会故事&lt;/a>。&lt;/p>
&lt;p>！&lt;a href="https://kubernetes.io/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG">2018年集体照&lt;/a>&lt;/p>
&lt;!--
*=QuartYard has a huge stage! Want to perform something in front of your contributor peers? Reach out to us! community@kubernetes.io
--->
&lt;p>*=QuartYard 有一个巨大的舞台！想要在您的贡献者同行面前做点什么？加入我们吧！ &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a>&lt;/p></description></item><item><title>Blog: OPA Gatekeeper：Kubernetes 的策略和管理</title><link>https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</guid><description>
&lt;!--
---
layout: blog
title: "OPA Gatekeeper: Policy and Governance for Kubernetes"
date: 2019-08-06
slug: OPA-Gatekeeper-Policy-and-Governance-for-Kubernetes
---
--->
&lt;!--
**Authors:** Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)
--->
&lt;p>&lt;strong>作者：&lt;/strong> Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)&lt;/p>
&lt;!--
The [Open Policy Agent Gatekeeper](https://github.com/open-policy-agent/gatekeeper) project can be leveraged to help enforce policies and strengthen governance in your Kubernetes environment. In this post, we will walk through the goals, history, and current state of the project.
--->
&lt;p>可以从项目 &lt;a href="https://github.com/open-policy-agent/gatekeeper">Open Policy Agent Gatekeeper&lt;/a> 中获得帮助，在 Kubernetes 环境下实施策略并加强治理。在本文中，我们将逐步介绍该项目的目标，历史和当前状态。&lt;/p>
&lt;!--
The following recordings from the Kubecon EU 2019 sessions are a great starting place in working with Gatekeeper:
* [Intro: Open Policy Agent Gatekeeper](https://youtu.be/Yup1FUc2Qn0)
* [Deep Dive: Open Policy Agent](https://youtu.be/n94_FNhuzy4)
--->
&lt;p>以下是 Kubecon EU 2019 会议的录音，帮助我们更好地开展与 Gatekeeper 合作：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/Yup1FUc2Qn0">简介：开放策略代理 Gatekeeper&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/n94_FNhuzy4">深入研究：开放策略代理&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Motivations
If your organization has been operating Kubernetes, you probably have been looking for ways to control what end-users can do on the cluster and ways to ensure that clusters are in compliance with company policies. These policies may be there to meet governance and legal requirements or to enforce best practices and organizational conventions. With Kubernetes, how do you ensure compliance without sacrificing development agility and operational independence?
--->
&lt;h2 id="出发点">出发点&lt;/h2>
&lt;p>如果您所在的组织一直在使用 Kubernetes，您可能一直在寻找如何控制终端用户在集群上的行为，以及如何确保集群符合公司政策。这些策略可能需要满足管理和法律要求，或者符合最佳执行方法和组织惯例。使用 Kubernetes，如何在不牺牲开发敏捷性和运营独立性的前提下确保合规性？&lt;/p>
&lt;!--
For example, you can enforce policies like:
* All images must be from approved repositories
* All ingress hostnames must be globally unique
* All pods must have resource limits
* All namespaces must have a label that lists a point-of-contact
--->
&lt;p>例如，您可以执行以下策略：&lt;/p>
&lt;ul>
&lt;li>所有镜像必须来自获得批准的存储库&lt;/li>
&lt;li>所有入口主机名必须是全局唯一的&lt;/li>
&lt;li>所有 Pod 必须有资源限制&lt;/li>
&lt;li>所有命名空间都必须具有列出联系的标签&lt;/li>
&lt;/ul>
&lt;!--
Kubernetes allows decoupling policy decisions from the API server by means of [admission controller webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) to intercept admission requests before they are persisted as objects in Kubernetes. [Gatekeeper](https://github.com/open-policy-agent/gatekeeper) was created to enable users to customize admission control via configuration, not code and to bring awareness of the cluster’s state, not just the single object under evaluation at admission time. Gatekeeper is a customizable admission webhook for Kubernetes that enforces policies executed by the [Open Policy Agent (OPA)](https://www.openpolicyagent.org), a policy engine for Cloud Native environments hosted by CNCF.
--->
&lt;p>在接收请求被持久化为 Kubernetes 中的对象之前，Kubernetes 允许通过 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission controller webhooks&lt;/a> 将策略决策与 API 服务器分离，从而拦截这些请求。&lt;a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper&lt;/a> 创建的目的是使用户能够通过配置（而不是代码）自定义控制许可，并使用户了解群集的状态，而不仅仅是针对评估状态的单个对象，在这些对象准许加入的时候。Gatekeeper 是 Kubernetes 的一个可定制的许可 webhook ，它由 &lt;a href="https://www.openpolicyagent.org">Open Policy Agent (OPA)&lt;/a> 强制执行， OPA 是 Cloud Native 环境下的策略引擎，由 CNCF 主办。&lt;/p>
&lt;!--
## Evolution
Before we dive into the current state of Gatekeeper, let’s take a look at how the Gatekeeper project has evolved.
--->
&lt;h2 id="发展">发展&lt;/h2>
&lt;p>在深入了解 Gatekeeper 的当前情况之前，让我们看一下 Gatekeeper 项目是如何发展的。&lt;/p>
&lt;!--
* Gatekeeper v1.0 - Uses OPA as the admission controller with the kube-mgmt sidecar enforcing configmap-based policies. It provides validating and mutating admission control. Donated by Styra.
* Gatekeeper v2.0 - Uses Kubernetes policy controller as the admission controller with OPA and kube-mgmt sidecars enforcing configmap-based policies. It provides validating and mutating admission control and audit functionality. Donated by Microsoft.
* Gatekeeper v3.0 - The admission controller is integrated with the [OPA Constraint Framework](https://github.com/open-policy-agent/frameworks/tree/master/constraint) to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable. Built with kubebuilder, it provides validating and, eventually, mutating (to be implemented) admission control and audit functionality. This enables the creation of policy templates for [Rego](https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/) policies, creation of policies as CRDs, and storage of audit results on policy CRDs. This project is a collaboration between Google, Microsoft, Red Hat, and Styra.
--->
&lt;ul>
&lt;li>Gatekeeper v1.0 - 使用 OPA 作为带有 kube-mgmt sidecar 的许可控制器，用来强制执行基于 configmap 的策略。这种方法实现了验证和转换许可控制。贡献方：Styra&lt;/li>
&lt;li>Gatekeeper v2.0 - 使用 Kubernetes 策略控制器作为许可控制器，OPA 和 kube-mgmt sidecar 实施基于 configmap 的策略。这种方法实现了验证和转换准入控制和审核功能。贡献方：Microsoft&lt;/li>
&lt;li>Gatekeeper v3.0 - 准入控制器与 &lt;a href="https://github.com/open-policy-agent/frameworks/tree/master/constraint">OPA Constraint Framework&lt;/a> 集成在一起，用来实施基于 CRD 的策略，并可以可靠地共享已完成声明配置的策略。使用 kubebuilder 进行构建，实现了验证以及最终转换（待完成）为许可控制和审核功能。这样就可以为 &lt;a href="https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/">Rego&lt;/a> 策略创建策略模板，将策略创建为 CRD 并存储审核结果到策略 CRD 上。该项目是 Google，Microsoft，Red Hat 和 Styra 合作完成的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-08-06-opa-gatekeeper/v3.png" alt="">&lt;/p>
&lt;!--
## Gatekeeper v3.0 Features
Now let’s take a closer look at the current state of Gatekeeper and how you can leverage all the latest features. Consider an organization that wants to ensure all objects in a cluster have departmental information provided as part of the object’s labels. How can you do this with Gatekeeper?
--->
&lt;h2 id="gatekeeper-v3-0-的功能">Gatekeeper v3.0 的功能&lt;/h2>
&lt;p>现在我们详细看一下 Gatekeeper 当前的状态，以及如何利用所有最新的功能。假设一个组织希望确保集群中的所有对象都有 department 信息，这些信息是对象标签的一部分。如何利用 Gatekeeper 完成这项需求？&lt;/p>
&lt;!--
### Validating Admission Control
Once all the Gatekeeper components have been [installed](https://github.com/open-policy-agent/gatekeeper) in your cluster, the API server will trigger the Gatekeeper admission webhook to process the admission request whenever a resource in the cluster is created, updated, or deleted.
During the validation process, Gatekeeper acts as a bridge between the API server and OPA. The API server will enforce all policies executed by OPA.
--->
&lt;h3 id="验证许可控制">验证许可控制&lt;/h3>
&lt;p>在集群中所有 Gatekeeper 组件都 &lt;a href="https://github.com/open-policy-agent/gatekeeper">安装&lt;/a> 完成之后，只要集群中的资源进行创建、更新或删除，API 服务器将触发 Gatekeeper 准入 webhook 来处理准入请求。&lt;/p>
&lt;p>在验证过程中，Gatekeeper 充当 API 服务器和 OPA 之间的桥梁。API 服务器将强制实施 OPA 执行的所有策略。&lt;/p>
&lt;!--
### Policies and Constraints
With the integration of the OPA Constraint Framework, a Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.
--->
&lt;h3 id="策略与-constraint">策略与 Constraint&lt;/h3>
&lt;p>结合 OPA Constraint Framework，Constraint 是一个声明，表示作者希望系统满足给定的一系列要求。Constraint 都使用 Rego 编写，Rego 是声明性查询语言，OPA 用 Rego 来枚举违背系统预期状态的数据实例。所有 Constraint 都遵循逻辑 AND。假使有一个 Constraint 不满足，那么整个请求都将被拒绝。&lt;/p>
&lt;!--
Before defining a Constraint, you need to create a Constraint Template that allows people to declare new Constraints. Each template describes both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint, much like arguments to a function.
For example, here is a Constraint template CRD that requires certain labels to be present on an arbitrary object.
--->
&lt;p>在定义 Constraint 之前，您需要创建一个 Constraint Template，允许大家声明新的 Constraint。每个模板都描述了强制执行 Constraint 的 Rego 逻辑和 Constraint 的模式，其中包括 CRD 的模式和传递到 enforces 中的参数，就像函数的参数一样。&lt;/p>
&lt;p>例如，以下是一个 Constraint 模板 CRD，它的请求是在任意对象上显示某些标签。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>templates.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ConstraintTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">crd&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">names&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">listKind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabelsList&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">plural&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">singular&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">validation&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Schema for the `parameters` field&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>array&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">items&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targets&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">target&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admission.k8s.gatekeeper.sh&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rego&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>|&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;span style="color:#b44;font-style:italic"> package k8srequiredlabels
&lt;/span>&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;span style="color:#b44;font-style:italic"> deny[{&amp;#34;msg&amp;#34;: msg, &amp;#34;details&amp;#34;: {&amp;#34;missing_labels&amp;#34;: missing}}] {
&lt;/span>&lt;span style="color:#b44;font-style:italic"> provided := {label | input.review.object.metadata.labels[label]}
&lt;/span>&lt;span style="color:#b44;font-style:italic"> required := {label | label := input.parameters.labels[_]}
&lt;/span>&lt;span style="color:#b44;font-style:italic"> missing := required - provided
&lt;/span>&lt;span style="color:#b44;font-style:italic"> count(missing) &amp;gt; 0
&lt;/span>&lt;span style="color:#b44;font-style:italic"> msg := sprintf(&amp;#34;you must provide labels: %v&amp;#34;, [missing])
&lt;/span>&lt;span style="color:#b44;font-style:italic"> }&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Once a Constraint template has been deployed in the cluster, an admin can now create individual Constraint CRDs as defined by the Constraint template. For example, here is a Constraint CRD that requires the label `hr` to be present on all namespaces.
--->
&lt;p>在集群中部署了 Constraint 模板后，管理员现在可以创建由 Constraint 模板定义的单个 Constraint CRD。例如，这里以下是一个 Constraint CRD，要求标签 &lt;code>hr&lt;/code> 出现在所有命名空间上。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-hr&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Similarly, another Constraint CRD that requires the label `finance` to be present on all namespaces can easily be created from the same Constraint template.
--->
&lt;p>类似地，可以从同一个 Constraint 模板轻松地创建另一个 Constraint CRD，该 Constraint CRD 要求所有命名空间上都有 &lt;code>finance&lt;/code> 标签。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-finance&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;finance&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
As you can see, with the Constraint framework, we can reliably share Regos via the Constraint templates, define the scope of enforcement with the match field, and provide user-defined parameters to the Constraints to create customized behavior for each Constraint.
--->
&lt;p>如您所见，使用 Constraint framework，我们可以通过 Constraint 模板可靠地共享 rego，使用匹配字段定义执行范围，并为 Constraint 提供用户定义的参数，从而为每个 Constraint 创建自定义行为。&lt;/p>
&lt;!--
### Audit
The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as `violations` listed in the `status` field of the relevant Constraint. --->
&lt;h3 id="审核">审核&lt;/h3>
&lt;p>根据群集中强制执行的 Constraint，审核功能可定期评估复制的资源，并检测先前存在的错误配置。Gatekeeper 将审核结果存储为 &lt;code>violations&lt;/code>，在相关 Constraint 的 &lt;code>status&lt;/code> 字段中列出。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-hr&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">auditTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2019-08-06T01:46:13Z&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">byPod&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">enforced&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">id&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gatekeeper-controller-manager-0&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">violations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">enforcementAction&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>deny&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Namespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">message: &amp;#39;you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: default
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: gatekeeper-system
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: kube-public
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&amp;#39;&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Data Replication
Audit requires replication of Kubernetes resources into OPA before they can be evaluated against the enforced Constraints. Data replication is also required by Constraints that need access to objects in the cluster other than the object under evaluation. For example, a Constraint that enforces uniqueness of ingress hostname must have access to all other ingresses in the cluster.
--->
&lt;h3 id="数据复制">数据复制&lt;/h3>
&lt;p>审核要求将 Kubernetes 复制到 OPA 中，然后才能根据强制的 Constraint 对其进行评估。数据复制同样也需要 Constraint，这些 Constraint 需要访问集群中除评估对象之外的对象。例如，一个 Constraint 要强制确定入口主机名的唯一性，就必须有权访问集群中的所有其他入口。&lt;/p>
&lt;!--
To configure Kubernetes data to be replicated, create a sync config resource with the resources to be replicated into OPA. For example, the below configuration replicates all namespace and pod resources to OPA.
--->
&lt;p>对 Kubernetes 数据进行复制，请使用复制到 OPA 中的资源创建 sync config 资源。例如，下面的配置将所有命名空间和 Pod 资源复制到 OPA。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>config.gatekeeper.sh/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;gatekeeper-system&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">sync&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">syncOnly&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Pod&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## Planned for Future
The community behind the Gatekeeper project will be focusing on providing mutating admission control to support mutation scenarios (for example: annotate objects automatically with departmental information when creating a new resource), support external data to inject context external to the cluster into the admission decisions, support dry run to see impact of a policy on existing resources in the cluster before enforcing it, and more audit functionalities.
--->
&lt;h2 id="未来计划">未来计划&lt;/h2>
&lt;p>Gatekeeper 项目背后的社区将专注于提供转换许可控制，可以用来支持转换方案（例如：在创建新资源时使用 department 信息自动注释对象），支持外部数据以将集群外部环境加入到许可决策中，支持试运行以便在执行策略之前了解策略对集群中现有资源的影响，还有更多的审核功能。&lt;/p>
&lt;!--
If you are interested in learning more about the project, check out the [Gatekeeper](https://github.com/open-policy-agent/gatekeeper) repo. If you are interested in helping define the direction of Gatekeeper, join the [#kubernetes-policy](https://openpolicyagent.slack.com/messages/CDTN970AX) channel on OPA Slack, and join our [weekly meetings](https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit) to discuss development, issues, use cases, etc.
--->
&lt;p>如果您有兴趣了解更多有关该项目的信息，请查看 &lt;a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper&lt;/a> 存储库。如果您有兴趣帮助确定 Gatekeeper 的方向，请加入 &lt;a href="https://openpolicyagent.slack.com/messages/CDTN970AX">#kubernetes-policy&lt;/a> OPA Slack 频道，并加入我们的 &lt;a href="https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit">周会&lt;/a> 一同讨论开发、任务、用例等。&lt;/p></description></item><item><title>Blog: 欢迎参加在上海举行的贡献者峰会</title><link>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid><description>
&lt;!-- ---
layout: blog
title: 'Join us at the Contributor Summit in Shanghai'
date: 2019-06-11
--- -->
&lt;p>&lt;strong>Author&lt;/strong>: Josh Berkus (Red Hat)&lt;/p>
&lt;!-- ![Picture of contributor panel at 2018 Shanghai contributor summit. Photo by Josh Berkus, licensed CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png) -->
&lt;p>![贡献者小组讨论掠影，摄于 2018 年上海贡献者峰会，作者 Josh Berkus, 许可证 CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png)&lt;/p>
&lt;!-- For the second year, we will have [a Contributor Summit event](https://www.lfasiallc.com/events/contributors-summit-china-2019/) the day before [KubeCon China](https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/) in Shanghai. If you already contribute to Kubernetes or would like to contribute, please consider attending and [register](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/). The Summit will be held June 24th, at the Shanghai Expo Center (the same location where KubeCon will take place), and will include a Current Contributor Day as well as the New Contributor Workshop and the Documentation Sprints. -->
&lt;p>连续第二年，我们将在 &lt;a href="https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/">KubeCon China&lt;/a> 之前举行一天的 &lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/">贡献者峰会&lt;/a>。
不管您是否已经是一名 Kubernetes 贡献者，还是想要加入社区队伍，贡献一份力量，都请考虑&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">注册&lt;/a>参加这次活动。
这次峰会将于六月 24 号，在上海世博中心（和 KubeCon 的举办地点相同）举行，
一天的活动将包含“现有贡献者活动”，以及“新贡献者工作坊”和“文档小组活动”。&lt;/p>
&lt;!-- ### Current Contributor Day -->
&lt;h3 id="现有贡献者活动">现有贡献者活动&lt;/h3>
&lt;!-- After last year's Contributor Day, our team received feedback that many of our contributors in Asia and Oceania would like content for current contributors as well. As such, we have added a Current Contributor track to the schedule. -->
&lt;p>去年的贡献者节之后，我们的团队收到了很多反馈意见，很多亚洲和大洋洲的贡献者也想要针对当前贡献者的峰会内容。
有鉴于此，我们在今年的安排中加入了当前贡献者的主题。&lt;/p>
&lt;!-- While we do not yet have a full schedule up, the topics covered in the current contributor track will include: -->
&lt;p>尽管我们还没有一个完整的时间安排，下面是当前贡献者主题所会包含的话题：&lt;/p>
&lt;!-- * How to write a KEP (Kubernetes Enhancement Proposal)
* Codebase and repository review
* Local Build &amp; Test troubleshooting session
* Guide to Non-Code Contribution opportunities
* SIG-Azure face-to-face meeting
* SIG-Scheduling face-to-face meeting
* Other SIG face-to-face meetings as we confirm them -->
&lt;ul>
&lt;li>如何撰写 Kubernetes 改进议案 (KEP)&lt;/li>
&lt;li>代码库研习&lt;/li>
&lt;li>本地构建以及测试调试&lt;/li>
&lt;li>不写代码的贡献机会&lt;/li>
&lt;li>SIG-Azure 面对面交流&lt;/li>
&lt;li>SIG-Scheduling 面对面交流&lt;/li>
&lt;li>其他兴趣小组的面对面机会&lt;/li>
&lt;/ul>
&lt;!-- The schedule will be on [the Community page](https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit) once it is complete. -->
&lt;p>整个计划安排将会在完全确定之后，整理放在&lt;a href="https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit">社区页面&lt;/a>上。&lt;/p>
&lt;!-- If your SIG wants to have a face-to-face meeting at Kubecon Shanghai, please contact [Josh Berkus](mailto:jberkus@redhat.com). -->
&lt;p>如果您的 SIG 想要在 Kubecon Shanghai 上进行面对面的交流，请联系 &lt;a href="mailto:jberkus@redhat.com">Josh Berkus&lt;/a>。&lt;/p>
&lt;!-- ### New Contributor Workshop -->
&lt;h3 id="新贡献者工作坊">新贡献者工作坊&lt;/h3>
&lt;!-- Students at [last year's New Contributor Workshop](/blog/2018/12/05/new-contributor-workshop-shanghai/) (NCW) found it to be extremely valuable, and the event helped to orient a few of the many Asian and Pacific developers looking to participate in the Kubernetes community. -->
&lt;p>参与过&lt;a href="https://kubernetes.io/blog/2018/12/05/new-contributor-workshop-shanghai/">去年新贡献者工作坊（NCW）&lt;/a>的学生觉得这项活动非常的有价值，
这项活动也帮助、引导了很多亚洲和大洋洲的开发者更多地参与到 Kubernetes 社区之中。&lt;/p>
&lt;!-- > "It's a one-stop-shop for becoming familiar with the community." said one participant. -->
&lt;blockquote>
&lt;p>“这次活动可以让人一次快速熟悉社区。”其中的一位参与者提到。&lt;/p>
&lt;/blockquote>
&lt;!-- If you have not contributed to Kubernetes before, or have only done one or two things, please consider [enrolling](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) in the NCW. -->
&lt;p>如果您之前从没有参与过 Kubernetes 的贡献，或者只是做过一次或两次贡献，都请考虑&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">注册参加&lt;/a>新贡献者工作坊。&lt;/p>
&lt;!-- > "Got to know the process from signing CLA to PR and made friends with other contributors." said another. -->
&lt;blockquote>
&lt;p>“熟悉了从 CLA 到 PR 的整个流程，也认识结交了很多贡献者。”另一位开发者提到。&lt;/p>
&lt;/blockquote>
&lt;!-- ### Documentation Sprints -->
&lt;h3 id="文档小组活动">文档小组活动&lt;/h3>
&lt;!-- Both old and new contributors on our Docs Team will spend a day both improving our documentation and translating it into other languages. If you are interested in having better documentation, fully localized into Chinese and other languages, please [sign up](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) to help with the Doc Sprints. -->
&lt;p>文档小组的新老贡献者都会聚首一天，讨论如何提升文档质量，以及将文档翻译成更多的语言。
如果您对翻译文档，将这些知识和信息翻译成中文和其他语言感兴趣的话，请在这里&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">注册&lt;/a>，报名参加文档小组活动。&lt;/p>
&lt;!-- ### Before you attend -->
&lt;h3 id="参与之前">参与之前&lt;/h3>
&lt;!-- Regardless of where you participate, everyone at the Contributor Summit should [sign the Kubernetes Contributor License Agreement](https://git.k8s.io/community/CLA.md#the-contributor-license-agreement) (CLA) before coming to the conference. You should also bring a laptop suitable for working on documentation or code development. -->
&lt;p>不论您参与的是哪一项活动，所有人都需要在到达贡献者峰会前签署 &lt;a href="https://git.k8s.io/community/CLA.md#the-contributor-license-agreement">Kubernetes CLA&lt;/a>。
您也同时需要考虑带一个合适的笔记本电脑，帮助文档写作或是编程开发。&lt;/p></description></item><item><title>Blog: 壮大我们的贡献者研讨会</title><link>https://kubernetes.io/zh/blog/2019/05/14/expanding-our-contributor-workshops/</link><pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/05/14/expanding-our-contributor-workshops/</guid><description>
&lt;!--
---
layout: blog
title: "Expanding our Contributor Workshops"
date: 2019-05-14
slug: expanding-our-contributor-workshops
---
-->
&lt;!--
**Authors:** Guinevere Saenger (GitHub) and Paris Pittman (Google)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Guinevere Saenger (GitHub) 和 Paris Pittman (Google)（谷歌）&lt;/p>
&lt;!--
**tl;dr** - learn about the contributor community with us and land your first PR! We have spots available in [Barcelona][eu] (registration **closes** on Wednesday May 15, so grab your spot!) and the upcoming [Shanghai][cn] Summit.
-->
&lt;p>&lt;strong>tl;dr&lt;/strong> - 与我们一起了解贡献者社区，并获得你的第一份 PR ! 我们在[巴塞罗那][欧洲]有空位（登记 在5月15号周三&lt;strong>结束&lt;/strong>，所以抓住这次机会！）并且在[上海][中国]有即将到来的峰会。&lt;/p>
&lt;!--
The Barcelona event is poised to be our biggest one yet, with more registered attendees than ever before!
-->
&lt;p>巴塞罗那的活动将是我们迄今为止最大的一次，登记的参与者比以往任何时候都多！&lt;/p>
&lt;!--
Have you always wanted to contribute to Kubernetes, but not sure where to begin?
Have you seen our community’s many code bases and seen places to improve? We have a workshop for you!
-->
&lt;p>你是否曾经想为 Kubernetes 做贡献，但不知道从哪里开始？你有没有曾经看过我们社区的许多代码库并认为需要改进的地方？我们为你准备了一个工作室！&lt;/p>
&lt;!--
KubeCon + CloudNativeCon Barcelona’s [new contributor workshop][ncw] will be the fourth one of its kind, and we’re really looking forward to it!
-->
&lt;p>KubeCon + CloudNativeCon 巴塞罗那的研讨会&lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/">新贡献者研讨会&lt;/a>将是第四个这样的研讨会，我们真的很期待！&lt;/p>
&lt;!--
The workshop was kickstarted last year at KubeConEU in Copenhagen, and so far we have taken it to Shanghai and Seattle, and now Barcelona, as well as some non-KubeCon locations.
-->
&lt;p>这个研讨会去年在哥本哈根的 KubeConEU 启动，到目前为止，我们已经把它带到了上海和西雅图，现在是巴塞罗那，以及一些非 KubeConEU 的地方。&lt;/p>
&lt;!--
We are constantly updating and improving the workshop content based on feedback from past sessions.
-->
&lt;p>我们根据以往课程的反馈，不断更新和改进研讨会的内容。&lt;/p>
&lt;!--
This time, we’re breaking up the participants by their experience and comfort level with open source and Kubernetes.
-->
&lt;p>这一次，我们将根据参与者对开源和 Kubernetes 的经验和适应程度来进行划分。&lt;/p>
&lt;!--
We’ll have developer setup and project workflow support for folks entirely new to open
source and Kubernetes as part of the 101 track, and hope to set up each
participant with their very own first issue to work on.
-->
&lt;p>作为101课程的一部分，我们将为完全不熟悉开源和 Kubernetes 的人提供开发人员设置和项目工作流支持，并希望为每个参与者开展他们自己的第一期工作。&lt;/p>
&lt;!--
In the 201 track, we will have a codebase walkthrough and local development and test demonstration for folks who have a bit more experience in open source but may be unfamiliar with our community’s development tools.
-->
&lt;p>在201课程中，我们将为那些在开源方面有更多经验但可能不熟悉我们社区开发工具的人进行代码库演练和本地开发和测试演示。&lt;/p>
&lt;!--
For both tracks, you will have a chance to get your hands dirty and have some fun. Because not every contributor works with code, and not every contribution is technical, we will spend the beginning of the workshop learning how our project is structured and organized, where to
find the right people, and where to get help when stuck.
-->
&lt;p>对于这两门课程，你将有机会亲自动手并体会到其中的乐趣。因为不是每个贡献者都使用代码，也不是每项贡献都是技术性的，所以我们将在研讨会开始时学习如何构建和组织项目，以及如何进行找到合适的人，以及遇到困难时在哪里寻求帮助。&lt;/p>
&lt;!--
## Mentoring Opportunities
-->
&lt;h2 id="辅导机会">辅导机会&lt;/h2>
&lt;!--
We will also bring back the SIG Meet-and-Greet where new contributors will have a chance to mingle with current contributors, perhaps find their dream SIG, learn what exciting areas they can help with, gain mentors, and make friends.
-->
&lt;p>我们还将回归 SIG Meet-and-Greet，在这里新入门的菜鸟贡献者将有机会与当值的贡献者交流，这也许会让他们找到他们梦想的 SIG，了解他们可以帮助哪些激动人心的领域，获得导师，结交朋友。&lt;/p>
&lt;!--
PS - there are also two mentoring sessions DURING KubeCon + CloudNativeCon on Thursday, May 23. [Sign up here][mentor]. 60% of the attendees during the Seattle event asked contributor questions.
-->
&lt;p>PS - 5月23日星期四，在 KubeCon+CloudNativeCon 会议期间会有两次导师会议。[在这里注册][导师]。在西雅图活动期间，60% 的与会者会向贡献者提问。&lt;/p>
&lt;!--
## Past Attendee Story - Vallery Lancy, Engineer at Lyft
-->
&lt;h2 id="曾经与会者的故事-vallery-lancy-lyft-的工程师">曾经与会者的故事 - Vallery Lancy，Lyft 的工程师&lt;/h2>
&lt;!--
We talked to a few of our past participants in a series of interviews that we will publish throughout the course of the year.
-->
&lt;p>在一系列采访中，我们与一些过去的参与者进行了交谈，这些采访将在今年全年公布。&lt;/p>
&lt;!--
In our first two clips, we meet Vallery Lancy, an Engineer at Lyft and one of 75 attendees at our recent Seattle edition of the workshop. She was poking around in the community for a while to see where she could jump in.
-->
&lt;p>在我们的前两个片段中，我们会提到 Vallery Lancy，Lyft 公司的工程师，也是我们最近西雅图版研讨会的75名与会者之一。她在社区里闲逛了一段时间，想看看能不能投身到某个领域当中。&lt;/p>
&lt;!--
Watch Vallery talk about her experience here:
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/uKg5WUcl6WU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
-->
&lt;p>在这里观看 Vallery 讲述她的经历：&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/uKg5WUcl6WU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>&lt;/p>
&lt;!--
What does Vallery say to folks curious about the workshops, or those attending the Barcelona edition?
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/niHiem7JmPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
-->
&lt;p>Vallery 和那些对研讨会感兴趣的人或者参加巴塞罗那会议的人说了些什么？&lt;/p>
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/niHiem7JmPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
&lt;!--
Be like Vallery and hundreds of previous New Contributor Workshop attendees: join us in Barcelona (or Shanghai - or San Diego!) for a unique experience without digging into our documentation!
[eu]: https://events.linuxfoundation.org/events/contributor-summit-europe-2019/
[cn]: https://www.lfasiallc.com/events/contributors-summit-china-2019/
[ncw]: https://events.linuxfoundation.org/events/contributor-summit-europe-2019/
[mentor]: http://bit.ly/mentor-bcn
-->
&lt;p>想变得和 Vallery 还有数百名参与之前的新贡献者研讨会的与会者一样的话：在巴塞罗那（或者上海-或者圣地亚哥！）加入我们！你会有一个与众不同的体验，而不再是死读我们的文档！&lt;/p>
&lt;p>Have the opportunity to meet with the experts and go step by step into your journey with your peers around you. We’re looking forward to seeing you there! &lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/">Register here&lt;/a>
--&amp;gt;
你将有机会与专家见面，并与周围的同龄人一步步走上属于你的道路。我们期待着在那里与你见面！&lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/"> 在这里注册&lt;/a>&lt;/p></description></item><item><title>Blog: 如何参与 Kubernetes 文档的本地化工作</title><link>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</link><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</guid><description>
&lt;p>&lt;strong>作者: Zach Corleissen（Linux 基金会）&lt;/strong>&lt;/p>
&lt;p>去年我们对 Kubernetes 网站进行了优化，加入了&lt;a href="https://kubernetes.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/">多语言内容的支持&lt;/a>。贡献者们踊跃响应，加入了多种新的本地化内容：截至 2019 年 4 月，Kubernetes 文档有了 9 个不同语言的未完成版本，其中有 6 个是 2019 年加入的。在每个 Kubernetes 文档页面的上方，读者都可以看到一个语言选择器，其中列出了所有可用语言。&lt;/p>
&lt;p>不论是完成度最高的&lt;a href="https://v1-12.docs.kubernetes.io/zh/">中文版 v1.12&lt;/a>，还是最新加入的&lt;a href="https://kubernetes.io/pt/">葡萄牙文版 v1.14&lt;/a>，各语言的本地化内容还未完成，这是一个进行中的项目。如果读者有兴趣对现有本地化工作提供支持，请继续阅读。&lt;/p>
&lt;h2 id="什么是本地化">什么是本地化&lt;/h2>
&lt;p>翻译是以词表意的问题。而本地化在此基础之上，还包含了过程和设计方面的工作。&lt;/p>
&lt;p>本地化和翻译很像，但是包含更多内容。除了进行翻译之外，本地化还要为编写和发布过程的框架进行优化。例如，Kubernetes.io 多数的站点浏览功能（按钮文字）都保存在&lt;a href="https://github.com/kubernetes/website/tree/master/i18n">单独的文件&lt;/a>之中。所以启动新本地化的过程中，需要包含加入对特定文件中字符串进行翻译的工作。&lt;/p>
&lt;p>本地化很重要，能够有效的降低 Kubernetes 的采纳和支持门槛。如果能用母语阅读 Kubernetes 文档，就能更轻松的开始使用 Kubernetes，并对其发展作出贡献。&lt;/p>
&lt;h2 id="如何启动本地化工作">如何启动本地化工作&lt;/h2>
&lt;p>不同语言的本地化工作都是单独的功能——和其它 Kubernetes 功能一致，贡献者们在一个 SIG 中进行本地化工作，分享出来进行评审，并加入项目。&lt;/p>
&lt;p>贡献者们在团队中进行内容的本地化工作。因为自己不能批准自己的 PR，所以一个本地化团队至少应该有两个人——例如意大利文的本地化团队有两个人。这个团队规模可能很大：中文团队有几十个成员。&lt;/p>
&lt;p>每个团队都有自己的工作流。有些团队手工完成所有的内容翻译；有些会使用带有翻译插件的编译器，并使用评审机来提供正确性的保障。SIG Docs 专注于输出的标准；这就给了本地化团队采用适合自己工作情况的工作流。这样一来，团队可以根据最佳实践进行协作，并以 Kubernetes 的社区精神进行分享。&lt;/p>
&lt;h2 id="为本地化工作添砖加瓦">为本地化工作添砖加瓦&lt;/h2>
&lt;p>如果你有兴趣为 Kubernetes 文档加入新语种的本地化内容，&lt;a href="https://kubernetes.io/docs/contribute/localization/">Kubernetes contribution guide&lt;/a> 中包含了这方面的相关内容。&lt;/p>
&lt;p>已经启动的的本地化工作同样需要支持。如果有兴趣为现存项目做出贡献，可以加入本地化团队的 Slack 频道，去做个自我介绍。各团队的成员会帮助你开始工作。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>语种&lt;/th>
&lt;th>Slack 频道&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>中文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CE3LNFYJ1/">#kubernetes-docs-zh&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>英文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/C1J0BPD2M/">#sig-docs&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>法文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CG838BFT9/">#kubernetes-docs-fr&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>德文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CH4UJ2BAL/">#kubernetes-docs-de&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>印地&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ14B9BDJ/">#kubernetes-docs-hi&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>印度尼西亚文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ1LUCUHM/">#kubernetes-docs-id&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>意大利文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CGB1MCK7X/">#kubernetes-docs-it&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>日文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CAG2M83S8/">#kubernetes-docs-ja&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>韩文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CA1MMR86S/">#kubernetes-docs-ko&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>葡萄牙文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ21AS0NA/">#kubernetes-docs-pt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>西班牙文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CH7GB2E3B/">#kubernetes-docs-es&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="下一步">下一步？&lt;/h2>
&lt;p>最新的&lt;a href="https://kubernetes.slack.com/messages/CJ14B9BDJ/">印地文本地化&lt;/a>工作正在启动。为什么不加入你的语言？&lt;/p>
&lt;p>身为 SIG Docs 的主席，我甚至希望本地化工作跳出文档范畴，直接为 Kubernetes 组件提供本地化支持。有什么组件是你希望支持不同语言的么？可以提交一个 &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">Kubernetes Enhancement Proposal&lt;/a> 来促成这一进步。&lt;/p></description></item><item><title>Blog: Kubernetes 1.14 稳定性改进中的进程ID限制</title><link>https://kubernetes.io/zh/blog/2019/04/15/kubernetes-1.14-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%94%B9%E8%BF%9B%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8Bid%E9%99%90%E5%88%B6/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/04/15/kubernetes-1.14-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%94%B9%E8%BF%9B%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8Bid%E9%99%90%E5%88%B6/</guid><description>
&lt;!--
---
title: 'Process ID Limiting for Stability Improvements in Kubernetes 1.14'
date: 2019-04-15
---
-->
&lt;!--
**Author: Derek Carr**
Have you ever seen someone take more than their fair share of the cookies? The one person who reaches in and grabs a half dozen fresh baked chocolate chip chunk morsels and skitters off like Cookie Monster exclaiming “Om nom nom nom.”
In some rare workloads, a similar occurrence was taking place inside Kubernetes clusters. With each Pod and Node, there comes a finite number of possible process IDs (PIDs) for all applications to share. While it is rare for any one process or pod to reach in and grab all the PIDs, some users were experiencing resource starvation due to this type of behavior. So in Kubernetes 1.14, we introduced an enhancement to mitigate the risk of a single pod monopolizing all of the PIDs available.
-->
&lt;p>&lt;strong>作者: Derek Carr&lt;/strong>&lt;/p>
&lt;p>你是否见过有人拿走了比属于他们那一份更多的饼干？ 一个人走过来，抓起半打新鲜烤制的大块巧克力饼干然后匆匆离去，就像饼干怪兽大喊 “Om nom nom nom”。&lt;/p>
&lt;p>在一些罕见的工作负载中，Kubernetes 集群内部也发生了类似的情况。每个 Pod 和 Node 都有有限数量的可能的进程 ID（PID），供所有应用程序共享。尽管很少有进程或 Pod 能够进入并获取所有 PID，但由于这种行为，一些用户会遇到资源匮乏的情况。 因此，在 Kubernetes 1.14 中，我们引入了一项增强功能，以降低单个 Pod 垄断所有可用 PID 的风险。&lt;/p>
&lt;!--
## Can You Spare Some PIDs?
Here, we’re talking about the greed of certain containers. Outside the ideal, runaway processes occur from time to time, particularly in clusters where testing is taking place. Thus, some wildly non-production-ready activity is happening.
In such a scenario, it’s possible for something akin to a fork bomb taking place inside a node. As resources slowly erode, being taken over by some zombie-like process that continually spawns children, other legitimate workloads begin to get bumped in favor of this inflating balloon of wasted processing power. This could result in other processes on the same pod being starved of their needed PIDs. It could also lead to interesting side effects as a node could fail and a replica of that pod is scheduled to a new machine where the process repeats across your entire cluster.
-->
&lt;h2 id="你能预留一些-pids-吗">你能预留一些 PIDs 吗？&lt;/h2>
&lt;p>在这里，我们谈论的是某些容器的贪婪性。 在理想情况之外，失控进程有时会发生，特别是在测试集群中。 因此，在这些集群中会发生一些混乱的非生产环境准备就绪的事情。&lt;/p>
&lt;p>在这种情况下，可能会在节点内部发生类似于 fork 炸弹耗尽 PID 的攻击。随着资源的缓慢腐蚀，被一些不断产生子进程的僵尸般的进程所接管，其他正常的工作负载会因为这些像气球般不断膨胀的浪费的处理能力而开始受到冲击。这可能导致同一 Pod 上的其他进程缺少所需的 PID。这也可能导致有趣的副作用，因为节点可能会发生故障，并且该Pod的副本将安排到新的机器上，至此，该过程将在整个群集中重复进行。&lt;/p>
&lt;!--
## Fixing the Problem
Thus, in Kubernetes 1.14, we have added a feature that allows for the configuration of a kubelet to limit the number of PIDs a given pod can consume. If that machine supports 32,768 PIDs and 100 pods, one can give each pod a budget of 300 PIDs to prevent total exhaustion of PIDs. If the admin wants to overcommit PIDs similar to cpu or memory, they may do so as well with some additional risks. Either way, no one pod can bring the whole machine down. This will generally prevent against simple fork bombs from taking over your cluster.
This change allows administrators to protect one pod from another, but does not ensure if all pods on the machine can protect the node, and the node agents themselves from falling over. Thus, we’ve introduced a feature in this release in alpha form that provides isolation of PIDs from end user workloads on a pod from the node agents (kubelet, runtime, etc.). The admin is able to reserve a specific number of PIDs--similar to how one reserves CPU or memory today--and ensure they are never consumed by pods on that machine. Once that graduates from alpha, to beta, then stable in future releases of Kubernetes, we’ll have protection against an easily starved Linux resource.
Get started with [Kubernetes 1.14](https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0).
-->
&lt;h2 id="解决问题">解决问题&lt;/h2>
&lt;p>因此，在 Kubernetes 1.14 中，我们添加了一个特性，允许通过配置 kubelet，限制给定 Pod 可以消耗的 PID 数量。如果该机器支持 32768 个 PIDs 和 100 个 Pod，则可以为每个 Pod 提供 300 个 PIDs 的预算，以防止 PIDs 完全耗尽。如果管理员想要像 CPU 或内存那样过度使用 PIDs，那么他们也可以配置超额使用，但是这样会有一些额外风险。不管怎样，没有一个Pod能搞坏整个机器。这通常会防止简单的分叉函数炸弹接管你的集群。&lt;/p>
&lt;p>此更改允许管理员保护一个 Pod 不受另一个 Pod 的影响，但不能确保计算机上的所有 Pod 都能保护节点和节点代理本身不受影响。因此，我们在这个版本中以 Alpha 的形式引入了这个一个特性，它提供了 PIDs 在节点代理（ kubelet、runtime 等）与 Pod 上的最终用户工作负载的分离。管理员可以预定特定数量的 pid（类似于今天如何预定 CPU 或内存），并确保它们不会被该计算机上的 pod 消耗。一旦从 Alpha 进入到 Beta，然后在将来的 Kubernetes 版本中稳定下来，我们就可以使用这个特性防止 Linux 资源耗尽。&lt;/p>
&lt;p>开始使用 &lt;a href="https://github.com/Kubernetes/Kubernetes/releases/tag/v1.14.0">Kubernetes 1.14&lt;/a>。&lt;/p>
&lt;!--
## Get Involved
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Node Special Interest Group](https://github.com/kubernetes/community/tree/master/sig-node).
### About the author:
Derek Carr is Senior Principal Software Engineer at Red Hat. He is a Kubernetes contributor and member of the Kubernetes Community Steering Committee.
-->
&lt;p>##参与其中&lt;/p>
&lt;p>如果您对此特性有反馈或有兴趣参与其设计与开发，请加入[节点特别兴趣小组](&lt;a href="https://github.com/kubernetes/community/tree/master/sig">https://github.com/kubernetes/community/tree/master/sig&lt;/a> Node)。&lt;/p>
&lt;p>###关于作者：
Derek Carr 是 Red Hat 高级首席软件工程师。他也是 Kubernetes 的贡献者和 Kubernetes 社区指导委员会的成员。&lt;/p></description></item><item><title>Blog: Raw Block Volume 支持进入 Beta</title><link>https://kubernetes.io/zh/blog/2019/03/07/raw-block-volume-%E6%94%AF%E6%8C%81%E8%BF%9B%E5%85%A5-beta/</link><pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/03/07/raw-block-volume-%E6%94%AF%E6%8C%81%E8%BF%9B%E5%85%A5-beta/</guid><description>
&lt;!--
---
title: Raw Block Volume support to Beta
date: 2019-03-07
---
--->
&lt;!--
**Authors:**
Ben Swartzlander (NetApp), Saad Ali (Google)
Kubernetes v1.13 moves raw block volume support to beta. This feature allows persistent volumes to be exposed inside containers as a block device instead of as a mounted file system.
--->
&lt;p>&lt;strong>作者：&lt;/strong>
Ben Swartzlander (NetApp), Saad Ali (Google)&lt;/p>
&lt;p>Kubernetes v1.13 中对原生数据块卷（Raw Block Volume）的支持进入 Beta 阶段。此功能允许将持久卷作为块设备而不是作为已挂载的文件系统暴露在容器内部。&lt;/p>
&lt;!--
## What are block devices?
Block devices enable random access to data in fixed-size blocks. Hard drives, SSDs, and CD-ROMs drives are all examples of block devices.
Typically persistent storage is implemented in a layered maner with a file system (like ext4) on top of a block device (like a spinning disk or SSD). Applications then read and write files instead of operating on blocks. The operating systems take care of reading and writing files, using the specified filesystem, to the underlying device as blocks.
It's worth noting that while whole disks are block devices, so are disk partitions, and so are LUNs from a storage area network (SAN) device.
--->
&lt;h2 id="什么是块设备">什么是块设备？&lt;/h2>
&lt;p>块设备允许对固定大小的块中的数据进行随机访问。硬盘驱动器、SSD 和 CD-ROM 驱动器都是块设备的例子。&lt;/p>
&lt;p>通常，持久性性存储是在通过在块设备（例如磁盘或 SSD）之上构造文件系统（例如 ext4）的分层方式实现的。这样应用程序就可以读写文件而不是操作数据块进。操作系统负责使用指定的文件系统将文件读写转换为对底层设备的数据块读写。&lt;/p>
&lt;p>值得注意的是，整个磁盘都是块设备，磁盘分区也是如此，存储区域网络（SAN）设备中的 LUN 也是一样的。&lt;/p>
&lt;!--
## Why add raw block volumes to kubernetes?
There are some specialized applications that require direct access to a block device because, for example, the file system layer introduces unneeded overhead. The most common case is databases, which prefer to organize their data directly on the underlying storage. Raw block devices are also commonly used by any software which itself implements some kind of storage service (software defined storage systems).
--->
&lt;h2 id="为什么要将-raw-block-volume-添加到-kubernetes">为什么要将 raw block volume 添加到 kubernetes？&lt;/h2>
&lt;p>有些特殊的应用程序需要直接访问块设备，原因例如，文件系统层会引入不必要的开销。最常见的情况是数据库，通常会直接在底层存储上组织数据。原生的块设备（Raw Block Devices）还通常由能自己实现某种存储服务的软件（软件定义的存储系统）使用。&lt;/p>
&lt;!--
From a programmer's perspective, a block device is a very large array of bytes, usually with some minimum granularity for reads and writes, often 512 bytes, but frequently 4K or larger.
As it becomes more common to run database software and storage infrastructure software inside of Kubernetes, the need for raw block device support in Kubernetes becomes more important.
--->
&lt;p>从程序员的角度来看，块设备是一个非常大的字节数组，具有某种最小读写粒度，通常为 512 个字节，大部分情况为 4K 或更大。&lt;/p>
&lt;p>随着在 Kubernetes 中运行数据库软件和存储基础架构软件变得越来越普遍，在 Kubernetes 中支持原生块设备的需求变得越来越重要。&lt;/p>
&lt;!--
## Which volume plugins support raw blocks?
As of the publishing of this blog, the following in-tree volumes types support raw blocks:
--->
&lt;h2 id="哪些卷插件支持-raw-block">哪些卷插件支持 raw block？&lt;/h2>
&lt;p>在发布此博客时，以下 in-tree 卷类型支持原生块设备：&lt;/p>
&lt;ul>
&lt;li>AWS EBS&lt;/li>
&lt;li>Azure Disk&lt;/li>
&lt;li>Cinder&lt;/li>
&lt;li>Fibre Channel&lt;/li>
&lt;li>GCE PD&lt;/li>
&lt;li>iSCSI&lt;/li>
&lt;li>Local volumes&lt;/li>
&lt;li>RBD (Ceph)&lt;/li>
&lt;li>Vsphere&lt;/li>
&lt;/ul>
&lt;!--
Out-of-tree [CSI volume drivers](https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/) may also support raw block volumes. Kubernetes CSI support for raw block volumes is currently alpha. See documentation [here](https://kubernetes-csi.github.io/docs/raw-block.html).
--->
&lt;p>Out-of-tree &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">CSI 卷驱动程序&lt;/a> 可能也支持原生数据块卷。Kubernetes CSI 对原生数据块卷的支持目前为 alpha 阶段。参考 &lt;a href="https://kubernetes-csi.github.io/docs/raw-block.html">这篇&lt;/a> 文档。&lt;/p>
&lt;!--
## Kubernetes raw block volume API
Raw block volumes share a lot in common with ordinary volumes. Both are requested by creating `PersistentVolumeClaim` objects which bind to `PersistentVolume` objects, and are attached to Pods in Kubernetes by including them in the volumes array of the `PodSpec`.
There are 2 important differences however. First, to request a raw block `PersistentVolumeClaim`, you must set `volumeMode = "Block"` in the `PersistentVolumeClaimSpec`. Leaving `volumeMode` blank is the same as specifying `volumeMode = "Filesystem"` which results in the traditional behavior. `PersistentVolumes` also have a `volumeMode` field in their `PersistentVolumeSpec`, and `"Block"` type PVCs can only bind to `"Block"` type PVs and `"Filesystem"` PVCs can only bind to `"Filesystem"` PVs.
--->
&lt;h2 id="kubernetes-raw-block-volume-的-api">Kubernetes raw block volume 的 API&lt;/h2>
&lt;p>原生数据块卷与普通存储卷有很多共同点。两者都通过创建与 &lt;code>PersistentVolume&lt;/code> 对象绑定的 &lt;code>PersistentVolumeClaim&lt;/code> 对象发起请求，并通过将它们加入到 &lt;code>PodSpec&lt;/code> 的 volumes 数组中来连接到 Kubernetes 中的 Pod。&lt;/p>
&lt;p>但是有两个重要的区别。首先，要请求原生数据块设备的 &lt;code>PersistentVolumeClaim&lt;/code> 必须在 &lt;code>PersistentVolumeClaimSpec&lt;/code> 中设置 &lt;code>volumeMode = &amp;quot;Block&amp;quot;&lt;/code>。&lt;code>volumeMode&lt;/code> 为空时与传统设置方式中的指定 &lt;code>volumeMode = &amp;quot;Filesystem&amp;quot;&lt;/code> 是一样的。&lt;code>PersistentVolumes&lt;/code> 在其 &lt;code>PersistentVolumeSpec&lt;/code> 中也有一个 &lt;code>volumeMode&lt;/code> 字段，&lt;code>&amp;quot;Block&amp;quot;&lt;/code> 类型的 PVC 只能绑定到 &lt;code>&amp;quot;Block&amp;quot;&lt;/code> 类型的 PV 上，而&lt;code>&amp;quot;Filesystem&amp;quot;&lt;/code> 类型的 PVC 只能绑定到 &lt;code>&amp;quot;Filesystem&amp;quot;&lt;/code> PV 上。&lt;/p>
&lt;!--
Secondly, when using a raw block volume in your Pods, you must specify a `VolumeDevice` in the Container portion of the `PodSpec` rather than a `VolumeMount`. `VolumeDevices` have `devicePaths` instead of `mountPaths`, and inside the container, applications will see a device at that path instead of a mounted file system.
Applications open, read, and write to the device node inside the container just like they would interact with any block device on a system in a non-containerized or virtualized context.
--->
&lt;p>其次，在 Pod 中使用原生数据块卷时，必须在 &lt;code>PodSpec&lt;/code> 的 Container 部分指定一个 &lt;code>VolumeDevice&lt;/code>，而不是 &lt;code>VolumeMount&lt;/code>。&lt;code>VolumeDevices&lt;/code> 具备 &lt;code>devicePaths&lt;/code> 而不是 &lt;code>mountPaths&lt;/code>，在容器中，应用程序将看到位于该路径的设备，而不是挂载了的文件系统。&lt;/p>
&lt;p>应用程序打开、读取和写入容器内的设备节点，就像它们在非容器化或虚拟环境中与系统上的任何块设备交互一样。&lt;/p>
&lt;!--
## Creating a new raw block PVC
First, ensure that the provisioner associated with the storage class you choose is one that support raw blocks. Then create the PVC.
--->
&lt;h2 id="创建一个新的原生块设备-pvc">创建一个新的原生块设备 PVC&lt;/h2>
&lt;p>首先，请确保与您选择的存储类关联的驱动支持原生块设备。然后创建 PVC。&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: my-pvc
spec:
accessModes:
- ReadWriteMany
volumeMode: Block
storageClassName: my-sc
resources:
requests:
storage: 1Gi
&lt;/code>&lt;/pre>&lt;!--
## Using a raw block PVC
When you use the PVC in a pod definition, you get to choose the device path for the block device rather than the mount path for the file system.
--->
&lt;h2 id="使用原生块-pvc">使用原生块 PVC&lt;/h2>
&lt;p>在 Pod 定义中使用 PVC 时，需要选择块设备的设备路径，而不是文件系统的安装路径。&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: my-pod
spec:
containers:
- name: my-container
image: busybox
command:
- sleep
- “3600”
volumeDevices:
- devicePath: /dev/block
name: my-volume
imagePullPolicy: IfNotPresent
volumes:
- name: my-volume
persistentVolumeClaim:
claimName: my-pvc
&lt;/code>&lt;/pre>&lt;!--
## As a storage vendor, how do I add support for raw block devices to my CSI plugin?
Raw block support for CSI plugins is still alpha, but support can be added today. The [CSI specification](https://github.com/container-storage-interface/spec/blob/master/spec.md) details how to handle requests for volume that have the `BlockVolume` capability instead of the `MountVolume` capability. CSI plugins can support both kinds of volumes, or one or the other. For more details see [documentation here](https://kubernetes-csi.github.io/docs/raw-block.html).
--->
&lt;h2 id="作为存储供应商-我如何在-csi-插件中添加对原生块设备的支持">作为存储供应商，我如何在 CSI 插件中添加对原生块设备的支持？&lt;/h2>
&lt;p>CSI 插件的原生块支持仍然是 alpha 版本，但是现在可以改进了。&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI 规范&lt;/a> 详细说明了如何处理具有 &lt;code>BlockVolume&lt;/code> 能力而不是 &lt;code>MountVolume&lt;/code> 能力的卷的请求。CSI 插件可以支持两种类型的卷，也可以支持其中一种或另一种。更多详细信息，请查看 &lt;a href="https://kubernetes-csi.github.io/docs/raw-block.html">这个文档&lt;/a>。&lt;/p>
&lt;!--
## Issues/gotchas
Because block devices are actually devices, it’s possible to do low-level actions on them from inside containers that wouldn’t be possible with file system volumes. For example, block devices that are actually SCSI disks support sending SCSI commands to the device using Linux ioctls.
--->
&lt;h2 id="问题-陷阱">问题/陷阱&lt;/h2>
&lt;p>由于块设备实质上还是设备，因此可以从容器内部对其进行底层操作，而文件系统的卷则无法执行这些操作。例如，实际上是块设备的 SCSI 磁盘支持使用 Linux ioctl 向设备发送 SCSI 命令。&lt;/p>
&lt;!--
By default, Linux won’t allow containers to send SCSI commands to disks from inside containers though. In order to do so, you must grant the `SYS_RAWIO` capability to the container security context to allow this. See documentation [here](/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container).
Also, while Kubernetes is guaranteed to deliver a block device to the container, there’s no guarantee that it’s actually a SCSI disk or any other kind of disk for that matter. The user must either ensure that the desired disk type is used with his pods, or only deploy applications that can handle a variety of block device types.
--->
&lt;p>默认情况下，Linux 不允许容器将 SCSI 命令从容器内部发送到磁盘。为此，必须向容器安全层级认证 &lt;code>SYS_RAWIO&lt;/code> 功能实现这种行为。请参阅 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">这篇&lt;/a> 文档。&lt;/p>
&lt;p>另外，尽管 Kubernetes 保证可以将块设备交付到容器中，但不能保证它实际上是 SCSI 磁盘或任何其他类型的磁盘。用户必须确保所需的磁盘类型与 Pod 一起使用，或只部署可以处理各种块设备类型的应用程序。&lt;/p>
&lt;!--
## How can I learn more?
Check out additional documentation on the snapshot feature here: [Raw Block Volume Support](/docs/concepts/storage/persistent-volumes/#raw-block-volume-support)
How do I get involved?
Join the Kubernetes storage SIG and the CSI community and help us add more great features and improve existing ones like raw block storage!
--->
&lt;h2 id="如何学习更多">如何学习更多？&lt;/h2>
&lt;p>在此处查看有关 snapshot 功能的其他文档：&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support">Raw Block Volume 支持&lt;/a>&lt;/p>
&lt;p>如何参与进来？&lt;/p>
&lt;p>加入 Kubernetes 存储 SIG 和 CSI 社区，帮助我们添加更多出色的功能并改进现有功能，就像 raw block 存储一样！&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">https://github.com/kubernetes/community/tree/master/sig-storage&lt;/a>
&lt;a href="https://github.com/container-storage-interface/community/blob/master/README.md">https://github.com/container-storage-interface/community/blob/master/README.md&lt;/a>&lt;/p>
&lt;!--
Special thanks to all the contributors who helped add block volume support to Kubernetes including:
--->
&lt;p>特别感谢所有为 Kubernetes 增加 block volume 支持的贡献者，包括：&lt;/p>
&lt;ul>
&lt;li>Ben Swartzlander (&lt;a href="https://github.com/bswartz">https://github.com/bswartz&lt;/a>)&lt;/li>
&lt;li>Brad Childs (&lt;a href="https://github.com/childsb">https://github.com/childsb&lt;/a>)&lt;/li>
&lt;li>Erin Boyd (&lt;a href="https://github.com/erinboyd">https://github.com/erinboyd&lt;/a>)&lt;/li>
&lt;li>Masaki Kimura (&lt;a href="https://github.com/mkimuram">https://github.com/mkimuram&lt;/a>)&lt;/li>
&lt;li>Matthew Wong (&lt;a href="https://github.com/wongma7">https://github.com/wongma7&lt;/a>)&lt;/li>
&lt;li>Michelle Au (&lt;a href="https://github.com/msau42">https://github.com/msau42&lt;/a>)&lt;/li>
&lt;li>Mitsuhiro Tanino (&lt;a href="https://github.com/mtanino">https://github.com/mtanino&lt;/a>)&lt;/li>
&lt;li>Saad Ali (&lt;a href="https://github.com/saad-ali">https://github.com/saad-ali&lt;/a>)&lt;/li>
&lt;/ul></description></item><item><title>Blog: 新贡献者工作坊上海站</title><link>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</guid><description>
&lt;!--
---
layout: blog
title: 'New Contributor Workshop Shanghai'
date: 2018-12-05
---
-->
&lt;!--
**Authors**: Josh Berkus (Red Hat), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ZTE)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Josh Berkus (红帽), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (中兴通讯)&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png"
alt="KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png"
alt="KubeCon 上海站新贡献者峰会与会者，摄影：Jerry Zhang"/> &lt;figcaption>
&lt;p>KubeCon 上海站新贡献者峰会与会者，摄影：Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
We recently completed our first New Contributor Summit in China, at the first KubeCon in China. It was very exciting to see all of the Chinese and Asian developers (plus a few folks from around the world) interested in becoming contributors. Over the course of a long day, they learned how, why, and where to contribute to Kubernetes, created pull requests, attended a panel of current contributors, and got their CLAs signed.
-->
&lt;p>最近，在中国的首次 KubeCon 上，我们完成了在中国的首次新贡献者峰会。看到所有中国和亚洲的开发者（以及来自世界各地的一些人）有兴趣成为贡献者，这令人非常兴奋。在长达一天的课程中，他们了解了如何、为什么以及在何处为 Kubernetes 作出贡献，创建了 PR，参加了贡献者圆桌讨论，并签署了他们的 CLA。&lt;/p>
&lt;!--
This was our second New Contributor Workshop (NCW), building on the one created and led by SIG Contributor Experience members in Copenhagen. Because of the audience, it was held in both Chinese and English, taking advantage of the superb simultaneous interpretation services the CNCF sponsored. Likewise, the NCW team included both English and Chinese-speaking members of the community: Yang Li, XiangPeng Zhao, Puja Abbassi, Noah Abrahams, Tim Pepper, Zach Corleissen, Sen Lu, and Josh Berkus. In addition to presenting and helping students, the bilingual members of the team translated all of the slides into Chinese. Fifty-one students attended.
-->
&lt;p>这是我们的第二届新贡献者工作坊（NCW），它由前一次贡献者体验 SIG 成员创建和领导的哥本哈根研讨会延伸而来。根据受众情况，本次活动采用了中英文两种语言，充分利用了 CNCF 赞助的一流的同声传译服务。同样，NCW 团队由社区成员组成，既有说英语的，也有说汉语的：Yang Li、XiangPeng Zhao、Puja Abbassi、Noah Abrahams、Tim Pepper、Zach Corleissen、Sen Lu 和 Josh Berkus。除了演讲和帮助学员外，团队的双语成员还将所有幻灯片翻译成了中文。共有五十一名学员参加。&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png"
alt="Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png"
alt="Noah Abrahams 讲解 Kubernetes 沟通渠道。摄影：Jerry Zhang"/> &lt;figcaption>
&lt;p>Noah Abrahams 讲解 Kubernetes 沟通渠道。摄影：Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
The NCW takes participants through the stages of contributing to Kubernetes, starting from deciding where to contribute, followed by an introduction to the SIG system and our repository structure. We also have "guest speakers" from Docs and Test Infrastructure who cover contributing in those areas. We finally wind up with some hands-on exercises in filing issues and creating and approving PRs.
-->
&lt;p>NCW 让参与者完成了为 Kubernetes 作出贡献的各个阶段，从决定在哪里作出贡献开始，接着介绍了 SIG 系统和我们的代码仓库结构。我们还有来自文档和测试基础设施领域的「客座讲者」，他们负责讲解有关的贡献。最后，我们在创建 issue、提交并批准 PR 的实践练习后，结束了工作坊。&lt;/p>
&lt;!--
Those hands-on exercises use a repository known as [the contributor playground](https://github.com/kubernetes-sigs/contributor-playground), created by SIG Contributor Experience as a place for new contributors to try out performing various actions on a Kubernetes repo. It has modified Prow and Tide automation, uses Owners files like in the real repositories. This lets students learn how the mechanics of contributing to our repositories work without disrupting normal development.
-->
&lt;p>这些实践练习使用一个名为&lt;a href="https://github.com/kubernetes-sigs/contributor-playground">贡献者游乐场&lt;/a>的代码仓库，由贡献者体验 SIG 创建，让新贡献者尝试在一个 Kubernetes 仓库中执行各种操作。它修改了 Prow 和 Tide 自动化，使用与真实代码仓库类似的 Owners 文件。这可以让学员了解为我们的仓库做出贡献的有关机制，同时又不妨碍正常的开发流程。&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png"
alt="Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus"/> &lt;figcaption>
&lt;p>Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png"
alt="Yang Li 讲到如何让你的 PR 通过评审。摄影：Josh Berkus"/> &lt;figcaption>
&lt;p>Yang Li 讲到如何让你的 PR 通过评审。摄影：Josh Berkus&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
Both the "Great Firewall" and the language barrier prevent contributing Kubernetes from China from being straightforward. What's more, because open source business models are not mature in China, the time for employees work on open source projects is limited.
-->
&lt;p>「防火长城」和语言障碍都使得在中国为 Kubernetes 作出贡献变得困难。而且，中国的开源商业模式并不成熟，员工在开源项目上工作的时间有限。&lt;/p>
&lt;!--
Chinese engineers are eager to participate in the development of Kubernetes, but many of them don't know where to start since Kubernetes is such a large project. With this workshop, we hope to help those who want to contribute, whether they wish to fix some bugs they encountered, improve or localize documentation, or they need to work with Kubernetes at their work. We are glad to see more and more Chinese contributors joining the community in the past few years, and we hope to see more of them in the future.
-->
&lt;p>中国工程师渴望参与 Kubernetes 的研发，但他们中的许多人不知道从何处开始，因为 Kubernetes 是一个如此庞大的项目。通过本次工作坊，我们希望帮助那些想要参与贡献的人，不论他们希望修复他们遇到的一些错误、改进或本地化文档，或者他们需要在工作中用到 Kubernetes。我们很高兴看到越来越多的中国贡献者在过去几年里加入社区，我们也希望将来可以看到更多。&lt;/p>
&lt;!--
"I have been participating in the Kubernetes community for about three years," said XiangPeng Zhao. "In the community, I notice that more and more Chinese developers are showing their interest in contributing to Kubernetes. However, it's not easy to start contributing to such a project. I tried my best to help those who I met in the community, but I think there might still be some new contributors leaving the community due to not knowing where to get help when in trouble. Fortunately, the community initiated NCW at KubeCon Copenhagen and held a second one at KubeCon Shanghai. I was so excited to be invited by Josh Berkus to help organize this workshop. During the workshop, I met community friends in person, mentored attendees in the exercises, and so on. All of this was a memorable experience for me. I also learned a lot as a contributor who already has years of contributing experience. I wish I had attended such a workshop when I started contributing to Kubernetes years ago."
-->
&lt;p>「我已经参与了 Kubernetes 社区大约三年」，XiangPeng Zhao 说，「在社区，我注意到越来越多的中国开发者表现出对 Kubernetes 贡献的兴趣。但是，开始为这样一个项目做贡献并不容易。我尽力帮助那些我在社区遇到的人，但是，我认为可能仍有一些新的贡献者离开社区，因为他们在遇到麻烦时不知道从哪里获得帮助。幸运的是，社区在 KubeCon 哥本哈根站发起了 NCW，并在 KubeCon 上海站举办了第二届。我很高兴受到 Josh Berkus 的邀请，帮助组织这个工作坊。在工作坊期间，我当面见到了社区里的朋友，在练习中指导了与会者，等等。所有这些对我来说都是难忘的经历。作为有着多年贡献者经验的我，也学习到了很多。我希望几年前我开始为 Kubernetes 做贡献时参加过这样的工作坊」。&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png"
alt="Panel of contributors. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>Panel of contributors. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png"
alt="贡献者圆桌讨论。摄影：Jerry Zhang"/> &lt;figcaption>
&lt;p>贡献者圆桌讨论。摄影：Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
The workshop ended with a panel of current contributors, featuring Lucas Käldström, Janet Kuo, Da Ma, Pengfei Ni, Zefeng Wang, and Chao Xu. The panel aimed to give both new and current contributors a look behind the scenes on the day-to-day of some of the most active contributors and maintainers, both from China and around the world. Panelists talked about where to begin your contributor's journey, but also how to interact with reviewers and maintainers. They further touched upon the main issues of contributing from China and gave attendees an outlook into exciting features they can look forward to in upcoming releases of Kubernetes.
-->
&lt;p>工作坊以现有贡献者圆桌讨论结束，嘉宾包括 Lucas Käldström、Janet Kuo、Da Ma、Pengfei Ni、Zefeng Wang 和 Chao Xu。这场圆桌讨论旨在让新的和现有的贡献者了解一些最活跃的贡献者和维护者的幕后日常工作，不论他们来自中国还是世界各地。嘉宾们讨论了从哪里开始贡献者的旅程，以及如何与评审者和维护者进行互动。他们进一步探讨了在中国参与贡献的主要问题，并向与会者预告了在 Kubernetes 的未来版本中可以期待的令人兴奋的功能。&lt;/p>
&lt;!--
After the workshop, XiangPeng Zhao chatted with some attendees on WeChat and Twitter about their experiences. They were very glad to have attended the NCW and had some suggestions on improving the workshop. One attendee, Mohammad, said, "I had a great time at the workshop and learned a lot about the entire process of k8s for a contributor." Another attendee, Jie Jia, said, "The workshop was wonderful. It systematically explained how to contribute to Kubernetes. The attendee could understand the process even if s/he knew nothing about that before. For those who were already contributors, they could also learn something new. Furthermore, I could make new friends from inside or outside of China in the workshop. It was awesome!"
-->
&lt;p>工作坊结束后，XiangPeng Zhao 和一些与会者就他们的经历在微信和 Twitter 上进行了交谈。他们很高兴参加了 NCW，并就改进工作坊提出了一些建议。一位名叫 Mohammad 的与会者说：「我在工作坊上玩得很开心，学习了参与 k8s 贡献的整个过程。」另一位与会者 Jie Jia 说：「工作坊非常精彩。它系统地解释了如何为 Kubernetes 做出贡献。即使参与者之前对此一无所知，他（她）也可以理解这个过程。对于那些已经是贡献者的人，他们也可以学习到新东西。此外，我还可以在工作坊上结识来自国内外的新朋友。真是棒极了！」&lt;/p>
&lt;!--
SIG Contributor Experience will continue to run New Contributor Workshops at each upcoming KubeCon, including Seattle, Barcelona, and the return to Shanghai in June 2019. If you failed to get into one this year, register for one at a future KubeCon. And, when you meet an NCW attendee, make sure to welcome them to the community.
-->
&lt;p>贡献者体验 SIG 将继续在未来的 KubeCon 上举办新贡献者工作坊，包括西雅图站、巴塞罗那站，然后在 2019 年六月回到上海。如果你今年未能参加，请在未来的 KubeCon 上注册。并且，如果你遇到工作坊的与会者，请务必欢迎他们加入社区。&lt;/p>
&lt;!--
Links:
-->
&lt;p>链接：&lt;/p>
&lt;!--
* English versions of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf) or [Google Docs with speaker notes](https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing)
* Chinese version of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf)
* [Contributor playground](https://github.com/kubernetes-sigs/contributor-playground)
-->
&lt;ul>
&lt;li>中文版幻灯片：&lt;a href="https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf">PDF&lt;/a>&lt;/li>
&lt;li>英文版幻灯片：&lt;a href="https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf">PDF&lt;/a> 或 &lt;a href="https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing">带有演讲者笔记的 Google Docs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/contributor-playground">贡献者游乐场&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 文档更新，国际版</title><link>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</link><pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</guid><description>
&lt;!--
---
layout: blog
title: 'Kubernetes Docs Updates, International Edition'
date: 2018-11-08
---
-->
&lt;!-- **Author**: Zach Corleissen (Linux Foundation) -->
&lt;p>&lt;strong>作者&lt;/strong>：Zach Corleissen （Linux 基金会）&lt;/p>
&lt;!-- As a co-chair of SIG Docs, I'm excited to share that Kubernetes docs have a fully mature workflow for localization (l10n). -->
&lt;p>作为文档特别兴趣小组（SIG Docs）的联合主席，我很高兴能与大家分享 Kubernetes 文档在本地化（l10n）方面所拥有的一个完全成熟的工作流。&lt;/p>
&lt;!-- ## Abbreviations galore -->
&lt;h2 id="丰富的缩写">丰富的缩写&lt;/h2>
&lt;!-- L10n is an abbreviation for _localization_. -->
&lt;p>L10n 是 &lt;em>localization&lt;/em> 的缩写。&lt;/p>
&lt;!-- I18n is an abbreviation for _internationalization_. -->
&lt;p>I18n 是 &lt;em>internationalization&lt;/em> 的缩写。&lt;/p>
&lt;!-- I18n is [what you do](https://www.w3.org/International/questions/qa-i18n) to make l10n easier. L10n is a fuller, more comprehensive process than translation (_t9n_). -->
&lt;p>I18n 定义了&lt;a href="https://www.w3.org/International/questions/qa-i18n">做什么&lt;/a> 能让 l10n 更容易。而 L10n 更全面，相比翻译（ &lt;em>t9n&lt;/em> ）具备更完善的流程。&lt;/p>
&lt;!-- ## Why localization matters -->
&lt;h2 id="为什么本地化很重要">为什么本地化很重要&lt;/h2>
&lt;!-- The goal of SIG Docs is to make Kubernetes easier to use for as many people as possible. -->
&lt;p>SIG Docs 的目标是让 Kubernetes 更容易为尽可能多的人使用。&lt;/p>
&lt;!-- One year ago, we looked at whether it was possible to host the output of a Chinese team working independently to translate the Kubernetes docs. After many conversations (including experts on OpenStack l10n), [much transformation](https://kubernetes.io/blog/2018/05/05/hugo-migration/), and [renewed commitment to easier localization](https://github.com/kubernetes/website/pull/10485), we realized that open source documentation is, like open source software, an ongoing exercise at the edges of what's possible. -->
&lt;p>一年前，我们研究了是否有可能由一个独立翻译 Kubernetes 文档的中国团队来主持文档输出。经过多次交谈（包括 OpenStack l10n 的专家），&lt;a href="https://kubernetes.io/blog/2018/05/05/hugo-migration/">多次转变&lt;/a>，以及&lt;a href="https://github.com/kubernetes/website/pull/10485">重新致力于更轻松的本地化&lt;/a>，我们意识到，开源文档就像开源软件一样，是在可能的边缘不断进行实践。&lt;/p>
&lt;!-- Consolidating workflows, language labels, and team-level ownership may seem like simple improvements, but these features make l10n scalable for increasing numbers of l10n teams. While SIG Docs continues to iterate improvements, we've paid off a significant amount of technical debt and streamlined l10n in a single workflow. That's great for the future as well as the present. -->
&lt;p>整合工作流程、语言标签和团队级所有权可能看起来像是十分简单的改进，但是这些功能使 l10n 可以扩展到规模越来越大的 l10n 团队。随着 SIG Docs 不断改进，我们已经在单一工作流程中偿还了大量技术债务并简化了 l10n。这对未来和现在都很有益。&lt;/p>
&lt;!-- ## Consolidated workflow -->
&lt;h2 id="整合的工作流程">整合的工作流程&lt;/h2>
&lt;!-- Localization is now consolidated in the [kubernetes/website](https://github.com/kubernetes/website) repository. We've configured the Kubernetes CI/CD system, [Prow](https://github.com/kubernetes/test-infra/tree/master/prow), to handle automatic language label assignment as well as team-level PR review and approval. -->
&lt;p>现在，本地化已整合到 &lt;a href="https://github.com/kubernetes/website">kubernetes/website&lt;/a> 存储库。我们已经配置了 Kubernetes CI/CD 系统，&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a> 来处理自动语言标签分配以及团队级 PR 审查和批准。&lt;/p>
&lt;!-- ### Language labels -->
&lt;h3 id="语言标签">语言标签&lt;/h3>
&lt;!-- Prow automatically applies language labels based on file path. Thanks to SIG Docs contributor [June Yi](https://github.com/kubernetes/test-infra/pull/9835), folks can also manually assign language labels in pull request (PR) comments. For example, when left as a comment on an issue or PR, this command assigns the label `language/ko` (Korean). -->
&lt;p>Prow 根据文件路径自动添加语言标签。感谢 SIG Docs 贡献者 &lt;a href="https://github.com/kubernetes/test-infra/pull/9835">June Yi&lt;/a>，他让人们还可以在 pull request（PR）注释中手动分配语言标签。例如，当为 issue 或 PR 留下下述注释时，将为之分配标签 &lt;code>language/ko&lt;/code>（Korean）。&lt;/p>
&lt;pre>&lt;code>/language ko
&lt;/code>&lt;/pre>&lt;!-- These repo labels let reviewers filter for PRs and issues by language. For example, you can now filter the k/website dashboard for [PRs with Chinese content](https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh). -->
&lt;p>这些存储库标签允许审阅者按语言过滤 PR 和 issue。例如，您现在可以过滤 kubernetes/website 面板中&lt;a href="https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh">具有中文内容的 PR&lt;/a>。&lt;/p>
&lt;!-- ### Team review -->
&lt;h3 id="团队审核">团队审核&lt;/h3>
&lt;!-- L10n teams can now review and approve their own PRs. For example, review and approval permissions for English are [assigned in an OWNERS file](https://github.com/kubernetes/website/blob/master/content/en/OWNERS) in the top subfolder for English content. -->
&lt;p>L10n 团队现在可以审查和批准他们自己的 PR。例如，英语的审核和批准权限在位于用于显示英语内容的顶级子文件夹中的 &lt;a href="https://github.com/kubernetes/website/blob/master/content/en/OWNERS">OWNERS 文件中指定&lt;/a>。&lt;/p>
&lt;!-- Adding `OWNERS` files to subdirectories lets localization teams review and approve changes without requiring a rubber stamp approval from reviewers who may lack fluency. -->
&lt;p>将 &lt;code>OWNERS&lt;/code> 文件添加到子目录可以让本地化团队审查和批准更改，而无需由可能并不擅长该门语言的审阅者进行批准。&lt;/p>
&lt;!-- ## What's next -->
&lt;h2 id="下一步是什么">下一步是什么&lt;/h2>
&lt;!-- We're looking forward to the [doc sprint in Shanghai](https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required) to serve as a resource for the Chinese l10n team. -->
&lt;p>我们期待着&lt;a href="https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required">上海的 doc sprint&lt;/a> 能作为中国 l10n 团队的资源。&lt;/p>
&lt;!-- We're excited to continue supporting the Japanese and Korean l10n teams, who are making excellent progress. -->
&lt;p>我们很高兴继续支持正在取得良好进展的日本和韩国 l10n 队伍。&lt;/p>
&lt;!-- If you're interested in localizing Kubernetes for your own language or region, check out our [guide to localizing Kubernetes docs](https://kubernetes.io/docs/contribute/localization/) and reach out to a [SIG Docs chair](https://github.com/kubernetes/community/tree/master/sig-docs#leadership) for support. -->
&lt;p>如果您有兴趣将 Kubernetes 本地化为您自己的语言或地区，请查看我们的&lt;a href="https://kubernetes.io/docs/contribute/localization/">本地化 Kubernetes 文档指南&lt;/a>，并联系 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs#leadership">SIG Docs 主席团&lt;/a>获取支持。&lt;/p>
&lt;!-- ### Get involved with SIG Docs -->
&lt;h3 id="加入sig-docs">加入SIG Docs&lt;/h3>
&lt;!-- If you're interested in Kubernetes documentation, come to a SIG Docs [weekly meeting](https://github.com/kubernetes/community/tree/master/sig-docs#meetings), or join [#sig-docs in Kubernetes Slack](https://kubernetes.slack.com/messages/C1J0BPD2M/details/). -->
&lt;p>如果您对 Kubernetes 文档感兴趣，请参加 SIG Docs &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs#meetings">每周会议&lt;/a>，或在 &lt;a href="https://kubernetes.slack.com/messages/C1J0BPD2M/details/">Kubernetes Slack 加入 #sig-docs&lt;/a>。&lt;/p></description></item><item><title>Blog: Kubernetes 2018 年北美贡献者峰会</title><link>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link><pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid><description>
&lt;!--
---
layout: "Blog"
title: "Kubernetes 2018 North American Contributor Summit"
date: 2018-10-16
---
-->
&lt;!--
**Authors:**
-->
&lt;p>&lt;strong>作者：&lt;/strong>&lt;/p>
&lt;!--
[Bob Killen][bob] (University of Michigan)
[Sahdev Zala][sahdev] (IBM),
[Ihor Dvoretskyi][ihor] (CNCF)
-->
&lt;p>&lt;a href="https://twitter.com/mrbobbytables">Bob Killen&lt;/a>（密歇根大学）
&lt;a href="https://twitter.com/sp_zala">Sahdev Zala&lt;/a>（IBM），
&lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>（CNCF）&lt;/p>
&lt;!--
The 2018 North American Kubernetes Contributor Summit to be hosted right before
[KubeCon + CloudNativeCon][kubecon] Seattle is shaping up to be the largest yet.
-->
&lt;p>2018 年北美 Kubernetes 贡献者峰会将在西雅图 &lt;a href="https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/">KubeCon + CloudNativeCon&lt;/a> 会议之前举办，这将是迄今为止规模最大的一次盛会。&lt;/p>
&lt;!--
It is an event that brings together new and current contributors alike to
connect and share face-to-face; and serves as an opportunity for existing
contributors to help shape the future of community development. For new
community members, it offers a welcoming space to learn, explore and put the
contributor workflow to practice.
-->
&lt;p>这是一个将新老贡献者聚集在一起，面对面交流和分享的活动；并为现有的贡献者提供一个机会，帮助塑造社区发展的未来。它为新的社区成员提供了一个学习、探索和实践贡献工作流程的良好空间。&lt;/p>
&lt;!--
Unlike previous Contributor Summits, the event now spans two-days with a more
relaxed ‘hallway’ track and general Contributor get-together to be hosted from
5-8pm on Sunday December 9th at the [Garage Lounge and Gaming Hall][garage], just
a short walk away from the Convention Center. There, contributors can enjoy
billiards, bowling, trivia and more; accompanied by a variety of food and drink.
-->
&lt;p>与之前的贡献者峰会不同，本次活动为期两天，有一个更为轻松的行程安排，一般贡献者将于 12 月 9 日（周日）下午 5 点至 8 点在距离会议中心仅几步远的 &lt;a href="https://www.garagebilliards.com/">Garage Lounge and Gaming Hall&lt;/a> 举办峰会。在那里，贡献者也可以进行台球、保龄球等娱乐活动，而且还有各种食品和饮料。&lt;/p>
&lt;!--
Things pick up the following day, Monday the 10th with three separate tracks:
-->
&lt;p>接下来的一天，也就是 10 号星期一，有三个独立的会议你可以选择参与：&lt;/p>
&lt;!--
### New Contributor Workshop:
A half day workshop aimed at getting new and first time contributors onboarded
and comfortable with working within the Kubernetes Community. Staying for the
duration is required; this is not a workshop you can drop into.
-->
&lt;h3 id="新贡献者研讨会">新贡献者研讨会：&lt;/h3>
&lt;p>为期半天的研讨会旨在让新贡献者加入社区，并营造一个良好的 Kubernetes 社区工作环境。
请在开会期间保持在场，该讨论会不允许随意进出。&lt;/p>
&lt;!--
### Current Contributor Track:
Reserved for those that are actively engaged with the development of the
project; the Current Contributor Track includes Talks, Workshops, Birds of a
Feather, Unconferences, Steering Committee Sessions, and more! Keep an eye on
the [schedule in GitHub][schedule] as content is frequently being updated.
-->
&lt;h3 id="当前贡献者追踪">当前贡献者追踪：&lt;/h3>
&lt;p>保留给那些积极参与项目开发的贡献者；目前的贡献者追踪包括讲座、研讨会、聚会、Unconferences 会议、指导委员会会议等等!
请留意 &lt;a href="https://git.k8s.io/community/events/2018/12-contributor-summit#agenda">GitHub 中的时间表&lt;/a>，因为内容经常更新。&lt;/p>
&lt;!--
### Docs Sprint:
SIG-Docs will have a curated list of issues and challenges to be tackled closer
to the event date.
-->
&lt;h3 id="docs-冲刺">Docs 冲刺：&lt;/h3>
&lt;p>SIG-Docs 将在活动日期临近的时候列出一个需要处理的问题和挑战列表。&lt;/p>
&lt;!--
## To Register:
To register for the Contributor Summit, see the [Registration section of the
Event Details in GitHub][register]. Please note that registrations are being
reviewed. If you select the “Current Contributor Track” and are not an active
contributor, you will be asked to attend the New Contributor Workshop, or asked
to be put on a waitlist. With thousands of contributors and only 300 spots, we
need to make sure the right folks are in the room.
-->
&lt;h2 id="注册">注册：&lt;/h2>
&lt;p>要注册贡献者峰会，请参阅 Git Hub 上的&lt;a href="https://git.k8s.io/community/events/2018/12-contributor-summit#registration">活动详情注册部分&lt;/a>。请注意报名正在审核中。
如果您选择了 “当前贡献者追踪”，而您却不是一个活跃的贡献者，您将被要求参加新贡献者研讨会，或者被要求进入候补名单。
成千上万的贡献者只有 300 个位置，我们需要确保正确的人被安排席位。&lt;/p>
&lt;!--
If you have any questions or concerns, please don’t hesitate to reach out to
the Contributor Summit Events Team at community@kubernetes.io.
-->
&lt;p>如果您有任何问题或疑虑，请随时通过 &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a> 联系贡献者峰会组织团队。&lt;/p>
&lt;!--
Look forward to seeing everyone there!
-->
&lt;p>期待在那里看到每个人！&lt;/p></description></item><item><title>Blog: 2018 年督导委员会选举结果</title><link>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</link><pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</guid><description>
&lt;!--
---
layout: blog
title: '2018 Steering Committee Election Results'
date: 2018-10-15
---
-->
&lt;!-- **Authors**: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google) -->
&lt;p>&lt;strong>作者&lt;/strong>: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google)&lt;/p>
&lt;!--
## Results
-->
&lt;h2 id="结果">结果&lt;/h2>
&lt;!--
The [Kubernetes Steering Committee Election](https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/) is now complete and the following candidates came ahead to secure two year terms that start immediately:
-->
&lt;p>&lt;a href="https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/">Kubernetes 督导委员会选举&lt;/a>现已完成，以下候选人获得了立即开始的两年任期：&lt;/p>
&lt;ul>
&lt;li>Aaron Crickenberger, Google, &lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>&lt;/li>
&lt;li>Davanum Srinivas, Huawei, &lt;a href="https://github.com/dims">@dims&lt;/a>&lt;/li>
&lt;li>Tim St. Clair, Heptio, &lt;a href="https://github.com/timothysc">@timothysc&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Big Thanks!
-->
&lt;h2 id="十分感谢">十分感谢！&lt;/h2>
&lt;!--
* Steering Committee Member Emeritus [Quinton Hoole](https://github.com/quinton-hoole) for his service to the community over the past year. We look forward to
* The candidates that came forward to run for election. May we always have a strong set of people who want to push community forward like yours in every election.
* All 307 voters who cast a ballot.
* And last but not least...Cornell University for hosting [CIVS](https://civs.cs.cornell.edu/)!
-->
&lt;ul>
&lt;li>督导委员会荣誉退休成员 &lt;a href="https://github.com/quinton-hoole">Quinton Hoole&lt;/a>，表扬他在过去一年为社区所作的贡献。我们期待着&lt;/li>
&lt;li>参加竞选的候选人。愿我们永远拥有一群强大的人，他们希望在每一次选举中都能像你们一样推动社区向前发展。&lt;/li>
&lt;li>共计 307 名选民参与投票。&lt;/li>
&lt;li>本次选举由康奈尔大学主办 &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>！&lt;/li>
&lt;/ul>
&lt;!--
## Get Involved with the Steering Committee
-->
&lt;h2 id="加入督导委员会">加入督导委员会&lt;/h2>
&lt;!--
You can follow along to Steering Committee [backlog items](https://git.k8s.io/steering/backlog.md) and weigh in by filing an issue or creating a PR against their [repo](https://github.com/kubernetes/steering). They meet bi-weekly on [Wednesdays at 8pm UTC](https://github.com/kubernetes/steering) and regularly attend Meet Our Contributors.
-->
&lt;p>你可以关注督导委员会的&lt;a href="https://git.k8s.io/steering/backlog.md">任务清单&lt;/a>，并通过向他们的&lt;a href="https://github.com/kubernetes/steering">代码仓库&lt;/a>提交 issue 或 PR 的方式来参与。他们也会在&lt;a href="https://github.com/kubernetes/steering">UTC 时间每周三晚 8 点&lt;/a>举行会议，并定期与我们的贡献者见面。&lt;/p>
&lt;!--
Steering Committee Meetings:
-->
&lt;p>督导委员会会议：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube 播放列表&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Meet Our Contributors Steering AMA’s:
-->
&lt;p>与我们的贡献者会面：&lt;/p>
&lt;!--
* [Oct 3 2018](https://youtu.be/x6Jm8p0K-IQ)
* [Sept 5 2018](https://youtu.be/UbxWV12Or58)
-->
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/x6Jm8p0K-IQ">2018 年 10 月 3 日&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/UbxWV12Or58">2018 年 7 月 5 日&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 中的拓扑感知数据卷供应</title><link>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</link><pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</guid><description>
&lt;!--
---
layout: blog
title: 'Topology-Aware Volume Provisioning in Kubernetes'
date: 2018-10-11
---
-->
&lt;!--
**Author**: Michelle Au (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Michelle Au（谷歌）&lt;/p>
&lt;!--
The multi-zone cluster experience with persistent volumes is improving in Kubernetes 1.12 with the topology-aware dynamic provisioning beta feature. This feature allows Kubernetes to make intelligent decisions when dynamically provisioning volumes by getting scheduler input on the best place to provision a volume for a pod. In multi-zone clusters, this means that volumes will get provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance.
-->
&lt;p>通过提供拓扑感知动态卷供应功能，具有持久卷的多区域集群体验在 Kubernetes 1.12 中得到了改进。此功能使得 Kubernetes 在动态供应卷时能做出明智的决策，方法是从调度器获得为 Pod 提供数据卷的最佳位置。在多区域集群环境，这意味着数据卷能够在满足你的 Pod 运行需要的合适的区域被供应，从而允许您跨故障域轻松部署和扩展有状态工作负载，从而提供高可用性和容错能力。&lt;/p>
&lt;!--
## Previous challenges
-->
&lt;h2 id="以前的挑战">以前的挑战&lt;/h2>
&lt;!--
Before this feature, running stateful workloads with zonal persistent disks (such as AWS ElasticBlockStore, Azure Disk, GCE PersistentDisk) in multi-zone clusters had many challenges. Dynamic provisioning was handled independently from pod scheduling, which meant that as soon as you created a PersistentVolumeClaim (PVC), a volume would get provisioned. This meant that the provisioner had no knowledge of what pods were using the volume, and any pod constraints it had that could impact scheduling.
-->
&lt;p>在此功能被提供之前，在多区域集群中使用区域化的持久磁盘（例如 AWS ElasticBlockStore，Azure Disk，GCE PersistentDisk）运行有状态工作负载存在许多挑战。动态供应独立于 Pod 调度处理，这意味着只要您创建了一个 PersistentVolumeClaim（PVC），一个卷就会被供应。这也意味着供应者不知道哪些 Pod 正在使用该卷，也不清楚任何可能影响调度的 Pod 约束。&lt;/p>
&lt;!--
This resulted in unschedulable pods because volumes were provisioned in zones that:
-->
&lt;p>这导致了不可调度的 Pod，因为在以下区域中配置了卷：&lt;/p>
&lt;!--
* did not have enough CPU or memory resources to run the pod
* conflicted with node selectors, pod affinity or anti-affinity policies
* could not run the pod due to taints
-->
&lt;ul>
&lt;li>没有足够的 CPU 或内存资源来运行 Pod&lt;/li>
&lt;li>与节点选择器、Pod 亲和或反亲和策略冲突&lt;/li>
&lt;li>由于污点（taint）不能运行 Pod&lt;/li>
&lt;/ul>
&lt;!--
Another common issue was that a non-StatefulSet pod using multiple persistent volumes could have each volume provisioned in a different zone, again resulting in an unschedulable pod.
-->
&lt;p>另一个常见问题是，使用多个持久卷的非有状态 Pod 可能会在不同的区域中配置每个卷，从而导致一个不可调度的 Pod。&lt;/p>
&lt;!--
Suboptimal workarounds included overprovisioning of nodes, or manual creation of volumes in the correct zones, making it difficult to dynamically deploy and scale stateful workloads.
-->
&lt;p>次优的解决方法包括节点超配，或在正确的区域中手动创建卷，但这会造成难以动态部署和扩展有状态工作负载的问题。&lt;/p>
&lt;!--
The topology-aware dynamic provisioning feature addresses all of the above issues.
-->
&lt;p>拓扑感知动态供应功能解决了上述所有问题。&lt;/p>
&lt;!--
## Supported Volume Types
-->
&lt;h2 id="支持的卷类型">支持的卷类型&lt;/h2>
&lt;!--
In 1.12, the following drivers support topology-aware dynamic provisioning:
-->
&lt;p>在 1.12 中，以下驱动程序支持拓扑感知动态供应：&lt;/p>
&lt;!--
* AWS EBS
* Azure Disk
* GCE PD (including Regional PD)
* CSI (alpha) - currently only the GCE PD CSI driver has implemented topology support
-->
&lt;ul>
&lt;li>AWS EBS&lt;/li>
&lt;li>Azure Disk&lt;/li>
&lt;li>GCE PD （包括 Regional PD）&lt;/li>
&lt;li>CSI（alpha） - 目前只有 GCE PD CSI 驱动实现了拓扑支持&lt;/li>
&lt;/ul>
&lt;!--
## Design Principles
-->
&lt;h2 id="设计原则">设计原则&lt;/h2>
&lt;!--
While the initial set of supported plugins are all zonal-based, we designed this feature to adhere to the Kubernetes principle of portability across environments. Topology specification is generalized and uses a similar label-based specification like in Pod nodeSelectors and nodeAffinity. This mechanism allows you to define your own topology boundaries, such as racks in on-premise clusters, without requiring modifications to the scheduler to understand these custom topologies.
-->
&lt;p>虽然最初支持的插件集都是基于区域的，但我们设计此功能时遵循 Kubernetes 跨环境可移植性的原则。
拓扑规范是通用的，并使用类似于基于标签的规范，如 Pod nodeSelectors 和 nodeAffinity。
该机制允许您定义自己的拓扑边界，例如内部部署集群中的机架，而无需修改调度程序以了解这些自定义拓扑。&lt;/p>
&lt;!--
In addition, the topology information is abstracted away from the pod specification, so a pod does not need knowledge of the underlying storage system’s topology characteristics. This means that you can use the same pod specification across multiple clusters, environments, and storage systems.
-->
&lt;p>此外，拓扑信息是从 Pod 规范中抽象出来的，因此 Pod 不需要了解底层存储系统的拓扑特征。
这意味着您可以在多个集群、环境和存储系统中使用相同的 Pod 规范。&lt;/p>
&lt;!--
## Getting Started
-->
&lt;h2 id="入门">入门&lt;/h2>
&lt;!--
To enable this feature, all you need to do is to create a StorageClass with `volumeBindingMode` set to `WaitForFirstConsumer`:
-->
&lt;p>要启用此功能，您需要做的就是创建一个将 &lt;code>volumeBindingMode&lt;/code> 设置为 &lt;code>WaitForFirstConsumer&lt;/code> 的 StorageClass：&lt;/p>
&lt;pre>&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: topology-aware-standard
provisioner: kubernetes.io/gce-pd
volumeBindingMode: WaitForFirstConsumer
parameters:
type: pd-standard
&lt;/code>&lt;/pre>&lt;!--
This new setting instructs the volume provisioner to not create a volume immediately, and instead, wait for a pod using an associated PVC to run through scheduling. Note that previous StorageClass `zone` and `zones` parameters do not need to be specified anymore, as pod policies now drive the decision of which zone to provision a volume in.
-->
&lt;p>这个新设置表明卷配置器不立即创建卷，而是等待使用关联的 PVC 的 Pod 通过调度运行。
请注意，不再需要指定以前的 StorageClass &lt;code>zone&lt;/code> 和 &lt;code>zones&lt;/code> 参数，因为现在在哪个区域中配置卷由 Pod 策略决定。&lt;/p>
&lt;!--
Next, create a pod and PVC with this StorageClass. This sequence is the same as before, but with a different StorageClass specified in the PVC. The following is a hypothetical example, demonstrating the capabilities of the new feature by specifying many pod constraints and scheduling policies:
-->
&lt;p>接下来，使用此 StorageClass 创建一个 Pod 和 PVC。
此过程与之前相同，但在 PVC 中指定了不同的 StorageClass。
以下是一个假设示例，通过指定许多 Pod 约束和调度策略来演示新功能特性：&lt;/p>
&lt;!--
* multiple PVCs in a pod
* nodeAffinity across a subset of zones
* pod anti-affinity on zones
-->
&lt;ul>
&lt;li>一个 Pod 多个 PVC&lt;/li>
&lt;li>跨子区域的节点亲和&lt;/li>
&lt;li>同一区域 Pod 反亲和&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: StatefulSet
metadata:
name: web
spec:
serviceName: &amp;quot;nginx&amp;quot;
replicas: 2
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
affinity:
nodeAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
nodeSelectorTerms:
- matchExpressions:
- key: failure-domain.beta.kubernetes.io/zone
operator: In
values:
- us-central1-a
- us-central1-f
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: app
operator: In
values:
- nginx
topologyKey: failure-domain.beta.kubernetes.io/zone
containers:
- name: nginx
image: gcr.io/google_containers/nginx-slim:0.8
ports:
- containerPort: 80
name: web
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
- name: logs
mountPath: /logs
volumeClaimTemplates:
- metadata:
name: www
spec:
accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
storageClassName: topology-aware-standard
resources:
requests:
storage: 10Gi
- metadata:
name: logs
spec:
accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
storageClassName: topology-aware-standard
resources:
requests:
storage: 1Gi
&lt;/code>&lt;/pre>&lt;!--
Afterwards, you can see that the volumes were provisioned in zones according to the policies set by the pod:
-->
&lt;p>之后，您可以看到根据 Pod 设置的策略在区域中配置卷：&lt;/p>
&lt;pre>&lt;code>$ kubectl get pv -o=jsonpath='{range .items[*]}{.spec.claimRef.name}{&amp;quot;\t&amp;quot;}{.metadata.labels.failure\-domain\.beta\.kubernetes\.io/zone}{&amp;quot;\n&amp;quot;}{end}'
www-web-0 us-central1-f
logs-web-0 us-central1-f
www-web-1 us-central1-a
logs-web-1 us-central1-a
&lt;/code>&lt;/pre>&lt;!--
## How can I learn more?
-->
&lt;h2 id="我怎样才能了解更多">我怎样才能了解更多？&lt;/h2>
&lt;!--
Official documentation on the topology-aware dynamic provisioning feature is available here:https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode
-->
&lt;p>有关拓扑感知动态供应功能的官方文档可在此处获取：https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&lt;/p>
&lt;!--
Documentation for CSI drivers is available at https://kubernetes-csi.github.io/docs/
-->
&lt;p>有关 CSI 驱动程序的文档，请访问：https://kubernetes-csi.github.io/docs/&lt;/p>
&lt;!--
## What’s next?
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;!--
We are actively working on improving this feature to support:
-->
&lt;p>我们正积极致力于改进此功能以支持：&lt;/p>
&lt;!--
* more volume types, including dynamic provisioning for local volumes
* dynamic volume attachable count and capacity limits per node
-->
&lt;ul>
&lt;li>更多卷类型，包括本地卷的动态供应&lt;/li>
&lt;li>动态容量可附加计数和每个节点的容量限制&lt;/li>
&lt;/ul>
&lt;!--
## How do I get involved?
-->
&lt;h2 id="我如何参与">我如何参与？&lt;/h2>
&lt;!--
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Kubernetes Storage Special-Interest-Group](https://github.com/kubernetes/community/tree/master/sig-storage) (SIG). We’re rapidly growing and always welcome new contributors.
-->
&lt;p>如果您对此功能有反馈意见或有兴趣参与设计和开发，请加入 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes 存储特别兴趣小组&lt;/a>（SIG）。我们正在快速成长，并始终欢迎新的贡献者。&lt;/p>
&lt;!--
Special thanks to all the contributors that helped bring this feature to beta, including Cheng Xing ([verult](https://github.com/verult)), Chuqiang Li ([lichuqiang](https://github.com/lichuqiang)), David Zhu ([davidz627](https://github.com/davidz627)), Deep Debroy ([ddebroy](https://github.com/ddebroy)), Jan Šafránek ([jsafrane](https://github.com/jsafrane)), Jordan Liggitt ([liggitt](https://github.com/liggitt)), Michelle Au ([msau42](https://github.com/msau42)), Pengfei Ni ([feiskyer](https://github.com/feiskyer)), Saad Ali ([saad-ali](https://github.com/saad-ali)), Tim Hockin ([thockin](https://github.com/thockin)), and Yecheng Fu ([cofyc](https://github.com/cofyc)).
-->
&lt;p>特别感谢帮助推出此功能的所有贡献者，包括 Cheng Xing (&lt;a href="https://github.com/verult">verult&lt;/a>)、Chuqiang Li (&lt;a href="https://github.com/lichuqiang">lichuqiang&lt;/a>)、David Zhu (&lt;a href="https://github.com/davidz627">davidz627&lt;/a>)、Deep Debroy (&lt;a href="https://github.com/ddebroy">ddebroy&lt;/a>)、Jan Šafránek (&lt;a href="https://github.com/jsafrane">jsafrane&lt;/a>)、Jordan Liggitt (&lt;a href="https://github.com/liggitt">liggitt&lt;/a>)、Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>)、Pengfei Ni (&lt;a href="https://github.com/feiskyer">feiskyer&lt;/a>)、Saad Ali (&lt;a href="https://github.com/saad-ali">saad-ali&lt;/a>)、Tim Hockin (&lt;a href="https://github.com/thockin">thockin&lt;/a>)，以及 Yecheng Fu (&lt;a href="https://github.com/cofyc">cofyc&lt;/a>)。&lt;/p></description></item><item><title>Blog: Kubernetes v1.12: RuntimeClass 简介</title><link>https://kubernetes.io/zh/blog/2018/10/10/kubernetes-v1.12-runtimeclass-%E7%AE%80%E4%BB%8B/</link><pubDate>Wed, 10 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/10/kubernetes-v1.12-runtimeclass-%E7%AE%80%E4%BB%8B/</guid><description>
&lt;!--
---
layout: blog
title: 'Kubernetes v1.12: Introducing RuntimeClass'
date: 2018-10-10
---
-->
&lt;!--
**Author**: Tim Allclair (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Tim Allclair (Google)&lt;/p>
&lt;!--
Kubernetes originally launched with support for Docker containers running native applications on a Linux host. Starting with [rkt](https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/) in Kubernetes 1.3 more runtimes were coming, which lead to the development of the [Container Runtime Interface](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) (CRI). Since then, the set of alternative runtimes has only expanded: projects like [Kata Containers](https://katacontainers.io/) and [gVisor](https://github.com/google/gvisor) were announced for stronger workload isolation, and Kubernetes' Windows support has been [steadily progressing](https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/).
-->
&lt;p>Kubernetes 最初是为了支持在 Linux 主机上运行本机应用程序的 Docker 容器而创建的。
从 Kubernetes 1.3中的 &lt;a href="https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/">rkt&lt;/a> 开始，更多的运行时间开始涌现，
这导致了&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">容器运行时接口（Container Runtime Interface）&lt;/a>（CRI）的开发。
从那时起，备用运行时集合越来越大：
为了加强工作负载隔离，&lt;a href="https://katacontainers.io/">Kata Containers&lt;/a> 和 &lt;a href="https://github.com/google/gvisor">gVisor&lt;/a> 等项目被发起，
并且 Kubernetes 对 Windows 的支持正在 &lt;a href="https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/">稳步发展&lt;/a> 。&lt;/p>
&lt;!--
With runtimes targeting so many different use cases, a clear need for mixed runtimes in a cluster arose. But all these different ways of running containers have brought a new set of problems to deal with:
-->
&lt;p>由于存在诸多针对不同用例的运行时，集群对混合运行时的需求变得明晰起来。
但是，所有这些不同的容器运行方式都带来了一系列新问题要处理：&lt;/p>
&lt;!--
- How do users know which runtimes are available, and select the runtime for their workloads?
- How do we ensure pods are scheduled to the nodes that support the desired runtime?
- Which runtimes support which features, and how can we surface incompatibilities to the user?
- How do we account for the varying resource overheads of the runtimes?
-->
&lt;ul>
&lt;li>用户如何知道哪些运行时可用，并为其工作负荷选择运行时？&lt;/li>
&lt;li>我们如何确保将 Pod 被调度到支持所需运行时的节点上？&lt;/li>
&lt;li>哪些运行时支持哪些功能，以及我们如何向用户显示不兼容性？&lt;/li>
&lt;li>我们如何考虑运行时的各种资源开销？&lt;/li>
&lt;/ul>
&lt;!--
**RuntimeClass** aims to solve these issues.
-->
&lt;p>&lt;strong>RuntimeClass&lt;/strong> 旨在解决这些问题。&lt;/p>
&lt;!--
## RuntimeClass in Kubernetes 1.12
-->
&lt;h2 id="kubernetes-1-12-中的-runtimeclass">Kubernetes 1.12 中的 RuntimeClass&lt;/h2>
&lt;!--
RuntimeClass was recently introduced as an alpha feature in Kubernetes 1.12. The initial implementation focuses on providing a runtime selection API, and paves the way to address the other open problems.
-->
&lt;p>最近，RuntimeClass 在 Kubernetes 1.12 中作为 alpha 功能引入。
最初的实现侧重于提供运行时选择 API ，并为解决其他未解决的问题铺平道路。&lt;/p>
&lt;!--
The RuntimeClass resource represents a container runtime supported in a Kubernetes cluster. The cluster provisioner sets up, configures, and defines the concrete runtimes backing the RuntimeClass. In its current form, a RuntimeClassSpec holds a single field, the **RuntimeHandler**. The RuntimeHandler is interpreted by the CRI implementation running on a node, and mapped to the actual runtime configuration. Meanwhile the PodSpec has been expanded with a new field, **RuntimeClassName**, which names the RuntimeClass that should be used to run the pod.
-->
&lt;p>RuntimeClass 资源代表 Kubernetes 集群中支持的容器运行时。
集群制备组件安装、配置和定义支持 RuntimeClass 的具体运行时。
在 RuntimeClassSpec 的当前形式中，只有一个字段，即 &lt;strong>RuntimeHandler&lt;/strong>。
RuntimeHandler 由在节点上运行的 CRI 实现解释，并映射到实际的运行时配置。
同时，PodSpec 被扩展添加了一个新字段 &lt;strong>RuntimeClassName&lt;/strong>，命名应该用于运行 Pod 的 RuntimeClass。&lt;/p>
&lt;!--
Why is RuntimeClass a pod level concept? The Kubernetes resource model expects certain resources to be shareable between containers in the pod. If the pod is made up of different containers with potentially different resource models, supporting the necessary level of resource sharing becomes very challenging. For example, it is extremely difficult to support a loopback (localhost) interface across a VM boundary, but this is a common model for communication between two containers in a pod.
-->
&lt;p>为什么 RuntimeClass 是 Pod 级别的概念？
Kubernetes 资源模型期望 Pod 中的容器之间可以共享某些资源。
如果 Pod 由具有不同资源模型的不同容器组成，支持必要水平的资源共享变得非常具有挑战性。
例如，要跨 VM 边界支持本地回路（localhost）接口非常困难，但这是 Pod 中两个容器之间通信的通用模型。&lt;/p>
&lt;!--
## What's next?
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;!--
The RuntimeClass resource is an important foundation for surfacing runtime properties to the control plane. For example, to implement scheduler support for clusters with heterogeneous nodes supporting different runtimes, we might add [NodeAffinity](/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity) terms to the RuntimeClass definition. Another area to address is managing the variable resource requirements to run pods of different runtimes. The [Pod Overhead proposal](https://docs.google.com/document/d/1EJKT4gyl58-kzt2bnwkv08MIUZ6lkDpXcxkHqCvvAp4/preview) was an early take on this that aligns nicely with the RuntimeClass design, and may be pursued further.
-->
&lt;p>RuntimeClass 资源是将运行时属性显示到控制平面的重要基础。
例如，要对具有支持不同运行时间的异构节点的群集实施调度程序支持，我们可以在 RuntimeClass 定义中添加
&lt;a href="https://kubernetes.io/zh/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">NodeAffinity&lt;/a>条件。
另一个需要解决的领域是管理可变资源需求以运行不同运行时的 Pod。
&lt;a href="https://docs.google.com/document/d/1EJKT4gyl58-kzt2bnwkv08MIUZ6lkDpXcxkHqCvvAp4/preview">Pod Overhead 提案&lt;/a>
是一项较早的尝试，与 RuntimeClass 设计非常吻合，并且可能会进一步推广。&lt;/p>
&lt;!--
Many other RuntimeClass extensions have also been proposed, and will be revisited as the feature continues to develop and mature. A few more extensions that are being considered include:
-->
&lt;p>人们还提出了许多其他 RuntimeClass 扩展，随着功能的不断发展和成熟，我们会重新讨论这些提议。
正在考虑的其他扩展包括：&lt;/p>
&lt;!--
- Surfacing optional features supported by runtimes, and better visibility into errors caused by incompatible features.
- Automatic runtime or feature discovery, to support scheduling decisions without manual configuration.
- Standardized or conformant RuntimeClass names that define a set of properties that should be supported across clusters with RuntimeClasses of the same name.
- Dynamic registration of additional runtimes, so users can install new runtimes on existing clusters with no downtime.
- "Fitting" a RuntimeClass to a pod's requirements. For instance, specifying runtime properties and letting the system match an appropriate RuntimeClass, rather than explicitly assigning a RuntimeClass by name.
-->
&lt;ul>
&lt;li>提供运行时支持的可选功能，并更好地查看由不兼容功能导致的错误。&lt;/li>
&lt;li>自动运行时或功能发现，支持无需手动配置的调度决策。&lt;/li>
&lt;li>标准化或一致的 RuntimeClass 名称，用于定义一组具有相同名称的 RuntimeClass 的集群应支持的属性。&lt;/li>
&lt;li>动态注册附加的运行时，因此用户可以在不停机的情况下在现有群集上安装新的运行时。&lt;/li>
&lt;li>根据 Pod 的要求“匹配” RuntimeClass。
例如，指定运行时属性并使系统与适当的 RuntimeClass 匹配，而不是通过名称显式分配 RuntimeClass。&lt;/li>
&lt;/ul>
&lt;!--
RuntimeClass will be under active development at least through 2019, and we’re excited to see the feature take shape, starting with the RuntimeClass alpha in Kubernetes 1.12.
-->
&lt;p>至少要到2019年，RuntimeClass 才会得到积极的开发，我们很高兴看到从 Kubernetes 1.12 中的 RuntimeClass alpha 开始，此功能得以形成。&lt;/p>
&lt;!--
## Learn More
-->
&lt;h2 id="学到更多">学到更多&lt;/h2>
&lt;!--
- Take it for a spin! As an alpha feature, there are some additional setup steps to use RuntimeClass. Refer to the [RuntimeClass documentation](/docs/concepts/containers/runtime-class/#runtime-class) for how to get it running.
- Check out the [RuntimeClass Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md) for more nitty-gritty design details.
- The [Sandbox Isolation Level Decision](https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview) documents the thought process that initially went into making RuntimeClass a pod-level choice.
- Join the discussions and help shape the future of RuntimeClass with the [SIG-Node community](https://github.com/kubernetes/community/tree/master/sig-node)
-->
&lt;ul>
&lt;li>试试吧！ 作为Alpha功能，还有一些其他设置步骤可以使用RuntimeClass。
有关如何使其运行，请参考 &lt;a href="https://kubernetes.io/zh/docs/concepts/containers/runtime-class/#runtime-class">RuntimeClass文档&lt;/a> 。&lt;/li>
&lt;li>查看 &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md">RuntimeClass Kubernetes 增强建议&lt;/a> 以获取更多细节设计细节。&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview">沙盒隔离级别决策&lt;/a>
记录了最初使 RuntimeClass 成为 Pod 级别选项的思考过程。&lt;/li>
&lt;li>加入讨论，并通过 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG-Node社区&lt;/a> 帮助塑造 RuntimeClass 的未来。&lt;/li>
&lt;/ul></description></item><item><title>Blog: KubeDirector：在 Kubernetes 上运行复杂状态应用程序的简单方法</title><link>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</guid><description>
&lt;!--
layout: blog
title: 'KubeDirector: The easy way to run complex stateful applications on Kubernetes'
date: 2018-10-03
-->
&lt;!--
**Author**: Thomas Phelan (BlueData)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Thomas Phelan（BlueData）&lt;/p>
&lt;!--
KubeDirector is an open source project designed to make it easy to run complex stateful scale-out application clusters on Kubernetes. KubeDirector is built using the custom resource definition (CRD) framework and leverages the native Kubernetes API extensions and design philosophy. This enables transparent integration with Kubernetes user/resource management as well as existing clients and tools.
-->
&lt;p>KubeDirector 是一个开源项目，旨在简化在 Kubernetes 上运行复杂的有状态扩展应用程序集群。KubeDirector 使用自定义资源定义（CRD）
框架构建，并利用了本地 Kubernetes API 扩展和设计哲学。这支持与 Kubernetes 用户/资源 管理以及现有客户端和工具的透明集成。&lt;/p>
&lt;!--
We recently [introduced the KubeDirector project](https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/), as part of a broader open source Kubernetes initiative we call BlueK8s. I’m happy to announce that the pre-alpha
code for [KubeDirector](https://github.com/bluek8s/kubedirector/) is now available. And in this blog post, I’ll show how it works.
-->
&lt;p>我们最近&lt;a href="https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/">介绍了 KubeDirector 项目&lt;/a>，作为我们称为 BlueK8s 的更广泛的 Kubernetes 开源项目的一部分。我很高兴地宣布 &lt;a href="https://github.com/bluek8s/kubedirector/">KubeDirector&lt;/a> 的
pre-alpha 代码现在已经可用。在这篇博客文章中，我将展示它是如何工作的。&lt;/p>
&lt;!--
KubeDirector provides the following capabilities:
-->
&lt;p>KubeDirector 提供以下功能：&lt;/p>
&lt;!--
* The ability to run non-cloud native stateful applications on Kubernetes without modifying the code. In other words, it’s not necessary to decompose these existing applications to fit a microservices design pattern.
* Native support for preserving application-specific configuration and state.
* An application-agnostic deployment pattern, minimizing the time to onboard new stateful applications to Kubernetes.
-->
&lt;ul>
&lt;li>无需修改代码即可在 Kubernetes 上运行非云原生有状态应用程序。换句话说，不需要分解这些现有的应用程序来适应微服务设计模式。&lt;/li>
&lt;li>本机支持保存特定于应用程序的配置和状态。&lt;/li>
&lt;li>与应用程序无关的部署模式，最大限度地减少将新的有状态应用程序装载到 Kubernetes 的时间。&lt;/li>
&lt;/ul>
&lt;!--
KubeDirector enables data scientists familiar with data-intensive distributed applications such as Hadoop, Spark, Cassandra, TensorFlow, Caffe2, etc. to run these applications on Kubernetes -- with a minimal learning curve and no need to write GO code. The applications controlled by KubeDirector are defined by some basic metadata and an associated package of configuration artifacts. The application metadata is referred to as a KubeDirectorApp resource.
-->
&lt;p>KubeDirector 使熟悉数据密集型分布式应用程序（如 Hadoop、Spark、Cassandra、TensorFlow、Caffe2 等）的数据科学家能够在 Kubernetes 上运行这些应用程序 -- 只需极少的学习曲线，无需编写 GO 代码。由 KubeDirector 控制的应用程序由一些基本元数据和相关的配置工件包定义。应用程序元数据称为 KubeDirectorApp 资源。&lt;/p>
&lt;!--
To understand the components of KubeDirector, clone the repository on [GitHub](https://github.com/bluek8s/kubedirector/) using a command similar to:
-->
&lt;p>要了解 KubeDirector 的组件，请使用类似于以下的命令在 &lt;a href="https://github.com/bluek8s/kubedirector/">GitHub&lt;/a> 上克隆存储库：&lt;/p>
&lt;pre>&lt;code>git clone http://&amp;lt;userid&amp;gt;@github.com/bluek8s/kubedirector.
&lt;/code>&lt;/pre>&lt;!--
The KubeDirectorApp definition for the Spark 2.2.1 application is located
in the file `kubedirector/deploy/example_catalog/cr-app-spark221e2.json`.
-->
&lt;p>Spark 2.2.1 应用程序的 KubeDirectorApp 定义位于文件 &lt;code>kubedirector/deploy/example_catalog/cr-app-spark221e2.json&lt;/code> 中。&lt;/p>
&lt;pre>&lt;code>~&amp;gt; cat kubedirector/deploy/example_catalog/cr-app-spark221e2.json
{
&amp;quot;apiVersion&amp;quot;: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;,
&amp;quot;kind&amp;quot;: &amp;quot;KubeDirectorApp&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;name&amp;quot; : &amp;quot;spark221e2&amp;quot;
},
&amp;quot;spec&amp;quot; : {
&amp;quot;systemctlMounts&amp;quot;: true,
&amp;quot;config&amp;quot;: {
&amp;quot;node_services&amp;quot;: [
{
&amp;quot;service_ids&amp;quot;: [
&amp;quot;ssh&amp;quot;,
&amp;quot;spark&amp;quot;,
&amp;quot;spark_master&amp;quot;,
&amp;quot;spark_worker&amp;quot;
],
…
&lt;/code>&lt;/pre>&lt;!--
The configuration of an application cluster is referred to as a KubeDirectorCluster resource. The
KubeDirectorCluster definition for a sample Spark 2.2.1 cluster is located in the file
`kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml`.
-->
&lt;p>应用程序集群的配置称为 KubeDirectorCluster 资源。示例 Spark 2.2.1 集群的 KubeDirectorCluster 定义位于文件
&lt;code>kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml&lt;/code> 中。&lt;/p>
&lt;pre>&lt;code>~&amp;gt; cat kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml
apiVersion: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;
kind: &amp;quot;KubeDirectorCluster&amp;quot;
metadata:
name: &amp;quot;spark221e2&amp;quot;
spec:
app: spark221e2
roles:
- name: controller
replicas: 1
resources:
requests:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
limits:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
- name: worker
replicas: 2
resources:
requests:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
limits:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
- name: jupyter
…
&lt;/code>&lt;/pre>&lt;!--
## Running Spark on Kubernetes with KubeDirector
-->
&lt;h2 id="使用-kubedirector-在-kubernetes-上运行-spark">使用 KubeDirector 在 Kubernetes 上运行 Spark&lt;/h2>
&lt;!--
With KubeDirector, it’s easy to run Spark clusters on Kubernetes.
-->
&lt;p>使用 KubeDirector，可以轻松在 Kubernetes 上运行 Spark 集群。&lt;/p>
&lt;!--
First, verify that Kubernetes (version 1.9 or later) is running, using the command `kubectl version`
-->
&lt;p>首先，使用命令 &lt;code>kubectl version&lt;/code> 验证 Kubernetes（版本 1.9 或更高）是否正在运行&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T18:02:47Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T17:53:03Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code>&lt;/pre>&lt;!--
Deploy the KubeDirector service and the example KubeDirectorApp resource definitions with the commands:
-->
&lt;p>使用以下命令部署 KubeDirector 服务和示例 KubeDirectorApp 资源定义：&lt;/p>
&lt;pre>&lt;code>cd kubedirector
make deploy
&lt;/code>&lt;/pre>&lt;!--
These will start the KubeDirector pod:
-->
&lt;p>这些将启动 KubeDirector pod：&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
kubedirector-58cf59869-qd9hb 1/1 Running 0 1m
&lt;/code>&lt;/pre>&lt;!--
List the installed KubeDirector applications with `kubectl get KubeDirectorApp`
-->
&lt;p>&lt;code>kubectl get KubeDirectorApp&lt;/code> 列出中已安装的 KubeDirector 应用程序&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get KubeDirectorApp
NAME AGE
cassandra311 30m
spark211up 30m
spark221e2 30m
&lt;/code>&lt;/pre>&lt;!--
Now you can launch a Spark 2.2.1 cluster using the example KubeDirectorCluster file and the
`kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml` command.
Verify that the Spark cluster has been started:
-->
&lt;p>现在，您可以使用示例 KubeDirectorCluster 文件和 &lt;code>kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml&lt;/code> 命令
启动 Spark 2.2.1 集群。验证 Spark 集群已经启动:&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
kubedirector-58cf59869-djdwl 1/1 Running 0 19m
spark221e2-controller-zbg4d-0 1/1 Running 0 23m
spark221e2-jupyter-2km7q-0 1/1 Running 0 23m
spark221e2-worker-4gzbz-0 1/1 Running 0 23m
spark221e2-worker-4gzbz-1 1/1 Running 0 23m
&lt;/code>&lt;/pre>&lt;!--
The running services now include the Spark services:
-->
&lt;p>现在运行的服务包括 Spark 服务：&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubedirector ClusterIP 10.98.234.194 &amp;lt;none&amp;gt; 60000/TCP 1d
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 1d
svc-spark221e2-5tg48 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 21s
svc-spark221e2-controller-tq8d6-0 NodePort 10.104.181.123 &amp;lt;none&amp;gt; 22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 20s
svc-spark221e2-jupyter-6989v-0 NodePort 10.105.227.249 &amp;lt;none&amp;gt; 22:30632/TCP,8888:30355/TCP 20s
svc-spark221e2-worker-d9892-0 NodePort 10.107.131.165 &amp;lt;none&amp;gt; 22:30358/TCP,8081:32144/TCP 20s
svc-spark221e2-worker-d9892-1 NodePort 10.110.88.221 &amp;lt;none&amp;gt; 22:30294/TCP,8081:31436/TCP 20s
&lt;/code>&lt;/pre>&lt;!--
Pointing the browser at port 31533 connects to the Spark Master UI:
-->
&lt;p>将浏览器指向端口 31533 连接到 Spark 主节点 UI：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-10-03-kubedirector/kubedirector.png" alt="kubedirector">&lt;/p>
&lt;!--
That’s all there is to it!
In fact, in the example above we also deployed a Jupyter notebook along with the Spark cluster.
-->
&lt;p>就是这样!
事实上，在上面的例子中，我们还部署了一个 Jupyter notebook 和 Spark 集群。&lt;/p>
&lt;!--
To start another application (e.g. Cassandra), just specify another KubeDirectorApp file:
-->
&lt;p>要启动另一个应用程序（例如 Cassandra），只需指定另一个 KubeDirectorApp 文件：&lt;/p>
&lt;pre>&lt;code>kubectl create -f deploy/example_clusters/cr-cluster-cassandra311.yaml
&lt;/code>&lt;/pre>&lt;!--
See the running Cassandra cluster:
-->
&lt;p>查看正在运行的 Cassandra 集群：&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
cassandra311-seed-v24r6-0 1/1 Running 0 1m
cassandra311-seed-v24r6-1 1/1 Running 0 1m
cassandra311-worker-rqrhl-0 1/1 Running 0 1m
cassandra311-worker-rqrhl-1 1/1 Running 0 1m
kubedirector-58cf59869-djdwl 1/1 Running 0 1d
spark221e2-controller-tq8d6-0 1/1 Running 0 22m
spark221e2-jupyter-6989v-0 1/1 Running 0 22m
spark221e2-worker-d9892-0 1/1 Running 0 22m
spark221e2-worker-d9892-1 1/1 Running 0 22m
&lt;/code>&lt;/pre>&lt;!--
Now you have a Spark cluster (with a Jupyter notebook) and a Cassandra cluster running on Kubernetes.
Use `kubectl get service` to see the set of services.
-->
&lt;p>现在，您有一个 Spark 集群（带有 Jupyter notebook ）和一个运行在 Kubernetes 上的 Cassandra 集群。
使用 &lt;code>kubectl get service&lt;/code> 查看服务集。&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubedirector ClusterIP 10.98.234.194 &amp;lt;none&amp;gt; 60000/TCP 1d
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 1d
svc-cassandra311-seed-v24r6-0 NodePort 10.96.94.204 &amp;lt;none&amp;gt; 22:31131/TCP,9042:30739/TCP 3m
svc-cassandra311-seed-v24r6-1 NodePort 10.106.144.52 &amp;lt;none&amp;gt; 22:30373/TCP,9042:32662/TCP 3m
svc-cassandra311-vhh29 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 3m
svc-cassandra311-worker-rqrhl-0 NodePort 10.109.61.194 &amp;lt;none&amp;gt; 22:31832/TCP,9042:31962/TCP 3m
svc-cassandra311-worker-rqrhl-1 NodePort 10.97.147.131 &amp;lt;none&amp;gt; 22:31454/TCP,9042:31170/TCP 3m
svc-spark221e2-5tg48 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 24m
svc-spark221e2-controller-tq8d6-0 NodePort 10.104.181.123 &amp;lt;none&amp;gt; 22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 24m
svc-spark221e2-jupyter-6989v-0 NodePort 10.105.227.249 &amp;lt;none&amp;gt; 22:30632/TCP,8888:30355/TCP 24m
svc-spark221e2-worker-d9892-0 NodePort 10.107.131.165 &amp;lt;none&amp;gt; 22:30358/TCP,8081:32144/TCP 24m
svc-spark221e2-worker-d9892-1 NodePort 10.110.88.221 &amp;lt;none&amp;gt; 22:30294/TCP,8081:31436/TCP 24m
&lt;/code>&lt;/pre>&lt;!--
## Get Involved
-->
&lt;h2 id="参与其中">参与其中&lt;/h2>
&lt;!--
KubeDirector is a fully open source, Apache v2 licensed, project – the first of multiple open source projects within a broader initiative we call BlueK8s.
The pre-alpha code for KubeDirector has just been released and we would love for you to join the growing community of developers, contributors, and adopters.
Follow [@BlueK8s](https://twitter.com/BlueK8s/) on Twitter and get involved through these channels:
-->
&lt;p>KubeDirector 是一个完全开放源码的 Apache v2 授权项目 – 在我们称为 BlueK8s 的更广泛的计划中，它是多个开放源码项目中的第一个。
KubeDirector 的 pre-alpha 代码刚刚发布，我们希望您加入到不断增长的开发人员、贡献者和使用者社区。
在 Twitter 上关注 &lt;a href="https://twitter.com/BlueK8s/">@BlueK8s&lt;/a>，并通过以下渠道参与:&lt;/p>
&lt;!--
* KubeDirector [chat room on Slack](https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/)
* KubeDirector [GitHub repo](https://github.com/bluek8s/kubedirector/)
-->
&lt;ul>
&lt;li>KubeDirector &lt;a href="https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/">Slack 聊天室&lt;/a>&lt;/li>
&lt;li>KubeDirector &lt;a href="https://github.com/bluek8s/kubedirector/">GitHub 仓库&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: 在 Kubernetes 上对 gRPC 服务器进行健康检查</title><link>https://kubernetes.io/zh/blog/2018/10/01/%E5%9C%A8-kubernetes-%E4%B8%8A%E5%AF%B9-grpc-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/01/%E5%9C%A8-kubernetes-%E4%B8%8A%E5%AF%B9-grpc-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</guid><description>
&lt;!--
---
layout: blog
title: 'Health checking gRPC servers on Kubernetes'
date: 2018-10-01
---
--->
&lt;!--
**Author**: [Ahmet Alp Balkan](https://twitter.com/ahmetb) (Google)
--->
&lt;p>&lt;strong>作者&lt;/strong>： &lt;a href="https://twitter.com/ahmetb">Ahmet Alp Balkan&lt;/a> (Google)&lt;/p>
&lt;!--
**Update (December 2021):** _Kubernetes now has built-in gRPC health probes starting in v1.23.
To learn more, see [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe).
This article was originally written about an external tool to achieve the same task._
-->
&lt;p>&lt;strong>更新（2021 年 12 月）：&lt;/strong> “Kubernetes 从 v1.23 开始具有内置 gRPC 健康探测。
了解更多信息，请参阅&lt;a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe">配置存活探针、就绪探针和启动探针&lt;/a>。
本文最初是为有关实现相同任务的外部工具所写。”&lt;/p>
&lt;!--
[gRPC](https://grpc.io) is on its way to becoming the lingua franca for
communication between cloud-native microservices. If you are deploying gRPC
applications to Kubernetes today, you may be wondering about the best way to
configure health checks. In this article, we will talk about
[grpc-health-probe](https://github.com/grpc-ecosystem/grpc-health-probe/), a
Kubernetes-native way to health check gRPC apps.
--->
&lt;p>&lt;a href="https://grpc.io">gRPC&lt;/a> 将成为本地云微服务间进行通信的通用语言。如果您现在将 gRPC 应用程序部署到 Kubernetes，您可能会想要了解配置健康检查的最佳方法。在本文中，我们将介绍 &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">grpc-health-probe&lt;/a>，这是 Kubernetes 原生的健康检查 gRPC 应用程序的方法。&lt;/p>
&lt;!--
If you're unfamiliar, Kubernetes [health
checks](/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
(liveness and readiness probes) is what's keeping your applications available
while you're sleeping. They detect unresponsive pods, mark them unhealthy, and
cause these pods to be restarted or rescheduled.
--->
&lt;p>如果您不熟悉，Kubernetes的 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">健康检查&lt;/a>（存活探针和就绪探针）可以使您的应用程序在睡眠时保持可用状态。当检测到没有回应的 Pod 时，会将其标记为不健康，并使这些 Pod 重新启动或重新安排。&lt;/p>
&lt;!--
Kubernetes [does not
support](https://github.com/kubernetes/kubernetes/issues/21493) gRPC health
checks natively. This leaves the gRPC developers with the following three
approaches when they deploy to Kubernetes:
[![options for health checking grpc on kubernetes today](/images/blog/2019-09-30-health-checking-grpc/options.png)](/images/blog/2019-09-30-health-checking-grpc/options.png)
--->
&lt;p>Kubernetes 原本 &lt;a href="https://github.com/kubernetes/kubernetes/issues/21493">不支持&lt;/a> gRPC 健康检查。gRPC 的开发人员在 Kubernetes 中部署时可以采用以下三种方法：&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/options.png">&lt;img src="https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/options.png" alt="当前在 kubernetes 上进行 gRPC 健康检查的选项">&lt;/a>&lt;/p>
&lt;!--
1. **httpGet probe:** Cannot be natively used with gRPC. You need to refactor
your app to serve both gRPC and HTTP/1.1 protocols (on different port
numbers).
2. **tcpSocket probe:** Opening a socket to gRPC server is not meaningful,
since it cannot read the response body.
3. **exec probe:** This invokes a program in a container's ecosystem
periodically. In the case of gRPC, this means you implement a health RPC
yourself, then write and ship a client tool with your container.
Can we do better? Absolutely.
--->
&lt;ol>
&lt;li>&lt;strong>httpGet prob：&lt;/strong> 不能与 gRPC 一起使用。您需要重构您的应用程序，必须同时支持 gRPC 和 HTTP/1.1 协议（在不同的端口号上）。&lt;/li>
&lt;li>&lt;strong>tcpSocket probe：&lt;/strong> 打开 gRPC 服务器的 Socket 是没有意义的，因为它无法读取响应主体。&lt;/li>
&lt;li>&lt;strong>exec probe：&lt;/strong> 将定期调用容器生态系统中的程序。对于 gRPC，这意味着您要自己实现健康 RPC，然后使用容器编写并交付客户端工具。&lt;/li>
&lt;/ol>
&lt;p>我们可以做得更好吗？这是肯定的。&lt;/p>
&lt;!--
## Introducing “grpc-health-probe”
To standardize the "exec probe" approach mentioned above, we need:
- a **standard** health check "protocol" that can be implemented in any gRPC
server easily.
- a **standard** health check "tool" that can query the health protocol easily.
--->
&lt;h2 id="介绍-grpc-health-probe">介绍 “grpc-health-probe”&lt;/h2>
&lt;p>为了使上述 &amp;quot;exec probe&amp;quot; 方法标准化，我们需要：&lt;/p>
&lt;ul>
&lt;li>可以在任何 gRPC 服务器中轻松实现的 &lt;strong>标准&lt;/strong> 健康检查 &amp;quot;协议&amp;quot; 。&lt;/li>
&lt;li>一种 &lt;strong>标准&lt;/strong> 健康检查 &amp;quot;工具&amp;quot; ，可以轻松查询健康协议。&lt;/li>
&lt;/ul>
&lt;!--
Thankfully, gRPC has a [standard health checking
protocol](https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md). It
can be used easily from any language. Generated code and the utilities for
setting the health status are shipped in nearly all language implementations of
gRPC.
--->
&lt;p>幸运的是，gRPC 具有 &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md">标准的健康检查协议&lt;/a>。可以用任何语言轻松调用它。几乎所有实现 gRPC 的语言都附带了生成的代码和用于设置健康状态的实用程序。&lt;/p>
&lt;!--
If you
[implement](https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto)
this health check protocol in your gRPC apps, you can then use a standard/common
tool to invoke this `Check()` method to determine server status.
--->
&lt;p>如果您在 gRPC 应用程序中 &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto">实现&lt;/a> 此健康检查协议，那么可以使用标准或通用工具调用 &lt;code>Check()&lt;/code> 方法来确定服务器状态。&lt;/p>
&lt;!--
The next thing you need is the "standard tool", and it's the
[**grpc-health-probe**](https://github.com/grpc-ecosystem/grpc-health-probe/).
--->
&lt;p>接下来您需要的是 &amp;quot;标准工具&amp;quot; &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">&lt;strong>grpc-health-probe&lt;/strong>&lt;/a>。&lt;/p>
&lt;a href='https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png'>
&lt;img width="768" title='grpc-health-probe on kubernetes'
src='https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png'/>
&lt;/a>
&lt;!--
With this tool, you can use the same health check configuration in all your gRPC
applications. This approach requires you to:
--->
&lt;p>使用此工具，您可以在所有 gRPC 应用程序中使用相同的健康检查配置。这种方法有以下要求：&lt;/p>
&lt;!--
1. Find the gRPC "health" module in your favorite language and start using it
(example [Go library](https://godoc.org/github.com/grpc/grpc-go/health)).
2. Ship the
[grpc_health_probe](https://github.com/grpc-ecosystem/grpc-health-probe/)
binary in your container.
3. [Configure](https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes)
Kubernetes "exec" probe to invoke the "grpc_health_probe" tool in the
container.
--->
&lt;ol>
&lt;li>用您喜欢的语言找到 gRPC 的 &amp;quot;健康&amp;quot; 模块并开始使用它（例如 &lt;a href="https://godoc.org/github.com/grpc/grpc-go/health">Go 库&lt;/a>）。&lt;/li>
&lt;li>将二进制文件 &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">grpc_health_probe&lt;/a> 送到容器中。&lt;/li>
&lt;li>&lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes">配置&lt;/a> Kubernetes 的 &amp;quot;exec&amp;quot; 检查模块来调用容器中的 &amp;quot;grpc_health_probe&amp;quot; 工具。&lt;/li>
&lt;/ol>
&lt;!--
In this case, executing "grpc_health_probe" will call your gRPC server over
`localhost`, since they are in the same pod.
--->
&lt;p>在这种情况下，执行 &amp;quot;grpc_health_probe&amp;quot; 将通过 &lt;code>localhost&lt;/code> 调用您的 gRPC 服务器，因为它们位于同一个容器中。&lt;/p>
&lt;!--
## What's next
**grpc-health-probe** project is still in its early days and it needs your
feedback. It supports a variety of features like communicating with TLS servers
and configurable connection/RPC timeouts.
--->
&lt;h2 id="下一步工作">下一步工作&lt;/h2>
&lt;p>&lt;strong>grpc-health-probe&lt;/strong> 项目仍处于初期阶段，需要您的反馈。它支持多种功能，例如与 TLS 服务器通信和配置延时连接/RPC。&lt;/p>
&lt;!--
If you are running a gRPC server on Kubernetes today, try using the gRPC Health
Protocol and try the grpc-health-probe in your deployments, and [give
feedback](https://github.com/grpc-ecosystem/grpc-health-probe/).
--->
&lt;p>如果您最近要在 Kubernetes 上运行 gRPC 服务器，请尝试使用 gRPC Health Protocol，并在您的 Deployment 中尝试 grpc-health-probe，然后 &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">进行反馈&lt;/a>。&lt;/p>
&lt;!--
## Further reading
- Protocol: [GRPC Health Checking Protocol](https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md) ([health.proto](https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto))
- Documentation: [Kubernetes liveness and readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
- Article: [Advanced Kubernetes Health Check Patterns](https://ahmet.im/blog/advanced-kubernetes-health-checks/)
--->
&lt;h2 id="更多内容">更多内容&lt;/h2>
&lt;ul>
&lt;li>协议： &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md">GRPC Health Checking Protocol&lt;/a> (&lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto">health.proto&lt;/a>)&lt;/li>
&lt;li>文档： &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">Kubernetes 存活和就绪探针&lt;/a>&lt;/li>
&lt;li>文章： &lt;a href="https://ahmet.im/blog/advanced-kubernetes-health-checks/">升级版 Kubernetes 健康检查模式&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: The Machines Can Do the Work, a Story of Kubernetes Testing, CI, and Automating the Contributor Experience</title><link>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</link><pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</guid><description>
&lt;hr>
&lt;h2 id="date-2019-08-29">layout: blog
title: '机器可以完成这项工作，一个关于 kubernetes 测试、CI 和自动化贡献者体验的故事'
date: 2019-08-29&lt;/h2>
&lt;!--
**Author**: Aaron Crickenberger (Google) and Benjamin Elder (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Aaron Crickenberger（谷歌）和 Benjamin Elder（谷歌）&lt;/p>
&lt;!--
_“Large projects have a lot of less exciting, yet, hard work. We value time spent automating repetitive work more highly than toil. Where that work cannot be automated, it is our culture to recognize and reward all types of contributions. However, heroism is not sustainable.”_ - [Kubernetes Community Values](https://git.k8s.io/community/values.md#automation-over-process)
-->
&lt;p>&lt;em>”大型项目有很多不那么令人兴奋，但却很辛苦的工作。比起辛苦工作，我们更重视把时间花在自动化重复性工作上，如果这项工作无法实现自动化，我们的文化就是承认并奖励所有类型的贡献。然而，英雄主义是不可持续的。“&lt;/em> - &lt;a href="https://git.k8s.io/community/values.md#automation-over-process">Kubernetes Community Values&lt;/a>&lt;/p>
&lt;!--
Like many open source projects, Kubernetes is hosted on GitHub. We felt the barrier to participation would be lowest if the project lived where developers already worked, using tools and processes developers already knew. Thus the project embraced the service fully: it was the basis of our workflow, our issue tracker, our documentation, our blog platform, our team structure, and more.
-->
&lt;p>像许多开源项目一样，Kubernetes 托管在 GitHub 上。 如果项目位于在开发人员已经工作的地方，使用的开发人员已经知道的工具和流程，那么参与的障碍将是最低的。 因此，该项目完全接受了这项服务：它是我们工作流程，问题跟踪，文档，博客平台，团队结构等的基础。&lt;/p>
&lt;!--
This strategy worked. It worked so well that the project quickly scaled past its contributors’ capacity as humans. What followed was an incredible journey of automation and innovation. We didn’t just need to rebuild our airplane mid-flight without crashing, we needed to convert it into a rocketship and launch into orbit. We needed machines to do the work.
-->
&lt;p>这个策略奏效了。 它运作良好，以至于该项目迅速超越了其贡献者的人类能力。 接下来是一次令人难以置信的自动化和创新之旅。 我们不仅需要在飞行途中重建我们的飞机而不会崩溃，我们需要将其转换为火箭飞船并发射到轨道。 我们需要机器来完成这项工作。&lt;/p>
&lt;!--
## The Work
-->
&lt;p>##　工作&lt;/p>
&lt;!--
Initially, we focused on the fact that we needed to support the sheer volume of tests mandated by a complex distributed system such as Kubernetes. Real world failure scenarios had to be exercised via end-to-end (e2e) tests to ensure proper functionality. Unfortunately, e2e tests were susceptible to flakes (random failures) and took anywhere from an hour to a day to complete.
-->
&lt;p>最初，我们关注的事实是，我们需要支持复杂的分布式系统（如 Kubernetes）所要求的大量测试。 真实世界中的故障场景必须通过端到端（e2e）测试来执行，确保正确的功能。 不幸的是，e2e 测试容易受到薄片（随机故障）的影响，并且需要花费一个小时到一天才能完成。&lt;/p>
&lt;!--
Further experience revealed other areas where machines could do the work for us:
-->
&lt;p>进一步的经验揭示了机器可以为我们工作的其他领域：&lt;/p>
&lt;!--
* PR Workflow
* Did the contributor sign our CLA?
* Did the PR pass tests?
* Is the PR mergeable?
* Did the merge commit pass tests?
* Triage
* Who should be reviewing PRs?
* Is there enough information to route an issue to the right people?
* Is an issue still relevant?
* Project Health
* What is happening in the project?
* What should we be paying attention to?
-->
&lt;ul>
&lt;li>Pull Request 工作流程
&lt;ul>
&lt;li>贡献者是否签署了我们的 CLA？&lt;/li>
&lt;li>Pull Request 通过测试吗？&lt;/li>
&lt;li>Pull Request 可以合并吗？&lt;/li>
&lt;li>合并提交是否通过了测试？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>鉴别分类
&lt;ul>
&lt;li>谁应该审查 Pull Request？&lt;/li>
&lt;li>是否有足够的信息将问题发送给合适的人？&lt;/li>
&lt;li>问题是否依旧存在？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>项目健康
&lt;ul>
&lt;li>项目中发生了什么？&lt;/li>
&lt;li>我们应该注意什么？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
As we developed automation to improve our situation, we followed a few guiding principles:
-->
&lt;p>当我们开发自动化来改善我们的情况时，我们遵循了以下几个指导原则：&lt;/p>
&lt;!--
* Follow the push/pull control loop patterns that worked well for Kubernetes
* Prefer stateless loosely coupled services that do one thing well
* Prefer empowering the entire community over empowering a few core contributors
* Eat our own dogfood and avoid reinventing wheels
-->
&lt;ul>
&lt;li>遵循适用于 Kubernetes 的推送/拉取控制循环模式&lt;/li>
&lt;li>首选无状态松散耦合服务&lt;/li>
&lt;li>更倾向于授权整个社区权利，而不是赋予少数核心贡献者权力&lt;/li>
&lt;li>做好自己的事，而不要重新造轮子&lt;/li>
&lt;/ul>
&lt;!--
## Enter Prow
-->
&lt;h2 id="了解-prow">了解 Prow&lt;/h2>
&lt;!--
This led us to create [Prow](https://git.k8s.io/test-infra/prow) as the central component for our automation. Prow is sort of like an [If This, Then That](https://ifttt.com/) for GitHub events, with a built-in library of [commands](https://prow.k8s.io/command-help), [plugins](https://prow.k8s.io/plugins), and utilities. We built Prow on top of Kubernetes to free ourselves from worrying about resource management and scheduling, and ensure a more pleasant operational experience.
-->
&lt;p>这促使我们创建 &lt;a href="https://git.k8s.io/test-infra/prow">Prow&lt;/a> 作为我们自动化的核心组件。 Prow有点像 &lt;a href="https://ifttt.com/">If This, Then That&lt;/a> 用于 GitHub 事件， 内置 &lt;a href="https://prow.k8s.io/command-help">commands&lt;/a>， &lt;a href="https://prow.k8s.io/plugins">plugins&lt;/a>， 和实用程序。 我们在 Kubernetes 之上建立了 Prow，让我们不必担心资源管理和日程安排，并确保更愉快的运营体验。&lt;/p>
&lt;!--
Prow lets us do things like:
-->
&lt;p>Prow 让我们做以下事情：&lt;/p>
&lt;!--
* Allow our community to triage issues/PRs by commenting commands such as “/priority critical-urgent”, “/assign mary” or “/close”
* Auto-label PRs based on how much code they change, or which files they touch
* Age out issues/PRs that have remained inactive for too long
* Auto-merge PRs that meet our PR workflow requirements
* Run CI jobs defined as [Knative Builds](https://github.com/knative/build), Kubernetes Pods, or Jenkins jobs
* Enforce org-wide and per-repo GitHub policies like [branch protection](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector) and [GitHub labels](https://github.com/kubernetes/test-infra/tree/master/label_sync)
-->
&lt;ul>
&lt;li>允许我们的社区通过评论诸如“/priority critical-urgent”，“/assign mary”或“/close”之类的命令对 issues/Pull Requests 进行分类&lt;/li>
&lt;li>根据用户更改的代码数量或创建的文件自动标记 Pull Requests&lt;/li>
&lt;li>标出长时间保持不活动状态 issues/Pull Requests&lt;/li>
&lt;li>自动合并符合我们PR工作流程要求的 Pull Requests&lt;/li>
&lt;li>运行定义为&lt;a href="https://github.com/knative/build">Knative Builds&lt;/a>的 Kubernetes Pods或 Jenkins jobs的 CI 作业&lt;/li>
&lt;li>实施组织范围和重构 GitHub 仓库策略，如&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector">Knative Builds&lt;/a>和&lt;a href="https://github.com/kubernetes/test-infra/tree/master/label_sync">GitHub labels&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Prow was initially developed by the engineering productivity team building Google Kubernetes Engine, and is actively contributed to by multiple members of Kubernetes SIG Testing. Prow has been adopted by several other open source projects, including Istio, JetStack, Knative and OpenShift. [Getting started with Prow](https://github.com/kubernetes/test-infra/tree/master/prow#getting-started) takes a Kubernetes cluster and `kubectl apply starter.yaml` (running pods on a Kubernetes cluster).
-->
&lt;p>Prow最初由构建 Google Kubernetes Engine 的工程效率团队开发，并由 Kubernetes SIG Testing 的多个成员积极贡献。 Prow 已被其他几个开源项目采用，包括 Istio，JetStack，Knative 和 OpenShift。 &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow#getting-started">Getting started with Prow&lt;/a>需要一个 Kubernetes 集群和 &lt;code>kubectl apply starter.yaml&lt;/code>（在 Kubernetes 集群上运行 pod）。&lt;/p>
&lt;!--
Once we had Prow in place, we began to hit other scaling bottlenecks, and so produced additional tooling to support testing at the scale required by Kubernetes, including:
-->
&lt;p>一旦我们安装了 Prow，我们就开始遇到其他的问题，因此需要额外的工具以支持 Kubernetes 所需的规模测试，包括：&lt;/p>
&lt;!--
- [Boskos](https://github.com/kubernetes/test-infra/tree/master/boskos): manages job resources (such as GCP projects) in pools, checking them out for jobs and cleaning them up automatically ([with monitoring](http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1))
- [ghProxy](https://github.com/kubernetes/test-infra/tree/master/ghproxy): a reverse proxy HTTP cache optimized for use with the GitHub API, to ensure our token usage doesn’t hit API limits ([with monitoring](http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;orgId=1))
- [Greenhouse](https://github.com/kubernetes/test-infra/tree/master/greenhouse): allows us to use a remote bazel cache to provide faster build and test results for PRs ([with monitoring](http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1))
- [Splice](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice): allows us to test and merge PRs in a batch, ensuring our merge velocity is not limited to our test velocity
- [Tide](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide): allows us to merge PRs selected via GitHub queries rather than ordered in a queue, allowing for significantly higher merge velocity in tandem with splice
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/boskos">Boskos&lt;/a>: 管理池中的作业资源（例如 GCP 项目），检查它们是否有工作并自动清理它们 (&lt;a href="http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/ghproxy">ghProxy&lt;/a>: 优化用于 GitHub API 的反向代理 HTTP 缓存，以确保我们的令牌使用不会达到 API 限制 (&lt;a href="http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;amp;orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/greenhouse">Greenhouse&lt;/a>: 允许我们使用远程 bazel 缓存为 Pull requests 提供更快的构建和测试结果 (&lt;a href="http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice">Splice&lt;/a>: 允许我们批量测试和合并 Pull requests，确保我们的合并速度不仅限于我们的测试速度&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide">Tide&lt;/a>: 允许我们合并通过 GitHub 查询选择的 Pull requests，而不是在队列中排序，允许显着更高合并速度与拼接一起&lt;/li>
&lt;/ul>
&lt;!--
## Scaling Project Health
-->
&lt;p>##　关注项目健康状况&lt;/p>
&lt;!--
With workflow automation addressed, we turned our attention to project health. We chose to use Google Cloud Storage (GCS) as our source of truth for all test data, allowing us to lean on established infrastructure, and allowed the community to contribute results. We then built a variety of tools to help individuals and the project as a whole make sense of this data, including:
-->
&lt;p>随着工作流自动化的实施，我们将注意力转向了项目健康。我们选择使用 Google Cloud Storage (GCS)作为所有测试数据的真实来源，允许我们依赖已建立的基础设施，并允许社区贡献结果。然后，我们构建了各种工具来帮助个人和整个项目理解这些数据，包括：&lt;/p>
&lt;!--
* [Gubernator](https://github.com/kubernetes/test-infra/tree/master/gubernator): display the results and test history for a given PR
* [Kettle](https://github.com/kubernetes/test-infra/tree/master/kettle): transfer data from GCS to a publicly accessible bigquery dataset
* [PR dashboard](https://k8s-gubernator.appspot.com/pr): a workflow-aware dashboard that allows contributors to understand which PRs require attention and why
* [Triage](https://storage.googleapis.com/k8s-gubernator/triage/index.html): identify common failures that happen across all jobs and tests
* [Testgrid](https://k8s-testgrid.appspot.com/): display test results for a given job across all runs, summarize test results across groups of jobs
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/gubernator">Gubernator&lt;/a>: 显示给定 Pull Request 的结果和测试历史&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/kettle">Kettle&lt;/a>: 将数据从 GCS 传输到可公开访问的 bigquery 数据集&lt;/li>
&lt;li>&lt;a href="https://k8s-gubernator.appspot.com/pr">PR dashboard&lt;/a>: 一个工作流程识别仪表板，允许参与者了解哪些 Pull Request 需要注意以及为什么&lt;/li>
&lt;li>&lt;a href="https://storage.googleapis.com/k8s-gubernator/triage/index.html">Triage&lt;/a>: 识别所有作业和测试中发生的常见故障&lt;/li>
&lt;li>&lt;a href="https://k8s-testgrid.appspot.com/">Testgrid&lt;/a>: 显示所有运行中给定作业的测试结果，汇总各组作业的测试结果&lt;/li>
&lt;/ul>
&lt;!--
We approached the Cloud Native Computing Foundation (CNCF) to develop DevStats to glean insights from our GitHub events such as:
-->
&lt;p>我们与云计算本地计算基金会（CNCF）联系，开发 DevStats，以便从我们的 GitHub 活动中收集见解，例如：&lt;/p>
&lt;!--
* [Which prow commands are people most actively using](https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1)
* [PR reviews by contributor over time](https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;var-period=d7&amp;var-repo_name=All&amp;var-reviewers=All)
* [Time spent in each phase of our PR workflow](https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1)
-->
&lt;ul>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1">Which prow commands are people most actively using&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;amp;var-period=d7&amp;amp;var-repo_name=All&amp;amp;var-reviewers=All">PR reviews by contributor over time&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1">Time spent in each phase of our PR workflow&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Into the Beyond
-->
&lt;h2 id="into-the-beyond">Into the Beyond&lt;/h2>
&lt;!--
Today, the Kubernetes project spans over 125 repos across five orgs. There are 31 Special Interests Groups and 10 Working Groups coordinating development within the project. In the last year the project has had [participation from over 13,800 unique developers](https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;var-period_name=Last%20year&amp;var-metric=contributions&amp;var-repogroup_name=All) on GitHub.
-->
&lt;p>今天，Kubernetes 项目跨越了5个组织125个仓库。有31个特殊利益集团和10个工作组在项目内协调发展。在过去的一年里，该项目有 &lt;a href="https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All">来自13800多名独立开发人员的参与&lt;/a>。&lt;/p>
&lt;!--
On any given weekday our Prow instance [runs over 10,000 CI jobs](http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;fullscreen&amp;orgId=1&amp;from=now-6M&amp;to=now); from March 2017 to March 2018 it ran 4.3 million jobs. Most of these jobs involve standing up an entire Kubernetes cluster, and exercising it using real world scenarios. They allow us to ensure all supported releases of Kubernetes work across cloud providers, container engines, and networking plugins. They make sure the latest releases of Kubernetes work with various optional features enabled, upgrade safely, meet performance requirements, and work across architectures.
-->
&lt;p>在任何给定的工作日，我们的 Prow 实例&lt;a href="http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;amp;fullscreen&amp;amp;orgId=1&amp;amp;from=now-6M&amp;amp;to=now">运行超过10,000个 CI 工作&lt;/a>; 从2017年3月到2018年3月，它有430万个工作岗位。 这些工作中的大多数涉及建立整个 Kubernetes 集群，并使用真实场景来实施它。 它们使我们能够确保所有受支持的 Kubernetes 版本跨云提供商，容器引擎和网络插件工作。 他们确保最新版本的 Kubernetes 能够启用各种可选功能，安全升级，满足性能要求，并跨架构工作。&lt;/p>
&lt;!--
With today’s [announcement from CNCF](https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google) – noting that Google Cloud has begun transferring ownership and management of the Kubernetes project’s cloud resources to CNCF community contributors, we are excited to embark on another journey. One that allows the project infrastructure to be owned and operated by the community of contributors, following the same open governance model that has worked for the rest of the project. Sound exciting to you? Come talk to us at #sig-testing on kubernetes.slack.com.
-->
&lt;p>今天&lt;a href="https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google">来自CNCF的公告&lt;/a> - 注意到 Google Cloud 有开始将 Kubernetes 项目的云资源的所有权和管理权转让给 CNCF 社区贡献者，我们很高兴能够开始另一个旅程。 允许项目基础设施由贡献者社区拥有和运营，遵循对项目其余部分有效的相同开放治理模型。 听起来令人兴奋。 请来 kubernetes.slack.com 上的 #sig-testing on kubernetes.slack.com 与我们联系。&lt;/p>
&lt;!--
Want to find out more? Come check out these resources:
-->
&lt;p>想了解更多？ 快来看看这些资源：&lt;/p>
&lt;!--
* [Prow: Testing the way to Kubernetes Next](https://elder.dev/posts/prow)
* [Automation and the Kubernetes Contributor Experience](https://www.youtube.com/watch?v=BsIC7gPkH5M)
-->
&lt;ul>
&lt;li>&lt;a href="https://elder.dev/posts/prow">Prow: Testing the way to Kubernetes Next&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=BsIC7gPkH5M">Automation and the Kubernetes Contributor Experience&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: 使用 CSI 和 Kubernetes 实现卷的动态扩容</title><link>https://kubernetes.io/zh/blog/2018/08/02/%E4%BD%BF%E7%94%A8-csi-%E5%92%8C-kubernetes-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%9A%84%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9/</link><pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/08/02/%E4%BD%BF%E7%94%A8-csi-%E5%92%8C-kubernetes-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%9A%84%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9/</guid><description>
&lt;!--
---
layout: blog
title: 'Dynamically Expand Volume with CSI and Kubernetes'
date: 2018-08-02
---
-->
&lt;!--
**Author**: Orain Xiong (Co-Founder, WoquTech)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Orain Xiong（联合创始人, WoquTech）&lt;/p>
&lt;!--
_There is a very powerful storage subsystem within Kubernetes itself, covering a fairly broad spectrum of use cases. Whereas, when planning to build a product-grade relational database platform with Kubernetes, we face a big challenge: coming up with storage. This article describes how to extend latest Container Storage Interface 0.2.0 and integrate with Kubernetes, and demonstrates the essential facet of dynamically expanding volume capacity._
-->
&lt;p>&lt;em>Kubernetes 本身有一个非常强大的存储子系统，涵盖了相当广泛的用例。而当我们计划使用 Kubernetes 构建产品级关系型数据库平台时，我们面临一个巨大的挑战：提供存储。本文介绍了如何扩展最新的 Container Storage Interface 0.2.0 和与 Kubernetes 集成，并演示了卷动态扩容的基本方面。&lt;/em>&lt;/p>
&lt;!--
## Introduction
-->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;!--
As we focalize our customers, especially in financial space, there is a huge upswell in the adoption of container orchestration technology.
-->
&lt;p>当我们专注于客户时，尤其是在金融领域，采用容器编排技术的情况大大增加。&lt;/p>
&lt;!--
They are looking forward to open source solutions to redesign already existing monolithic applications, which have been running for several years on virtualization infrastructure or bare metal.
-->
&lt;p>他们期待着能用开源解决方案重新设计已经存在的整体应用程序，这些应用程序已经在虚拟化基础架构或裸机上运行了几年。&lt;/p>
&lt;!--
Considering extensibility and the extent of technical maturity, Kubernetes and Docker are at the very top of the list. But migrating monolithic applications to a distributed orchestration like Kubernetes is challenging, the relational database is critical for the migration.
-->
&lt;p>考虑到可扩展性和技术成熟程度，Kubernetes 和 Docker 排在我们选择列表的首位。但是将整体应用程序迁移到类似于 Kubernetes 之类的分布式容器编排平台上很具有挑战性，其中关系数据库对于迁移来说至关重要。&lt;/p>
&lt;!--
With respect to the relational database, we should pay attention to storage. There is a very powerful storage subsystem within Kubernetes itself. It is very useful and covers a fairly broad spectrum of use cases. When planning to run a relational database with Kubernetes in production, we face a big challenge: coming up with storage. There are still some fundamental functionalities which are left unimplemented. Specifically, dynamically expanding volume. It sounds boring but is highly required, except for actions like create and delete and mount and unmount.
-->
&lt;p>关于关系数据库，我们应该注意存储。Kubernetes 本身内部有一个非常强大的存储子系统。它非常有用，涵盖了相当广泛的用例。当我们计划在生产环境中使用 Kubernetes 运行关系型数据库时，我们面临一个巨大挑战：提供存储。目前，仍有一些基本功能尚未实现。特别是，卷的动态扩容。这听起来很无聊，但在除创建，删除，安装和卸载之类的操作外，它是非常必要的。&lt;/p>
&lt;!--
Currently, expanding volume is only available with those storage provisioners:
-->
&lt;p>目前，扩展卷仅适用于这些存储供应商：&lt;/p>
&lt;ul>
&lt;li>gcePersistentDisk&lt;/li>
&lt;li>awsElasticBlockStore&lt;/li>
&lt;li>OpenStack Cinder&lt;/li>
&lt;li>glusterfs&lt;/li>
&lt;li>rbd&lt;/li>
&lt;/ul>
&lt;!--
In order to enable this feature, we should set feature gate `ExpandPersistentVolumes` true and turn on the `PersistentVolumeClaimResize` admission plugin. Once `PersistentVolumeClaimResize` has been enabled, resizing will be allowed by a Storage Class whose `allowVolumeExpansion` field is set to true.
-->
&lt;p>为了启用此功能，我们应该将特性开关 &lt;code>ExpandPersistentVolumes&lt;/code> 设置为 true 并打开 &lt;code>PersistentVolumeClaimResize&lt;/code> 准入插件。 一旦启用了 &lt;code>PersistentVolumeClaimResize&lt;/code>，则其对应的 &lt;code>allowVolumeExpansion&lt;/code> 字段设置为 true 的存储类将允许调整大小。&lt;/p>
&lt;!--
Unfortunately, dynamically expanding volume through the Container Storage Interface (CSI) and Kubernetes is unavailable, even though the underlying storage providers have this feature.
-->
&lt;p>不幸的是，即使基础存储提供者具有此功能，也无法通过容器存储接口（CSI）和 Kubernetes 动态扩展卷。&lt;/p>
&lt;!--
This article will give a simplified view of CSI, followed by a walkthrough of how to introduce a new expanding volume feature on the existing CSI and Kubernetes. Finally, the article will demonstrate how to dynamically expand volume capacity.
-->
&lt;p>本文将给出 CSI 的简化视图，然后逐步介绍如何在现有 CSI 和 Kubernetes 上引入新的扩展卷功能。最后，本文将演示如何动态扩展卷容量。&lt;/p>
&lt;!--
## Container Storage Interface (CSI)
-->
&lt;h2 id="容器存储接口-csi">容器存储接口（CSI）&lt;/h2>
&lt;!--
To have a better understanding of what we're going to do, the first thing we need to know is what the Container Storage Interface is. Currently, there are still some problems for already existing storage subsystem within Kubernetes. Storage driver code is maintained in the Kubernetes core repository which is difficult to test. But beyond that, Kubernetes needs to give permissions to storage vendors to check code into the Kubernetes core repository. Ideally, that should be implemented externally.
-->
&lt;p>为了更好地了解我们将要做什么，我们首先需要知道什么是容器存储接口。当前，Kubernetes 中已经存在的存储子系统仍然存在一些问题。 存储驱动程序代码在 Kubernetes 核心存储库中维护，这很难测试。 但是除此之外，Kubernetes 还需要授予存储供应商许可，以将代码签入 Kubernetes 核心存储库。 理想情况下，这些应在外部实施。&lt;/p>
&lt;!--
CSI is designed to define an industry standard that will enable storage providers who enable CSI to be available across container orchestration systems that support CSI.
-->
&lt;p>CSI 旨在定义行业标准，该标准将使支持 CSI 的存储提供商能够在支持 CSI 的容器编排系统中使用。&lt;/p>
&lt;!--
This diagram depicts a kind of high-level Kubernetes archetypes integrated with CSI:
![csi diagram](/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png)
-->
&lt;p>该图描述了一种与 CSI 集成的高级 Kubernetes 原型：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png" alt="csi diagram">&lt;/p>
&lt;!--
* Three new external components are introduced to decouple Kubernetes and Storage Provider logic
* Blue arrows present the conventional way to call against API Server
* Red arrows present gRPC to call against Volume Driver
-->
&lt;ul>
&lt;li>引入了三个新的外部组件以解耦 Kubernetes 和存储提供程序逻辑&lt;/li>
&lt;li>蓝色箭头表示针对 API 服务器进行调用的常规方法&lt;/li>
&lt;li>红色箭头显示 gRPC 以针对 Volume Driver 进行调用&lt;/li>
&lt;/ul>
&lt;!--
For more details, please visit: https://github.com/container-storage-interface/spec/blob/master/spec.md
-->
&lt;p>更多详细信息，请访问：https://github.com/container-storage-interface/spec/blob/master/spec.md&lt;/p>
&lt;!--
## Extend CSI and Kubernetes
-->
&lt;h2 id="扩展-csi-和-kubernetes">扩展 CSI 和 Kubernetes&lt;/h2>
&lt;!--
In order to enable the feature of expanding volume atop Kubernetes, we should extend several components including CSI specification, “in-tree” volume plugin, external-provisioner and external-attacher.
-->
&lt;p>为了实现在 Kubernetes 上扩展卷的功能，我们应该扩展几个组件，包括 CSI 规范，“in-tree” 卷插件，external-provisioner 和 external-attacher。&lt;/p>
&lt;!--
## Extend CSI spec
-->
&lt;h2 id="扩展csi规范">扩展CSI规范&lt;/h2>
&lt;!--
The feature of expanding volume is still undefined in latest CSI 0.2.0. The new 3 RPCs, including `RequiresFSResize` and `ControllerResizeVolume` and `NodeResizeVolume`, should be introduced.
-->
&lt;p>最新的 CSI 0.2.0 仍未定义扩展卷的功能。应该引入新的3个 RPC，包括 &lt;code>RequiresFSResize&lt;/code>， &lt;code>ControllerResizeVolume&lt;/code> 和 &lt;code>NodeResizeVolume&lt;/code>。&lt;/p>
&lt;pre>&lt;code>service Controller {
rpc CreateVolume (CreateVolumeRequest)
returns (CreateVolumeResponse) {}
……
rpc RequiresFSResize (RequiresFSResizeRequest)
returns (RequiresFSResizeResponse) {}
rpc ControllerResizeVolume (ControllerResizeVolumeRequest)
returns (ControllerResizeVolumeResponse) {}
}
service Node {
rpc NodeStageVolume (NodeStageVolumeRequest)
returns (NodeStageVolumeResponse) {}
……
rpc NodeResizeVolume (NodeResizeVolumeRequest)
returns (NodeResizeVolumeResponse) {}
}
&lt;/code>&lt;/pre>&lt;!--
## Extend “In-Tree” Volume Plugin
-->
&lt;h2 id="扩展-in-tree-卷插件">扩展 “In-Tree” 卷插件&lt;/h2>
&lt;!--
In addition to the extend CSI specification, the `csiPlugin﻿` interface within Kubernetes should also implement `expandablePlugin`. The `csiPlugin` interface will expand `PersistentVolumeClaim` representing for `ExpanderController`.
-->
&lt;p>除了扩展的 CSI 规范之外，Kubernetes 中的 &lt;code>csiPlugin&lt;/code> 接口还应该实现 &lt;code>expandablePlugin&lt;/code>。&lt;code>csiPlugin&lt;/code> 接口将扩展代表 &lt;code>ExpanderController&lt;/code> 的 &lt;code>PersistentVolumeClaim&lt;/code>。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ExpandableVolumePlugin &lt;span style="color:#a2f;font-weight:bold">interface&lt;/span> {
VolumePlugin
&lt;span style="color:#00a000">ExpandVolumeDevice&lt;/span>(spec Spec, newSize resource.Quantity, oldSize resource.Quantity) (resource.Quantity, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>)
&lt;span style="color:#00a000">RequiresFSResize&lt;/span>() &lt;span style="color:#0b0;font-weight:bold">bool&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Implement Volume Driver
-->
&lt;h3 id="实现卷驱动程序">实现卷驱动程序&lt;/h3>
&lt;!--
Finally, to abstract complexity of the implementation, we should hard code the separate storage provider management logic into the following functions which is well-defined in the CSI specification:
-->
&lt;p>最后，为了抽象化实现的复杂性，我们应该将单独的存储提供程序管理逻辑硬编码为以下功能，这些功能在 CSI 规范中已明确定义：&lt;/p>
&lt;ul>
&lt;li>CreateVolume&lt;/li>
&lt;li>DeleteVolume&lt;/li>
&lt;li>ControllerPublishVolume&lt;/li>
&lt;li>ControllerUnpublishVolume&lt;/li>
&lt;li>ValidateVolumeCapabilities&lt;/li>
&lt;li>ListVolumes&lt;/li>
&lt;li>GetCapacity&lt;/li>
&lt;li>ControllerGetCapabilities&lt;/li>
&lt;li>RequiresFSResize&lt;/li>
&lt;li>ControllerResizeVolume&lt;/li>
&lt;/ul>
&lt;!--
## Demonstration
Let’s demonstrate this feature with a concrete user case.
* Create storage class for CSI storage provisioner
-->
&lt;h2 id="展示">展示&lt;/h2>
&lt;p>让我们以具体的用户案例来演示此功能。&lt;/p>
&lt;ul>
&lt;li>为 CSI 存储供应商创建存储类&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">allowVolumeExpansion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csiProvisionerSecretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>orain-test&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csiProvisionerSecretNamespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfsplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">reclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeBindingMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Immediate&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
* Deploy CSI Volume Driver including storage provisioner `csi-qcfsplugin` across Kubernetes cluster
* Create PVC `qcfs-pvc` which will be dynamically provisioned by storage class `csi-qcfs`
-->
&lt;ul>
&lt;li>
&lt;p>在 Kubernetes 集群上部署包括存储供应商 &lt;code>csi-qcfsplugin&lt;/code> 在内的 CSI 卷驱动&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 PVC &lt;code>qcfs-pvc&lt;/code>，它将由存储类 &lt;code>csi-qcfs&lt;/code> 动态配置&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>qcfs-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">...&lt;/span>.&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfs&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
* Create MySQL 5.7 instance to use PVC `qcfs-pvc`
* In order to mirror the exact same production-level scenario, there are actually two different types of workloads including:
* Batch insert to make MySQL consuming more file system capacity
* Surge query request
* Dynamically expand volume capacity through edit pvc `qcfs-pvc` configuration
-->
&lt;ul>
&lt;li>创建 MySQL 5.7 实例以使用 PVC &lt;code>qcfs-pvc&lt;/code>&lt;/li>
&lt;li>为了反映完全相同的生产级别方案，实际上有两种不同类型的工作负载，包括：
     * 批量插入使 MySQL 消耗更多的文件系统容量
     * 浪涌查询请求&lt;/li>
&lt;li>通过编辑 pvc &lt;code>qcfs-pvc&lt;/code> 配置动态扩展卷容量&lt;/li>
&lt;/ul>
&lt;!--
The Prometheus and Grafana integration allows us to visualize corresponding critical metrics.
![prometheus grafana](/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png)
We notice that the middle reading shows MySQL datafile size increasing slowly during bulk inserting. At the same time, the bottom reading shows file system expanding twice in about 20 minutes, from 300 GiB to 400 GiB and then 500 GiB. Meanwhile, the upper reading shows the whole process of expanding volume immediately completes and hardly impacts MySQL QPS.
-->
&lt;p>Prometheus 和 Grafana 的集成使我们可以可视化相应的关键指标。&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png" alt="prometheus grafana">&lt;/p>
&lt;p>我们注意到中间的读数显示在批量插入期间 MySQL 数据文件的大小缓慢增加。 同时，底部读数显示文件系统在大约20分钟内扩展了两次，从 300 GiB 扩展到 400 GiB，然后扩展到 500 GiB。 同时，上半部分显示，扩展卷的整个过程立即完成，几乎不会影响 MySQL QPS。&lt;/p>
&lt;!--
## Conclusion
Regardless of whatever infrastructure applications have been running on, the database is always a critical resource. It is essential to have a more advanced storage subsystem out there to fully support database requirements. This will help drive the more broad adoption of cloud native technology.
-->
&lt;h2 id="结论">结论&lt;/h2>
&lt;p>不管运行什么基础结构应用程序，数据库始终是关键资源。拥有更高级的存储子系统以完全支持数据库需求至关重要。这将有助于推动云原生技术的更广泛采用。&lt;/p></description></item><item><title>Blog: 动态 Kubelet 配置</title><link>https://kubernetes.io/zh/blog/2018/07/11/%E5%8A%A8%E6%80%81-kubelet-%E9%85%8D%E7%BD%AE/</link><pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/11/%E5%8A%A8%E6%80%81-kubelet-%E9%85%8D%E7%BD%AE/</guid><description>
&lt;!--
---
layout: blog
title: 'Dynamic Kubelet Configuration'
date: 2018-07-11
---
-->
&lt;!--
**Author**: Michael Taufen (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Michael Taufen (Google)&lt;/p>
&lt;!--
**Editor’s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/) on what’s new in Kubernetes 1.11**
-->
&lt;p>&lt;strong>编者注：这篇文章是&lt;a href="https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/">一系列深度文章&lt;/a> 的一部分，这个系列介绍了 Kubernetes 1.11 中的新增功能&lt;/strong>&lt;/p>
&lt;!--
## Why Dynamic Kubelet Configuration?
-->
&lt;h2 id="为什么要进行动态-kubelet-配置">为什么要进行动态 Kubelet 配置？&lt;/h2>
&lt;!--
Kubernetes provides API-centric tooling that significantly improves workflows for managing applications and infrastructure. Most Kubernetes installations, however, run the Kubelet as a native process on each host, outside the scope of standard Kubernetes APIs.
-->
&lt;p>Kubernetes 提供了以 API 为中心的工具，可显着改善用于管理应用程序和基础架构的工作流程。
但是，在大多数的 Kubernetes 安装中，kubelet 在每个主机上作为本机进程运行，因此
未被标准 Kubernetes API 覆盖。&lt;/p>
&lt;!--
In the past, this meant that cluster administrators and service providers could not rely on Kubernetes APIs to reconfigure Kubelets in a live cluster. In practice, this required operators to either ssh into machines to perform manual reconfigurations, use third-party configuration management automation tools, or create new VMs with the desired configuration already installed, then migrate work to the new machines. These approaches are environment-specific and can be expensive.
-->
&lt;p>过去，这意味着集群管理员和服务提供商无法依靠 Kubernetes API 在活动集群中重新配置 Kubelets。
实际上，这要求操作员要 SSH 登录到计算机以执行手动重新配置，要么使用第三方配置管理自动化工具，
或创建已经安装了所需配置的新 VM，然后将工作迁移到新计算机上。
这些方法是特定于环境的，并且可能很耗时费力。&lt;/p>
&lt;!--
Dynamic Kubelet configuration gives cluster administrators and service providers the ability to reconfigure Kubelets in a live cluster via Kubernetes APIs.
-->
&lt;p>动态 Kubelet 配置使集群管理员和服务提供商能够通过 Kubernetes API 在活动集群中重新配置 Kubelet。&lt;/p>
&lt;!--
## What is Dynamic Kubelet Configuration?
-->
&lt;h2 id="什么是动态-kubelet-配置">什么是动态 Kubelet 配置？&lt;/h2>
&lt;!--
Kubernetes v1.10 made it possible to configure the Kubelet via a beta [config file](/docs/tasks/administer-cluster/kubelet-config-file/) API. Kubernetes already provides the ConfigMap abstraction for storing arbitrary file data in the API server.
-->
&lt;p>Kubernetes v1.10 使得可以通过 Beta 版本的&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/kubelet-config-file/">配置文件&lt;/a>
API 配置 kubelet。
Kubernetes 已经提供了用于在 API 服务器中存储任意文件数据的 ConfigMap 抽象。&lt;/p>
&lt;!--
Dynamic Kubelet configuration extends the Node object so that a Node can refer to a ConfigMap that contains the same type of config file. When a Node is updated to refer to a new ConfigMap, the associated Kubelet will attempt to use the new configuration.
-->
&lt;p>动态 Kubelet 配置扩展了 Node 对象，以便 Node 可以引用包含相同类型配置文件的 ConfigMap。
当节点更新为引用新的 ConfigMap 时，关联的 Kubelet 将尝试使用新的配置。&lt;/p>
&lt;!--
## How does it work?
-->
&lt;h2 id="它是如何工作的">它是如何工作的？&lt;/h2>
&lt;!--
Dynamic Kubelet configuration provides the following core features:
-->
&lt;p>动态 Kubelet 配置提供以下核心功能：&lt;/p>
&lt;!--
* Kubelet attempts to use the dynamically assigned configuration.
* Kubelet "checkpoints" configuration to local disk, enabling restarts without API server access.
* Kubelet reports assigned, active, and last-known-good configuration sources in the Node status.
* When invalid configuration is dynamically assigned, Kubelet automatically falls back to a last-known-good configuration and reports errors in the Node status.
-->
&lt;ul>
&lt;li>Kubelet 尝试使用动态分配的配置。&lt;/li>
&lt;li>Kubelet 将其配置已检查点的形式保存到本地磁盘，无需 API 服务器访问即可重新启动。&lt;/li>
&lt;li>Kubelet 在 Node 状态中报告已指定的、活跃的和最近已知良好的配置源。&lt;/li>
&lt;li>当动态分配了无效的配置时，Kubelet 会自动退回到最后一次正确的配置，并在 Node 状态中报告错误。&lt;/li>
&lt;/ul>
&lt;!--
To use the dynamic Kubelet configuration feature, a cluster administrator or service provider will first post a ConfigMap containing the desired configuration, then set each Node.Spec.ConfigSource.ConfigMap reference to refer to the new ConfigMap. Operators can update these references at their preferred rate, giving them the ability to perform controlled rollouts of new configurations.
-->
&lt;p>要使用动态 Kubelet 配置功能，群集管理员或服务提供商将首先发布包含所需配置的 ConfigMap，
然后设置每个 Node.Spec.ConfigSource.ConfigMap 引用以指向新的 ConfigMap。
运营商可以以他们喜欢的速率更新这些参考，从而使他们能够执行新配置的受控部署。&lt;/p>
&lt;!--
Each Kubelet watches its associated Node object for changes. When the Node.Spec.ConfigSource.ConfigMap reference is updated, the Kubelet will "checkpoint" the new ConfigMap by writing the files it contains to local disk. The Kubelet will then exit, and the OS-level process manager will restart it. Note that if the Node.Spec.ConfigSource.ConfigMap reference is not set, the Kubelet uses the set of flags and config files local to the machine it is running on.
-->
&lt;p>每个 Kubelet 都会监视其关联的 Node 对象的更改。
更新 Node.Spec.ConfigSource.ConfigMap 引用后，
Kubelet 将通过将其包含的文件通过检查点机制写入本地磁盘保存新的 ConfigMap。
然后，Kubelet 将退出，而操作系统级进程管理器将重新启动它。
请注意，如果未设置 Node.Spec.ConfigSource.ConfigMap 引用，
则 Kubelet 将使用其正在运行的计算机本地的一组标志和配置文件。&lt;/p>
&lt;!--
Once restarted, the Kubelet will attempt to use the configuration from the new checkpoint. If the new configuration passes the Kubelet's internal validation, the Kubelet will update Node.Status.Config to reflect that it is using the new configuration. If the new configuration is invalid, the Kubelet will fall back to its last-known-good configuration and report an error in Node.Status.Config.
-->
&lt;p>重新启动后，Kubelet 将尝试使用来自新检查点的配置。
如果新配置通过了 Kubelet 的内部验证，则 Kubelet 将更新
Node.Status.Config 用以反映它正在使用新配置。
如果新配置无效，则 Kubelet 将退回到其最后一个正确的配置，并在 Node.Status.Config 中报告错误。&lt;/p>
&lt;!--
Note that the default last-known-good configuration is the combination of Kubelet command-line flags with the Kubelet's local configuration file. Command-line flags that overlap with the config file always take precedence over both the local configuration file and dynamic configurations, for backwards-compatibility.
-->
&lt;p>请注意，默认的最后一次正确配置是 Kubelet 命令行标志与 Kubelet 的本地配置文件的组合。
与配置文件重叠的命令行标志始终优先于本地配置文件和动态配置，以实现向后兼容。&lt;/p>
&lt;!--
See the following diagram for a high-level overview of a configuration update for a single Node:
-->
&lt;p>有关单个节点的配置更新的高级概述，请参见下图：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-07-11-dynamic-kubelet-configuration/kubelet-diagram.png" alt="kubelet-diagram">&lt;/p>
&lt;!--
## How can I learn more?
-->
&lt;h2 id="我如何了解更多">我如何了解更多？&lt;/h2>
&lt;!--
Please see the official tutorial at /docs/tasks/administer-cluster/reconfigure-kubelet/, which contains more in-depth details on user workflow, how a configuration becomes "last-known-good," how the Kubelet "checkpoints" config, and possible failure modes.
-->
&lt;p>请参阅/docs/tasks/administer-cluster/reconfigure-kubelet/上的官方教程，
其中包含有关用户工作流，某配置如何成为“最新的正确的”配置，Kubelet 如何对配置执行“检查点”操作等，
更多详细信息，以及可能的故障模式。&lt;/p></description></item><item><title>Blog: 用于 Kubernetes 集群 DNS 的 CoreDNS GA 正式发布</title><link>https://kubernetes.io/zh/blog/2018/07/10/%E7%94%A8%E4%BA%8E-kubernetes-%E9%9B%86%E7%BE%A4-dns-%E7%9A%84-coredns-ga-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/10/%E7%94%A8%E4%BA%8E-kubernetes-%E9%9B%86%E7%BE%A4-dns-%E7%9A%84-coredns-ga-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</guid><description>
&lt;!--
---
layout: blog
title: "CoreDNS GA for Kubernetes Cluster DNS"
date: 2018-07-10
---
--->
&lt;!--
**Author**: John Belamaric (Infoblox)
**Editor’s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/) on what’s new in Kubernetes 1.11**
--->
&lt;p>&lt;strong>作者&lt;/strong>：John Belamaric (Infoblox)&lt;/p>
&lt;p>**编者注：这篇文章是 &lt;a href="https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/">系列深度文章&lt;/a> 中的一篇，介绍了 Kubernetes 1.11 新增的功能&lt;/p>
&lt;!--
## Introduction
In Kubernetes 1.11, [CoreDNS](https://coredns.io) has reached General Availability (GA) for DNS-based service discovery, as an alternative to the kube-dns addon. This means that CoreDNS will be offered as an option in upcoming versions of the various installation tools. In fact, the kubeadm team chose to make it the default option starting with Kubernetes 1.11.
--->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>在 Kubernetes 1.11 中，&lt;a href="https://coredns.io">CoreDNS&lt;/a> 已经达到基于 DNS 服务发现的 General Availability (GA)，可以替代 kube-dns 插件。这意味着 CoreDNS 会作为即将发布的安装工具的选项之一上线。实际上，从 Kubernetes 1.11 开始，kubeadm 团队选择将它设为默认选项。&lt;/p>
&lt;!--
DNS-based service discovery has been part of Kubernetes for a long time with the kube-dns cluster addon. This has generally worked pretty well, but there have been some concerns around the reliability, flexibility and security of the implementation.
CoreDNS is a general-purpose, authoritative DNS server that provides a backwards-compatible, but extensible, integration with Kubernetes. It resolves the issues seen with kube-dns, and offers a number of unique features that solve a wider variety of use cases.
In this article, you will learn about the differences in the implementations of kube-dns and CoreDNS, and some of the helpful extensions offered by CoreDNS.
--->
&lt;p>很久以来， kube-dns 集群插件一直是 Kubernetes 的一部分，用来实现基于 DNS 的服务发现。
通常，此插件运行平稳，但对于实现的可靠性、灵活性和安全性仍存在一些疑虑。&lt;/p>
&lt;p>CoreDNS 是通用的、权威的 DNS 服务器，提供与 Kubernetes 向后兼容但可扩展的集成。它解决了 kube-dns 遇到的问题，并提供了许多独特的功能，可以解决各种用例。&lt;/p>
&lt;p>在本文中，您将了解 kube-dns 和 CoreDNS 的实现有何差异，以及 CoreDNS 提供的一些非常有用的扩展。&lt;/p>
&lt;!--
## Implementation differences
In kube-dns, several containers are used within a single pod: `kubedns`, `dnsmasq`, and `sidecar`. The `kubedns`
container watches the Kubernetes API and serves DNS records based on the [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md), `dnsmasq` provides caching and stub domain support, and `sidecar` provides metrics and health checks.
--->
&lt;h2 id="实现差异">实现差异&lt;/h2>
&lt;p>在 kube-dns 中，一个 Pod 中使用多个 容器：&lt;code>kubedns&lt;/code>、&lt;code>dnsmasq&lt;/code>、和 &lt;code>sidecar&lt;/code>。&lt;code>kubedns&lt;/code> 容器监视 Kubernetes API 并根据 &lt;a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS 规范&lt;/a> 提供 DNS 记录，&lt;code>dnsmasq&lt;/code> 提供缓存和存根域支持，&lt;code>sidecar&lt;/code> 提供指标和健康检查。&lt;/p>
&lt;!--
This setup leads to a few issues that have been seen over time. For one, security vulnerabilities in `dnsmasq` have led to the need
for a security-patch release of Kubernetes in the past. Additionally, because `dnsmasq` handles the stub domains,
but `kubedns` handles the External Services, you cannot use a stub domain in an external service, which is very
limiting to that functionality (see [dns#131](https://github.com/kubernetes/dns/issues/131)).
All of these functions are done in a single container in CoreDNS, which is running a process written in Go. The
different plugins that are enabled replicate (and enhance) the functionality found in kube-dns.
--->
&lt;p>随着时间的推移，此设置会导致一些问题。一方面，以往 &lt;code>dnsmasq&lt;/code> 中的安全漏洞需要通过发布 Kubernetes 的安全补丁来解决。但是，由于 &lt;code>dnsmasq&lt;/code> 处理存根域，而 &lt;code>kubedns&lt;/code> 处理外部服务，因此您不能在外部服务中使用存根域，导致这个功能具有局限性（请参阅 &lt;a href="https://github.com/kubernetes/dns/issues/131">dns#131&lt;/a>）。&lt;/p>
&lt;p>在 CoreDNS 中，所有这些功能都是在一个容器中完成的，该容器运行用 Go 编写的进程。所启用的不同插件可复制（并增强）在 kube-dns 中存在的功能。&lt;/p>
&lt;!--
## Configuring CoreDNS
In kube-dns, you can [modify a ConfigMap](https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/) to change the behavior of your service discovery. This allows the addition of
features such as serving stub domains, modifying upstream nameservers, and enabling federation.
--->
&lt;h2 id="配置-coredns">配置 CoreDNS&lt;/h2>
&lt;p>在 kube-dns 中，您可以 &lt;a href="https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/">修改 ConfigMap&lt;/a> 来更改服务发现的行为。用户可以添加诸如为存根域提供服务、修改上游名称服务器以及启用联盟之类的功能。&lt;/p>
&lt;!--
In CoreDNS, you similarly can modify the ConfigMap for the CoreDNS [Corefile](https://coredns.io/2017/07/23/corefile-explained/) to change how service discovery
works. This Corefile configuration offers many more options than you will find in kube-dns, since it is the
primary configuration file that CoreDNS uses for configuration of all of its features, even those that are not
Kubernetes related.
When upgrading from kube-dns to CoreDNS using `kubeadm`, your existing ConfigMap will be used to generate the
customized Corefile for you, including all of the configuration for stub domains, federation, and upstream nameservers. See [Using CoreDNS for Service Discovery](/docs/tasks/administer-cluster/coredns/) for more details.
--->
&lt;p>在 CoreDNS 中，您可以类似地修改 CoreDNS &lt;a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile&lt;/a> 的 ConfigMap，以更改服务发现的工作方式。这种 Corefile 配置提供了比 kube-dns 中更多的选项，因为它是 CoreDNS 用于配置所有功能的主要配置文件，即使与 Kubernetes 不相关的功能也可以操作。&lt;/p>
&lt;p>使用 &lt;code>kubeadm&lt;/code> 将 kube-dns 升级到 CoreDNS 时，现有的 ConfigMap 将被用来为您生成自定义的 Corefile，包括存根域、联盟和上游名称服务器的所有配置。更多详细信息，请参见
&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/coredns/">使用 CoreDNS 进行服务发现&lt;/a>。&lt;/p>
&lt;!--
## Bug fixes and enhancements
There are several open issues with kube-dns that are resolved in CoreDNS, either in default configuration or with some customized configurations.
--->
&lt;h2 id="错误修复和增强">错误修复和增强&lt;/h2>
&lt;p>在 CoreDNS 中解决了 kube-dn 的多个未解决问题，无论是默认配置还是某些自定义配置。&lt;/p>
&lt;!--
* [dns#55 - Custom DNS entries for kube-dns](https://github.com/kubernetes/dns/issues/55) may be handled using the "fallthrough" mechanism in the [kubernetes plugin](https://coredns.io/plugins/kubernetes), using the [rewrite plugin](https://coredns.io/plugins/rewrite), or simply serving a subzone with a different plugin such as the [file plugin](https://coredns.io/plugins/file).
* [dns#116 - Only one A record set for headless service with pods having single hostname](https://github.com/kubernetes/dns/issues/116). This issue is fixed without any additional configuration.
* [dns#131 - externalName not using stubDomains settings](https://github.com/kubernetes/dns/issues/131). This issue is fixed without any additional configuration.
* [dns#167 - enable skyDNS round robin A/AAAA records](https://github.com/kubernetes/dns/issues/167). The equivalent functionality can be configured using the [load balance plugin](https://coredns.io/plugins/loadbalance).
* [dns#190 - kube-dns cannot run as non-root user](https://github.com/kubernetes/dns/issues/190). This issue is solved today by using a non-default image, but it will be made the default CoreDNS behavior in a future release.
* [dns#232 - fix pod hostname to be podname for dns srv records](https://github.com/kubernetes/dns/issues/232) is an enhancement that is supported through the "endpoint_pod_names" feature described below.
--->
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/55">dns#55 - kube-dns 的自定义 DNS 条目&lt;/a> 可以使用 &lt;a href="https://coredns.io/plugins/kubernetes">kubernetes 插件&lt;/a> 中的 &amp;quot;fallthrough&amp;quot; 机制，使用 &lt;a href="https://coredns.io/plugins/rewrite">rewrite 插件&lt;/a>，或者分区使用不同的插件，例如 &lt;a href="https://coredns.io/plugins/file">file 插件&lt;/a>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/116">dns#116 - 对具有相同主机名的、提供无头服务服务的 Pod 仅设置了一个 A 记录&lt;/a>。无需任何其他配置即可解决此问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/131">dns#131 - externalName 未使用 stubDomains 设置&lt;/a>。无需任何其他配置即可解决此问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/167">dns#167 - 允许 skyDNS 为 A/AAAA 记录提供轮换&lt;/a>。可以使用 &lt;a href="https://coredns.io/plugins/loadbalance">负载均衡插件&lt;/a> 配置等效功能。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/190">dns#190 - kube-dns 无法以非 root 用户身份运行&lt;/a>。今天，通过使用 non-default 镜像解决了此问题，但是在将来的版本中，它将成为默认的 CoreDNS 行为。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/232">dns#232 - 在 dns srv 记录中修复 pod hostname 为 podname&lt;/a> 是通过下面提到的 &amp;quot;endpoint_pod_names&amp;quot; 功能进行支持的增强功能。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
## Metrics
The functional behavior of the default CoreDNS configuration is the same as kube-dns. However,
one difference you need to be aware of is that the published metrics are not the same. In kube-dns,
you get separate metrics for `dnsmasq` and `kubedns` (skydns). In CoreDNS there is a completely
different set of metrics, since it is all a single process. You can find more details on these
metrics on the CoreDNS [Prometheus plugin](https://coredns.io/plugins/metrics/) page.
--->
&lt;h2 id="指标">指标&lt;/h2>
&lt;p>CoreDNS 默认配置的功能性行为与 kube-dns 相同。但是，你需要了解的差别之一是二者发布的指标是不同的。在 kube-dns 中，您将分别获得 &lt;code>dnsmasq&lt;/code> 和 &lt;code>kubedns&lt;/code>（skydns）的度量值。在 CoreDNS 中，存在一组完全不同的指标，因为它们在同一个进程中。您可以在 CoreDNS &lt;a href="https://coredns.io/plugins/metrics/">Prometheus 插件&lt;/a> 页面上找到有关这些指标的更多详细信息。&lt;/p>
&lt;!--
## Some special features
The standard CoreDNS Kubernetes configuration is designed to be backwards compatible with the prior
kube-dns behavior. But with some configuration changes, CoreDNS can allow you to modify how the
DNS service discovery works in your cluster. A number of these features are intended to still be
compliant with the [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md);
they enhance functionality but remain backward compatible. Since CoreDNS is not
*only* made for Kubernetes, but is instead a general-purpose DNS server, there are many things you
can do beyond that specification.
--->
&lt;h2 id="一些特殊功能">一些特殊功能&lt;/h2>
&lt;p>标准的 CoreDNS Kubernetes 配置旨在与以前的 kube-dns 在行为上向后兼容。但是，通过进行一些配置更改，CoreDNS 允许您修改 DNS 服务发现在群集中的工作方式。这些功能中的许多功能仍要符合 &lt;a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS规范&lt;/a>；它们在增强了功能的同时保持向后兼容。由于 CoreDNS 并非 &lt;em>仅&lt;/em> 用于 Kubernetes，而是通用的 DNS 服务器，因此您可以做很多超出该规范的事情。&lt;/p>
&lt;!--
### Pods verified mode
In kube-dns, pod name records are "fake". That is, any "a-b-c-d.namespace.pod.cluster.local" query will
return the IP address "a.b.c.d". In some cases, this can weaken the identity guarantees offered by TLS. So,
CoreDNS offers a "pods verified" mode, which will only return the IP address if there is a pod in the
specified namespace with that IP address.
--->
&lt;h3 id="pod-验证模式">Pod 验证模式&lt;/h3>
&lt;p>在 kube-dns 中，Pod 名称记录是 &amp;quot;伪造的&amp;quot;。也就是说，任何 &amp;quot;a-b-c-d.namespace.pod.cluster.local&amp;quot; 查询都将返回 IP 地址 &amp;quot;a.b.c.d&amp;quot;。在某些情况下，这可能会削弱 TLS 提供的身份确认。因此，CoreDNS 提供了一种 &amp;quot;Pod 验证&amp;quot; 的模式，该模式仅在指定名称空间中存在具有该 IP 地址的 Pod 时才返回 IP 地址。&lt;/p>
&lt;!--
### Endpoint names based on pod names
In kube-dns, when using a headless service, you can use an SRV request to get a list of
all endpoints for the service:
--->
&lt;h3 id="基于-pod-名称的端点名称">基于 Pod 名称的端点名称&lt;/h3>
&lt;p>在 kube-dns 中，使用无头服务时，可以使用 SRV 请求获取该服务的所有端点的列表：&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 10 33 0 6234396237313665.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6662363165353239.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6338633437303230.headless.default.svc.cluster.local.
dnstools#
&lt;/code>&lt;/pre>&lt;!--
However, the endpoint DNS names are (for practical purposes) random. In CoreDNS, by default, you get endpoint
DNS names based upon the endpoint IP address:
--->
&lt;p>但是，端点 DNS 名称（出于实际目的）是随机的。在 CoreDNS 中，默认情况下，您所获得的端点 DNS 名称是基于端点 IP 地址生成的：&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-14.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-18.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-4.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-9.headless.default.svc.cluster.local.
&lt;/code>&lt;/pre>&lt;!--
For some applications, it is desirable to have the pod name for this, rather than the pod IP
address (see for example [kubernetes#47992](https://github.com/kubernetes/kubernetes/issues/47992) and [coredns#1190](https://github.com/coredns/coredns/pull/1190)). To enable this in CoreDNS, you specify the "endpoint_pod_names" option in your Corefile, which results in this:
--->
&lt;p>对于某些应用程序，你会希望在这里使用 Pod 名称，而不是 Pod IP 地址（例如，参见 &lt;a href="https://github.com/kubernetes/kubernetes/issues/47992">kubernetes#47992&lt;/a> 和 &lt;a href="https://github.com/coredns/coredns/pull/1190">coredns#1190&lt;/a>）。要在 CoreDNS 中启用此功能，请在 Corefile 中指定 &amp;quot;endpoint_pod_names&amp;quot; 选项，结果如下：&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-qv84p.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-zc8lx.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-q7lf2.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-566rt.headless.default.svc.cluster.local.
&lt;/code>&lt;/pre>&lt;!--
### Autopath
CoreDNS also has a special feature to improve latency in DNS requests for external names. In Kubernetes, the
DNS search path for pods specifies a long list of suffixes. This enables the use of short names when requesting
services in the cluster - for example, "headless" above, rather than "headless.default.svc.cluster.local". However,
when requesting an external name - "infoblox.com", for example - several invalid DNS queries are made by the client,
requiring a roundtrip from the client to kube-dns each time (actually to `dnsmasq` and then to `kubedns`, since [negative caching is disabled](https://github.com/kubernetes/dns/issues/121)):
--->
&lt;h3 id="自动路径">自动路径&lt;/h3>
&lt;p>CoreDNS 还具有一项特殊功能，可以改善 DNS 中外部名称请求的延迟。在 Kubernetes 中，Pod 的 DNS 搜索路径指定了一长串后缀。这一特点使得你可以针对集群中服务使用短名称 - 例如，上面的 &amp;quot;headless&amp;quot;，而不是 &amp;quot;headless.default.svc.cluster.local&amp;quot;。但是，当请求一个外部名称（例如 &amp;quot;infoblox.com&amp;quot;）时，客户端会进行几个无效的 DNS 查询，每次都需要从客户端到 kube-dns 往返（实际上是到 &lt;code>dnsmasq&lt;/code>，然后到 &lt;code>kubedns&lt;/code>），因为 &lt;a href="https://github.com/kubernetes/dns/issues/121">禁用了负缓存&lt;/a>）&lt;/p>
&lt;ul>
&lt;li>infoblox.com.default.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.your-internal-domain.com -&amp;gt; NXDOMAIN&lt;/li>
&lt;/ul>
&lt;!--
* infoblox.com -> returns a valid record
--->
&lt;ul>
&lt;li>infoblox.com -&amp;gt; 返回有效记录&lt;/li>
&lt;/ul>
&lt;!--
In CoreDNS, an optional feature called [autopath](https://coredns.io/plugins/autopath) can be enabled that will cause this search path to be followed
*in the server*. That is, CoreDNS will figure out from the source IP address which namespace the client pod is in,
and it will walk this search list until it gets a valid answer. Since the first 3 of these are resolved internally
within CoreDNS itself, it cuts out all of the back and forth between the client and server, reducing latency.
--->
&lt;p>在 CoreDNS 中，可以启用 &lt;a href="https://coredns.io/plugins/autopath">autopath&lt;/a> 的可选功能，该功能使搜索路径在 &lt;em>服务器端&lt;/em> 遍历。也就是说，CoreDNS 将基于源 IP 地址判断客户端 Pod 所在的命名空间，并且遍历此搜索列表，直到获得有效答案为止。由于其中的前三个是在 CoreDNS 本身内部解决的，因此它消除了客户端和服务器之间所有的来回通信，从而减少了延迟。&lt;/p>
&lt;!--
### A few other Kubernetes specific features
In CoreDNS, you can use standard DNS zone transfer to export the entire DNS record set. This is useful for
debugging your services as well as importing the cluster zone into other DNS servers.
You can also filter by namespaces or a label selector. This can allow you to run specific CoreDNS instances that will only server records that match the filters, exposing only a limited set of your services via DNS.
--->
&lt;h3 id="其他一些特定于-kubernetes-的功能">其他一些特定于 Kubernetes 的功能&lt;/h3>
&lt;p>在 CoreDNS 中，您可以使用标准 DNS 区域传输来导出整个 DNS 记录集。这对于调试服务以及将集群区导入其他 DNS 服务器很有用。&lt;/p>
&lt;p>您还可以按名称空间或标签选择器进行过滤。这样，您可以运行特定的 CoreDNS 实例，该实例仅服务与过滤器匹配的记录，从而通过 DNS 公开受限的服务集。&lt;/p>
&lt;!--
## Extensibility
In addition to the features described above, CoreDNS is easily extended. It is possible to build custom versions
of CoreDNS that include your own features. For example, this ability has been used to extend CoreDNS to do recursive resolution
with the [unbound plugin](https://coredns.io/explugins/unbound), to server records directly from a database with the [pdsql plugin](https://coredns.io/explugins/pdsql), and to allow multiple CoreDNS instances to share a common level 2 cache with the [redisc plugin](https://coredns.io/explugins/redisc).
Many other interesting extensions have been added, which you will find on the [External Plugins](https://coredns.io/explugins/) page of the CoreDNS site. One that is really interesting for Kubernetes and Istio users is the [kubernetai plugin](https://coredns.io/explugins/kubernetai), which allows a single CoreDNS instance to connect to multiple Kubernetes clusters and provide service discovery across all of them.
--->
&lt;h2 id="可扩展性">可扩展性&lt;/h2>
&lt;p>除了上述功能之外，CoreDNS 还可轻松扩展，构建包含您独有的功能的自定义版本的 CoreDNS。例如，这一能力已被用于扩展 CoreDNS 来使用 &lt;a href="https://coredns.io/explugins/unbound">unbound 插件&lt;/a> 进行递归解析、使用 &lt;a href="https://coredns.io/explugins/pdsql">pdsql 插件&lt;/a> 直接从数据库提供记录，以及使用 &lt;a href="https://coredns.io/explugins/redisc">redisc 插件&lt;/a> 与多个 CoreDNS 实例共享一个公共的 2 级缓存。&lt;/p>
&lt;p>已添加的还有许多其他有趣的扩展，您可以在 CoreDNS 站点的 &lt;a href="https://coredns.io/explugins/">外部插件&lt;/a> 页面上找到这些扩展。Kubernetes 和 Istio 用户真正感兴趣的是 &lt;a href="https://coredns.io/explugins/kubernetai">kubernetai 插件&lt;/a>，它允许单个 CoreDNS 实例连接到多个 Kubernetes 集群并在所有集群中提供服务发现 。&lt;/p>
&lt;!--
## What's Next?
CoreDNS is an independent project, and as such is developing many features that are not directly
related to Kubernetes. However, a number of these will have applications within Kubernetes. For example,
the upcoming integration with policy engines will allow CoreDNS to make intelligent choices about which endpoint
to return when a headless service is requested. This could be used to route traffic to a local pod, or
to a more responsive pod. Many other features are in development, and of course as an open source project, we welcome you to suggest and contribute your own features!
The features and differences described above are a few examples. There is much more you can do with CoreDNS.
You can find out more on the [CoreDNS Blog](https://coredns.io/blog).
--->
&lt;h2 id="下一步工作">下一步工作&lt;/h2>
&lt;p>CoreDNS 是一个独立的项目，许多与 Kubernetes 不直接相关的功能正在开发中。但是，其中许多功能将在 Kubernetes 中具有对应的应用。例如，与策略引擎完成集成后，当请求无头服务时，CoreDNS 能够智能地选择返回哪个端点。这可用于将流量分流到到本地 Pod 或响应更快的 Pod。更多的其他功能正在开发中，当然作为一个开源项目，我们欢迎您提出建议并贡献自己的功能特性！&lt;/p>
&lt;p>上述特征和差异是几个示例。CoreDNS 还可以做更多的事情。您可以在 &lt;a href="https://coredns.io/blog">CoreDNS 博客&lt;/a> 上找到更多信息。&lt;/p>
&lt;!--
### Get involved with CoreDNS
CoreDNS is an incubated [CNCF](https:://cncf.io) project.
We're most active on Slack (and GitHub):
--->
&lt;h3 id="参与-coredns">参与 CoreDNS&lt;/h3>
&lt;p>CoreDNS 是一个 &lt;a href="https:://cncf.io">CNCF&lt;/a> 孵化项目。&lt;/p>
&lt;p>我们在 Slack（和 GitHub）上最活跃：&lt;/p>
&lt;ul>
&lt;li>Slack: #coredns on &lt;a href="https://slack.cncf.io">https://slack.cncf.io&lt;/a>&lt;/li>
&lt;li>GitHub: &lt;a href="https://github.com/coredns/coredns">https://github.com/coredns/coredns&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
More resources can be found:
--->
&lt;p>更多资源请浏览：&lt;/p>
&lt;ul>
&lt;li>Website: &lt;a href="https://coredns.io">https://coredns.io&lt;/a>&lt;/li>
&lt;li>Blog: &lt;a href="https://blog.coredns.io">https://blog.coredns.io&lt;/a>&lt;/li>
&lt;li>Twitter: &lt;a href="https://twitter.com/corednsio">@corednsio&lt;/a>&lt;/li>
&lt;li>Mailing list/group: &lt;a href="mailto:coredns-discuss@googlegroups.com">coredns-discuss@googlegroups.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: IPVS-Based In-Cluster Load Balancing Deep Dive</title><link>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</link><pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</guid><description>
&lt;!--
Author: Jun Du(Huawei), Haibin Xie(Huawei), Wei Liang(Huawei)
Editor’s note: this post is part of a series of in-depth articles on what’s new in Kubernetes 1.11
-->
&lt;p>作者: Jun Du(华为), Haibin Xie(华为), Wei Liang(华为)&lt;/p>
&lt;p>注意: 这篇文章出自 系列深度文章 介绍 Kubernetes 1.11 的新特性&lt;/p>
&lt;!--
Introduction
Per the Kubernetes 1.11 release blog post , we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.
-->
&lt;p>介绍&lt;/p>
&lt;p>根据 Kubernetes 1.11 发布的博客文章, 我们宣布基于 IPVS 的集群内部服务负载均衡已达到一般可用性。 在这篇博客中，我们将带您深入了解该功能。&lt;/p>
&lt;!--
What Is IPVS?
IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.
IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.
-->
&lt;p>什么是 IPVS ?&lt;/p>
&lt;p>IPVS (IP Virtual Server)是在 Netfilter 上层构建的，并作为 Linux 内核的一部分，实现传输层负载均衡。&lt;/p>
&lt;p>IPVS 集成在 LVS（Linux Virtual Server，Linux 虚拟服务器）中，它在主机上运行，并在物理服务器集群前作为负载均衡器。IPVS 可以将基于 TCP 和 UDP 服务的请求定向到真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。 因此，IPVS 自然支持 Kubernetes 服务。&lt;/p>
&lt;!--
Why IPVS for Kubernetes?
As Kubernetes grows in usage, the scalability of its resources becomes more and more important. In particular, the scalability of services is paramount to the adoption of Kubernetes by developers/companies running large workloads.
Kube-proxy, the building block of service routing has relied on the battle-hardened iptables to implement the core supported Service types such as ClusterIP and NodePort. However, iptables struggles to scale to tens of thousands of Services because it is designed purely for firewalling purposes and is based on in-kernel rule lists.
Even though Kubernetes already support 5000 nodes in release v1.6, the kube-proxy with iptables is actually a bottleneck to scale the cluster to 5000 nodes. One example is that with NodePort Service in a 5000-node cluster, if we have 2000 services and each services have 10 pods, this will cause at least 20000 iptable records on each worker node, and this can make the kernel pretty busy.
On the other hand, using IPVS-based in-cluster service load balancing can help a lot for such cases. IPVS is specifically designed for load balancing and uses more efficient data structures (hash tables) allowing for almost unlimited scale under the hood.
-->
&lt;p>为什么为 Kubernetes 选择 IPVS ?&lt;/p>
&lt;p>随着 Kubernetes 的使用增长，其资源的可扩展性变得越来越重要。特别是，服务的可扩展性对于运行大型工作负载的开发人员/公司采用 Kubernetes 至关重要。&lt;/p>
&lt;p>Kube-proxy 是服务路由的构建块，它依赖于经过强化攻击的 iptables 来实现支持核心的服务类型，如 ClusterIP 和 NodePort。 但是，iptables 难以扩展到成千上万的服务，因为它纯粹是为防火墙而设计的，并且基于内核规则列表。&lt;/p>
&lt;p>尽管 Kubernetes 在版本v1.6中已经支持5000个节点，但使用 iptables 的 kube-proxy 实际上是将集群扩展到5000个节点的瓶颈。 一个例子是，在5000节点集群中使用 NodePort 服务，如果我们有2000个服务并且每个服务有10个 pod，这将在每个工作节点上至少产生20000个 iptable 记录，这可能使内核非常繁忙。&lt;/p>
&lt;p>另一方面，使用基于 IPVS 的集群内服务负载均衡可以为这种情况提供很多帮助。 IPVS 专门用于负载均衡，并使用更高效的数据结构（哈希表），允许几乎无限的规模扩张。&lt;/p>
&lt;!--
IPVS-based Kube-proxy
Parameter Changes
Parameter: --proxy-mode In addition to existing userspace and iptables modes, IPVS mode is configured via --proxy-mode=ipvs. It implicitly uses IPVS NAT mode for service port mapping.
-->
&lt;p>基于 IPVS 的 Kube-proxy&lt;/p>
&lt;p>参数更改&lt;/p>
&lt;p>参数: --proxy-mode 除了现有的用户空间和 iptables 模式，IPVS 模式通过--proxy-mode = ipvs 进行配置。 它隐式使用 IPVS NAT 模式进行服务端口映射。&lt;/p>
&lt;!--
Parameter: --ipvs-scheduler
A new kube-proxy parameter has been added to specify the IPVS load balancing algorithm, with the parameter being --ipvs-scheduler. If it’s not configured, then round-robin (rr) is the default value.
- rr: round-robin
- lc: least connection
- dh: destination hashing
- sh: source hashing
- sed: shortest expected delay
- nq: never queue
In the future, we can implement Service specific scheduler (potentially via annotation), which has higher priority and overwrites the value.
-->
&lt;p>参数: --ipvs-scheduler&lt;/p>
&lt;p>添加了一个新的 kube-proxy 参数来指定 IPVS 负载均衡算法，参数为 --ipvs-scheduler。 如果未配置，则默认为 round-robin 算法（rr）。&lt;/p>
&lt;ul>
&lt;li>rr: round-robin&lt;/li>
&lt;li>lc: least connection&lt;/li>
&lt;li>dh: destination hashing&lt;/li>
&lt;li>sh: source hashing&lt;/li>
&lt;li>sed: shortest expected delay&lt;/li>
&lt;li>nq: never queue&lt;/li>
&lt;/ul>
&lt;p>将来，我们可以实现特定于服务的调度程序（可能通过注释），该调度程序具有更高的优先级并覆盖该值。&lt;/p>
&lt;!--
Parameter: --cleanup-ipvs Similar to the --cleanup-iptables parameter, if true, cleanup IPVS configuration and IPTables rules that are created in IPVS mode.
Parameter: --ipvs-sync-period Maximum interval of how often IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.
Parameter: --ipvs-min-sync-period Minimum interval of how often the IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.
-->
&lt;p>参数: --cleanup-ipvs 类似于 --cleanup-iptables 参数，如果为 true，则清除在 IPVS 模式下创建的 IPVS 配置和 IPTables 规则。&lt;/p>
&lt;p>参数: --ipvs-sync-period 刷新 IPVS 规则的最大间隔时间（例如'5s'，'1m'）。 必须大于0。&lt;/p>
&lt;p>参数: --ipvs-min-sync-period 刷新 IPVS 规则的最小间隔时间间隔（例如'5s'，'1m'）。 必须大于0。&lt;/p>
&lt;!--
Parameter: --ipvs-exclude-cidrs A comma-separated list of CIDR's which the IPVS proxier should not touch when cleaning up IPVS rules because IPVS proxier can't distinguish kube-proxy created IPVS rules from user original IPVS rules. If you are using IPVS proxier with your own IPVS rules in the environment, this parameter should be specified, otherwise your original rule will be cleaned.
-->
&lt;p>参数: --ipvs-exclude-cidrs 清除 IPVS 规则时 IPVS 代理不应触及的 CIDR 的逗号分隔列表，因为 IPVS 代理无法区分 kube-proxy 创建的 IPVS 规则和用户原始规则 IPVS 规则。 如果您在环境中使用 IPVS proxier 和您自己的 IPVS 规则，则应指定此参数，否则将清除原始规则。&lt;/p>
&lt;!--
Design Considerations
IPVS Service Network Topology
When creating a ClusterIP type Service, IPVS proxier will do the following three things:
- Make sure a dummy interface exists in the node, defaults to kube-ipvs0
- Bind Service IP addresses to the dummy interface
- Create IPVS virtual servers for each Service IP address respectively
-->
&lt;p>设计注意事项&lt;/p>
&lt;p>IPVS 服务网络拓扑&lt;/p>
&lt;p>创建 ClusterIP 类型服务时，IPVS proxier 将执行以下三项操作：&lt;/p>
&lt;ul>
&lt;li>确保节点中存在虚拟接口，默认为 kube-ipvs0&lt;/li>
&lt;li>将服务 IP 地址绑定到虚拟接口&lt;/li>
&lt;li>分别为每个服务 IP 地址创建 IPVS 虚拟服务器&lt;/li>
&lt;/ul>
&lt;!--
Here comes an example:
# kubectl describe svc nginx-service
Name: nginx-service
...
Type: ClusterIP
IP: 10.102.128.4
Port: http 3080/TCP
Endpoints: 10.244.0.235:8080,10.244.1.237:8080
Session Affinity: None
# ip addr
...
73: kube-ipvs0: &lt;BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN qlen 1000
link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
inet 10.102.128.4/32 scope global kube-ipvs0
valid_lft forever preferred_lft forever
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-> RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr
-> 10.244.0.235:8080 Masq 1 0 0
-> 10.244.1.237:8080 Masq 1 0 0
-->
&lt;p>这是一个例子:&lt;/p>
&lt;pre>&lt;code># kubectl describe svc nginx-service
Name: nginx-service
...
Type: ClusterIP
IP: 10.102.128.4
Port: http 3080/TCP
Endpoints: 10.244.0.235:8080,10.244.1.237:8080
Session Affinity: None
# ip addr
...
73: kube-ipvs0: &amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state DOWN qlen 1000
link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
inet 10.102.128.4/32 scope global kube-ipvs0
valid_lft forever preferred_lft forever
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-&amp;gt; RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr
-&amp;gt; 10.244.0.235:8080 Masq 1 0 0
-&amp;gt; 10.244.1.237:8080 Masq 1 0 0
&lt;/code>&lt;/pre>
&lt;!--
Please note that the relationship between a Kubernetes Service and IPVS virtual servers is 1:N. For example, consider a Kubernetes Service that has more than one IP address. An External IP type Service has two IP addresses - ClusterIP and External IP. Then the IPVS proxier will create 2 IPVS virtual servers - one for Cluster IP and another one for External IP. The relationship between a Kubernetes Endpoint (each IP+Port pair) and an IPVS virtual server is 1:1.
Deleting of a Kubernetes service will trigger deletion of the corresponding IPVS virtual server, IPVS real servers and its IP addresses bound to the dummy interface.
Port Mapping
There are three proxy modes in IPVS: NAT (masq), IPIP and DR. Only NAT mode supports port mapping. Kube-proxy leverages NAT mode for port mapping. The following example shows IPVS mapping Service port 3080 to Pod port 8080.
-->
&lt;p>请注意，Kubernetes 服务和 IPVS 虚拟服务器之间的关系是“1：N”。 例如，考虑具有多个 IP 地址的 Kubernetes 服务。 外部 IP 类型服务有两个 IP 地址 - 集群IP和外部 IP。 然后，IPVS 代理将创建2个 IPVS 虚拟服务器 - 一个用于集群 IP，另一个用于外部 IP。 Kubernetes 的 endpoint（每个IP +端口对）与 IPVS 虚拟服务器之间的关系是“1：1”。&lt;/p>
&lt;p>删除 Kubernetes 服务将触发删除相应的 IPVS 虚拟服务器，IPVS 物理服务器及其绑定到虚拟接口的 IP 地址。&lt;/p>
&lt;p>端口映射&lt;/p>
&lt;p>IPVS 中有三种代理模式：NAT（masq），IPIP 和 DR。 只有 NAT 模式支持端口映射。 Kube-proxy 利用 NAT 模式进行端口映射。 以下示例显示 IPVS 服务端口3080到Pod端口8080的映射。&lt;/p>
&lt;pre>&lt;code>TCP 10.102.128.4:3080 rr
-&amp;gt; 10.244.0.235:8080 Masq 1 0 0
-&amp;gt; 10.244.1.237:8080 Masq 1 0
&lt;/code>&lt;/pre>
&lt;!--
Session Affinity
IPVS supports client IP session affinity (persistent connection). When a Service specifies session affinity, the IPVS proxier will set a timeout value (180min=10800s by default) in the IPVS virtual server. For example:
-->
&lt;p>会话关系&lt;/p>
&lt;p>IPVS 支持客户端 IP 会话关联（持久连接）。 当服务指定会话关系时，IPVS 代理将在 IPVS 虚拟服务器中设置超时值（默认为180分钟= 10800秒）。 例如：&lt;/p>
&lt;pre>&lt;code># kubectl describe svc nginx-service
Name: nginx-service
...
IP: 10.102.128.4
Port: http 3080/TCP
Session Affinity: ClientIP
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-&amp;gt; RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr persistent 10800
&lt;/code>&lt;/pre>
&lt;!--
Iptables &amp; Ipset in IPVS Proxier
IPVS is for load balancing and it can't handle other workarounds in kube-proxy, e.g. packet filtering, hairpin-masquerade tricks, SNAT, etc.
IPVS proxier leverages iptables in the above scenarios. Specifically, ipvs proxier will fall back on iptables in the following 4 scenarios:
- kube-proxy start with --masquerade-all=true
- Specify cluster CIDR in kube-proxy startup
- Support Loadbalancer type service
- Support NodePort type service
However, we don't want to create too many iptables rules. So we adopt ipset for the sake of decreasing iptables rules. The following is the table of ipset sets that IPVS proxier maintains:
-->
&lt;p>IPVS 代理中的 Iptables 和 Ipset&lt;/p>
&lt;p>IPVS 用于负载均衡，它无法处理 kube-proxy 中的其他问题，例如 包过滤，数据包欺骗，SNAT 等&lt;/p>
&lt;p>IPVS proxier 在上述场景中利用 iptables。 具体来说，ipvs proxier 将在以下4种情况下依赖于 iptables：&lt;/p>
&lt;ul>
&lt;li>kube-proxy 以 --masquerade-all = true 开头&lt;/li>
&lt;li>在 kube-proxy 启动中指定集群 CIDR&lt;/li>
&lt;li>支持 Loadbalancer 类型服务&lt;/li>
&lt;li>支持 NodePort 类型的服务&lt;/li>
&lt;/ul>
&lt;p>但是，我们不想创建太多的 iptables 规则。 所以我们采用 ipset 来减少 iptables 规则。 以下是 IPVS proxier 维护的 ipset 集表：&lt;/p>
&lt;!--
set name members usage
KUBE-CLUSTER-IP All Service IP + port masquerade for cases that masquerade-all=true or clusterCIDR specified
KUBE-LOOP-BACK All Service IP + port + IP masquerade for resolving hairpin issue
KUBE-EXTERNAL-IP Service External IP + port masquerade for packets to external IPs
KUBE-LOAD-BALANCER Load Balancer ingress IP + port masquerade for packets to Load Balancer type service
KUBE-LOAD-BALANCER-LOCAL Load Balancer ingress IP + port with externalTrafficPolicy=local accept packets to Load Balancer with externalTrafficPolicy=local
KUBE-LOAD-BALANCER-FW Load Balancer ingress IP + port with loadBalancerSourceRanges Drop packets for Load Balancer type Service with loadBalancerSourceRanges specified
KUBE-LOAD-BALANCER-SOURCE-CIDR Load Balancer ingress IP + port + source CIDR accept packets for Load Balancer type Service with loadBalancerSourceRanges specified
KUBE-NODE-PORT-TCP NodePort type Service TCP port masquerade for packets to NodePort(TCP)
KUBE-NODE-PORT-LOCAL-TCP NodePort type Service TCP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
KUBE-NODE-PORT-UDP NodePort type Service UDP port masquerade for packets to NodePort(UDP)
KUBE-NODE-PORT-LOCAL-UDP NodePort type service UDP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
-->
&lt;p>设置名称 成员 用法
KUBE-CLUSTER-IP 所有服务 IP + 端口 masquerade-all=true 或 clusterCIDR 指定的情况下进行伪装
KUBE-LOOP-BACK 所有服务 IP +端口+ IP 解决数据包欺骗问题
KUBE-EXTERNAL-IP 服务外部 IP +端口 将数据包伪装成外部 IP
KUBE-LOAD-BALANCER 负载均衡器入口 IP +端口 将数据包伪装成 Load Balancer 类型的服务
KUBE-LOAD-BALANCER-LOCAL 负载均衡器入口 IP +端口 以及 externalTrafficPolicy=local 接受数据包到 Load Balancer externalTrafficPolicy=local
KUBE-LOAD-BALANCER-FW 负载均衡器入口 IP +端口 以及 loadBalancerSourceRanges 使用指定的 loadBalancerSourceRanges 丢弃 Load Balancer类型Service的数据包
KUBE-LOAD-BALANCER-SOURCE-CIDR 负载均衡器入口 IP +端口 + 源 CIDR 接受 Load Balancer 类型 Service 的数据包，并指定loadBalancerSourceRanges
KUBE-NODE-PORT-TCP NodePort 类型服务 TCP 将数据包伪装成 NodePort（TCP）
KUBE-NODE-PORT-LOCAL-TCP NodePort 类型服务 TCP 端口，带有 externalTrafficPolicy=local 接受数据包到 NodePort 服务 使用 externalTrafficPolicy=local
KUBE-NODE-PORT-UDP NodePort 类型服务 UDP 端口 将数据包伪装成 NodePort(UDP)
KUBE-NODE-PORT-LOCAL-UDP NodePort 类型服务 UDP 端口 使用 externalTrafficPolicy=local 接受数据包到NodePort服务 使用 externalTrafficPolicy=local&lt;/p>
&lt;!--
In general, for IPVS proxier, the number of iptables rules is static, no matter how many Services/Pods we have.
-->
&lt;p>通常，对于 IPVS proxier，无论我们有多少 Service/ Pod，iptables 规则的数量都是静态的。&lt;/p>
&lt;!--
Run kube-proxy in IPVS Mode
Currently, local-up scripts, GCE scripts, and kubeadm support switching IPVS proxy mode via exporting environment variables (KUBE_PROXY_MODE=ipvs) or specifying flag (--proxy-mode=ipvs). Before running IPVS proxier, please ensure IPVS required kernel modules are already installed.
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
Finally, for Kubernetes v1.10, feature gate SupportIPVSProxyMode is set to true by default. For Kubernetes v1.11, the feature gate is entirely removed. However, you need to enable --feature-gates=SupportIPVSProxyMode=true explicitly for Kubernetes before v1.10.
-->
&lt;p>在 IPVS 模式下运行 kube-proxy&lt;/p>
&lt;p>目前，本地脚本，GCE 脚本和 kubeadm 支持通过导出环境变量（KUBE_PROXY_MODE=ipvs）或指定标志（--proxy-mode=ipvs）来切换 IPVS 代理模式。 在运行IPVS 代理之前，请确保已安装 IPVS 所需的内核模块。&lt;/p>
&lt;pre>&lt;code>ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
&lt;/code>&lt;/pre>
&lt;p>最后，对于 Kubernetes v1.10，“SupportIPVSProxyMode” 默认设置为 “true”。 对于 Kubernetes v1.11 ，该选项已完全删除。 但是，您需要在v1.10之前为Kubernetes 明确启用 --feature-gates = SupportIPVSProxyMode = true。&lt;/p>
&lt;!--
Get Involved
The simplest way to get involved with Kubernetes is by joining one of the many Special Interest Groups (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly community meeting, and through the channels below.
Thank you for your continued feedback and support.
Post questions (or answer questions) on Stack Overflow
Join the community portal for advocates on K8sPort
Follow us on Twitter @Kubernetesio for latest updates
Chat with the community on Slack
Share your Kubernetes story
-->
&lt;p>参与其中&lt;/p>
&lt;p>参与 Kubernetes 的最简单方法是加入众多&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">特别兴趣小组&lt;/a> (SIG）中与您的兴趣一致的小组。 你有什么想要向 Kubernetes 社区广播的吗？ 在我们的每周&lt;a href="https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting">社区会议&lt;/a>或通过以下渠道分享您的声音。&lt;/p>
&lt;p>感谢您的持续反馈和支持。
在&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>上发布问题（或回答问题）&lt;/p>
&lt;p>加入&lt;a href="http://k8sport.org/">K8sPort&lt;/a>的倡导者社区门户网站&lt;/p>
&lt;p>在 Twitter 上关注我们 &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>获取最新更新&lt;/p>
&lt;p>在&lt;a href="http://slack.k8s.io/">Slack&lt;/a>上与社区聊天&lt;/p>
&lt;p>分享您的 Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">故事&lt;/a>&lt;/p></description></item><item><title>Blog: Airflow在Kubernetes中的使用（第一部分）：一种不同的操作器</title><link>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</guid><description>
&lt;!--
Author: Daniel Imberman (Bloomberg LP)
-->
&lt;p>作者: Daniel Imberman (Bloomberg LP)&lt;/p>
&lt;!--
## Introduction
As part of Bloomberg's continued commitment to developing the Kubernetes ecosystem, we are excited to announce the Kubernetes Airflow Operator; a mechanism for Apache Airflow, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.
-->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>作为Bloomberg [继续致力于开发Kubernetes生态系统]的一部分（https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/），我们很高兴能够宣布Kubernetes Airflow Operator的发布; &lt;a href="https://airflow.apache.org/">Apache Airflow&lt;/a>的机制，一种流行的工作流程编排框架，使用Kubernetes API可以在本机启动任意的Kubernetes Pod。&lt;/p>
&lt;!--
## What Is Airflow?
Apache Airflow is one realization of the DevOps philosophy of "Configuration As Code." Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.
-->
&lt;h2 id="什么是airflow">什么是Airflow?&lt;/h2>
&lt;p>Apache Airflow是DevOps“Configuration As Code”理念的一种实现。 Airflow允许用户使用简单的Python对象DAG（有向无环图）启动多步骤流水线。 您可以在易于阅读的UI中定义依赖关系，以编程方式构建复杂的工作流，并监视调度的作业。&lt;/p>
&lt;p>&amp;lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow_dags.png”width =“85％”alt =“Airflow DAGs”/&amp;gt;&lt;/p>
&lt;p>&amp;lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow.png”width =“85％”alt =“Airflow UI”/&amp;gt;&lt;/p>
&lt;!--
## Why Airflow on Kubernetes?
Since its inception, Airflow's greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.
To address this issue, we've utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an "any job you want" workflow orchestrator.
-->
&lt;h2 id="为什么在kubernetes上使用airflow">为什么在Kubernetes上使用Airflow?&lt;/h2>
&lt;p>自成立以来，Airflow的最大优势在于其灵活性。 Airflow提供广泛的服务集成，包括Spark和HBase，以及各种云提供商的服务。 Airflow还通过其插件框架提供轻松的可扩展性。但是，该项目的一个限制是Airflow用户仅限于执行时Airflow站点上存在的框架和客户端。单个组织可以拥有各种Airflow工作流程，范围从数据科学流到应用程序部署。用例中的这种差异会在依赖关系管理中产生问题，因为两个团队可能会在其工作流程使用截然不同的库。&lt;/p>
&lt;p>为了解决这个问题，我们使Kubernetes允许用户启动任意Kubernetes pod和配置。 Airflow用户现在可以在其运行时环境，资源和机密上拥有全部权限，基本上将Airflow转变为“您想要的任何工作”工作流程协调器。&lt;/p>
&lt;!--
## The Kubernetes Operator
Before we move any further, we should clarify that an Operator in Airflow is a task definition. When a user creates a DAG, they would use an operator like the "SparkSubmitOperator" or the "PythonOperator" to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.
Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:
-->
&lt;h2 id="kubernetes运营商">Kubernetes运营商&lt;/h2>
&lt;p>在进一步讨论之前，我们应该澄清Airflow中的&lt;a href="https://airflow.apache.org/concepts.html#operators">Operator&lt;/a>是一个任务定义。 当用户创建DAG时，他们将使用像“SparkSubmitOperator”或“PythonOperator”这样的operator分别提交/监视Spark作业或Python函数。 Airflow附带了Apache Spark，BigQuery，Hive和EMR等框架的内置运算符。 它还提供了一个插件入口点，允许DevOps工程师开发自己的连接器。&lt;/p>
&lt;p>Airflow用户一直在寻找更易于管理部署和ETL流的方法。 在增加监控的同时，任何解耦流程的机会都可以减少未来的停机等问题。 以下是Airflow Kubernetes Operator提供的好处：&lt;/p>
&lt;!--
* Increased flexibility for deployments:
Airflow's plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.
-->
&lt;ul>
&lt;li>提高部署灵活性：&lt;/li>
&lt;/ul>
&lt;p>Airflow的插件API一直为希望在其DAG中测试新功能的工程师提供了重要的福利。 不利的一面是，每当开发人员想要创建一个新的operator时，他们就必须开发一个全新的插件。 现在，任何可以在Docker容器中运行的任务都可以通过完全相同的运算符访问，而无需维护额外的Airflow代码。&lt;/p>
&lt;!--
* Flexibility of configurations and dependencies:
For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires SciPy and another that requires NumPy, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.
-->
&lt;ul>
&lt;li>配置和依赖的灵活性：&lt;/li>
&lt;/ul>
&lt;p>对于在静态Airflow工作程序中运行的operator，依赖关系管理可能变得非常困难。 如果开发人员想要运行一个需要&lt;a href="https://www.scipy.org">SciPy&lt;/a>的任务和另一个需要&lt;a href="http://www.numpy.org">NumPy&lt;/a>的任务，开发人员必须维护所有Airflow节点中的依赖关系或将任务卸载到其他计算机（如果外部计算机以未跟踪的方式更改，则可能导致错误）。 自定义Docker镜像允许用户确保任务环境，配置和依赖关系完全是幂等的。&lt;/p>
&lt;!--
* Usage of kubernetes secrets for added security:
Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.
-->
&lt;ul>
&lt;li>使用kubernetes Secret以增加安全性：&lt;/li>
&lt;/ul>
&lt;p>处理敏感数据是任何开发工程师的核心职责。 Airflow用户总有机会在严格条款的基础上隔离任何API密钥，数据库密码和登录凭据。 使用Kubernetes运算符，用户可以利用Kubernetes Vault技术存储所有敏感数据。 这意味着Airflow工作人员将永远无法访问此信息，并且可以容易地请求仅使用他们需要的密码信息构建pod。&lt;/p>
&lt;!--
# Architecture
The Kubernetes Operator uses the Kubernetes Python Client to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you've defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.
-->
&lt;p>＃架构&lt;/p>
&lt;p>&amp;lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow-architecture.png”width =“85％”alt =“Airflow Architecture”/&amp;gt;&lt;/p>
&lt;p>Kubernetes Operator使用&lt;a href="https://github.com/kubernetes-client/Python">Kubernetes Python客户端&lt;/a>生成由APIServer处理的请求（1）。 然后，Kubernetes将使用您定义的需求启动您的pod（2）。映像文件中将加载环境变量，Secret和依赖项，执行单个命令。 一旦启动作业，operator只需要监视跟踪日志的状况（3）。 用户可以选择将日志本地收集到调度程序或当前位于其Kubernetes集群中的任何分布式日志记录服务。&lt;/p>
&lt;!--
# Using the Kubernetes Operator
## A Basic Example
The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the passing-task pod should complete, while the failing-task pod returns a failure to the Airflow webserver.
-->
&lt;p>＃使用Kubernetes Operator&lt;/p>
&lt;p>##一个基本的例子&lt;/p>
&lt;p>以下DAG可能是我们可以编写的最简单的示例，以显示Kubernetes Operator的工作原理。 这个DAG在Kubernetes上创建了两个pod：一个带有Python的Linux发行版和一个没有它的基本Ubuntu发行版。 Python pod将正确运行Python请求，而没有Python的那个将向用户报告失败。 如果Operator正常工作，则应该完成“passing-task”pod，而“falling-task”pod则向Airflow网络服务器返回失败。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> DAG
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">datetime&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> datetime, timedelta
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow.contrib.operators.kubernetes_pod_operator&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> KubernetesPodOperator
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow.operators.dummy_operator&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> DummyOperator
default_args &lt;span style="color:#666">=&lt;/span> {
&lt;span style="color:#b44">&amp;#39;owner&amp;#39;&lt;/span>: &lt;span style="color:#b44">&amp;#39;airflow&amp;#39;&lt;/span>,
&lt;span style="color:#b44">&amp;#39;depends_on_past&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;start_date&amp;#39;&lt;/span>: datetime&lt;span style="color:#666">.&lt;/span>utcnow(),
&lt;span style="color:#b44">&amp;#39;email&amp;#39;&lt;/span>: [&lt;span style="color:#b44">&amp;#39;airflow@example.com&amp;#39;&lt;/span>],
&lt;span style="color:#b44">&amp;#39;email_on_failure&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;email_on_retry&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;retries&amp;#39;&lt;/span>: &lt;span style="color:#666">1&lt;/span>,
&lt;span style="color:#b44">&amp;#39;retry_delay&amp;#39;&lt;/span>: timedelta(minutes&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">5&lt;/span>)
}
dag &lt;span style="color:#666">=&lt;/span> DAG(
&lt;span style="color:#b44">&amp;#39;kubernetes_sample&amp;#39;&lt;/span>, default_args&lt;span style="color:#666">=&lt;/span>default_args, schedule_interval&lt;span style="color:#666">=&lt;/span>timedelta(minutes&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">10&lt;/span>))
start &lt;span style="color:#666">=&lt;/span> DummyOperator(task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;run_this_first&amp;#39;&lt;/span>, dag&lt;span style="color:#666">=&lt;/span>dag)
passing &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;Python:3.6&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
labels&lt;span style="color:#666">=&lt;/span>{&lt;span style="color:#b44">&amp;#34;foo&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;bar&amp;#34;&lt;/span>},
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;passing-test&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;passing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
failing &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;ubuntu:1604&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
labels&lt;span style="color:#666">=&lt;/span>{&lt;span style="color:#b44">&amp;#34;foo&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;bar&amp;#34;&lt;/span>},
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;fail&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;failing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
passing&lt;span style="color:#666">.&lt;/span>set_upstream(start)
failing&lt;span style="color:#666">.&lt;/span>set_upstream(start)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## But how does this relate to my workflow?
While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.
### 1: PR in github
Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR'ing your code, and merge to the master branch to trigger an automated CI build.
### 2: CI/CD via Jenkins -> Docker Image
Generate your Docker images and bump release version within your Jenkins build.
### 3: Airflow launches task
Finally, update your DAGs to reflect the new release version and you should be ready to go!
-->
&lt;p>##但这与我的工作流程有什么关系？&lt;/p>
&lt;p>虽然这个例子只使用基本映像，但Docker的神奇之处在于，这个相同的DAG可以用于您想要的任何图像/命令配对。 以下是推荐的CI / CD管道，用于在Airflow DAG上运行生产就绪代码。&lt;/p>
&lt;h3 id="1-github中的pr">1：github中的PR&lt;/h3>
&lt;p>使用Travis或Jenkins运行单元和集成测试，请您的朋友PR您的代码，并合并到主分支以触发自动CI构建。&lt;/p>
&lt;h3 id="2-ci-cd构建jenkins-docker-image">2：CI / CD构建Jenkins - &amp;gt; Docker Image&lt;/h3>
&lt;p>&lt;a href="https://getintodevops.com/blog/building-your-first-Docker-image-with-jenkins-2-guide-for-developers">在Jenkins构建中生成Docker镜像和缓冲版本&lt;/a>。&lt;/p>
&lt;h3 id="3-airflow启动任务">3：Airflow启动任务&lt;/h3>
&lt;p>最后，更新您的DAG以反映新版本，您应该准备好了！&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">
production_task &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
&lt;span style="color:#080;font-style:italic"># image=&amp;#34;my-production-job:release-1.0.1&amp;#34;, &amp;lt;-- old release&lt;/span>
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;my-production-job:release-1.0.2&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;fail&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;failing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
# Launching a test deployment
Since the Kubernetes Operator is not yet released, we haven't released an official helm chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:
## Step 1: Set your kubeconfig to point to a kubernetes cluster
## Step 2: Clone the Airflow Repo:
Run git clone https://github.com/apache/incubator-airflow.git to clone the official Airflow repo.
## Step 3: Run
To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:
-->
&lt;p>＃启动测试部署&lt;/p>
&lt;p>由于Kubernetes运营商尚未发布，我们尚未发布官方&lt;a href="https://helm.sh/">helm&lt;/a> 图表或operator（但两者目前都在进行中）。 但是，我们在下面列出了基本部署的说明，并且正在积极寻找测试人员来尝试这一新功能。 要试用此系统，请按以下步骤操作：&lt;/p>
&lt;p>##步骤1：将kubeconfig设置为指向kubernetes集群&lt;/p>
&lt;p>##步骤2：clone Airflow 仓库：&lt;/p>
&lt;p>运行git clone https：// github.com / apache / incubator-airflow.git来clone官方Airflow仓库。&lt;/p>
&lt;p>##步骤3：运行&lt;/p>
&lt;p>为了运行这个基本Deployment，我们正在选择我们目前用于Kubernetes Executor的集成测试脚本（将在本系列的下一篇文章中对此进行解释）。 要启动此部署，请运行以下三个命令：&lt;/p>
&lt;pre>&lt;code>
sed -ie &amp;quot;s/KubernetesExecutor/LocalExecutor/g&amp;quot; scripts/ci/kubernetes/kube/configmaps.yaml
./scripts/ci/kubernetes/Docker/build.sh
./scripts/ci/kubernetes/kube/deploy.sh
&lt;/code>&lt;/pre>&lt;!--
Before we move on, let's discuss what these commands are doing:
### sed -ie "s/KubernetesExecutor/LocalExecutor/g" scripts/ci/kubernetes/kube/configmaps.yaml
The Kubernetes Executor is another Airflow feature that allows for dynamic allocation of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.
### ./scripts/ci/kubernetes/Docker/build.sh
This script will tar the Airflow master source code build a Docker container based on the Airflow distribution
### ./scripts/ci/kubernetes/kube/deploy.sh
Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml
## Step 4: Log into your webserver
Now that your Airflow instance is running let's take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run
-->
&lt;p>在我们继续之前，让我们讨论这些命令正在做什么：&lt;/p>
&lt;h3 id="sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml">sed -ie“s / KubernetesExecutor / LocalExecutor / g”scripts / ci / kubernetes / kube / configmaps.yaml&lt;/h3>
&lt;p>Kubernetes Executor是另一种Airflow功能，允许动态分配任务已解决幂等pod的问题。我们将其切换到LocalExecutor的原因只是一次引入一个功能。如果您想尝试Kubernetes Executor，欢迎您跳过此步骤，但我们将在以后的文章中详细介绍。&lt;/p>
&lt;h3 id="scripts-ci-kubernetes-docker-build-sh">./scripts/ci/kubernetes/Docker/build.sh&lt;/h3>
&lt;p>此脚本将对Airflow主分支代码进行打包，以根据Airflow的发行文件构建Docker容器&lt;/p>
&lt;h3 id="scripts-ci-kubernetes-kube-deploy-sh">./scripts/ci/kubernetes/kube/deploy.sh&lt;/h3>
&lt;p>最后，我们在您的群集上创建完整的Airflow部署。这包括Airflow配置，postgres后端，webserver +调度程序以及之间的所有必要服务。需要注意的一点是，提供的角色绑定是集群管理员，因此如果您没有该集群的权限级别，可以在scripts / ci / kubernetes / kube / airflow.yaml中进行修改。&lt;/p>
&lt;p>##步骤4：登录您的网络服务器&lt;/p>
&lt;p>现在您的Airflow实例正在运行，让我们来看看UI！用户界面位于Airflow pod的8080端口，因此只需运行即可&lt;/p>
&lt;pre>&lt;code>
WEB=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}' | grep &amp;quot;airflow&amp;quot; | head -1)
kubectl port-forward $WEB 8080:8080
&lt;/code>&lt;/pre>&lt;!--
Now the Airflow UI will exist on http://localhost:8080. To log in simply enter airflow/airflow and you should have full access to the Airflow web UI.
## Step 5: Upload a test document
To modify/add your own DAGs, you can use kubectl cp to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:
-->
&lt;p>现在，Airflow UI将存在于http://localhost:8080上。 要登录，只需输入airflow /airflow，您就可以完全访问Airflow Web UI。&lt;/p>
&lt;p>##步骤5：上传测试文档&lt;/p>
&lt;p>要修改/添加自己的DAG，可以使用kubectl cp将本地文件上传到Airflow调度程序的DAG文件夹中。 然后，Airflow将读取新的DAG并自动将其上传到其系统。 以下命令将任何本地文件上载到正确的目录中：&lt;/p>
&lt;p>kubectl cp &lt;local file> &lt;namespace>/&lt;pod>:/root/airflow/dags -c scheduler&lt;/p>
&lt;!--
## Step 6: Enjoy!
# So when will I be able to use this?
While this feature is still in the early stages, we hope to see it released for wide release in the next few months.
# Get Involved
This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the 1.10 release branch of Airflow (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributors can have a huge influence on the future of these features.
For those interested in joining these efforts, I'd recommend checkint out these steps:
* Join the airflow-dev mailing list at dev@airflow.apache.org.
* File an issue in Apache Airflow JIRA
* Join our SIG-BigData meetings on Wednesdays at 10am PST.
* Reach us on slack at #sig-big-data on kubernetes.slack.com
Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.
-->
&lt;p>##步骤6：使用它！&lt;/p>
&lt;p>#那么我什么时候可以使用它？&lt;/p>
&lt;p>虽然此功能仍处于早期阶段，但我们希望在未来几个月内发布该功能以进行广泛发布。&lt;/p>
&lt;p>#参与其中&lt;/p>
&lt;p>此功能只是将Apache Airflow集成到Kubernetes中的多项主要工作的开始。 Kubernetes Operator已合并到&lt;a href="https://github.com/apache/incubator-airflow/tree/v1-10-test">Airflow的1.10发布分支&lt;/a>（实验模式中的执行模块），以及完整的k8s本地调度程序称为Kubernetes Executor（即将发布文章）。这些功能仍处于早期采用者/贡献者可能对这些功能的未来产生巨大影响的阶段。&lt;/p>
&lt;p>对于有兴趣加入这些工作的人，我建议按照以下步骤：&lt;/p>
&lt;p>*加入airflow-dev邮件列表dev@airflow.apache.org。&lt;/p>
&lt;p>*在[Apache Airflow JIRA]中提出问题（https://issues.apache.org/jira/projects/AIRFLOW/issues/）&lt;/p>
&lt;p>*周三上午10点太平洋标准时间加入我们的SIG-BigData会议。&lt;/p>
&lt;p>*在kubernetes.slack.com上的＃sig-big-data找到我们。&lt;/p>
&lt;p>特别感谢Apache Airflow和Kubernetes社区，特别是Grant Nicholas，Ben Goldberg，Anirudh Ramanathan，Fokko Dreisprong和Bolke de Bruin，感谢您对这些功能的巨大帮助以及我们未来的努力。&lt;/p></description></item><item><title>Blog: Kubernetes 内的动态 Ingress</title><link>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</link><pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</guid><description>
&lt;!--
title: Dynamic Ingress in Kubernetes
date: 2018-06-07
Author: Richard Li (Datawire)
-->
&lt;p>作者: Richard Li (Datawire)&lt;/p>
&lt;!--
Kubernetes makes it easy to deploy applications that consist of many microservices, but one of the key challenges with this type of architecture is dynamically routing ingress traffic to each of these services. One approach is Ambassador, a Kubernetes-native open source API Gateway built on the Envoy Proxy. Ambassador is designed for dynamic environment where services may come and go frequently.
Ambassador is configured using Kubernetes annotations. Annotations are used to configure specific mappings from a given Kubernetes service to a particular URL. A mapping can include a number of annotations for configuring a route. Examples include rate limiting, protocol, cross-origin request sharing, traffic shadowing, and routing rules.
-->
&lt;p>Kubernetes 可以轻松部署由许多微服务组成的应用程序，但这种架构的关键挑战之一是动态地将流量路由到这些服务中的每一个。
一种方法是使用 &lt;a href="https://www.getambassador.io">Ambassador&lt;/a>，
一个基于 &lt;a href="https://www.envoyproxy.io">Envoy Proxy&lt;/a> 构建的 Kubernetes 原生开源 API 网关。
Ambassador 专为动态环境而设计，这类环境中的服务可能被频繁添加或删除。&lt;/p>
&lt;p>Ambassador 使用 Kubernetes 注解进行配置。
注解用于配置从给定 Kubernetes 服务到特定 URL 的具体映射关系。
每个映射中可以包括多个注解，用于配置路由。
注解的例子有速率限制、协议、跨源请求共享（CORS）、流量影射和路由规则等。&lt;/p>
&lt;!--
## A Basic Ambassador Example
Ambassador is typically installed as a Kubernetes deployment, and is also available as a Helm chart. To configure Ambassador, create a Kubernetes service with the Ambassador annotations. Here is an example that configures Ambassador to route requests to /httpbin/ to the public httpbin.org service:
-->
&lt;h2 id="一个简单的-ambassador-示例">一个简单的 Ambassador 示例&lt;/h2>
&lt;p>Ambassador 通常作为 Kubernetes Deployment 来安装，也可以作为 Helm Chart 使用。
配置 Ambassador 时，请使用 Ambassador 注解创建 Kubernetes 服务。
下面是一个例子，用来配置 Ambassador，将针对 /httpbin/ 的请求路由到公共的 httpbin.org 服务：&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Service
metadata:
name: httpbin
annotations:
getambassador.io/config: |
---
apiVersion: ambassador/v0
kind: Mapping
name: httpbin_mapping
prefix: /httpbin/
service: httpbin.org:80
host_rewrite: httpbin.org
spec:
type: ClusterIP
ports:
- port: 80
&lt;/code>&lt;/pre>&lt;!--
A mapping object is created with a prefix of /httpbin/ and a service name of httpbin.org. The host_rewrite annotation specifies that the HTTP host header should be set to httpbin.org.
-->
&lt;p>例子中创建了一个 Mapping 对象，其 prefix 设置为 /httpbin/，service 名称为 httpbin.org。
其中的 host_rewrite 注解指定 HTTP 的 host 头部字段应设置为 httpbin.org。&lt;/p>
&lt;!--
## Kubeflow
Kubeflow provides a simple way to easily deploy machine learning infrastructure on Kubernetes. The Kubeflow team needed a proxy that provided a central point of authentication and routing to the wide range of services used in Kubeflow, many of which are ephemeral in nature.
&lt;center>&lt;i>Kubeflow architecture, pre-Ambassador&lt;/center>&lt;/i>
-->
&lt;h2 id="kubeflow">Kubeflow&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubeflow/kubeflow">Kubeflow&lt;/a> 提供了一种简单的方法，用于在 Kubernetes 上轻松部署机器学习基础设施。
Kubeflow 团队需要一个代理，为 Kubeflow 中所使用的各种服务提供集中化的认证和路由能力；Kubeflow 中许多服务本质上都是生命期很短的。&lt;/p>
&lt;center>&lt;i>Kubeflow architecture, pre-Ambassador&lt;/center>&lt;/i>
&lt;!--
## Service configuration
With Ambassador, Kubeflow can use a distributed model for configuration. Instead of a central configuration file, Ambassador allows each service to configure its route in Ambassador via Kubernetes annotations. Here is a simplified example configuration:
-->
&lt;h2 id="服务配置">服务配置&lt;/h2>
&lt;p>有了 Ambassador，Kubeflow 可以使用分布式模型进行配置。
Ambassador 不使用集中的配置文件，而是允许每个服务通过 Kubernetes 注解在 Ambassador 中配置其路由。
下面是一个简化的配置示例：&lt;/p>
&lt;pre>&lt;code>---
apiVersion: ambassador/v0
kind: Mapping
name: tfserving-mapping-test-post
prefix: /models/test/
rewrite: /model/test/:predict
method: POST
service: test.kubeflow:8000
&lt;/code>&lt;/pre>&lt;!--
In this example, the “test” service uses Ambassador annotations to dynamically configure a route to the service, triggered only when the HTTP method is a POST, and the annotation also specifies a rewrite rule.
-->
&lt;p>示例中，“test” 服务使用 Ambassador 注解来为服务动态配置路由。
所配置的路由仅在 HTTP 方法是 POST 时触发；注解中同时还给出了一条重写规则。&lt;/p>
&lt;!--
With Ambassador, Kubeflow manages routing easily with Kubernetes annotations. Kubeflow configures a single ingress object that directs traffic to Ambassador, then creates services with Ambassador annotations as needed to direct traffic to specific backends. For example, when deploying TensorFlow services, Kubeflow creates and and annotates a K8s service so that the model will be served at https://&lt;ingress host>/models/&lt;model name>/. Kubeflow can also use the Envoy Proxy to do the actual L7 routing. Using Ambassador, Kubeflow takes advantage of additional routing configuration like URL rewriting and method-based routing.
If you’re interested in using Ambassador with Kubeflow, the standard Kubeflow install automatically installs and configures Ambassador.
If you’re interested in using Ambassador as an API Gateway or Kubernetes ingress solution for your non-Kubeflow services, check out the Getting Started with Ambassador guide.
## Kubeflow and Ambassador
-->
&lt;h2 id="kubeflow-和-ambassador">Kubeflow 和 Ambassador&lt;/h2>
&lt;p>通过 Ambassador，Kubeflow 可以使用 Kubernetes 注解轻松管理路由。
Kubeflow 配置同一个 Ingress 对象，将流量定向到 Ambassador，然后根据需要创建具有 Ambassador 注解的服务，以将流量定向到特定后端。
例如，在部署 TensorFlow 服务时，Kubeflow 会创建 Kubernetes 服务并为其添加注解，
以便用户能够在 &lt;code>https://&amp;lt;ingress主机&amp;gt;/models/&amp;lt;模型名称&amp;gt;/&lt;/code> 处访问到模型本身。
Kubeflow 还可以使用 Envoy Proxy 来进行实际的 L7 路由。
通过 Ambassador，Kubeflow 能够更充分地利用 URL 重写和基于方法的路由等额外的路由配置能力。&lt;/p>
&lt;p>如果您对在 Kubeflow 中使用 Ambassador 感兴趣，标准的 Kubeflow 安装会自动安装和配置 Ambassador。&lt;/p>
&lt;p>如果您有兴趣将 Ambassador 用作 API 网关或 Kubernetes 的 Ingress 解决方案，
请参阅 &lt;a href="https://www.getambassador.io/user-guide/getting-started">Ambassador 入门指南&lt;/a>。&lt;/p></description></item><item><title>Blog: Kubernetes 这四年</title><link>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</link><pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</guid><description>
&lt;!--
**Author**: Joe Beda (CTO and Founder, Heptio)
On June 6, 2014 I checked in the [first commit](https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56) of what would become the public repository for Kubernetes. Many would assume that is where the story starts. It is the beginning of history, right? But that really doesn’t tell the whole story.
-->
&lt;p>&lt;strong>作者&lt;/strong>：Joe Beda( Heptio 首席技术官兼创始人)
2014 年 6 月 6 日，我检查了 Kubernetes 公共代码库的&lt;a href="https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56">第一次 commit&lt;/a> 。许多人会认为这是故事开始的地方。这难道不是一切开始的地方吗？但这的确不能把整个过程说清楚。&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-06-06-4-years-of-k8s/k8s-first-commit.png" alt="k8s_first_commit">&lt;/p>
&lt;!--
The cast leading up to that commit was large and the success for Kubernetes since then is owed to an ever larger cast.
Kubernetes was built on ideas that had been proven out at Google over the previous ten years with Borg. And Borg, itself, owed its existence to even earlier efforts at Google and beyond.
Concretely, Kubernetes started as some prototypes from Brendan Burns combined with ongoing work from me and Craig McLuckie to better align the internal Google experience with the Google Cloud experience. Brendan, Craig, and I really wanted people to use this, so we made the case to build out this prototype as an open source project that would bring the best ideas from Borg out into the open.
After we got the nod, it was time to actually build the system. We took Brendan’s prototype (in Java), rewrote it in Go, and built just enough to get the core ideas across. By this time the team had grown to include Ville Aikas, Tim Hockin, Brian Grant, Dawn Chen and Daniel Smith. Once we had something working, someone had to sign up to clean things up to get it ready for public launch. That ended up being me. Not knowing the significance at the time, I created a new repo, moved things over, and checked it in. So while I have the first public commit to the repo, there was work underway well before that.
The version of Kubernetes at that point was really just a shadow of what it was to become. The core concepts were there but it was very raw. For example, Pods were called Tasks. That was changed a day before we went public. All of this led up to the public announcement of Kubernetes on June 10th, 2014 in a keynote from Eric Brewer at the first DockerCon. You can watch that video here:
-->
&lt;p>第一次 commit 涉及的人员众多，自那以后 Kubernetes 的成功归功于更大的开发者阵容。
Kubernetes 建立在过去十年曾经在 Google 的 Borg 集群管理系统中验证过的思路之上。而 Borg 本身也是 Google 和其他公司早期努力的结果。
具体而言，Kubernetes 最初是从 Brendan Burns 的一些原型开始，结合我和 Craig McLuckie 正在进行的工作，以更好地将 Google 内部实践与 Google Cloud 的经验相结合。 Brendan，Craig 和我真的希望人们使用它，所以我们建议将这个原型构建为一个开源项目，将 Borg 的最佳创意带给大家。
在我们所有人同意后，就开始着手构建这个系统了。我们采用了 Brendan 的原型（Java 语言），用 Go 语言重写了它，并且以上述核心思想去构建该系统。到这个时候，团队已经成长为包括 Ville Aikas，Tim Hockin，Brian Grant，Dawn Chen 和 Daniel Smith。一旦我们有了一些工作需求，有人必须承担一些脱敏的工作，以便为公开发布做好准备。这个角色最终由我承担。当时，我不知道这件事情的重要性，我创建了一个新的仓库，把代码搬过来，然后进行了检查。所以在我第一次提交 public commit 之前，就有工作已经启动了。
那时 Kubernetes 的版本只是现在版本的简单雏形。核心概念已经有了，但非常原始。例如，Pods 被称为 Tasks，这在我们推广前一天就被替换。2014年6月10日 Eric Brewe 在第一届 DockerCon 上的演讲中正式发布了 Kubernetes 。您可以在此处观看该视频：&lt;/p>
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/YrxnVKZeqK8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
&lt;!--
But, however raw, that modest start was enough to pique the interest of a community that started strong and has only gotten stronger. Over the past four years Kubernetes has exceeded the expectations of all of us that were there early on. We owe the Kubernetes community a huge debt. The success the project has seen is based not just on code and technology but also the way that an amazing group of people have come together to create something special. The best expression of this is the [set of Kubernetes values](https://github.com/kubernetes/steering/blob/master/values.md) that Sarah Novotny helped curate.
Here is to another 4 years and beyond! 🎉🎉🎉
-->
&lt;p>但是，无论多么原始，这小小的一步足以激起一个开始强大而且变得更强大的社区的兴趣。在过去的四年里，Kubernetes 已经超出了我们所有人的期望。我们对 Kubernetes 社区的所有人员表示感谢。该项目所取得的成功不仅基于代码和技术，还基于一群出色的人聚集在一起所做的有意义的事情。Sarah Novotny 策划的一套 &lt;a href="https://github.com/kubernetes/steering/blob/master/values.md">Kubernetes 价值观&lt;/a>是以上最好的表现形式。
让我们一起期待下一个4年！🎉🎉🎉&lt;/p></description></item></channel></rss>