<!doctype html><html lang=pt-br class=no-js>
<head>
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JPP6RFM2BP"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JPP6RFM2BP')</script>
<link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/>
<link rel=alternate hreflang=zh href=https://kubernetes.io/zh/docs/concepts/>
<link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/>
<link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/>
<link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/>
<link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/>
<link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/>
<link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/>
<link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/>
<link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/>
<link rel=alternate hreflang=pl href=https://kubernetes.io/pl/docs/concepts/>
<link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/concepts/>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.87.0">
<link rel=canonical type=text/html href=https://kubernetes.io/pt-br/docs/concepts/>
<link rel="shortcut icon" type=image/png href=/images/favicon.png>
<link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=manifest href=/manifest.webmanifest>
<link rel=apple-touch-icon href=/images/kubernetes-192x192.png>
<title>Conceitos | Kubernetes</title><meta property="og:title" content="Conceitos">
<meta property="og:description" content="Orquestração de contêineres em nível de produção">
<meta property="og:type" content="website">
<meta property="og:url" content="https://kubernetes.io/pt-br/docs/concepts/"><meta property="og:site_name" content="Kubernetes">
<meta itemprop=name content="Conceitos">
<meta itemprop=description content="Orquestração de contêineres em nível de produção"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Conceitos">
<meta name=twitter:description content="Orquestração de contêineres em nível de produção">
<link href=/scss/main.css rel=stylesheet>
<script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script>
<meta name=theme-color content="#326ce5">
<link rel=stylesheet href=/css/feature-states.css>
<meta name=description content="A seção de Conceitos irá te ajudar a aprender mais sobre as partes do ecossistema Kubernetes e as abstrações que o Kubernetes usa para representar seu cluster.
Ela irá lhe ajudar a obter um entendimento mais profundo sobre como o Kubernetes funciona.">
<meta property="og:description" content="A seção de Conceitos irá te ajudar a aprender mais sobre as partes do ecossistema Kubernetes e as abstrações que o Kubernetes usa para representar seu cluster.
Ela irá lhe ajudar a obter um entendimento mais profundo sobre como o Kubernetes funciona.">
<meta name=twitter:description content="A seção de Conceitos irá te ajudar a aprender mais sobre as partes do ecossistema Kubernetes e as abstrações que o Kubernetes usa para representar seu cluster.
Ela irá lhe ajudar a obter um entendimento mais profundo sobre como o Kubernetes funciona.">
<meta property="og:url" content="https://kubernetes.io/pt-br/docs/concepts/">
<meta property="og:title" content="Conceitos">
<meta name=twitter:title content="Conceitos">
<meta name=twitter:image content="https://kubernetes.io/images/favicon.png">
<meta name=twitter:image:alt content="Kubernetes">
<meta property="og:image" content="/images/kubernetes-horizontal-color.png">
<meta property="og:type" content="article">
<script src=/js/script.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary>
<a class=navbar-brand href=/pt-br/></a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-2 mb-lg-0">
<a class="nav-link active" href=/pt-br/docs/>Documentação</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/pt-br/blog/>Kubernetes Blog</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/pt-br/partners/>Parceiros</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/pt-br/community/>Comunidade</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/pt-br/case-studies/>Casos de estudo</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
Versões
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/pt-br/docs/concepts/>v1.27</a>
<a class=dropdown-item href=https://v1-26.docs.kubernetes.io/pt-br/docs/concepts/>v1.26</a>
<a class=dropdown-item href=https://v1-25.docs.kubernetes.io/pt-br/docs/concepts/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/pt-br/docs/concepts/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/pt-br/docs/concepts/>v1.23</a>
</div>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
Português
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/docs/concepts/>English</a>
<a class=dropdown-item href=/zh/docs/concepts/>中文 Chinese</a>
<a class=dropdown-item href=/ko/docs/concepts/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/concepts/>日本語 Japanese</a>
<a class=dropdown-item href=/fr/docs/concepts/>Français</a>
<a class=dropdown-item href=/it/docs/concepts/>Italiano</a>
<a class=dropdown-item href=/de/docs/concepts/>Deutsch</a>
<a class=dropdown-item href=/es/docs/concepts/>Español</a>
<a class=dropdown-item href=/id/docs/concepts/>Bahasa Indonesia</a>
<a class=dropdown-item href=/ru/docs/concepts/>Русский</a>
<a class=dropdown-item href=/pl/docs/concepts/>Polski</a>
<a class=dropdown-item href=/uk/docs/concepts/>Українська</a>
</div>
</li>
</ul>
</div>
<button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
Essa é a versão completa de impressão dessa seção
<a href=# onclick="return print(),!1">Clique aqui para imprimir</a>.
</p><p>
<a href=/pt-br/docs/concepts/>Retornar à visualização normal</a>.
</p>
</div>
<h1 class=title>Conceitos</h1>
<ul>
<li>1: <a href=#pg-a935ff8c59eb116b43494255cc67f69a>Intervalos de limite</a></li>
<li>2: <a href=#pg-94ddc6e901c30f256138db11d09f05a3>Cotas de Recursos</a></li>
<li>3: <a href=#pg-0554ac387412eaf4e6e89b2f847dacde>Visão Geral</a></li>
<ul>
<li>3.1: <a href=#pg-45bdca6129cf540121623e903c18ba46>O que é Kubernetes?</a></li>
<li>3.2: <a href=#pg-13b0f1dbe89228e3d76d2ac231e245f1>Componentes do Kubernetes</a></li>
<li>3.3: <a href=#pg-110f33530cf761140cb1dab536baef04>Objetos do Kubernetes</a></li>
<ul>
<li>3.3.1: <a href=#pg-f37749a83c2916b63279ea60f3cfe53e>Nomes</a></li>
<li>3.3.2: <a href=#pg-1127165f472b7181b9c1d5a0b187d620>Namespaces</a></li>
<li>3.3.3: <a href=#pg-046c03090d47bc4b89b818dc645c3865>Seletores de Campos</a></li>
</ul>
</ul>
<li>4: <a href=#pg-ffd12528a12882b282e1bd19e29f9e75>Volumes Persistentes</a></li>
<li>5: <a href=#pg-2bf36ccd6b3dbeafecf87c39761b07c7>Arquitetura do Kubernetes</a></li>
<ul>
<li>5.1: <a href=#pg-c0251def6da29b30afebfb04549f1703>Comunicação entre Nó e Control Plane</a></li>
<li>5.2: <a href=#pg-bc804b02614d67025b4c788f1ca87fbc>Conceitos sobre Cloud Controller Manager</a></li>
<li>5.3: <a href=#pg-ca8819042a505291540e831283da66df>Controladores</a></li>
</ul>
<li>6: <a href=#pg-a5f7383c83ab9eb9cd0e3c4c020b3ae6>Contêineres</a></li>
<ul>
<li>6.1: <a href=#pg-16042b4652ad19e565c7263824029a43>Imagens</a></li>
<li>6.2: <a href=#pg-643212488f778acf04bebed65ba34441>Ambiente de Contêiner</a></li>
<li>6.3: <a href=#pg-a858027489648786a3b16264e451272b>Classes de execução</a></li>
<li>6.4: <a href=#pg-e6941d969d81540208a3e78bc56f43bc>Hooks de Ciclo de Vida do Contêiner</a></li>
</ul>
<li>7: <a href=#pg-0a0a7eca3e302a3c08f8c85e15d337fd>Serviços, balanceamento de carga e conectividade</a></li>
<ul>
<li>7.1: <a href=#pg-ded1daafdcd293023ee333728007ca61>Políticas de rede</a></li>
</ul>
<li>8: <a href=#pg-275bea454e1cf4c5adeca4058b5af988>Configuração</a></li>
<ul>
<li>8.1: <a href=#pg-ddef6fd0e47bb51c6f05e8e7fb11d2dd>Melhores Práticas de Configuração</a></li>
<li>8.2: <a href=#pg-6b5ccadd699df0904e8e9917c5450c4b>ConfigMaps</a></li>
<li>8.3: <a href=#pg-e511ed821ada65d0053341dbd8ad2bb5>Secrets</a></li>
<li>8.4: <a href=#pg-436057b96151ecb8a4a9a9f456b5d0fc>Gerenciamento de recursos em Pods e contêineres</a></li>
<li>8.5: <a href=#pg-ab6d20f33ad930a67ee7ef57bff6c75e>Organizando o acesso ao cluster usando arquivos kubeconfig</a></li>
</ul>
<li>9: <a href=#pg-712cb3c03ff14a39e5a83a6d9b71d203>Segurança</a></li>
<ul>
<li>9.1: <a href=#pg-04eeb110d75afc8acb2cf7a3db743985>Visão Geral da Segurança Cloud Native</a></li>
</ul>
<li>10: <a href=#pg-c21d05f31057c5bcd2ebdd01f4e62a0e>Escalonamento</a></li>
<ul>
<li>10.1: <a href=#pg-ede4960b56a3529ee0bfe7c8fe2d09a5>Taints e Tolerâncias</a></li>
<li>10.2: <a href=#pg-598f36d691ab197f9d995784574b0a12>Escalonador do Kubernetes</a></li>
<li>10.3: <a href=#pg-da22fe2278df236f71efbe672f392677>Sobrecarga de Pod</a></li>
</ul>
<li>11: <a href=#pg-285a3785fd3d20f437c28d87ca4dadca>Administração de Cluster</a></li>
<ul>
<li>11.1: <a href=#pg-fb494ea3b1874bd753dcd11c3f35c2dc>Visão Geral da Administração de Cluster</a></li>
<li>11.2: <a href=#pg-2bf9a93ab5ba014fb6ff70b22c29d432>Certificates</a></li>
<li>11.3: <a href=#pg-d649067a69d8d5c7e71564b42b96909e>Conectividade do Cluster</a></li>
<li>11.4: <a href=#pg-c4b1e87a84441f8a90699a345ce48d68>Arquitetura de Log</a></li>
<li>11.5: <a href=#pg-5cc31ecfba86467f8884856412cfb6b2>Logs de Sistema</a></li>
<li>11.6: <a href=#pg-cbfd3654996eae9fcdef009f70fa83f0>Métricas para componentes do sistema Kubernetes</a></li>
<li>11.7: <a href=#pg-2e05a56491965ae320c2662590b2ca18>Configurando o Garbage Collection do kubelet</a></li>
<li>11.8: <a href=#pg-08e94e6a480e0d6b2de72d84a1b97617>Proxies no Kubernetes</a></li>
<li>11.9: <a href=#pg-85d633ae590aa20ec024f1b7af1d74fc>Instalando Complementos</a></li>
</ul>
<li>12: <a href=#pg-7e0d97616b15e2c383c6a0a96ec442cb>Extendendo o Kubernetes</a></li>
<ul>
<li>12.1: <a href=#pg-0af41d3bd7c785621b58b7564793396a>Extendendo a API do Kubernetes</a></li>
<ul>
<li>12.1.1: <a href=#pg-1ea4977c0ebf97569bf54a477faa7fa5>Extendendo a API do Kubernetes com a camada de agregação</a></li>
</ul>
<li>12.2: <a href=#pg-c8937cdc9df96f3328becf04f8211292>Extensões de Computação, armazenamento e redes</a></li>
<ul>
<li>12.2.1: <a href=#pg-1ac2260db9ecccbf0303a899bc27ce6d>Plugins de rede</a></li>
</ul>
<li>12.3: <a href=#pg-3131452556176159fb269593c1a52012>Padrão Operador</a></li>
</ul>
</ul>
<div class=content>
<p>A seção de Conceitos irá te ajudar a aprender mais sobre as partes do ecossistema Kubernetes e as abstrações que o Kubernetes usa para representar seu <a class=glossary-tooltip title="Um conjunto de servidores de processamento, também chamados de nós, que executam aplicações containerizadas. Todo cluster possui ao menos um servidor de processamento (worker node)." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-cluster" target=_blank aria-label=cluster>cluster</a>.</p>
<p>Ela irá lhe ajudar a obter um entendimento mais profundo sobre como o Kubernetes funciona.</p>
</div>
</div>
<div class=td-content>
<h1 id=pg-a935ff8c59eb116b43494255cc67f69a>1 - Intervalos de limite</h1>
<p>Por padrão, os cointêineres são executados com <a href=/docs/concepts/configuration/manage-resources-containers/>recursos computacionais</a> ilimitados em um cluster Kubernetes. Com cotas de recursos, os administradores de cluster podem restringir o consumo e a criação de recursos baseado no <a class=glossary-tooltip title="Uma abstração utilizada pelo Kubernetes para suportar múltiplos clusters virtuais no mesmo cluster físico." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>. Dentro de um <em>namespace</em>, pod ou contêiner pode haver o consumo de quantidade de CPU e memória definidos de acordo com a cota de recursos do <em>namespace</em>. Existe a preocupação de que um Pod ou contêiner possa monopolizar todos os recursos disponíveis, justamente por conta disso existe o conceito de <em>Limit Range</em>, ou intervalos de limite, que pode ser definido como uma política utilizada para a restrição de alocação de recursos (para pods ou contêineres) em um <em>namespace</em>.</p>
<p>Um <em>LimitRange</em> fornece restrições que podem:</p>
<ul>
<li>Aplicar o uso mínimo e máximo de recursos computacionais por pod ou contêiner em um <em>namespace</em>.</li>
<li>Impor a solicitação de armazenamento mínimo e máximo por <em>PersistentVolumeClaim</em> em um <em>namespace</em>.</li>
<li>Impor a proporção entre solicitação e limite para um recurso em um <em>namespace</em>.</li>
<li>Definir a solicitação/limite padrão para recursos computacionais em um <em>namespace</em> e utilizá-los automaticamente nos contêineres em tempo de execução.</li>
</ul>
<h2 id=ativando-o-limitrange>Ativando o LimitRange</h2>
<p>O suporte ao <em>LimitRange</em> foi ativado por padrão desde o Kubernetes 1.10.</p>
<p>Um <em>LimitRange</em> é aplicado em um <em>namespace</em> específico quando há um objeto <em>LimitRange</em> nesse <em>namespace</em>.</p>
<p>O nome de um objeto <em>LimitRange</em> deve ser um <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nome de subdomínio DNS</a> válido.</p>
<h3 id=visão-geral-do-limit-range>Visão geral do Limit Range</h3>
<ul>
<li>O administrador cria um <em>LimitRange</em> em um <em>namespace</em>.</li>
<li>Os usuários criam recursos como pods, contêineres e <em>PersistentVolumeClaims</em> no <em>namespace</em>.</li>
<li>O controlador de admissão <code>LimitRanger</code> impõe padrões e limites para todos os pods e contêineres que não definem os requisitos de recursos computacionais e rastreia o uso para garantir que não exceda o mínimo, o máximo e a proporção de recursos definidos em qualquer <em>LimitRange</em> presente no <em>namespace</em>.</li>
<li>Se estiver criando ou atualizando um recurso (Pod, Container, <em>PersistentVolumeClaim</em>) que viola uma restrição <em>LimitRange</em>, a solicitação ao servidor da API falhará com um código de status HTTP <code>403 FORBIDDEN</code> e uma mensagem explicando a restrição violada.</li>
<li>Se um <em>LimitRange</em> for ativado em um <em>namespace</em> para recursos computacionais como <code>cpu</code> e <code>memória</code>, os usuários deverão especificar solicitações ou limites para esses valores. Caso contrário, o sistema pode rejeitar a criação do pod.</li>
<li>As validações de <em>LimitRange</em> ocorrem apenas no estágio de Admissão de Pod, não em Pods em Execução.</li>
</ul>
<p>Alguns exemplos de políticas que podem ser criadas utilizando os intervalos de limite são:</p>
<ul>
<li>Em um cluster de 2 nós com capacidade de 8 GiB de RAM e 16 núcleos, restrinja os Pods em um namespace para solicitar 100m de CPU com um limite máximo de 500m para CPU e solicitar 200Mi para memória com um limite máximo de 600Mi para memória.</li>
<li>Defina o limite e a solicitação de CPU padrão para 150m e a solicitação padrão de memória para 300Mi para contêineres iniciados sem solicitações de CPU e memória em suas especificações.</li>
</ul>
<p>Caso os limites totais do namespace sejam menores que a soma dos limites dos Pods/Contêineres, pode haver contenção por recursos. Nesse caso, os contêineres ou Pods não serão criados.</p>
<p>Nem a contenção nem as alterações em um <em>LimitRange</em> afetarão os recursos já criados.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<p>Consulte o <a href=https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md>documento de design LimitRanger</a> para obter mais informações.</p>
<p>Para exemplos de uso de limites, leia:</p>
<ul>
<li><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Como configurar restrições mínimas e máximas de CPU por <em>namespace</em></a>.</li>
<li><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>Como configurar restrições de memória mínima e máxima por <em>namespace</em></a>.</li>
<li><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>como configurar solicitações e limites de CPU padrão por <em>namespace</em></a>.</li>
<li><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>como configurar solicitações e limites de memória padrão por <em>namespace</em></a>.</li>
<li><a href=/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage>como configurar o consumo mínimo e máximo de armazenamento por <em>namespace</em></a>.</li>
<li>Um <a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>exemplo detalhado de configuração de cota por <em>namespace</em></a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-94ddc6e901c30f256138db11d09f05a3>2 - Cotas de Recursos</h1>
<p>Quando vários usuários ou equipes compartilham um cluster com um número fixo de nós,
há uma preocupação de que uma equipe possa usar mais do que é justo durante o compartilhamento de recursos.</p>
<p>As cotas de recursos são uma ferramenta para os administradores resolverem essa preocupação.</p>
<p>Uma cota de recurso, definida por um objeto <code>ResourceQuota</code>, fornece restrições que limitam
consumo de recursos agregados por <em>namespace</em>. Pode limitar a quantidade de objetos que podem
ser criado em um <em>namespace</em> por tipo, bem como a quantidade total de recursos computacionais que podem
ser consumidos por recursos nesse <em>namespace</em>.</p>
<p>As cotas de recursos funcionam assim:</p>
<ul>
<li>
<p>Diferentes equipes trabalham em diferentes <em>namespaces</em>. Atualmente, isso é voluntário, mas o suporte para tornar isso obrigatório por meio de ACLs está planejado.</p>
</li>
<li>
<p>O administrador cria uma <code>ResourceQuota</code> para cada <em>namespace</em>.</p>
</li>
<li>
<p>Os usuários criam recursos (pods, serviços, etc.) no <em>namespace</em> e o sistema de cotas rastreia o uso para garantir que ele não exceda os limites de recursos definidos em um <code>ResourceQuota</code>.</p>
</li>
<li>
<p>Se a criação ou atualização de um recurso violar uma restrição de cota, a solicitação falhará com código de status HTTP <code>403 FORBIDDEN</code> acompanhado de uma mensagem explicando a restrição que foi violada.</p>
</li>
<li>
<p>Se a cota estiver habilitada em um <em>namespace</em> para recursos computacionais como <code>cpu</code> e <code>memória</code>, os usuários devem especificar solicitações ou limites para esses valores; caso contrário, o sistema de cotas poderá rejeitar a criação de pods. Dica: use o controlador de admissão <code>LimitRanger</code> para forçar padrões para pods que não exigem recursos computacionais.</p>
<p>Veja o <a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>passo a passo</a>
para um exemplo de como evitar este problema.</p>
</li>
</ul>
<p>O nome de um objeto <code>ResourceQuota</code> deve ser um <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nome do subdomínio DNS</a> válido.</p>
<p>Exemplos de políticas que podem ser criadas usando <em>namespaces</em> e cotas são:</p>
<ul>
<li>Em um cluster com capacidade de 32 GiB de RAM e 16 núcleos, deixe a equipe A usar 20 GiB e 10 núcleos, deixe B usar 10GiB e 4 núcleos e mantenha 2GiB e 2 núcleos em reserva para alocação futura.</li>
<li>Limite o <em>namespace</em> "testing" para usar 1 núcleo e 1GiB de RAM. Deixe o namespace "produção" usar qualquer quantia.</li>
</ul>
<p>Caso a capacidade total do cluster seja menor que a soma das cotas dos <em>namespaces</em>, pode haver contenção de recursos. Isso é tratado por ordem de chegada.</p>
<p>Nem a contenção nem as alterações na cota afetarão os recursos já criados.</p>
<h2 id=ativando-a-cota-de-recursos>Ativando a cota de recursos</h2>
<p>O suporte à cota de recursos é ativado por padrão para muitas distribuições do Kubernetes. Isto é
ativado quando a flag <a class=glossary-tooltip title="O componente da camada de gerenciamento que serve a API do Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label="API server">API server</a> <code>--enable-admission-plugins=</code> tem <code>ResourceQuota</code> como
um de seus argumentos.</p>
<p>Uma cota de recurso é aplicada em um <em>namespace</em> específico quando há um <code>ResourceQuota</code> nesse <em>namespace</em>.</p>
<h2 id=cota-de-recursos-computacionais>Cota de recursos computacionais</h2>
<p>Você pode limitar a soma total de <a href=/docs/concepts/configuration/manage-resources-containers/>recursos computacionais</a> que pode ser solicitado em um determinado <em>namespace</em>.</p>
<p>Os seguintes tipos de recursos são suportados:</p>
<table>
<thead>
<tr>
<th>Nome do Recurso</th>
<th>Descrição</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>limits.cpu</code></td>
<td>Em todos os pods em um estado não terminal, a soma dos limites de CPU não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>limits.memory</code></td>
<td>Em todos os pods em um estado não terminal, a soma dos limites de memória não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>requests.cpu</code></td>
<td>Em todos os pods em um estado não terminal, a soma das solicitações da CPU não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>requests.memory</code></td>
<td>Em todos os pods em um estado não terminal, a soma das solicitações de memória não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>hugepages-&lt;size></code></td>
<td>Em todos os pods em um estado não terminal, o número de solicitações de grandes páginas do tamanho especificado não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>cpu</code></td>
<td>O mesmo que <code>requests.cpu</code></td>
</tr>
<tr>
<td><code>memory</code></td>
<td>O mesmo que <code>requests.memory</code></td>
</tr>
</tbody>
</table>
<h3 id=cota-de-recursos-para-recursos-estendidos>Cota de recursos para recursos estendidos</h3>
<p>Além dos recursos mencionados acima, na versão 1.10, suporte a cotas para <a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>recursos estendidos</a> foi adicionado.</p>
<p>Como o <code>overcommit</code> não é permitido para recursos estendidos, não faz sentido especificar tanto <code>requests</code> e <code>limits</code> para o mesmo recurso estendido em uma cota. Portanto, para recursos estendidos, apenas itens de cota com prefixo <code>requests.</code> é permitido por enquanto.</p>
<p>Tome o recurso GPU como exemplo, se o nome do recurso for <code>nvidia.com/gpu</code> e você quiser limitar o número total de GPUs solicitadas em um <em>namespace</em> para 4, você pode definir uma cota da seguinte maneira:</p>
<ul>
<li><code>requests.nvidia.com/gpu: 4</code></li>
</ul>
<p>Veja <a href=#viewing-and-setting-quotas>como visualizar e definir cotas</a> para mais informações.</p>
<h2 id=cota-de-recursos-de-armazenamento>Cota de recursos de armazenamento</h2>
<p>Você pode limitar a soma total de <a href=/docs/concepts/storage/persistent-volumes/>recursos de armazenamento</a> que podem ser solicitados em um determinado <em>namespace</em>.</p>
<p>Além disso, você pode limitar o consumo de recursos de armazenamento com base na classe de armazenamento associada.</p>
<table>
<thead>
<tr>
<th>Nome do recurso</th>
<th>Descrição</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>requests.storage</code></td>
<td>Em todas as solicitações de volume persistentes, a soma das solicitações de armazenamento não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>persistentvolumeclaims</code></td>
<td>O número total de <a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>PersistentVolumeClaims</a> que podem existir no namespace.</td>
</tr>
<tr>
<td><code>&lt;storage-class-name>.storageclass.storage.k8s.io/requests.storage</code></td>
<td>Em todas as solicitações de volume persistentes associadas ao <code>&lt;storage-class-name></code>, a soma das solicitações de armazenamento não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>&lt;storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims</code></td>
<td>Em todas as declarações de volume persistentes associadas ao storage-class-name, o número total de <a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>declarações de volume persistente</a> que podem existir no namespace.</td>
</tr>
</tbody>
</table>
<p>Por exemplo, se um operador deseja cotar armazenamento com classe de armazenamento <code>gold</code> separada da classe de armazenamento <code>bronze</code>, o operador pode definir uma cota da seguinte forma:</p>
<ul>
<li><code>gold.storageclass.storage.k8s.io/requests.storage: 500Gi</code></li>
<li><code>bronze.storageclass.storage.k8s.io/requests.storage: 100Gi</code></li>
</ul>
<p>Na versão 1.8, o suporte de cota para armazenamento temporário local foi adicionado como um recurso alfa:</p>
<table>
<thead>
<tr>
<th>Nome do Recurso</th>
<th>Descrição</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>requests.ephemeral-storage</code></td>
<td>Em todos os pods no <em>namespace</em>, a soma das solicitações de armazenamento local efêmero não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>limits.ephemeral-storage</code></td>
<td>Em todos os pods no <em>namespace</em>, a soma dos limites de armazenamento temporário local não pode exceder esse valor.</td>
</tr>
<tr>
<td><code>ephemeral-storage</code></td>
<td>O mesmo que <code>requests.ephemeral-storage</code>.</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Ao usar um tempo de execução do contêiner CRI, os logs do contêiner serão contabilizados na cota de armazenamento efêmero. Isso pode resultar no despejo inesperado de pods que esgotaram suas cotas de armazenamento. Consulte <a href=/docs/concepts/cluster-administration/logging/>Arquitetura de registro</a> para mais detalhes.
</div>
<h2 id=cota-de-contagem-de-objetos>Cota de contagem de objetos</h2>
<p>Você pode definir cotas para o número total de determinados recursos de todos os padrões, tipos de recursos com <em>namespace</em> usando a seguinte sintaxe:</p>
<ul>
<li><code>count/&lt;resource>.&lt;group></code> para recursos de grupos não principais</li>
<li><code>count/&lt;resource></code> para recursos do grupo principal</li>
</ul>
<p>Exemplo de conjunto de recursos que os usuários podem querer colocar na cota de contagem de objetos:</p>
<ul>
<li><code>count/persistentvolumeclaims</code></li>
<li><code>count/services</code></li>
<li><code>count/secrets</code></li>
<li><code>count/configmaps</code></li>
<li><code>count/replicationcontrollers</code></li>
<li><code>count/deployments.apps</code></li>
<li><code>count/replicasets.apps</code></li>
<li><code>count/statefulsets.apps</code></li>
<li><code>count/jobs.batch</code></li>
<li><code>count/cronjobs.batch</code></li>
</ul>
<p>A mesma sintaxe pode ser usada para recursos personalizados. Por exemplo, para criar uma cota em um recurso personalizado <code>widgets</code> no grupo de API <code>example.com</code>, use <code>count/widgets.example.com</code>.</p>
<p>Ao usar a cota de recurso <code>count/*</code>, um objeto é cobrado na cota se existir no armazenamento do servidor. Esses tipos de cotas são úteis para proteger contra o esgotamento dos recursos de armazenamento. Por exemplo, você pode desejar limitar o número de segredos em um servidor devido ao seu grande tamanho. Muitos segredos em um cluster podem
na verdade, impedir que servidores e controladores sejam iniciados. Você pode definir uma cota para projetos para proteger contra um <code>CronJob</code> mal configurado. <code>CronJobs</code> que criam muitos <code>Jobs</code> em um <em>namespace</em> podem levar a uma negação de serviço.</p>
<p>Também é possível fazer uma cota de contagem de objetos genéricos em um conjunto limitado de recursos.
Os seguintes tipos são suportados:</p>
<table>
<thead>
<tr>
<th>Nome do Recurso</th>
<th>Descrição</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>configmaps</code></td>
<td>O número total de <code>ConfigMaps</code> que podem existir no namespace.</td>
</tr>
<tr>
<td><code>persistentvolumeclaims</code></td>
<td>O número total de <a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>PersistentVolumeClaims</a> que podem existir no namespace.</td>
</tr>
<tr>
<td><code>pods</code></td>
<td>O número total de pods em um estado não terminal que pode existir no namespace. Um pod está em um estado terminal se <code>.status.phase in (Failed, Succeeded)</code> for verdadeiro.</td>
</tr>
<tr>
<td><code>replicationcontrollers</code></td>
<td>O número total de <code>ReplicationControllers</code> que podem existir no <em>namespace</em>.</td>
</tr>
<tr>
<td><code>resourcequotas</code></td>
<td>O número total de <code>ResourceQuotas</code> que podem existir no <em>namespace</em>.</td>
</tr>
<tr>
<td><code>services</code></td>
<td>O número total de Serviços que podem existir no <em>namespace</em>.</td>
</tr>
<tr>
<td><code>services.loadbalancers</code></td>
<td>O número total de serviços do tipo <code>LoadBalancer</code> que podem existir no <em>namespace</em>.</td>
</tr>
<tr>
<td><code>services.nodeports</code></td>
<td>O número total de serviços do tipo <code>NodePort</code> que podem existir no <em>namespace</em>.</td>
</tr>
<tr>
<td><code>secrets</code></td>
<td>O número total de segredos que podem existir no <em>namespace</em>.</td>
</tr>
</tbody>
</table>
<p>Por exemplo, a cota de <code>pods</code> conta e impõe um número máximo de <code>pods</code> criados em um único <em>namespace</em> que não é terminal. Você pode querer definir uma cota <code>pods</code>em um <em>namespace</em> para evitar o caso em que um usuário cria muitos <code>pods</code> pequenos e esgota o fornecimento de IPs de pod do cluster.</p>
<h2 id=escopos-de-cota>Escopos de cota</h2>
<p>Cada cota pode ter um conjunto associado de <code>scopes</code>. Uma cota só medirá o uso de um recurso se corresponder
a interseção de escopos enumerados.</p>
<p>Quando um escopo é adicionado à cota, ele limita o número de recursos aos quais ele dá suporte a aqueles que pertencem ao escopo. Os recursos especificados na cota fora do conjunto permitido resultam em um erro de validação.</p>
<table>
<thead>
<tr>
<th>Escopo</th>
<th>Descrição</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Terminating</code></td>
<td>Pods correspondentes onde <code>.spec.activeDeadlineSeconds >= 0</code></td>
</tr>
<tr>
<td><code>NotTerminating</code></td>
<td>Pods correspondentes onde <code>.spec.activeDeadlineSeconds is nil</code></td>
</tr>
<tr>
<td><code>BestEffort</code></td>
<td>Pods correspondentes que tenham a qualidade de serviço de melhor esforço.</td>
</tr>
<tr>
<td><code>NotBestEffort</code></td>
<td>Pods correspondentes que não têm qualidade de serviço de melhor esforço.</td>
</tr>
<tr>
<td><code>PriorityClass</code></td>
<td>Corresponde aos pods que fazem referência à <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption>classe de prioridade</a> especificada.</td>
</tr>
<tr>
<td><code>CrossNamespacePodAffinity</code></td>
<td>Corresponde a pods que tenham <a href=/docs/concepts/scheduling-eviction/assign-pod-node>termos de (anti)afinidade</a> de <em>namespace</em> cruzado.</td>
</tr>
</tbody>
</table>
<p>O escopo <code>BestEffort</code> restringe uma cota ao rastreamento do seguinte recurso:</p>
<ul>
<li><code>pods</code></li>
</ul>
<p>Os escopos <code>Termination</code>, <code>NotTerminate</code>, <code>NotBestEffort</code> e <code>PriorityClass</code>restringem uma cota para rastrear os seguintes recursos:</p>
<ul>
<li><code>pods</code></li>
<li><code>cpu</code></li>
<li><code>memory</code></li>
<li><code>requests.cpu</code></li>
<li><code>requests.memory</code></li>
<li><code>limits.cpu</code></li>
<li><code>limits.memory</code></li>
</ul>
<p>Observe que você não pode especificar os escopos <code>Terminate</code> e o <code>NotTerminate</code>na mesma cota, e você também não pode especificar o <code>BestEffort</code> e<code>NotBestEffort</code> na mesma cota.</p>
<p>O <code>scopeSelector</code> suporta os seguintes valores no campo <code>operator</code>:</p>
<ul>
<li><code>In</code></li>
<li><code>NotIn</code></li>
<li><code>Exists</code></li>
<li><code>DoesNotExist</code></li>
</ul>
<p>Ao usar um dos seguintes valores como o <code>scopeName</code> ao definir o<code>scopeSelector</code>, o <code>operator</code> deve ser <code>Exists</code>.</p>
<ul>
<li><code>Terminating</code></li>
<li><code>NotTerminating</code></li>
<li><code>BestEffort</code></li>
<li><code>NotBestEffort</code></li>
</ul>
<p>Se o <code>operator</code> for <code>In</code> ou <code>NotIn</code>, o campo <code>values</code> deve ter pelo menos um valor. Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- middle<span style=color:#bbb>
</span></code></pre></div><p>Se o <code>operator</code> for <code>Exists</code> ou <code>DoesNotExist</code>, o campo <code>values</code> <em>NÃO</em> deve ser especificado.</p>
<h3 id=cota-de-recursos-por-classe-de-prioridade>Cota de recursos por classe de prioridade</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>
<p>Os pods podem ser criados em uma <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority>prioridade</a> específica. Você pode controlar o consumo de recursos do sistema de um pod com base na prioridade de um pod, usando o <code>scopeSelector</code>
campo na especificação de cota.</p>
<p>Uma cota é correspondida e consumida apenas se <code>scopeSelector</code> na especificação de cota selecionar o pod.</p>
<p>Quando a cota está no escopo da classe de prioridade usando o campo <code>scopeSelector</code>, objeto de cota
está restrito a rastrear apenas os seguintes recursos:</p>
<ul>
<li><code>pods</code></li>
<li><code>cpu</code></li>
<li><code>memory</code></li>
<li><code>ephemeral-storage</code></li>
<li><code>limits.cpu</code></li>
<li><code>limits.memory</code></li>
<li><code>limits.ephemeral-storage</code></li>
<li><code>requests.cpu</code></li>
<li><code>requests.memory</code></li>
<li><code>requests.ephemeral-storage</code></li>
</ul>
<p>Este exemplo cria um objeto de cota e o corresponde a pods em prioridades específicas. O exemplo
funciona da seguinte forma:</p>
<ul>
<li>Os pods no cluster têm uma das três classes de prioridade, "baixa", "média", "alta".</li>
<li>Um objeto de cota é criado para cada prioridade.</li>
</ul>
<p>Salve o seguinte YAML em um arquivo <code>quota.yml</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-high<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1000&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Gi<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>operator </span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;high&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-medium<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>20Gi<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>operator </span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;medium&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-low<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>operator </span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;low&#34;</span>]<span style=color:#bbb>
</span></code></pre></div><p>Aplique o YAML usando <code>kubectl create</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create -f ./quota.yml
</code></pre></div><pre><code>resourcequota/pods-high created
resourcequota/pods-medium created
resourcequota/pods-low created
</code></pre><p>Verifique se a cota <code>Used</code> é <code>0</code> usando <code>kubectl describe quota</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe quota
</code></pre></div><pre><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     1k
memory      0     200Gi
pods        0     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><p>Crie um pod com prioridade "high". Salve o seguinte YAML em um arquivo <code>high-priority-pod.yml</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>ubuntu<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;/bin/sh&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;while true; do echo hello; sleep 10;done&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityClassName</span>:<span style=color:#bbb> </span>high<span style=color:#bbb>
</span></code></pre></div><p>Applique com <code>kubectl create</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create -f ./high-priority-pod.yml
</code></pre></div><p>Verifique se as estatísticas "Used" para a cota de prioridade "high", <code>pods-high</code> foram alteradas e se
as outras duas cotas permanecem inalteradas.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe quota
</code></pre></div><pre><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         500m  1k
memory      10Gi  200Gi
pods        1     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><h3 id=cota-de-afinidade-de-pod-entre-namespaces>Cota de afinidade de pod entre <em>namespaces</em></h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>
<p>Os operadores podem usar o escopo de cota <code>CrossNamespacePodAffinity</code> para limitar quais <em>namespaces</em> têm permissão para ter pods com termos de afinidade que cruzam <em>namespaces</em>. Especificamente, ele controla quais pods são permitidos para definir os campos <code>namespaces</code> ou <code>namespaceSelector</code> em termos de afinidade de pod.</p>
<p>Impedir que os usuários usem termos de afinidade entre <em>namespaces</em> pode ser desejável, pois um pod
com restrições antiafinidade pode bloquear pods de todos os outros <em>namespaces</em> de ser agendado em um domínio de falha.</p>
<p>O uso desses operadores de escopo pode impedir certos <em>namespaces</em> (<code>foo-ns</code> no exemplo abaixo) de ter pods que usam afinidade de pod entre <em>namespaces</em> criando um objeto de cota de recurso nesse <em>namespace</em> com escopo <code>CrossNamespaceAffinity</code> e limite rígido de 0:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>disable-cross-namespace-affinity<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>foo-ns<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>CrossNamespaceAffinity<span style=color:#bbb>
</span></code></pre></div><p>Se os operadores quiserem proibir o uso de <code>namespaces</code> e <code>namespaceSelector</code> por padrão, e
permitir apenas para <em>namespaces</em> específicos, eles podem configurar <code>CrossNamespaceAffinity</code>como um recurso limitado definindo o sinalizador kube-apiserver --admission-control-config-file
para o caminho do seguinte arquivo de configuração:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>AdmissionConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>plugins</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;ResourceQuota&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>configuration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuotaConfiguration<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>limitedResources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>resource</span>:<span style=color:#bbb> </span>pods<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchScopes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>CrossNamespaceAffinity<span style=color:#bbb>
</span></code></pre></div><p>Com a configuração acima, os pods podem usar <code>namespaces</code> e <code>namespaceSelector</code> apenas na afinidade do pod se o <em>namespace</em> em que foram criados tiver um objeto de cota de recurso com escopo <code>CrossNamespaceAffinity</code> e um limite rígido maior ou igual ao número de pods usando esses campos.</p>
<p>Esse recurso é beta e ativado por padrão. Você pode desativá-lo usando o <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>PodAffinityNamespaceSelector</code> no kube-apiserver e no kube-scheduler.</p>
<h2 id=requests-vs-limits>Solicitações comparadas aos limites</h2>
<p>Ao alocar recursos computacionais, cada contêiner pode especificar uma solicitação e um valor limite para CPU ou memória. A cota pode ser configurada para cotar qualquer valor.</p>
<p>Se a cota tiver um valor especificado para <code>requests.cpu</code> ou <code>requests.memory</code>, ela exigirá que cada container faça uma solicitação explícita para esses recursos. Se a cota tiver um valor especificado para <code>limits.cpu</code> ou <code>limits.memory</code>, em seguida exige que cada contêiner de entrada especifique um limite explícito para esses recursos.</p>
<h2 id=como-visualizar-e-definir-cotas>Como visualizar e definir cotas</h2>
<p>O Kubectl é compatível com a criação, atualização e visualização de cotas:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace myspace
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF &gt; compute-resources.yaml
</span><span style=color:#b44>apiVersion: v1
</span><span style=color:#b44>kind: ResourceQuota
</span><span style=color:#b44>metadata:
</span><span style=color:#b44>  name: compute-resources
</span><span style=color:#b44>spec:
</span><span style=color:#b44>  hard:
</span><span style=color:#b44>    requests.cpu: &#34;1&#34;
</span><span style=color:#b44>    requests.memory: 1Gi
</span><span style=color:#b44>    limits.cpu: &#34;2&#34;
</span><span style=color:#b44>    limits.memory: 2Gi
</span><span style=color:#b44>    requests.nvidia.com/gpu: 4
</span><span style=color:#b44>EOF</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create -f ./compute-resources.yaml --namespace<span style=color:#666>=</span>myspace
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF &gt; object-counts.yaml
</span><span style=color:#b44>apiVersion: v1
</span><span style=color:#b44>kind: ResourceQuota
</span><span style=color:#b44>metadata:
</span><span style=color:#b44>  name: object-counts
</span><span style=color:#b44>spec:
</span><span style=color:#b44>  hard:
</span><span style=color:#b44>    configmaps: &#34;10&#34;
</span><span style=color:#b44>    persistentvolumeclaims: &#34;4&#34;
</span><span style=color:#b44>    pods: &#34;4&#34;
</span><span style=color:#b44>    replicationcontrollers: &#34;20&#34;
</span><span style=color:#b44>    secrets: &#34;10&#34;
</span><span style=color:#b44>    services: &#34;10&#34;
</span><span style=color:#b44>    services.loadbalancers: &#34;2&#34;
</span><span style=color:#b44>EOF</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create -f ./object-counts.yaml --namespace<span style=color:#666>=</span>myspace
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get quota --namespace<span style=color:#666>=</span>myspace
</code></pre></div><pre><code class=language-none data-lang=none>NAME                    AGE
compute-resources       30s
object-counts           32s
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe quota compute-resources --namespace<span style=color:#666>=</span>myspace
</code></pre></div><pre><code class=language-none data-lang=none>Name:                    compute-resources
Namespace:               myspace
Resource                 Used  Hard
--------                 ----  ----
limits.cpu               0     2
limits.memory            0     2Gi
requests.cpu             0     1
requests.memory          0     1Gi
requests.nvidia.com/gpu  0     4
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe quota object-counts --namespace<span style=color:#666>=</span>myspace
</code></pre></div><pre><code class=language-none data-lang=none>Name:                   object-counts
Namespace:              myspace
Resource                Used    Hard
--------                ----    ----
configmaps              0       10
persistentvolumeclaims  0       4
pods                    0       4
replicationcontrollers  0       20
secrets                 1       10
services                0       10
services.loadbalancers  0       2
</code></pre><p>Kubectl also supports object count quota for all standard namespaced resources
using the syntax <code>count/&lt;resource>.&lt;group></code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create namespace myspace
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create quota <span style=color:#a2f>test</span> --hard<span style=color:#666>=</span>count/deployments.apps<span style=color:#666>=</span>2,count/replicasets.apps<span style=color:#666>=</span>4,count/pods<span style=color:#666>=</span>3,count/secrets<span style=color:#666>=</span><span style=color:#666>4</span> --namespace<span style=color:#666>=</span>myspace
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create deployment nginx --image<span style=color:#666>=</span>nginx --namespace<span style=color:#666>=</span>myspace --replicas<span style=color:#666>=</span><span style=color:#666>2</span>
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe quota --namespace<span style=color:#666>=</span>myspace
</code></pre></div><pre><code>Name:                         test
Namespace:                    myspace
Resource                      Used  Hard
--------                      ----  ----
count/deployments.apps        1     2
count/pods                    2     3
count/replicasets.apps        1     4
count/secrets                 1     4
</code></pre><h2 id=capacidade-e-cota-de-cluster>Capacidade e cota de Cluster</h2>
<p><code>ResourceQuotas</code> são independentes da capacidade do cluster. Eles estão expresso em unidades absolutas. Portanto, se você adicionar nós ao cluster, isso <em>não</em>
dá automaticamente a cada <em>namespace</em> a capacidade de consumir mais recursos.</p>
<p>Às vezes, políticas mais complexas podem ser necessárias, como:</p>
<ul>
<li>Divida proporcionalmente os recursos totais do cluster entre várias equipes.</li>
<li>Permita que cada locatário aumente o uso de recursos conforme necessário, mas tenha um generoso limite para evitar o esgotamento acidental de recursos.</li>
<li>Detecte a demanda de um <em>namespace</em>, adicione nós e aumente a cota.</li>
</ul>
<p>Tais políticas podem ser implementadas usando <code>ResourceQuotas</code> como blocos de construção, por
escrevendo um "controlador" que observa o uso da cota e ajusta os limites rígidos da cota de cada <em>namespace</em> de acordo com outros sinais.</p>
<p>Observe que a cota de recursos divide os recursos agregados do cluster, mas não cria restrições em torno dos nós: pods de vários <em>namespaces</em> podem ser executados no mesmo nó.</p>
<h2 id=limite-de-consumo-de-classe-de-prioridade-por-padrão>Limite de consumo de classe de prioridade por padrão</h2>
<p>Pode ser desejado que os pods com uma prioridade particular, por exemplo. "cluster-services",
deve ser permitido em um <em>namespace</em>, se, e somente se, existir um objeto de cota correspondente.</p>
<p>Com este mecanismo, os operadores podem restringir o uso de certas classes de prioridade para um número limitado de <em>namespaces</em> , e nem todos poderão consumir essas classes de prioridade por padrão.</p>
<p>Para impor isso, a flag <code>kube-apiserver</code> <code>--admission-control-config-file</code> deve ser
usada para passar o caminho para o seguinte arquivo de configuração:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>AdmissionConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>plugins</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;ResourceQuota&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>configuration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuotaConfiguration<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>limitedResources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>resource</span>:<span style=color:#bbb> </span>pods<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchScopes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;cluster-services&#34;</span>]<span style=color:#bbb>
</span></code></pre></div><p>Em seguida, crie um objeto de cota de recurso no <em>namespace</em> <code>kube-system</code>:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/policy/priority-class-resourcequota.yaml download=policy/priority-class-resourcequota.yaml><code>policy/priority-class-resourcequota.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('policy-priority-class-resourcequota-yaml')" title="Copy policy/priority-class-resourcequota.yaml to clipboard">
</img>
</div>
<div class=includecode id=policy-priority-class-resourcequota-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-cluster-services<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>operator </span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;cluster-services&#34;</span>]</code></pre></div>
</div>
</div>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system
</code></pre></div><pre><code class=language-none data-lang=none>resourcequota/pods-cluster-services created
</code></pre><p>Nesse caso, a criação de um pod será permitida se:</p>
<ol>
<li>O <code>priorityClassName</code> do pod não foi especificado.</li>
<li>O <code>priorityClassName</code> do pod é especificado com um valor diferente de <code>cluster-services</code>.</li>
<li>O <code>priorityClassName</code> do pod está definido como <code>cluster-services</code>, ele deve ser criado no namespace <code>kube-system</code> e passou na verificação de cota de recursos.</li>
</ol>
<p>Uma solicitação de criação de pod é rejeitada caso seu <code>priorityClassName</code> estiver definido como <code>cluster-services</code> e deve ser criado em um <em>namespace</em> diferente de <code>kube-system</code>.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Veja o <a href=https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_resource_quota.md>documento de design de cota de recursos</a> para mais informações.</li>
<li>Veja um <a href=/docs/tasks/administer-cluster/quota-api-object/>exemplo detalhado de como usar a cota de recursos</a>.</li>
<li>Leia o <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-priority-resourcequota.md>documento de design de suporte de cota para prioridade de classe</a>.</li>
<li>Veja <a href=https://github.com/kubernetes/kubernetes/pull/36765>recursos limitados</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-0554ac387412eaf4e6e89b2f847dacde>3 - Visão Geral</h1>
<div class=lead>Obtenha uma visão em alto-nível do Kubernetes e dos componentes a partir dos quais ele é construído.</div>
</div>
<div class=td-content>
<h1 id=pg-45bdca6129cf540121623e903c18ba46>3.1 - O que é Kubernetes?</h1>
<div class=lead>Kubernetes é um plataforma de código aberto, portável e extensiva para o gerenciamento de cargas de trabalho e serviços distribuídos em contêineres, que facilita tanto a configuração declarativa quanto a automação. Ele possui um ecossistema grande, e de rápido crescimento. Serviços, suporte, e ferramentas para Kubernetes estão amplamente disponíveis.</div>
<p>Essa página é uma visão geral do Kubernetes.</p>
<p>Kubernetes é um plataforma de código aberto, portável e extensiva para o gerenciamento de cargas de trabalho e serviços distribuídos em contêineres, que facilita tanto a configuração declarativa quanto a automação. Ele possui um ecossistema grande, e de rápido crescimento. Serviços, suporte, e ferramentas para Kubernetes estão amplamente disponíveis.</p>
<p>O Google tornou Kubernetes um projeto de código-aberto em 2014. O Kubernetes combina <a href=/blog/2015/04/borg-predecessor-to-kubernetes/>mais de 15 anos de experiência do Google</a> executando cargas de trabalho produtivas em escala, com as melhores idéias e práticas da comunidade.</p>
<p>O nome <strong>Kubernetes</strong> tem origem no Grego, significando <em>timoneiro</em> ou <em>piloto</em>. <strong>K8s</strong> é a abreviação derivada pela troca das oito letras "ubernete" por "8", se tornado <em>K"8"s</em>.</p>
<h2 id=voltando-no-tempo>Voltando no tempo</h2>
<p>Vamos dar uma olhada no porque o Kubernetes é tão útil, voltando no tempo.</p>
<p><img src=/images/docs/Container_Evolution.svg alt="Evolução das implantações"></p>
<p><strong>Era da implantação tradicional:</strong> No início, as organizações executavam aplicações em servidores físicos. Não havia como definir limites de recursos para aplicações em um mesmo servidor físico, e isso causava problemas de alocação de recursos. Por exemplo, se várias aplicações fossem executadas em um mesmo servidor físico, poderia haver situações em que uma aplicação ocupasse a maior parte dos recursos e, como resultado, o desempenho das outras aplicações seria inferior. Uma solução para isso seria executar cada aplicação em um servidor físico diferente. Mas isso não escalava, pois os recursos eram subutilizados, e se tornava custoso para as organizações manter muitos servidores físicos.</p>
<p><strong>Era da implantação virtualizada:</strong> Como solução, a virtualização foi introduzida. Esse modelo permite que você execute várias máquinas virtuais (VMs) em uma única CPU de um servidor físico. A virtualização permite que as aplicações sejam isoladas entre as VMs, e ainda fornece um nível de segurança, pois as informações de uma aplicação não podem ser acessadas livremente por outras aplicações.</p>
<p>A virtualização permite melhor utilização de recursos em um servidor físico, e permite melhor escalabilidade porque uma aplicação pode ser adicionada ou atualizada facilmente, reduz os custos de hardware e muito mais. Com a virtualização, você pode apresentar um conjunto de recursos físicos como um cluster de máquinas virtuais descartáveis.</p>
<p>Cada VM é uma máquina completa que executa todos os componentes, incluindo seu próprio sistema operacional, além do hardware virtualizado.</p>
<p><strong>Era da implantação em contêineres:</strong> Contêineres são semelhantes às VMs, mas têm propriedades de isolamento flexibilizados para compartilhar o sistema operacional (SO) entre as aplicações. Portanto, os contêineres são considerados leves. Semelhante a uma VM, um contêiner tem seu próprio sistema de arquivos, compartilhamento de CPU, memória, espaço de processo e muito mais. Como eles estão separados da infraestrutura subjacente, eles são portáveis entre nuvens e distribuições de sistema operacional.</p>
<p>Contêineres se tornaram populares porque eles fornecem benefícios extra, tais como:</p>
<ul>
<li>Criação e implantação ágil de aplicações: aumento da facilidade e eficiência na criação de imagem de contêiner comparado ao uso de imagem de VM.</li>
<li>Desenvolvimento, integração e implantação contínuos: fornece capacidade de criação e de implantação de imagens de contêiner de forma confiável e frequente, com a funcionalidade de efetuar reversões rápidas e eficientes (devido à imutabilidade da imagem).</li>
<li>Separação de interesses entre Desenvolvimento e Operações: crie imagens de contêineres de aplicações no momento de construção/liberação em vez de no momento de implantação, desacoplando as aplicações da infraestrutura.</li>
<li>A capacidade de observação (Observabilidade) não apenas apresenta informações e métricas no nível do sistema operacional, mas também a integridade da aplicação e outros sinais.</li>
<li>Consistência ambiental entre desenvolvimento, teste e produção: funciona da mesma forma em um laptop e na nuvem.</li>
<li>Portabilidade de distribuição de nuvem e sistema operacional: executa no Ubuntu, RHEL, CoreOS, localmente, nas principais nuvens públicas e em qualquer outro lugar.</li>
<li>Gerenciamento centrado em aplicações: eleva o nível de abstração da execução em um sistema operacional em hardware virtualizado à execução de uma aplicação em um sistema operacional usando recursos lógicos.</li>
<li>Microserviços fracamente acoplados, distribuídos, elásticos e livres: as aplicações são divididas em partes menores e independentes e podem ser implantados e gerenciados dinamicamente - não uma pilha monolítica em execução em uma grande máquina de propósito único.</li>
<li>Isolamento de recursos: desempenho previsível de aplicações.</li>
<li>Utilização de recursos: alta eficiência e densidade.</li>
</ul>
<h2 id=why-you-need-kubernetes-and-what-can-it-do>Por que você precisa do Kubernetes e o que ele pode fazer</h2>
<p>Os contêineres são uma boa maneira de agrupar e executar suas aplicações. Em um ambiente de produção, você precisa gerenciar os contêineres que executam as aplicações e garantir que não haja tempo de inatividade. Por exemplo, se um contêiner cair, outro contêiner precisa ser iniciado. Não seria mais fácil se esse comportamento fosse controlado por um sistema?</p>
<p>É assim que o Kubernetes vem ao resgate! O Kubernetes oferece uma estrutura para executar sistemas distribuídos de forma resiliente. Ele cuida do escalonamento e do recuperação à falha de sua aplicação, fornece padrões de implantação e muito mais. Por exemplo, o Kubernetes pode gerenciar facilmente uma implantação no método canário para seu sistema.</p>
<p>O Kubernetes oferece a você:</p>
<ul>
<li><strong>Descoberta de serviço e balanceamento de carga</strong>
O Kubernetes pode expor um contêiner usando o nome DNS ou seu próprio endereço IP. Se o tráfego para um contêiner for alto, o Kubernetes pode balancear a carga e distribuir o tráfego de rede para que a implantação seja estável.</li>
<li><strong>Orquestração de armazenamento</strong>
O Kubernetes permite que você monte automaticamente um sistema de armazenamento de sua escolha, como armazenamentos locais, provedores de nuvem pública e muito mais.</li>
<li><strong>Lançamentos e reversões automatizadas</strong>
Você pode descrever o estado desejado para seus contêineres implantados usando o Kubernetes, e ele pode alterar o estado real para o estado desejado em um ritmo controlada. Por exemplo, você pode automatizar o Kubernetes para criar novos contêineres para sua implantação, remover os contêineres existentes e adotar todos os seus recursos para o novo contêiner.</li>
<li><strong>Empacotamento binário automático</strong>
Você fornece ao Kubernetes um cluster de nós que pode ser usado para executar tarefas nos contêineres. Você informa ao Kubernetes de quanta CPU e memória (RAM) cada contêiner precisa. O Kubernetes pode encaixar contêineres em seus nós para fazer o melhor uso de seus recursos.</li>
<li><strong>Autocorreção</strong>
O Kubernetes reinicia os contêineres que falham, substitui os contêineres, elimina os contêineres que não respondem à verificação de integridade definida pelo usuário e não os anuncia aos clientes até que estejam prontos para servir.</li>
<li><strong>Gerenciamento de configuração e de segredos</strong>
O Kubernetes permite armazenar e gerenciar informações confidenciais, como senhas, tokens OAuth e chaves SSH. Você pode implantar e atualizar segredos e configuração de aplicações sem reconstruir suas imagens de contêiner e sem expor segredos em sua pilha de configuração.</li>
</ul>
<h2 id=o-que-o-kubernetes-não-é>O que o Kubernetes não é</h2>
<p>O Kubernetes não é um sistema PaaS (plataforma como serviço) tradicional e completo. Como o Kubernetes opera no nível do contêiner, e não no nível do hardware, ele fornece alguns recursos geralmente aplicáveis comuns às ofertas de PaaS, como implantação, escalonamento, balanceamento de carga, e permite que os usuários integrem suas soluções de <em>logging</em>, monitoramento e alerta. No entanto, o Kubernetes não é monolítico, e essas soluções padrão são opcionais e conectáveis. O Kubernetes fornece os blocos de construção para a construção de plataformas de desenvolvimento, mas preserva a escolha e flexibilidade do usuário onde é importante.</p>
<p>Kubernetes:</p>
<ul>
<li>Não limita os tipos de aplicações suportadas. O Kubernetes visa oferecer suporte a uma variedade extremamente diversa de cargas de trabalho, incluindo cargas de trabalho sem estado, com estado e de processamento de dados. Se uma aplicação puder ser executada em um contêiner, ele deve ser executado perfeitamente no Kubernetes.</li>
<li>Não implanta código-fonte e não constrói sua aplicação. Os fluxos de trabalho de integração contínua, entrega e implantação (CI/CD) são determinados pelas culturas e preferências da organização, bem como pelos requisitos técnicos.</li>
<li>Não fornece serviços em nível de aplicação, tais como middleware (por exemplo, barramentos de mensagem), estruturas de processamento de dados (por exemplo, Spark), bancos de dados (por exemplo, MySQL), caches, nem sistemas de armazenamento em cluster (por exemplo, Ceph), como serviços integrados. Esses componentes podem ser executados no Kubernetes e/ou podem ser acessados por aplicações executadas no Kubernetes por meio de mecanismos portáteis, como o <a href=https://openservicebrokerapi.org/>Open Service Broker</a>.</li>
<li>Não dita soluções de <em>logging</em>, monitoramento ou alerta. Ele fornece algumas integrações como prova de conceito e mecanismos para coletar e exportar métricas.</li>
<li>Não fornece nem exige um sistema/idioma de configuração (por exemplo, Jsonnet). Ele fornece uma API declarativa que pode ser direcionada por formas arbitrárias de especificações declarativas.</li>
<li>Não fornece nem adota sistemas abrangentes de configuração de máquinas, manutenção, gerenciamento ou autocorreção.</li>
<li>Adicionalmente, o Kubernetes não é um mero sistema de orquestração. Na verdade, ele elimina a necessidade de orquestração. A definição técnica de orquestração é a execução de um fluxo de trabalho definido: primeiro faça A, depois B e depois C. Em contraste, o Kubernetes compreende um conjunto de processos de controle independentes e combináveis que conduzem continuamente o estado atual em direção ao estado desejado fornecido. Não importa como você vai de A para C. O controle centralizado também não é necessário. Isso resulta em um sistema que é mais fácil de usar e mais poderoso, robusto, resiliente e extensível.</li>
</ul>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Dê uma olhada em <a href=/pt-br/docs/concepts/overview/components/>Componentes do Kubernetes</a>.</li>
<li>Pronto para <a href=/docs/setup/>Iniciar</a>?</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-13b0f1dbe89228e3d76d2ac231e245f1>3.2 - Componentes do Kubernetes</h1>
<div class=lead>Um cluster Kubernetes consiste de componentes que representam a camada de gerenciamento, e um conjunto de máquinas chamadas nós.</div>
<p>Ao implantar o Kubernetes, você obtém um cluster.
<p><p>Um cluster Kubernetes consiste em um conjunto de servidores de processamento, chamados <a class=glossary-tooltip title="Um Nó é uma máquina de trabalho no Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nós>nós</a>, que executam aplicações containerizadas. Todo cluster possui ao menos um servidor de processamento (<em>worker node</em>).</p></p>
<p>O servidor de processamento hospeda os <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> que são componentes de uma aplicação. O <a class=glossary-tooltip title="A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="ambiente de gerenciamento">ambiente de gerenciamento</a> gerencia os nós de processamento e os Pods no cluster. Em ambientes de produção, o ambiente de gerenciamento geralmente executa em múltiplos computadores e um cluster geralmente executa em múltiplos nós (<em>nodes</em>) , provendo tolerância a falhas e alta disponibilidade.</p></p>
<p>Este documento descreve os vários componentes que você precisa ter para implantar um cluster Kubernetes completo e funcional.</p>
<p>Esse é o diagrama de um cluster Kubernetes com todos os componentes interligados.</p>
<p><img src=/images/docs/components-of-kubernetes.svg alt="Componentes do Kubernetes"></p>
<h2 id=componentes-da-camada-de-gerenciamento>Componentes da camada de gerenciamento</h2>
<p>Os componentes da camada de gerenciamento tomam decisões globais sobre o cluster (por exemplo, agendamento de <em>pods</em>), bem como detectam e respondem aos eventos do cluster (por exemplo, iniciando um novo <em><a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=pod>pod</a></em> quando o campo <code>replicas</code> de um <em>Deployment</em> não está atendido).</p>
<p>Os componentes da camada de gerenciamento podem ser executados em qualquer máquina do cluster. Contudo, para simplificar, os <em>scripts</em> de configuração normalmente iniciam todos os componentes da camada de gerenciamento na mesma máquina, e não executa contêineres de usuário nesta máquina. Veja <a href=/docs/admin/high-availability/>Construindo clusters de alta disponibilidade</a> para um exemplo de configuração de múltiplas VMs para camada de gerenciamento (<em>multi-main-VM</em>).</p>
<h3 id=kube-apiserver>kube-apiserver</h3>
<p>O servidor de API é um componente da <a class=glossary-tooltip title="A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="Camada de gerenciamento">Camada de gerenciamento</a> do Kubernetes que expõe a API do Kubernetes.
O servidor de API é o <em>front end</em> para a camada de gerenciamento do Kubernetes.</p>
<p>A principal implementação de um servidor de API do Kubernetes é <a href=/docs/reference/generated/kube-apiserver/>kube-apiserver</a>.
O kube-apiserver foi projetado para ser escalonado horizontalmente — ou seja, ele pode ser escalado com a implantação de mais instâncias.
Você pode executar várias instâncias do kube-apiserver e balancear (balanceamento de carga, etc) o tráfego entre essas instâncias.</p>
<h3 id=etcd>etcd</h3>
<p>Armazenamento do tipo Chave-Valor consistente e em alta-disponibilidade usado como repositório de apoio do Kubernetes para todos os dados do cluster.</p>
<p>Se o seu cluster Kubernetes usa <strong>etcd</strong> como seu armazenamento de apoio, certifique-se de ter um plano de <a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster>back up</a> para seus dados.</p>
<p>Você pode encontrar informações detalhadas sobre o etcd na seção oficial da <a href=https://etcd.io/docs/>documentação</a>.</p>
<h3 id=kube-scheduler>kube-scheduler</h3>
<p>Componente da camada de gerenciamento que observa os <em><a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=pods>pods</a></em> recém-criados sem nenhum <a class=glossary-tooltip title="Um Nó é uma máquina de trabalho no Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nó>nó</a> atribuído, e seleciona um nó para executá-los.</p>
<p>Os fatores levados em consideração para as decisões de agendamento incluem:
requisitos de recursos individuais e coletivos, hardware/software/política de restrições, especificações de afinidade e antiafinidade, localidade de dados, interferência entre cargas de trabalho, e prazos.</p>
<h3 id=kube-controller-manager>kube-controller-manager</h3>
<p>Componente da camada de gerenciamento que executa os processos de <a class=glossary-tooltip title="Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a>.</p>
<p>Logicamente, cada <em><a class=glossary-tooltip title="Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a></em> está em um processo separado, mas para reduzir a complexidade, eles todos são compilados num único binário e executam em um processo único.</p>
<p>Alguns tipos desses controladores são:</p>
<ul>
<li>Controlador de nó: responsável por perceber e responder quando os nós caem.</li>
<li>Controlador de <em>Job</em>: Observa os objetos <em>Job</em> que representam tarefas únicas e, em seguida, cria <em>pods</em> para executar essas tarefas até a conclusão.</li>
<li>Controlador de <em>endpoints</em>: preenche o objeto <em>Endpoints</em> (ou seja, junta os Serviços e os <em>pods</em>).</li>
<li>Controladores de conta de serviço e de <em>token</em>: crie contas padrão e <em>tokens</em> de acesso de API para novos <em>namespaces</em>.</li>
</ul>
<h3 id=cloud-controller-manager>cloud-controller-manager</h3>
Um componente da <a class=glossary-tooltip title="A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="camada de gerenciamento">camada de gerenciamento</a> do Kubernetes
que incorpora a lógica de controle específica da nuvem. O gerenciador de controle de nuvem permite que você vincule seu
<em>cluster</em> na API do seu provedor de nuvem, e separar os componentes que interagem com essa plataforma de nuvem a partir de componentes que apenas interagem com seu cluster.
<p>O cloud-controller-manager executa apenas controladores que são específicos para seu provedor de nuvem.
Se você estiver executando o Kubernetes em suas próprias instalações ou em um ambiente de aprendizagem dentro de seu
próprio PC, o cluster não possui um gerenciador de controlador de nuvem.</p>
<p>Tal como acontece com o kube-controller-manager, o cloud-controller-manager combina vários ciclos de controle logicamente independentes em um binário único que você executa como um processo único. Você pode escalar horizontalmente (executar mais de uma cópia) para melhorar o desempenho ou para auxiliar na tolerância a falhas.</p>
<p>Os seguintes controladores podem ter dependências de provedor de nuvem:</p>
<ul>
<li>Controlador de nó: para verificar junto ao provedor de nuvem para determinar se um nó foi excluído da nuvem após parar de responder.</li>
<li>Controlador de rota: para configurar rotas na infraestrutura de nuvem subjacente.</li>
<li>Controlador de serviço: Para criar, atualizar e excluir balanceadores de carga do provedor de nuvem.</li>
</ul>
<h2 id=node-components>Node Components</h2>
<p>Os componentes de nó são executados em todos os nós, mantendo os <em>pods</em> em execução e fornecendo o ambiente de execução do Kubernetes.</p>
<h3 id=kubelet>kubelet</h3>
<p>Um agente que é executado em cada <a class=glossary-tooltip title="Um Nó é uma máquina de trabalho no Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> no cluster. Ele garante que os <a class=glossary-tooltip title="Uma imagem executável leve e portável que contém software e todas as suas dependências." data-toggle=tooltip data-placement=top href=/docs/concepts/containers/ target=_blank aria-label=contêineres>contêineres</a> estejam sendo executados em um <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>.</p>
<p>O kubelet utiliza um conjunto de PodSpecs que são fornecidos por vários mecanismos e garante que os contêineres descritos nesses PodSpecs estejam funcionando corretamente. O kubelet não gerencia contêineres que não foram criados pelo Kubernetes.</p>
<h3 id=kube-proxy>kube-proxy</h3>
<p>kube-proxy é um <em>proxy</em> de rede executado em cada <a class=glossary-tooltip title="Um Nó é uma máquina de trabalho no Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nó>nó</a> no seu <em>cluster</em>,
implementando parte do conceito de <a class=glossary-tooltip title="Uma forma abstrata de expor uma aplicação que está executando em um conjunto de Pods como um serviço de rede." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=serviço>serviço</a> do Kubernetes.</p>
<p><a href=/docs/reference/command-line-tools-reference/kube-proxy/>kube-proxy</a>
mantém regras de rede nos nós. Estas regras de rede permitem a comunicação de rede com seus <em>pods</em> a partir de sessões de rede dentro ou fora de seu <em>cluster</em>.</p>
<p>kube-proxy usa a camada de filtragem de pacotes do sistema operacional se houver uma e estiver disponível. Caso contrário, o kube-proxy encaminha o tráfego ele mesmo.</p>
<h3 id=container-runtime>Container runtime</h3>
<p>O agente de execução (<em>runtime</em>) de contêiner é o software responsável por executar os contêineres.</p>
<p>O Kubernetes suporta diversos agentes de execução de contêineres: <a class=glossary-tooltip title="Docker is a software technology providing operating-system-level virtualization also known as containers." data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a>, <a class=glossary-tooltip title="Um agente de execução de contêiner com enfase em simplicidade, robustez e portabilidade" data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a>, <a class=glossary-tooltip title="Um agente de execução leve de contêineres criado especificamente para o Kubernetes" data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a>, e qualquer implementação do <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Kubernetes CRI (Container Runtime Interface)</a>.</p>
<h2 id=addons>Addons</h2>
<p>Complementos (<em>addons</em>) usam recursos do Kubernetes (<a class=glossary-tooltip title="Ensures a copy of a Pod is running across a set of nodes in a cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a>, <a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>, etc) para implementar funcionalidades do cluster. Como fornecem funcionalidades em nível do cluster, recursos de <em>addons</em> que necessitem ser criados dentro de um <em>namespace</em> pertencem ao <em>namespace</em> <code>kube-system</code>.</p>
<p>Alguns <em>addons</em> selecionados são descritos abaixo; para uma lista estendida dos <em>addons</em> disponíveis, por favor consulte <a href=/docs/concepts/cluster-administration/addons/>Addons</a>.</p>
<h3 id=dns>DNS</h3>
<p>Embora os outros complementos não sejam estritamente necessários, todos os clusters do Kubernetes devem ter um <a href=/docs/concepts/services-networking/dns-pod-service/>DNS do cluster</a>, já que muitos exemplos dependem disso.</p>
<p>O DNS do cluster é um servidor DNS, além de outros servidores DNS em seu ambiente, que fornece registros DNS para serviços do Kubernetes.</p>
<p>Os contêineres iniciados pelo Kubernetes incluem automaticamente esse servidor DNS em suas pesquisas DNS.</p>
<h3 id=web-ui-dashboard>Web UI (Dashboard)</h3>
<p><a href=/docs/tasks/access-application-cluster/web-ui-dashboard/>Dashboard</a> é uma interface de usuário Web, de uso geral, para clusters do Kubernetes. Ele permite que os usuários gerenciem e solucionem problemas de aplicações em execução no cluster, bem como o próprio cluster.</p>
<h3 id=monitoramento-de-recursos-do-contêiner>Monitoramento de recursos do contêiner</h3>
<p><a href=/docs/tasks/debug-application-cluster/resource-usage-monitoring/>Monitoramento de recursos do contêiner</a> registra métricas de série temporal genéricas sobre os contêineres em um banco de dados central e fornece uma interface de usuário para navegar por esses dados.</p>
<h3 id=logging-a-nivel-do-cluster>Logging a nivel do cluster</h3>
<p>Um mecanismo de <a href=/docs/concepts/cluster-administration/logging/><em>logging</em> a nível do cluster</a> é responsável por guardar os <em>logs</em> dos contêineres em um armazenamento central de <em>logs</em> com um interface para navegação/pesquisa.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Aprenda sobre <a href=/docs/concepts/architecture/nodes/>Nós</a>.</li>
<li>Aprenda sobre <a href=/docs/concepts/architecture/controller/>Controladores</a>.</li>
<li>Aprenda sobre <a href=/docs/concepts/scheduling-eviction/kube-scheduler/>kube-scheduler</a>.</li>
<li>Leia a <a href=https://etcd.io/docs/>documentação</a> oficial do <strong>etcd</strong>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-110f33530cf761140cb1dab536baef04>3.3 - Objetos do Kubernetes</h1>
</div>
<div class=td-content>
<h1 id=pg-f37749a83c2916b63279ea60f3cfe53e>3.3.1 - Nomes</h1>
<p>Cada objeto em um cluster possui um Nome que é único para aquele tipo de recurso.
Todo objeto do Kubernetes também possui um UID que é único para todo o cluster.</p>
<p>Por exemplo, você pode ter apenas um Pod chamado "myapp-1234", porém você pode ter um Pod
e um Deployment ambos com o nome "myapp-1234".</p>
<p>Para atributos não únicos providenciados por usuário, Kubernetes providencia <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> e <a href=/docs/concepts/overview/working-with-objects/annotations/>annotations</a>.</p>
<h2 id=nomes>Nomes</h2>
<p>Recursos Kubernetes podem ter nomes com até 253 caracteres. Os caracteres permitidos em nomes são: dígitos (0-9), letras minúsculas (a-z), <code>-</code>, e <code>.</code>.</p>
<p>A seguir, um exemplo para um Pod chamado <code>nginx-demo</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.7.9<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Alguns tipos de recursos possuem restrições adicionais em seus nomes.
</div>
<h2 id=uids>UIDs</h2>
<p>Kubernetes UIDs são identificadores únicos universais (também chamados de UUIDs).
UUIDs utilizam padrões ISO/IEC 9834-8 e ITU-T X.667.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia sobre <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> em Kubernetes.</li>
<li>Consulte o documento de design <a href=https://git.k8s.io/community/contributors/design-proposals/architecture/identifiers.md>Identificadores e Nomes em Kubernetes</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1127165f472b7181b9c1d5a0b187d620>3.3.2 - Namespaces</h1>
<p>No Kubernetes, <em>namespaces</em> disponibilizam um mecanismo para isolar grupos de recursos dentro de um único cluster. Nomes de recursos precisam ser únicos dentro de um namespace, porém podem se repetir em diferentes namespaces. Escopos baseados em namespaces são aplicáveis apenas para objetos com namespace <em>(como: Deployments, Services, etc)</em> e não em objetos que abrangem todo o cluster <em>(como: StorageClass, Nodes, PersistentVolumes, etc)</em>.</p>
<h2 id=quando-utilizar-múltiplos-namespaces>Quando Utilizar Múltiplos Namespaces</h2>
<p>Namespaces devem ser utilizados em ambientes com múltiplos usuários espalhados por diversos times ou projetos. Para clusters com poucos ou até algumas dezenas de usuários, você não deveria precisar criar ou pensar a respeito de namespaces. Comece a utilizar namespaces quando você precisar das funcionalidades que eles oferecem.</p>
<p>Namespaces oferecem escopo para nomes. Nomes de recursos precisam ser únicos dentro de um namespace, porém não em diferentes namespaces. Namespaces não podem ser aninhados dentro de outros namespaces e cada recurso Kubernetes pode pertencer à apenas um namespace.</p>
<p>Namespaces nos permitem dividir os recursos do cluster entre diferentes usuários (via <a href=/docs/concepts/policy/resource-quotas/>resource quota</a>).</p>
<p>Não é necessário utilizar múltiplos namespaces para separar recursos levemente diferentes, como diferentes versões de um mesmo software: use <a class=glossary-tooltip title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a> para distinguir recursos dentro de um mesmo namespace.</p>
<h2 id=trabalhando-com-namespaces>Trabalhando com Namespaces</h2>
<p>Criação e eliminação de namespaces estão descritas na
<a href=/docs/tasks/administer-cluster/namespaces>documentação de namespaces do guia de administradores</a>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Evite criar namespaces com o prefixo <code>kube-</code>, já que este prefixo é reservado para namespaces do sistema Kubernetes.
</div>
<h3 id=visualizando-namespaces>Visualizando namespaces</h3>
<p>Você pode obter uma lista dos namespaces atuais dentro de um cluster com:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get namespace
</code></pre></div><pre><code>NAME              STATUS   AGE
default           Active   1d
kube-node-lease   Active   1d
kube-public       Active   1d
kube-system       Active   1d
</code></pre><p>O Kubernetes é inicializado com quatro namespaces:</p>
<ul>
<li><code>default</code> O namespace padrão para objetos sem namespace</li>
<li><code>kube-system</code> O namespace para objetos criados pelo sistema Kubernetes</li>
<li><code>kube-public</code> Este namespace é criado automaticamente e é legível por todos os usuários (incluindo usuários não autenticados). Este namespace é reservado principalmente para uso do cluster, no caso de alguns recursos que precisem ser visíveis e legíveis publicamente por todo o cluster. O aspecto público deste namespace é apenas uma convenção, não um requisito.</li>
<li><code>kube-node-lease</code> Este namespace contém os objetos de <a href=/docs/reference/kubernetes-api/cluster-resources/lease-v1/>Lease</a> associados com cada node. Node leases permitem que o kubelet envie <a href=/docs/concepts/architecture/nodes/#heartbeats>heartbeats</a> para que a camada de gerenciamento detecte falhas nos nodes.</li>
</ul>
<h3 id=preparando-o-namespace-para-uma-requisição>Preparando o namespace para uma requisição</h3>
<p>Para preparar o namespace para a requisição atual, utilize o parâmetro <code>--namespace</code>. Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl run nginx --image<span style=color:#666>=</span>nginx --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt;
kubectl get pods --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt;
</code></pre></div><h3 id=configurando-a-preferência-de-namespaces>Configurando a preferência de namespaces</h3>
<p>Você pode salvar permanentemente o namespace para todos os comandos <code>kubectl</code> subsequentes no mesmo contexto:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config set-context --current --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt;
<span style=color:#080;font-style:italic># Validando</span>
kubectl config view --minify | grep namespace:
</code></pre></div><h2 id=namespaces-e-dns>Namespaces e DNS</h2>
<p>Quando você cria um <a href=/docs/concepts/services-networking/service/>Serviço</a>, ele cria uma
<a href=/docs/concepts/services-networking/dns-pod-service/>entrada DNS</a> correspondente.
Esta entrada possui o formato: <code>&lt;service-name>.&lt;namespace-name>.svc.cluster.local</code>, de forma que se um contêiner utilizar apenas <code>&lt;service-name></code> ele será resolvido para um serviço que é local ao namespace.
Isso é útil para utilizar a mesma configuração em vários namespaces, por exemplo em Desenvolvimento, <code>Staging</code> e Produç. Se você quiser acessar múltiplos namespaces, precisará utilizar um <em>Fully Qualified Domain Name</em> (FQDN).</p>
<h2 id=nem-todos-os-objetos-pertencem-a-algum-namespace>Nem todos os objetos pertencem a algum Namespace</h2>
<p>A maior parte dos recursos Kubernetes (como Pods, Services, controladores de replicação e outros) pertencem a algum namespace. Entretanto, recursos de namespaces não pertencem a nenhum namespace. Além deles, recursos de baixo nível, como <a href=/docs/concepts/architecture/nodes/>nodes</a> e persistentVolumes, também não pertencem a nenhum namespace.</p>
<p>Para visualizar quais recursos Kubernetes pertencem ou não a algum namespace, utilize:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># Em um namespace</span>
kubectl api-resources --namespaced<span style=color:#666>=</span><span style=color:#a2f>true</span>

<span style=color:#080;font-style:italic># Sem namespace</span>
kubectl api-resources --namespaced<span style=color:#666>=</span><span style=color:#a2f>false</span>
</code></pre></div><h2 id=rotulamento-automático>Rotulamento Automático</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes 1.21 [beta]</code>
</div>
<p>A camada de gerenciamento Kubernetes configura um <a class=glossary-tooltip title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=label>label</a> imutável <code>kubernetes.io/metadata.name</code> em todos os namespaces se a
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>NamespaceDefaultLabelName</code> estiver habilitada. O valor do label é o nome do namespace.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia sobre <a href=/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace>a criação de um novo namespace</a>.</li>
<li>Leia sobre <a href=/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace>a eliminação de um namespace</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-046c03090d47bc4b89b818dc645c3865>3.3.3 - Seletores de Campos</h1>
<p>Os <em>Seletores de Campos</em> permitem que você <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects>selecione recursos do Kubernetes</a> baseado no valor de um ou mais campos de um recurso. Seguem alguns exemplos de buscas utilizando seletores de campos:</p>
<ul>
<li><code>metadata.name=my-service</code></li>
<li><code>metadata.namespace!=default</code></li>
<li><code>status.phase=Pending</code></li>
</ul>
<p>O comando <code>kubectl</code>, mostrado a seguir, seleciona todos os Pods nos quais o valor do campo <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase><code>status.phase</code></a> é <code>Running</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods --field-selector status.phase<span style=color:#666>=</span>Running
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Seletores de campos são essencialmente <em>filtros</em> de recursos. Por padrão, nenhum seletor/filtro é aplicado, de forma que todos os recursos do tipo especificado são selecionados. Isso faz com que as seguintes pesquisas utilizando <code>kubectl</code> sejam equivalentes: <code>kubectl get pods</code> e <code>kubectl get pods --field-selector ""</code>
</div>
<h2 id=campos-suportados>Campos suportados</h2>
<p>Os campos de seleção suportados variam dependendo do tipo de recurso Kubernetes. Todos os tipos de recursos suportam os campos <code>metadata.name</code> e <code>metadata.namespace</code>. Utilizar campos não suportados produz um erro. Como por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get ingress --field-selector foo.bar<span style=color:#666>=</span>baz
</code></pre></div><pre><code>Error from server (BadRequest): Unable to find &quot;ingresses&quot; that match label selector &quot;&quot;, field selector &quot;foo.bar=baz&quot;: &quot;foo.bar&quot; is not a known field selector: only &quot;metadata.name&quot;, &quot;metadata.namespace&quot;
</code></pre><h2 id=operadores-suportados>Operadores suportados</h2>
<p>Você pode utilizar os operadores <code>=</code>, <code>==</code> e <code>!=</code> com seletores de campos (<code>=</code> e <code>==</code> significam a mesma coisa). Por exemplo, o comando <code>kubectl</code> a seguir seleciona todos os Kubernetes Services que não estão no namespace <code>default</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get services  --all-namespaces --field-selector metadata.namespace!<span style=color:#666>=</span>default
</code></pre></div><h2 id=seletores-em-cadeia>Seletores em cadeia</h2>
<p>Assim como <a href=/docs/concepts/overview/working-with-objects/labels>label</a> e outros tipos de seletores, podem ser utilizados em cadeia através de uma lista separada por vírgula. O comando <code>kubectl</code> a seguir seleciona todos os Pods nos quais <code>status.phase</code> não é igual a <code>Running</code> e <code>spec.restartPolicy</code> é igual a <code>Always</code></p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods --field-selector<span style=color:#666>=</span>status.phase!<span style=color:#666>=</span>Running,spec.restartPolicy<span style=color:#666>=</span>Always
</code></pre></div><h2 id=múltiplos-tipos-de-recursos>Múltiplos tipos de recursos</h2>
<p>Você pode utilizar seletores de campos através de múltiplos tipos de recursos. Por exemplo, o comando <code>kubectl</code> a seguir seleciona todos Statefulsets e Services que não estão presentes no namespace <code>default</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!<span style=color:#666>=</span>default
</code></pre></div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ffd12528a12882b282e1bd19e29f9e75>4 - Volumes Persistentes</h1>
<p>Esse documento descreve o estado atual dos <em>volumes persistentes</em> no Kubernetes. Sugerimos que esteja familiarizado com <a href=/docs/concepts/storage/volumes/>volumes</a>.</p>
<h2 id=introdução>Introdução</h2>
<p>O gerenciamento de armazenamento é uma questão bem diferente do gerenciamento de instâncias computacionais. O subsistema PersistentVolume provê uma API para usuários e administradores que mostra de forma detalhada de como o armazenamento é provido e como ele é consumido. Para isso, nós introduzimos duas novas APIs: PersistentVolume e PersistentVolumeClaim.</p>
<p>Um <em>PersistentVolume</em> (PV) é uma parte do armazenamento dentro do cluster que tenha sido provisionada por um administrador, ou dinamicamente utilizando <a href=/docs/concepts/storage/storage-classes/>Classes de Armazenamento</a>. Isso é um recurso dentro do cluster da mesma forma que um nó também é. PVs são plugins de volume da mesma forma que Volumes, porém eles têm um ciclo de vida independente de qualquer Pod que utilize um PV. Essa API tem por objetivo mostrar os detalhes da implementação do armazenamento, seja ele NFS, iSCSI, ou um armazenamento específico de um provedor de cloud pública.</p>
<p>Uma_PersistentVolumeClaim_ (PVC) é uma requisição para armazenamento por um usuário. É similar a um Pod. Pods utilizam recursos do nó e PVCs utilizam recursos do PV. Pods podem solicitar níveis específicos de recursos (CPU e Memória). Claims podem solicitar tamanho e modos de acesso específicos (exemplo: montagem como ReadWriteOnce, ReadOnlyMany ou ReadWriteMany, veja <a href=#modos-de-acesso>Modos de Acesso</a>).</p>
<p>Enquanto as PersistentVolumeClaims permitem que um usuário utilize recursos de armazenamento de forma limitada, é comum que usuários precisem de PersistentVolumes com diversas propriedades, como desempenho, para problemas diversos. Os administradores de cluster precisam estar aptos a oferecer uma variedade de PersistentVolumes que difiram em tamanho e modo de acesso, sem expor os usuários a detalhes de como esses volumes são implementados. Para necessidades como essas, temos o recurso de <em>StorageClass</em>.</p>
<p>Veja os <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/>exemplos de passo a passo de forma detalhada</a>.</p>
<h2 id=requisição-e-ciclo-de-vida-de-um-volume>Requisição e ciclo de vida de um volume</h2>
<p>PVs são recursos dentro um cluster. PVCs são requisições para esses recursos e também atuam como uma validação da solicitação desses recursos. O ciclo de vida da interação entre PVs e PVCs funcionam da seguinte forma:</p>
<h3 id=provisionamento>Provisionamento</h3>
<p>Existem duas formas de provisionar um PV: estaticamente ou dinamicamente.</p>
<h4 id=estático>Estático</h4>
<p>O administrador do cluster cria uma determinada quantidade de PVs. Eles possuem todos os detalhes do armazenamento os quais estão atrelados, que neste caso fica disponível para utilização por um usuário dentro do cluster. Eles estão presentes na API do Kubernetes e disponíveis para utilização.</p>
<h4 id=dinâmico>Dinâmico</h4>
<p>Quando nenhum dos PVs estáticos, que foram criados anteriormente pelo administrador, satisfazem os critérios de uma PersistentVolumeClaim enviado por um usuário, o cluster pode tentar realizar um provisionamento dinâmico para atender a essa PVC. Esse provisionamento é baseado em StorageClasses: a PVC deve solicitar uma <a href=/docs/concepts/storage/storage-classes/>classe de armazenamento</a> e o administrador deve ter previamente criado e configurado essa classe para que o provisionamento dinâmico possa ocorrer. Requisições que solicitam a classe <code>""</code> efetivamente desabilitam o provisionamento dinâmico para elas mesmas.</p>
<p>Para habilitar o provisionamento de armazenamento dinâmico baseado em classe de armazenamento, o administrador do cluster precisa habilitar o <a href=/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass>controle de admissão</a> <code>DefaultStorageClass</code> no servidor da API. Isso pode ser feito, por exemplo, garantindo que <code>DefaultStorageClass</code> esteja entre aspas simples, ordenado por uma lista de valores para a flag <code>--enable-admission-plugins</code>, componente do servidor da API. Para mais informações sobre os comandos das flags do servidor da API, consulte a documentação <a href=/docs/admin/kube-apiserver/>kube-apiserver</a>.</p>
<h3 id=binding>Binding</h3>
<p>Um usuário cria, ou em caso de um provisionamento dinâmico já ter criado, uma PersistentVolumeClaim solicitando uma quantidade específica de armazenamento e um determinado modo de acesso. Um controle de loop no master monitora por novas PVCs, encontra um PV (se possível) que satisfaça os requisitos e realiza o bind. Se o PV foi provisionado dinamicamente por uma PVC, o loop sempre vai fazer o bind desse PV com essa PVC em específico. Caso contrário, o usuário vai receber no mínimo o que ele havia solicitado, porém, o volume possa exceder em relação à solicitação inicial. Uma vez realizado esse processo, PersistentVolumeClaim sempre vai ter um bind exclusivo, sem levar em conta como o isso aconteceu. Um bind entre uma PVC e um PV é um mapeamento de um para um, utilizando o ClaimRef que é um bind bidirecional entre o PersistentVolume e o PersistentVolumeClaim.</p>
<p>As requisições permanecerão sem bind se o volume solicitado não existir. O bind ocorrerá somente se os requisitos forem atendidos exatamente da mesma forma como solicitado. Por exemplo, um bind de uma PVC de 100 GB não ocorrerá num cluster que foi provisionado com vários PVs de 50 GB. O bind ocorrerá somente no momento em que um PV de 100 GB for adicionado.</p>
<h3 id=utilização>Utilização</h3>
<p>Pods utilizam requisições como volumes. O cluster inspeciona a requisição para encontrar o volume atrelado a ela e monta esse volume para um Pod. Para volumes que suportam múltiplos modos de acesso, o usuário especifica qual o modo desejado quando utiliza essas requisições.</p>
<p>Uma vez que o usuário tem a requisição atrelada a um PV, ele pertence ao usuário pelo tempo que ele precisar. Usuários agendam Pods e acessam seus PVs requisitados através da seção <code>persistentVolumeClaim</code> no bloco <code>volumes</code> do Pod. Para mais detalhes sobre isso, veja <a href=#requisi%C3%A7%C3%B5es-como-volumes>Requisições como Volumes</a>.</p>
<h3 id=proteção-de-uso-de-um-objeto-de-armazenamento>Proteção de Uso de um Objeto de Armazenamento</h3>
<p>O propósito da funcionalidade do Objeto de Armazenamento em Proteção de Uso é garantir que as PersistentVolumeClaims (PVCs) que estejam sendo utilizadas por um Pod e PersistentVolume (PVs) que pertençam aos PVCs não sejam removidos do sistema, pois isso pode resultar numa perda de dados.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Uma PVC está sendo utilizada por um Pod quando existe um Pod que está usando essa PVC.
</div>
<p>Se um usuário deleta uma PVC que está sendo utilizada por um Pod, esta PVC não é removida imediatamente. A remoção da PVC é adiada até que a PVC não esteja mais sendo utilizado por nenhum Pod. Se um administrador deleta um PV que está atrelado a uma PVC, o PV não é removido imediatamente também. A remoção do PV é adiada até que o PV não esteja mais atrelado à PVC.</p>
<p>Note que uma PVC é protegida quando o status da PVC é <code>Terminating</code> e a lista <code>Finalizers</code> contém <code>kubernetes.io/pvc-protection</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pvc hostpath
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        &lt;none&gt;
Annotations:   volume.beta.kubernetes.io/storage-class<span style=color:#666>=</span>example-hostpath
               volume.beta.kubernetes.io/storage-provisioner<span style=color:#666>=</span>example.com/hostpath
Finalizers:    <span style=color:#666>[</span>kubernetes.io/pvc-protection<span style=color:#666>]</span>
...
</code></pre></div><p>Note que um PV é protegido quando o status da PVC é <code>Terminating</code> e a lista <code>Finalizers</code> contém <code>kubernetes.io/pv-protection</code> também:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pv task-pv-volume
Name:            task-pv-volume
Labels:          <span style=color:#b8860b>type</span><span style=color:#666>=</span><span style=color:#a2f>local</span>
Annotations:     &lt;none&gt;
Finalizers:      <span style=color:#666>[</span>kubernetes.io/pv-protection<span style=color:#666>]</span>
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath <span style=color:#666>(</span>bare host directory volume<span style=color:#666>)</span>
    Path:          /tmp/data
    HostPathType:
Events:            &lt;none&gt;
</code></pre></div><h3 id=recuperação>Recuperação</h3>
<p>Quando um usuário não precisar mais utilizar um volume, ele pode deletar a PVC pela API, que, permite a recuperação do recurso. A política de recuperação para um PersistentVolume diz ao cluster o que fazer com o volume após ele ter sido liberado da sua requisição. Atualmente, volumes podem ser Retidos, Reciclados ou Deletados.</p>
<h4 id=retenção>Retenção</h4>
<p>A política <code>Retain</code> permite a recuperação de forma manual do recurso. Quando a PersistentVolumeClaim é deletada, ela continua existindo e o volume é considerado "livre". Mas ele ainda não está disponível para outra requisição porque os dados da requisição anterior ainda permanecem no volume. Um administrador pode manualmente recuperar o volume executando os seguintes passos:</p>
<ol>
<li>Deletar o PersistentVolume. O armazenamento associado à infraestrutura externa (AWS EBS, GCE PD, Azure Disk ou Cinder volume) ainda continuará existindo após o PV ser deletado.</li>
<li>Limpar os dados de forma manual no armazenamento associado.</li>
<li>Deletar manualmente o armazenamento associado. Caso você queira utilizar o mesmo armazenamento, crie um novo PersistentVolume com esse armazenamento.</li>
</ol>
<h4 id=deletar>Deletar</h4>
<p>Para plugins de volume que suportam a política de recuperação <code>Delete</code>, a deleção vai remover o tanto o PersistentVolume do Kubernetes, quanto o armazenamento associado à infraestrutura externa, como AWS EBS, GCE PD, Azure Disk, ou Cinder volume. Volumes que foram provisionados dinamicamente herdam a <a href=#pol%C3%ADtica-de-reten%C3%A7%C3%A3o>política de retenção da sua StorageClass</a>, que por padrão é <code>Delete</code>. O administrador precisa configurar a StorageClass de acordo com as necessidades dos usuários. Caso contrário, o PV deve ser editado ou reparado após sua criação. Veja <a href=/docs/tasks/administer-cluster/change-pv-reclaim-policy/>Alterar a política de retenção de um PersistentVolume</a>.</p>
<h4 id=reciclar>Reciclar</h4>
<div class="alert alert-danger warning callout" role=alert>
<strong>Aviso:</strong> A política de retenção <code>Recycle</code> está depreciada. Ao invés disso, recomendamos a utilização de provisionamento dinâmico.
</div>
<p>Em caso do volume plugin ter suporte a essa operação, a política de retenção <code>Recycle</code> faz uma limpeza básica (<code>rm -rf /thevolume/*</code>) no volume e torna ele disponível novamente para outra requisição.</p>
<p>Contudo, um administrador pode configurar um template personalizado de um Pod reciclador utilizando a linha de comando do gerenciamento de controle do Kubernetes como descrito em <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>referência</a>.
O Pod reciclador personalizado deve conter a spec <code>volume</code> como é mostrado no exemplo abaixo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pv-recycler<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>vol<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/any/path/it/will/be/replaced<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pv-recycler<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;k8s.gcr.io/busybox&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;/bin/sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \&#34;$(ls -A /scrub)\&#34; || exit 1&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>vol<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/scrub<span style=color:#bbb>
</span></code></pre></div><p>Contudo, o caminho especificado no Pod reciclador personalizado em <code>volumes</code> é substituído pelo caminho do volume que está sendo reciclado.</p>
<h3 id=reservando-um-persistentvolume>Reservando um PersistentVolume</h3>
<p>A camada de gerenciamento pode <a href=#binding>fazer o bind de um PersistentVolumeClaims com PersistentVolumes equivalentes</a> no cluster. Contudo, se você quer que uma PVC faça um bind com um PV específico, é preciso fazer o pré-bind deles.</p>
<p>Especificando um PersistentVolume na PersistentVolumeClaim, você declara um bind entre uma PVC e um PV específico. O bind ocorrerá se o PersistentVolume existir e não estiver reservado por uma PersistentVolumeClaims através do seu campo <code>claimRef</code>.</p>
<p>O bind ocorre independentemente se algum volume atender ao critério, incluindo afinidade de nó. A camada de gerenciamento verifica se a <a href=/docs/concepts/storage/storage-classes/>classe de armazenamento</a>, modo de acesso e tamanho do armazenamento solicitado ainda são válidos.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo-pvc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># Empty string must be explicitly set otherwise default StorageClass will be set</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeName</span>:<span style=color:#bbb> </span>foo-pv<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></code></pre></div><p>Esse método não garante nenhum privilégio de bind no PersistentVolume. Para evitar que alguma outra PersistentVolumeClaims possa usar o PV que você especificar, você precisa primeiro reservar esse volume de armazenamento. Especifique sua PersistentVolumeClaim no campo <code>claimRef</code> do PV para que outras PVCs não façam bind nele.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo-pv<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>claimRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo-pvc<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></code></pre></div><p>Isso é útil se você deseja utilizar PersistentVolumes que possuem suas <code>claimPolicy</code> configuradas para <code>Retain</code>, incluindo situações onde você estiver reutilizando um PV existente.</p>
<h3 id=expandindo-requisições-de-volumes-persistentes>Expandindo Requisições de Volumes Persistentes</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>
<p>Agora, o suporte à expansão de PersistentVolumeClaims (PVCs) já é habilitado por padrão. Você pode expandir os tipos de volumes abaixo:</p>
<ul>
<li>gcePersistentDisk</li>
<li>awsElasticBlockStore</li>
<li>Cinder</li>
<li>glusterfs</li>
<li>rbd</li>
<li>Azure File</li>
<li>Azure Disk</li>
<li>Portworx</li>
<li>FlexVolumes</li>
<li><a class=glossary-tooltip title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a></li>
</ul>
<p>Você só pode expandir uma PVC se o campo da classe de armazenamento <code>allowVolumeExpansion</code> é <code>true</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>gluster-vol-default<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/glusterfs<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resturl</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;http://192.168.10.100:8080&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restusuário</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>secretNamespace</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>allowVolumeExpansion</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></code></pre></div><p>Para solicitar um volume maior para uma PVC, edite a PVC e especifique um tamanho maior. Isso irá fazer com o que volume atrelado ao respectivo PersistentVolume seja expandido. Nunca um PersistentVolume é criado para satisfazer a requisição. Ao invés disso, um volume existente é redimensionado.</p>
<h4 id=expansão-de-volume-csi>Expansão de volume CSI</h4>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code>
</div>
<p>O suporte à expansão de volumes CSI é habilitada por padrão, porém é necessário um driver CSI específico para suportar a expansão do volume. Verifique a documentação do driver CSI específico para mais informações.</p>
<h4 id=redimensionando-um-volume-que-contém-um-sistema-de-arquivo>Redimensionando um volume que contém um sistema de arquivo</h4>
<p>Só podem ser redimensionados os volumes que contém os seguintes sistemas de arquivo: XFS, Ext3 ou Ext4.</p>
<p>Quando um volume contém um sistema de arquivo, o sistema de arquivo somente é redimensionado quando um novo Pod está utilizando a PersistentVolumeClaim no modo <code>ReadWrite</code>. A expansão de sistema de arquivo é feita quando um Pod estiver inicializando ou quando um Pod estiver em execução e o respectivo sistema de arquivo tenha suporte para expansão a quente.</p>
<p>FlexVolumes permitem redimensionamento se o <code>RequiresFSResize</code> do drive é configurado como <code>true</code>. O FlexVolume pode ser redimensionado na reinicialização do Pod.</p>
<h4 id=redimensionamento-de-uma-persistentvolumeclaim-em-uso>Redimensionamento de uma PersistentVolumeClaim em uso</h4>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code>
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> A Expansão de PVCs em uso está disponível como beta desde o Kubernetes 1.15, e como alpha desde a versão 1.11. A funcionalidade <code>ExpandInUsePersistentVolumes</code> precisa ser habilitada, o que já está automático para vários clusters que possuem funcionalidades beta. Verifique a documentação <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> para mais informações.
</div>
<p>Neste caso, você não precisa deletar e recriar um Pod ou um deployment que está sendo utilizado por uma PVC existente.
Automaticamente, qualquer PVC em uso fica disponível para o Pod assim que o sistema de arquivo for expandido.
Essa funcionalidade não tem efeito em PVCs que não estão em uso por um Pod ou deployment. Você deve criar um Pod que utilize a PVC antes que a expansão seja completada.</p>
<p>Da mesma forma que outros tipos de volumes - volumes FlexVolume também podem ser expandidos quando estiverem em uso por um Pod.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Redimensionamento de FlexVolume somente é possível quando o respectivo driver suportar essa operação.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Expandir volumes do tipo EBS é uma operação que toma muito tempo. Além disso, só é possível fazer uma modificação por volume a cada 6 horas.
</div>
<h4 id=recuperação-em-caso-de-falha-na-expansão-de-volumes>Recuperação em caso de falha na expansão de volumes</h4>
<p>Se a expansão do respectivo armazenamento falhar, o administrador do cluster pode recuperar manualmente o estado da Persistent Volume Claim (PVC) e cancelar as solicitações de redimensionamento. Caso contrário, as tentativas de solicitação de redimensionamento ocorrerão de forma contínua pelo controlador sem nenhuma intervenção do administrador.</p>
<ol>
<li>Marque o PersistentVolume(PV) que estiver atrelado à PersistentVolumeClaim(PVC) com a política de recuperação <code>Retain</code>.</li>
<li>Delete a PVC. Desde que o PV tenha a política de recuperação <code>Retain</code> - nenhum dado será perdido quando a PVC for recriada.</li>
<li>Delete a entrada <code>claimRef</code> da especificação do PV para que uma PVC possa fazer bind com ele. Isso deve tornar o PV <code>Available</code>.</li>
<li>Recrie a PVC com um tamanho menor que o PV e configure o campo <code>volumeName</code> da PCV com o nome do PV. Isso deve fazer o bind de uma nova PVC a um PV existente.</li>
<li>Não esqueça de restaurar a política de recuperação do PV.</li>
</ol>
<h2 id=tipos-de-volumes-persistentes>Tipos de volumes persistentes</h2>
<p>Tipos de PersistentVolume são implementados como plugins. Atualmente o Kubernetes suporta os plugins abaixo:</p>
<ul>
<li><a href=/docs/concepts/storage/volumes/#awselasticblockstore><code>awsElasticBlockStore</code></a> - AWS Elastic Block Store (EBS)</li>
<li><a href=/docs/concepts/storage/volumes/#azuredisk><code>azureDisk</code></a> - Azure Disk</li>
<li><a href=/docs/concepts/storage/volumes/#azurefile><code>azureFile</code></a> - Azure File</li>
<li><a href=/docs/concepts/storage/volumes/#cephfs><code>cephfs</code></a> - CephFS volume</li>
<li><a href=/docs/concepts/storage/volumes/#cinder><code>cinder</code></a> - Cinder (OpenStack block storage)
(<strong>depreciado</strong>)</li>
<li><a href=/docs/concepts/storage/volumes/#csi><code>csi</code></a> - Container Storage Interface (CSI)</li>
<li><a href=/docs/concepts/storage/volumes/#fc><code>fc</code></a> - Fibre Channel (FC) storage</li>
<li><a href=/docs/concepts/storage/volumes/#flexVolume><code>flexVolume</code></a> - FlexVolume</li>
<li><a href=/docs/concepts/storage/volumes/#flocker><code>flocker</code></a> - Flocker storage</li>
<li><a href=/docs/concepts/storage/volumes/#gcepersistentdisk><code>gcePersistentDisk</code></a> - GCE Persistent Disk</li>
<li><a href=/docs/concepts/storage/volumes/#glusterfs><code>glusterfs</code></a> - Glusterfs volume</li>
<li><a href=/docs/concepts/storage/volumes/#hostpath><code>hostPath</code></a> - HostPath volume
(somente para teste de nó único; ISSO NÃO FUNCIONARÁ num cluster multi-nós; ao invés disso, considere a utilização de volume <code>local</code>.)</li>
<li><a href=/docs/concepts/storage/volumes/#iscsi><code>iscsi</code></a> - iSCSI (SCSI over IP) storage</li>
<li><a href=/docs/concepts/storage/volumes/#local><code>local</code></a> - storage local montados nos nós.</li>
<li><a href=/docs/concepts/storage/volumes/#nfs><code>nfs</code></a> - Network File System (NFS) storage</li>
<li><code>photonPersistentDisk</code> - Controlador Photon para disco persistente.
(Esse tipo de volume não funciona mais desde a removação do provedor de cloud correspondente.)</li>
<li><a href=/docs/concepts/storage/volumes/#portworxvolume><code>portworxVolume</code></a> - Volume Portworx</li>
<li><a href=/docs/concepts/storage/volumes/#quobyte><code>quobyte</code></a> - Volume Quobyte</li>
<li><a href=/docs/concepts/storage/volumes/#rbd><code>rbd</code></a> - Volume Rados Block Device (RBD)</li>
<li><a href=/docs/concepts/storage/volumes/#scaleio><code>scaleIO</code></a> - Volume ScaleIO
(<strong>depreciado</strong>)</li>
<li><a href=/docs/concepts/storage/volumes/#storageos><code>storageos</code></a> - Volume StorageOS</li>
<li><a href=/docs/concepts/storage/volumes/#vspherevolume><code>vsphereVolume</code></a> - Volume vSphere VMDK</li>
</ul>
<h2 id=volumes-persistentes>Volumes Persistentes</h2>
<p>Cada PV contém uma <code>spec</code> e um status, que é a especificação e o status do volume. O nome do PersistentVolume deve ser um <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS</a> válido.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pv0003<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>capacity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>5Gi<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>persistentVolumeReclaimPolicy</span>:<span style=color:#bbb> </span>Retain<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>mountOptions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- hard<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- nfsvers=4.1<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nfs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/tmp<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>server</span>:<span style=color:#bbb> </span><span style=color:#666>172.17.0.2</span><span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Talvez sejam necessários programas auxiliares para um determinado tipo de volume utilizar um PersistentVolume no cluster. Neste exemplo, o PersistentVolume é do tipo NFS e o programa auxiliar <em>/sbin/mount.nfs</em> é necessário para suportar a montagem dos sistemas de arquivos NFS.
</div>
<h3 id=capacidade>Capacidade</h3>
<p>Geralmente, um PV terá uma capacidade de armazenamento específica. Isso é configurado usando o atributo <code>capacity</code> do PV. Veja o <a href=https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md>Modelo de Recurso</a> do Kubernetes para entender as unidades aceitas pelo atributo <code>capacity</code>.</p>
<p>Atualmente, o tamanho do armazenamento é o único recurso que pode ser configurado ou solicitado. Os futuros atributos podem incluir IOPS, throughput, etc.</p>
<h3 id=modo-do-volume>Modo do Volume</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>
<p>O Kubernetes suporta dois <code>volumeModes</code> de PersistentVolumes: <code>Filesystem</code> e <code>Block</code>.</p>
<p><code>volumeMode</code> é um parâmetro opcional da API.
<code>Filesystem</code> é o modo padrão utilizado quando o parâmetro <code>volumeMode</code> é omitido.</p>
<p>Um volume com <code>volumeMode: Filesystem</code> é <em>montado</em> em um diretório nos Pods. Se o volume for de um dispositivo de bloco e ele estiver vazio, o Kubernetes cria o sistema de arquivo no dispositivo antes de fazer a montagem pela primeira vez.</p>
<p>Você pode configurar o valor do <code>volumeMode</code> para <code>Block</code> para utilizar um disco bruto como volume. Esse volume é apresentado num Pod como um dispositivo de bloco, sem nenhum sistema de arquivo. Esse modo é útil para prover ao Pod a forma mais rápida para acessar um volume, sem nenhuma camada de sistema de arquivo entre o Pod e o volume. Por outro lado, a aplicação que estiver rodando no Pod deverá saber como tratar um dispositivo de bloco. Veja <a href=#raw-block-volume-support>Suporte a Volume de Bloco Bruto</a> para um exemplo de como utilizar o volume como <code>volumeMode: Block</code> num Pod.</p>
<h3 id=modos-de-acesso>Modos de Acesso</h3>
<p>Um PersistentVolume pode ser montado num host das mais variadas formas suportadas pelo provedor. Como mostrado na tabela abaixo, os provedores terão diferentes capacidades e cada modo de acesso do PV são configurados nos modos específicos suportados para cada volume em particular. Por exemplo, o NFS pode suportar múltiplos clientes read/write, mas um PV NFS específico pode ser exportado no server como read-only. Cada PV recebe seu próprio modo de acesso que descreve suas capacidades específicas.</p>
<p>Os modos de acesso são:</p>
<ul>
<li>ReadWriteOnce -- o volume pode ser montado como leitura-escrita por um nó único</li>
<li>ReadOnlyMany -- o volume pode ser montado como somente-leitura por vários nós</li>
<li>ReadWriteMany -- o volume pode ser montado como leitura-escrita por vários nós</li>
</ul>
<p>Na linha de comando, os modos de acesso ficam abreviados:</p>
<ul>
<li>RWO - ReadWriteOnce</li>
<li>ROX - ReadOnlyMany</li>
<li>RWX - ReadWriteMany</li>
</ul>
<blockquote>
<p><strong>Importante!</strong> Um volume somente pode ser montado utilizando um único modo de acesso por vez, independente se ele suportar mais de um. Por exemplo, um GCEPersistentDisk pode ser montado como ReadWriteOnce por um único nó ou ReadOnlyMany por vários nós, porém não simultaneamente.</p>
</blockquote>
<table>
<thead>
<tr>
<th style=text-align:left>Plugin de Volume</th>
<th style=text-align:center>ReadWriteOnce</th>
<th style=text-align:center>ReadOnlyMany</th>
<th style=text-align:center>ReadWriteMany</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left>AWSElasticBlockStore</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>AzureFile</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
</tr>
<tr>
<td style=text-align:left>AzureDisk</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>CephFS</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
</tr>
<tr>
<td style=text-align:left>Cinder</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>CSI</td>
<td style=text-align:center>depende do driver</td>
<td style=text-align:center>depende do driver</td>
<td style=text-align:center>depende do driver</td>
</tr>
<tr>
<td style=text-align:left>FC</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>FlexVolume</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>depende do driver</td>
</tr>
<tr>
<td style=text-align:left>Flocker</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>GCEPersistentDisk</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>Glusterfs</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
</tr>
<tr>
<td style=text-align:left>HostPath</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>iSCSI</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>Quobyte</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
</tr>
<tr>
<td style=text-align:left>NFS</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
</tr>
<tr>
<td style=text-align:left>RBD</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>VsphereVolume</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
<td style=text-align:center>(funcionam quando os Pods são do tipo collocated)</td>
</tr>
<tr>
<td style=text-align:left>PortworxVolume</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
<td style=text-align:center>✓</td>
</tr>
<tr>
<td style=text-align:left>ScaleIO</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
</tr>
<tr>
<td style=text-align:left>StorageOS</td>
<td style=text-align:center>✓</td>
<td style=text-align:center>-</td>
<td style=text-align:center>-</td>
</tr>
</tbody>
</table>
<h3 id=classe>Classe</h3>
<p>Um PV pode ter uma classe, que é especificada na configuração do atributo <code>storageClassName</code> com o nome da <a href=/docs/concepts/storage/storage-classes/>StorageClass</a>. Um PV de uma classe específica só pode ser atrelado a requisições PVCs dessa mesma classe. Um PV sem <code>storageClassName</code> não possui nenhuma classe e pode ser montado somente a PVCs que não solicitem nenhuma classe em específico.</p>
<p>No passado, a notação <code>volume.beta.kubernetes.io/storage-class</code> era utilizada no lugar do atributo <code>storageClassName</code>. Essa notação ainda funciona. Contudo, ela será totalmente depreciada numa futura versão do Kubernetes.</p>
<h3 id=política-de-retenção>Política de Retenção</h3>
<p>Atualmente as políticas de retenção são:</p>
<ul>
<li>Retain -- recuperação manual</li>
<li>Recycle -- limpeza básica (<code>rm -rf /thevolume/*</code>)</li>
<li>Delete -- o volume de armazenamento associado, como AWS EBS, GCE PD, Azure Disk ou OpenStack Cinder é deletado</li>
</ul>
<p>Atualmente, somente NFS e HostPath suportam reciclagem. Volumes AWS EBS, GCE PD, Azure Disk e Cinder suportam delete.</p>
<h3 id=opções-de-montagem>Opções de Montagem</h3>
<p>Um administrador do Kubernetes pode especificar opções de montagem adicionais quando um Volume Persistente é montado num nó.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Nem todos os tipos de Volume Persistente suportam opções de montagem.
</div>
<p>Seguem os tipos de volumes que suportam opções de montagem.</p>
<ul>
<li>AWSElasticBlockStore</li>
<li>AzureDisk</li>
<li>AzureFile</li>
<li>CephFS</li>
<li>Cinder (OpenStack block storage)</li>
<li>GCEPersistentDisk</li>
<li>Glusterfs</li>
<li>NFS</li>
<li>Quobyte Volumes</li>
<li>RBD (Ceph Block Device)</li>
<li>StorageOS</li>
<li>VsphereVolume</li>
<li>iSCSI</li>
</ul>
<p>Não há validação em relação às opções de montagem. A montagem irá falhar se houver alguma opção inválida.</p>
<p>No passado, a notação <code>volume.beta.kubernetes.io/mount-options</code> era usada no lugar do atributo <code>mountOptions</code>. Essa notação ainda funciona. Contudo, ela será totalmente depreciada numa futura versão do Kubernetes.</p>
<h3 id=afinidade-de-nó>Afinidade de Nó</h3>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Para a maioria dos tipos de volume, a configuração desse campo não se faz necessária. Isso é automaticamente populado pelos seguintes volumes de bloco do tipo: <a href=/docs/concepts/storage/volumes/#awselasticblockstore>AWS EBS</a>, <a href=/docs/concepts/storage/volumes/#gcepersistentdisk>GCE PD</a> e <a href=/docs/concepts/storage/volumes/#azuredisk>Azure Disk</a>. Você precisa deixar isso configurado para volumes do tipo <a href=/docs/concepts/storage/volumes/#local>local</a>.
</div>
<p>Um PV pode especificar uma <a href=/docs/reference/generated/kubernetes-api/v1.23/#volumenodeaffinity-v1-core>afinidade de nó</a> para definir restrições em relação ao limite de nós que podem acessar esse volume. Pods que utilizam um PV serão somente reservados para nós selecionados pela afinidade de nó.</p>
<h3 id=estado>Estado</h3>
<p>Um volume sempre estará em um dos seguintes estados:</p>
<ul>
<li>Available -- um recurso que está livre e ainda não foi atrelado a nenhuma requisição</li>
<li>Bound -- um volume atrelado a uma requisição</li>
<li>Released -- a requisição foi deletada, mas o curso ainda não foi recuperado pelo cluster</li>
<li>Failed -- o volume fracassou na sua recuperação automática</li>
</ul>
<p>A CLI mostrará o nome do PV que foi atrelado à PVC</p>
<h2 id=persistentvolumeclaims>PersistentVolumeClaims</h2>
<p>Cada PVC contém uma <code>spec</code> e um status, que é a especificação e estado de uma requisição. O nome de um objeto PersistentVolumeClaim precisa ser um <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS</a> válido.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myclaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>8Gi<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>release</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;stable&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- {<span style=color:green;font-weight:700>key: environment, operator: In, values</span>:<span style=color:#bbb> </span>[dev]}<span style=color:#bbb>
</span></code></pre></div><h3 id=modos-de-acesso-1>Modos de Acesso</h3>
<p>As requisições usam as mesmas convenções que os volumes quando eles solicitam um armazenamento com um modo de acesso específico.</p>
<h3 id=modos-de-volume>Modos de Volume</h3>
<p>As requisições usam as mesmas convenções que os volumes quando eles indicam o tipo de volume, seja ele um sistema de arquivo ou dispositivo de bloco.</p>
<h3 id=recursos>Recursos</h3>
<p>Assim como Pods, as requisições podem solicitar quantidades específicas de recurso. Neste caso, a solicitação é por armazenamento. O mesmo <a href=https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md>modelo de recurso</a> vale para volumes e requisições.</p>
<h3 id=seletor>Seletor</h3>
<p>Requisições podem especifiar um <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>seletor de rótulo</a> para posteriormente filtrar um grupo de volumes. Somente os volumes que possuam rótulos que satisfaçam os critérios do seletor podem ser atrelados à requisição. O seletor pode conter dois campos:</p>
<ul>
<li><code>matchLabels</code> - o volume deve ter um rótulo com esse valor</li>
<li><code>matchExpressions</code> - uma lista de requisitos, como chave, lista de valores e operador relacionado aos valores e chaves. São operadores válidos: In, NotIn, Exists e DoesNotExist.</li>
</ul>
<p>Todos os requisitos de <code>matchLabels</code> e <code>matchExpressions</code>, são do tipo AND - todos eles juntos devem ser atendidos.</p>
<h3 id=classe-1>Classe</h3>
<p>Uma requisição pode solicitar uma classe específica através da <a href=/docs/concepts/storage/storage-classes/>StorageClass</a> utilizando o atributo <code>storageClassName</code>. Neste caso o bind ocorrerá somente com os PVs que possuírem a mesma classe do <code>storageClassName</code> dos PVCs.</p>
<p>As PVCs não precisam necessariamente solicitar uma classe. Uma PVC com sua <code>storageClassName</code> configurada como <code>""</code> sempre solicitará um PV sem classe, dessa forma ela sempre será atrelada a um PV sem classe (que não tenha nenhuma notação, ou seja, igual a <code>""</code>). Uma PVC sem <code>storageClassName</code> não é a mesma coisa e será tratada pelo cluster de forma diferente, porém isso dependerá se o <a href=/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass>puglin de admissão</a> <code>DefaultStorageClass</code> estiver habilitado.</p>
<ul>
<li>Se o plugin de admissão estiver habilitado, o administrador poderá especificar a StorageClass padrão. Todas as PVCs que não tiverem <code>storageClassName</code> podem ser atreladas somente a PVs que atendam a esse padrão. A especificação de uma StorageClass padrão é feita através da notação <code>storageclass.kubernetes.io/is-default-class</code> recebendo o valor <code>true</code> no objeto da StorageClass. Se o administrador não especificar nenhum padrão, o cluster vai tratar a criação de uma PVC como se o plugin de admissão estivesse desabilitado. Se mais de um valor padrão for especificado, o plugin de admissão proíbe a criação de todas as PVCs.</li>
<li>Se o plugin de admissão estiver desabilitado, não haverá nenhuma notação para a StorageClass padrão. Todas as PVCs que não tiverem <code>storageClassName</code> poderão ser atreladas somente aos PVs que não possuem classe. Neste caso, as PVCs que não tiverem <code>storageClassName</code> são tratadas da mesma forma como as PVCs que possuem suas <code>storageClassName</code> configuradas como <code>""</code>.</li>
</ul>
<p>Dependendo do modo de instalação, uma StorageClass padrão pode ser implantada num cluster Kubernetes durante a instalação pelo addon manager.</p>
<p>Quando uma PVC especifica um <code>selector</code> para solicitar uma StorageClass, os requisitos são do tipo AND: somente um PV com a classe solicitada e com o rótulo requisitado pode ser atrelado à PVC.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Atualmente, uma PVC que tenha <code>selector</code> não pode ter um PV dinamicamente provisionado.
</div>
<p>No passado, a notação <code>volume.beta.kubernetes.io/storage-class</code> era usada no lugar do atributo <code>storageClassName</code> Essa notação ainda funciona. Contudo, ela será totalmente depreciada numa futura versão do Kubernetes.</p>
<h2 id=requisições-como-volumes>Requisições como Volumes</h2>
<p>Os Pods podem ter acesso ao armazenamento utilizando a requisição como um volume. Para isso, a requisição tem que estar no mesmo namespace que o Pod. Ao localizar a requisição no namespace do Pod, o cluster passa o PersistentVolume para a requisição.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfrontend<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/var/www/html&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>myclaim<span style=color:#bbb>
</span></code></pre></div><h3 id=sobre-namespaces>Sobre Namespaces</h3>
<p>Os binds dos PersistentVolumes são exclusivos e, desde que as PersistentVolumeClaims são objetos do namespace, fazer a montagem das requisições com "Muitos" nós (<code>ROX</code>, <code>RWX</code>) é possível somente para um namespace.</p>
<h3 id=persistentvolumes-do-tipo-hostpath>PersistentVolumes do tipo <code>hostPath</code></h3>
<p>Um PersistentVolume do tipo <code>hostPath</code> utiliza um arquivo ou diretório no nó para emular um network-attached storage (NAS). Veja <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume>um exemplo de volume do tipo <code>hostPath</code></a>.</p>
<h2 id=suporte-a-volume-de-bloco-bruto>Suporte a Volume de Bloco Bruto</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>
<p>Os plugins de volume abaixo suportam volumes de bloco bruto, incluindo provisionamento dinâmico onde for aplicável:</p>
<ul>
<li>AWSElasticBlockStore</li>
<li>AzureDisk</li>
<li>CSI</li>
<li>FC (Fibre Channel)</li>
<li>GCEPersistentDisk</li>
<li>iSCSI</li>
<li>Local volume</li>
<li>OpenStack Cinder</li>
<li>RBD (Ceph Block Device)</li>
<li>VsphereVolume</li>
</ul>
<h3 id=persistent-volume-using-a-raw-block-volume>Utilização de PersistentVolume com Volume de Bloco Bruto</h3>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>block-pv<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>capacity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Block<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>persistentVolumeReclaimPolicy</span>:<span style=color:#bbb> </span>Retain<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fc</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetWWNs</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;50060e801049cfd1&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>lun</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></code></pre></div><h3 id=persistent-volume-claim-requesting-a-raw-block-volume>Requisição de PersistentVolumeClaim com Volume de Bloco Bruto</h3>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>block-pvc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Block<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></code></pre></div><h3 id=especificação-de-pod-com-dispositivo-de-bloco-bruto-no-contêiner>Especificação de Pod com Dispositivo de Bloco Bruto no contêiner</h3>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod-with-block-volume<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fc-container<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>fedora:26<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;/bin/sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;tail -f /dev/null&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeDevices</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>data<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>devicePath</span>:<span style=color:#bbb> </span>/dev/xvda<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>data<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>block-pvc<span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Quando adicionar um dispositivo de bloco bruto num Pod, você especifica o caminho do dispositivo no contêiner ao invés de um ponto de montagem.
</div>
<h3 id=bind-de-volumes-de-bloco>Bind de Volumes de Bloco</h3>
<p>Se um usuário solicita um volume de bloco bruto através do campo <code>volumeMode</code> na <code>spec</code> da PersistentVolumeClaim, as regras de bind agora têm uma pequena diferença em relação às versões anteriores que não consideravam esse modo como parte da <code>spec</code>.
A tabela abaixo mostra as possíveis combinações que um usuário e um administrador pode especificar para requisitar um dispositivo de bloco bruto. A tabela indica se o volume será ou não atrelado com base nas combinações:
Matriz de bind de volume para provisionamento estático de volumes:</p>
<table>
<thead>
<tr>
<th>PV volumeMode</th>
<th style=text-align:center>PVC volumeMode</th>
<th style=text-align:right>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>unspecified</td>
<td style=text-align:center>unspecified</td>
<td style=text-align:right>BIND</td>
</tr>
<tr>
<td>unspecified</td>
<td style=text-align:center>Block</td>
<td style=text-align:right>NO BIND</td>
</tr>
<tr>
<td>unspecified</td>
<td style=text-align:center>Filesystem</td>
<td style=text-align:right>BIND</td>
</tr>
<tr>
<td>Block</td>
<td style=text-align:center>unspecified</td>
<td style=text-align:right>NO BIND</td>
</tr>
<tr>
<td>Block</td>
<td style=text-align:center>Block</td>
<td style=text-align:right>BIND</td>
</tr>
<tr>
<td>Block</td>
<td style=text-align:center>Filesystem</td>
<td style=text-align:right>NO BIND</td>
</tr>
<tr>
<td>Filesystem</td>
<td style=text-align:center>Filesystem</td>
<td style=text-align:right>BIND</td>
</tr>
<tr>
<td>Filesystem</td>
<td style=text-align:center>Block</td>
<td style=text-align:right>NO BIND</td>
</tr>
<tr>
<td>Filesystem</td>
<td style=text-align:center>unspecified</td>
<td style=text-align:right>BIND</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> O provisionamento estático de volumes é suportado somente na versão alpha. Os administradores devem tomar cuidado ao considerar esses valores quando estiverem trabalhando com dispositivos de bloco bruto.
</div>
<h2 id=snapshot-de-volume-e-restauração-de-volume-a-partir-de-um-snapshot>Snapshot de Volume e Restauração de Volume a partir de um Snapshot</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>
<p>O snapshot de volume é suportado somente pelo plugin de volume CSI. Veja <a href=/docs/concepts/storage/volume-snapshots/>Snapshot de Volume</a> para mais detalhes.
Plugins de volume in-tree estão depreciados. Você pode consultar sobre os plugins de volume depreciados em <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md>Perguntas Frequentes sobre Plugins de Volume</a>.</p>
<h3 id=create-persistent-volume-claim-from-volume-snapshot>Criar uma PersistentVolumeClaim a partir de um Snapshot de Volume</h3>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>restore-pvc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>csi-hostpath-sc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSource</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></code></pre></div><h2 id=clonagem-de-volume>Clonagem de Volume</h2>
<p>A <a href=/docs/concepts/storage/volume-pvc-datasource/>Clonagem de Volume</a> é possível somente com plugins de volume CSI.</p>
<h3 id=create-persistent-volume-claim-from-an-existing-pvc>Criação de PersistentVolumeClaim a partir de uma PVC já existente</h3>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloned-pvc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>my-csi-plugin<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSource</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>existing-src-pvc-name<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></code></pre></div><h2 id=boas-práticas-de-configuração>Boas Práticas de Configuração</h2>
<p>Se você está criando templates ou exemplos que rodam numa grande quantidade de clusters e que precisam de armazenamento persistente, recomendamos que utilize o padrão abaixo:</p>
<ul>
<li>
<p>Inclua objetos PersistentVolumeClaim em seu pacote de configuração (com Deployments, ConfigMaps, etc.).</p>
</li>
<li>
<p>Não inclua objetos PersistentVolume na configuração, pois o usuário que irá instanciar a configuração talvez não tenha permissão para criar PersistentVolume.</p>
</li>
<li>
<p>Dê ao usuário a opção dele informar o nome de uma classe de armazenamento quando instanciar o template.</p>
<ul>
<li>Se o usuário informar o nome de uma classe de armazenamento, coloque esse valor no campo <code>persistentVolumeClaim.storageClassName</code>. Isso fará com que a PVC encontre a classe de armazenamento correta se o cluster tiver a StorageClasses habilitado pelo administrador.</li>
<li>Se o usuário não informar o nome da classe de armazenamento, deixe o campo <code>persistentVolumeClaim.storageClassName</code> sem nenhum valor (vazio). Isso fará com que o PV seja provisionado automaticamente no cluster para o usuário com o StorageClass padrão. Muitos ambientes de cluster já possuem uma StorageClass padrão, ou então os administradores podem criar suas StorageClass de acordo com seus critérios.</li>
</ul>
</li>
<li>
<p>Durante suas tarefas de administração, busque por PVCs que após um tempo não estão sendo atreladas, pois, isso talvez indique que o cluster não tem provisionamento dinâmico (onde o usuário deveria criar um PV que satisfaça os critérios da PVC) ou cluster não tem um sistema de armazenamento (onde usuário não pode realizar um deploy solicitando PVCs).</p>
<h2 id=próximos-passos>Próximos passos</h2>
</li>
</ul>
<ul>
<li>Saiba mais sobre <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume>Criando um PersistentVolume</a>.</li>
<li>Saiba mais sobre <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim>Criando um PersistentVolumeClaim</a>.</li>
<li>Leia a <a href=https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md>documentação sobre planejamento de Armazenamento Persistente</a>.</li>
</ul>
<h3 id=referência>Referência</h3>
<ul>
<li><a href=/docs/reference/generated/kubernetes-api/v1.23/#persistentvolume-v1-core>PersistentVolume</a></li>
<li><a href=/docs/reference/generated/kubernetes-api/v1.23/#persistentvolumespec-v1-core>PersistentVolumeSpec</a></li>
<li><a href=/docs/reference/generated/kubernetes-api/v1.23/#persistentvolumeclaim-v1-core>PersistentVolumeClaim</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2bf36ccd6b3dbeafecf87c39761b07c7>5 - Arquitetura do Kubernetes</h1>
</div>
<div class=td-content>
<h1 id=pg-c0251def6da29b30afebfb04549f1703>5.1 - Comunicação entre Nó e Control Plane</h1>
<p>Este documento cataloga os caminhos de comunicação entre o control plane (o
apiserver) e o cluster Kubernetes. A intenção é permitir que os usuários
personalizem sua instalação para proteger a configuração de rede
então o cluster pode ser executado em uma rede não confiável (ou em IPs totalmente públicos em um
provedor de nuvem).</p>
<h2 id=nó-para-o-control-plane>Nó para o Control Plane</h2>
<p>Todos os caminhos de comunicação do cluster para o control plane terminam no
apiserver (nenhum dos outros componentes do control plane são projetados para expor
Serviços remotos). Em uma implantação típica, o apiserver é configurado para escutar
conexões remotas em uma porta HTTPS segura (443) com uma ou mais clientes <a href=/docs/reference/access-authn-authz/authentication/>autenticação</a> habilitado.
Uma ou mais formas de <a href=/docs/reference/access-authn-authz/authorization/>autorização</a>
deve ser habilitado, especialmente se <a href=/docs/reference/access-authn-authz/authentication/#anonymous-requests>requisições anônimas</a>
ou <a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>tokens da conta de serviço</a>
são autorizados.</p>
<p>Os nós devem ser provisionados com o certificado root público para o cluster
de tal forma que eles podem se conectar de forma segura ao apiserver junto com o cliente válido
credenciais. Por exemplo, em uma implantação padrão do GKE, as credenciais do cliente
fornecidos para o kubelet estão na forma de um certificado de cliente. Vejo
<a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>bootstrapping TLS do kubelet</a>
para provisionamento automatizado de certificados de cliente kubelet.</p>
<p>Os pods que desejam se conectar ao apiserver podem fazê-lo com segurança, aproveitando
conta de serviço para que o Kubernetes injetará automaticamente o certificado raiz público
certificado e um token de portador válido no pod quando ele é instanciado.
O serviço <code>kubernetes</code> (no namespace <code>default</code>) é configurado com um IP virtual
endereço que é redirecionado (via kube-proxy) para o endpoint com HTTPS no
apiserver.</p>
<p>Os componentes do control plane também se comunicam com o apiserver do cluster através da porta segura.</p>
<p>Como resultado, o modo de operação padrão para conexões do cluster
(nodes e pods em execução nos Nodes) para o control plane é protegido por padrão
e pode passar por redes não confiáveis ​​e/ou públicas.</p>
<h2 id=control-plane-para-o-nó>Control Plane para o nó</h2>
<p>Existem dois caminhos de comunicação primários do control plane (apiserver) para os nós.
O primeiro é do apiserver para o processo do kubelet que é executado em
cada nó no cluster. O segundo é do apiserver para qualquer nó, pod,
ou serviço através da funcionalidade de proxy do apiserver.</p>
<h3 id=apiserver-para-o-kubelet>apiserver para o kubelet</h3>
<p>As conexões do apiserver ao kubelet são usadas para:</p>
<ul>
<li>Buscar logs para pods.</li>
<li>Anexar (através de kubectl) pods em execução.</li>
<li>Fornecer a funcionalidade de encaminhamento de porta do kubelet.</li>
</ul>
<p>Essas conexões terminam no endpoint HTTPS do kubelet. Por padrão,
o apiserver não verifica o certificado de serviço do kubelet,
o que torna a conexão sujeita a ataques man-in-the-middle, o que o torna
<strong>inseguro</strong> para passar por redes não confiáveis ​​e / ou públicas.</p>
<p>Para verificar essa conexão, use a flag <code>--kubelet-certificate-authority</code> para
fornecer o apiserver com um pacote de certificado raiz para usar e verificar o
certificado de serviço da kubelet.</p>
<p>Se isso não for possível, use o <a href=/docs/concepts/architecture/master-node-communication/#ssh-tunnels>SSH túnel</a>
entre o apiserver e kubelet se necessário para evitar a conexão ao longo de um
rede não confiável ou pública.</p>
<p>Finalmente, <a href=/docs/admin/kubelet-authentication-authorization/>Autenticação e/ou autorização do Kubelet</a>
deve ser ativado para proteger a API do kubelet.</p>
<h3 id=apiserver-para-nós-pods-e-serviços>apiserver para nós, pods e serviços</h3>
<p>As conexões a partir do apiserver para um nó, pod ou serviço padrão para simples
conexões HTTP não são autenticadas nem criptografadas. Eles
podem ser executados em uma conexão HTTPS segura prefixando <code>https:</code> no nó,
pod, ou nome do serviço no URL da API, mas eles não validarão o certificado
fornecido pelo ponto de extremidade HTTPS, nem fornece credenciais de cliente, enquanto
a conexão será criptografada, não fornecerá nenhuma garantia de integridade.
Estas conexões <strong>não são atualmente seguras</strong> para serem usados por redes não confiáveis ​​e/ou públicas.</p>
<h3 id=ssh-túnel>SSH Túnel</h3>
<p>O Kubernetes suporta túneis SSH para proteger os caminhos de comunicação do control plane para os nós. Nesta configuração, o apiserver inicia um túnel SSH para cada nó
no cluster (conectando ao servidor ssh escutando na porta 22) e passa
todo o tráfego destinado a um kubelet, nó, pod ou serviço através do túnel.
Este túnel garante que o tráfego não seja exposto fora da rede aos quais
os nós estão sendo executados.</p>
<p>Atualmente, os túneis SSH estão obsoletos, portanto, você não deve optar por usá-los, a menos que saiba o que está fazendo. O serviço Konnectivity é um substituto para este canal de comunicação.</p>
<h3 id=konnectivity-service>Konnectivity service</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>
<p>Como uma substituição aos túneis SSH, o serviço Konnectivity fornece proxy de nível TCP para a comunicação do control plane para o cluster. O serviço Konnectivity consiste em duas partes: o servidor Konnectivity na rede control plane e os agentes Konnectivity na rede dos nós. Os agentes Konnectivity iniciam conexões com o servidor Konnectivity e mantêm as conexões de rede. Depois de habilitar o serviço Konnectivity, todo o tráfego do control plane para os nós passa por essas conexões.</p>
<p>Veja a <a href=docs/tasks/extend-kubernetes/setup-konnectivity/>tarefa do Konnectivity</a> para configurar o serviço Konnectivity no seu cluster.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-bc804b02614d67025b4c788f1ca87fbc>5.2 - Conceitos sobre Cloud Controller Manager</h1>
<p>O conceito do Cloud Controller Manager (CCM) (não confundir com o binário) foi originalmente criado para permitir que o código específico de provedor de nuvem e o núcleo do Kubernetes evoluíssem independentemente um do outro. O Cloud Controller Manager é executado junto com outros componentes principais, como o Kubernetes controller manager, o servidor de API e o scheduler. Também pode ser iniciado como um addon do Kubernetes, caso em que é executado em cima do Kubernetes.</p>
<p>O design do Cloud Controller Manager é baseado em um mecanismo de plug-in que permite que novos provedores de nuvem se integrem facilmente ao Kubernetes usando plug-ins. Existem planos para integrar novos provedores de nuvem no Kubernetes e para migrar provedores de nuvem que estão utilizando o modelo antigo para o novo modelo de CCM.</p>
<p>Este documento discute os conceitos por trás do Cloud Controller Manager e fornece detalhes sobre suas funções associadas.</p>
<p>Aqui está a arquitetura de um cluster Kubernetes sem o Cloud Controller Manager:</p>
<p><img src=/images/docs/pre-ccm-arch.png alt="Pre CCM Kube Arch"></p>
<h2 id=projeto-de-arquitetura-design>Projeto de Arquitetura (Design)</h2>
<p>No diagrama anterior, o Kubernetes e o provedor de nuvem são integrados através de vários componentes diferentes:</p>
<ul>
<li>Kubelet</li>
<li>Kubernetes controller manager</li>
<li>Kubernetes API server</li>
</ul>
<p>O CCM consolida toda a lógica que depende da nuvem dos três componentes anteriores para criar um único ponto de integração com a nuvem. A nova arquitetura com o CCM se parece com isso:</p>
<p><img src=/images/docs/post-ccm-arch.png alt="CCM Kube Arch"></p>
<h2 id=componentes-do-ccm>Componentes do CCM</h2>
<p>O CCM separa algumas das funcionalidades do KCM (Kubernetes Controller Manager) e o executa como um processo separado. Especificamente, isso elimina os controladores no KCM que dependem da nuvem. O KCM tem os seguintes loops de controlador dependentes de nuvem:</p>
<ul>
<li>Node controller</li>
<li>Volume controller</li>
<li>Route controller</li>
<li>Service controller</li>
</ul>
<p>Na versão 1.9, o CCM executa os seguintes controladores da lista anterior:</p>
<ul>
<li>Node controller</li>
<li>Route controller</li>
<li>Service controller</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> O Volume Controller foi deliberadamente escolhido para não fazer parte do CCM. Devido à complexidade envolvida e devido aos esforços existentes para abstrair a lógica de volume específica do fornecedor, foi decidido que o Volume Controller não será movido para o CCM.
</div>
<p>O plano original para suportar volumes usando o CCM era usar volumes Flex para suportar volumes plugáveis. No entanto, um esforço concorrente conhecido como CSI está sendo planejado para substituir o Flex.</p>
<p>Considerando essas dinâmicas, decidimos ter uma medida de intervalo intermediário até que o CSI esteja pronto.</p>
<h2 id=funções-do-ccm>Funções do CCM</h2>
<p>O CCM herda suas funções de componentes do Kubernetes que são dependentes de um provedor de nuvem. Esta seção é estruturada com base nesses componentes.</p>
<h3 id=1-kubernetes-controller-manager>1. Kubernetes Controller Manager</h3>
<p>A maioria das funções do CCM é derivada do KCM. Conforme mencionado na seção anterior, o CCM executa os seguintes ciclos de controle:</p>
<ul>
<li>Node Controller</li>
<li>Route Controller</li>
<li>Service Controller</li>
</ul>
<h4 id=node-controller>Node Controller</h4>
<p>O Node Controller é responsável por inicializar um nó obtendo informações sobre os nós em execução no cluster do provedor de nuvem. O Node Controller executa as seguintes funções:</p>
<ol>
<li>Inicializar um node com labels de região/zona específicos para a nuvem.</li>
<li>Inicialize um node com detalhes de instância específicos da nuvem, por exemplo, tipo e tamanho.</li>
<li>Obtenha os endereços de rede e o nome do host do node.</li>
<li>No caso de um node não responder, verifique a nuvem para ver se o node foi excluído da nuvem.
Se o node foi excluído da nuvem, exclua o objeto Node do Kubernetes.</li>
</ol>
<h4 id=route-controller>Route Controller</h4>
<p>O Route Controller é responsável por configurar as rotas na nuvem apropriadamente, de modo que os contêineres em diferentes nodes no cluster do Kubernetes possam se comunicar entre si. O Route Controller é aplicável apenas para clusters do Google Compute Engine.</p>
<h4 id=service-controller>Service controller</h4>
<p>O Service controller é responsável por ouvir os eventos de criação, atualização e exclusão do serviço. Com base no estado atual dos serviços no Kubernetes, ele configura os balanceadores de carga da nuvem (como o ELB, o Google LB ou o Oracle Cloud Infrastrucutre LB) para refletir o estado dos serviços no Kubernetes. Além disso, garante que os back-ends de serviço para balanceadores de carga da nuvem estejam atualizados.</p>
<h3 id=2-kubelet>2. Kubelet</h3>
<p>O Node Controller contém a funcionalidade dependente da nuvem do kubelet. Antes da introdução do CCM, o kubelet era responsável por inicializar um nó com detalhes específicos da nuvem, como endereços IP, rótulos de região / zona e informações de tipo de instância. A introdução do CCM mudou esta operação de inicialização do kubelet para o CCM.</p>
<p>Nesse novo modelo, o kubelet inicializa um nó sem informações específicas da nuvem. No entanto, ele adiciona uma marca (taint) ao nó recém-criado que torna o nó não programável até que o CCM inicialize o nó com informações específicas da nuvem. Em seguida, remove essa mancha (taint).</p>
<h2 id=mecanismo-de-plugins>Mecanismo de plugins</h2>
<p>O Cloud Controller Manager usa interfaces Go para permitir implementações de qualquer nuvem a ser conectada. Especificamente, ele usa a Interface CloudProvider definida<a href=https://github.com/kubernetes/cloud-provider/blob/9b77dc1c384685cb732b3025ed5689dd597a5971/cloud.go#L42-L62>aqui</a>.</p>
<p>A implementação dos quatro controladores compartilhados destacados acima, e algumas estruturas que ficam junto com a interface compartilhada do provedor de nuvem, permanecerão no núcleo do Kubernetes. Implementações específicas para provedores de nuvem serão construídas fora do núcleo e implementarão interfaces definidas no núcleo.</p>
<p>Para obter mais informações sobre o desenvolvimento de plug-ins, consulte<a href=/docs/tasks/administer-cluster/developing-cloud-controller-manager/>Desenvolvendo o Cloud Controller Manager</a>.</p>
<h2 id=autorização>Autorização</h2>
<p>Esta seção divide o acesso necessário em vários objetos da API pelo CCM para executar suas operações.</p>
<h3 id=node-controller-1>Node Controller</h3>
<p>O Node Controller só funciona com objetos Node. Ele requer acesso total para obter, listar, criar, atualizar, corrigir, assistir e excluir objetos Node.</p>
<p>v1/Node:</p>
<ul>
<li>Get</li>
<li>List</li>
<li>Create</li>
<li>Update</li>
<li>Patch</li>
<li>Watch</li>
<li>Delete</li>
</ul>
<h3 id=rote-controller>Rote Controller</h3>
<p>O Rote Controller escuta a criação do objeto Node e configura as rotas apropriadamente. Isso requer acesso a objetos Node.</p>
<p>v1/Node:</p>
<ul>
<li>Get</li>
</ul>
<h3 id=service-controller-1>Service Controller</h3>
<p>O Service Controller escuta eventos de criação, atualização e exclusão de objeto de serviço e, em seguida, configura pontos de extremidade para esses serviços de forma apropriada.</p>
<p>Para acessar os Serviços, é necessário listar e monitorar o acesso. Para atualizar os Serviços, ele requer patch e atualização de acesso.</p>
<p>Para configurar endpoints para os Serviços, é necessário acesso para criar, listar, obter, assistir e atualizar.</p>
<p>v1/Service:</p>
<ul>
<li>List</li>
<li>Get</li>
<li>Watch</li>
<li>Patch</li>
<li>Update</li>
</ul>
<h3 id=outros>Outros</h3>
<p>A implementação do núcleo do CCM requer acesso para criar eventos e, para garantir a operação segura, requer acesso para criar ServiceAccounts.</p>
<p>v1/Event:</p>
<ul>
<li>Create</li>
<li>Patch</li>
<li>Update</li>
</ul>
<p>v1/ServiceAccount:</p>
<ul>
<li>Create</li>
</ul>
<p>O RBAC ClusterRole para o CCM se parece com isso:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- events<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#39;*&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- nodes/status<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- services<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- serviceaccounts<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- persistentvolumes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- endpoints<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></code></pre></div><h2 id=implementações-de-provedores-de-nuvem>Implementações de Provedores de Nuvem</h2>
<p>Os seguintes provedores de nuvem implementaram CCMs:</p>
<ul>
<li><a href=https://github.com/digitalocean/digitalocean-cloud-controller-manager>Digital Ocean</a></li>
<li><a href=https://github.com/oracle/oci-cloud-controller-manager>Oracle</a></li>
<li><a href=https://github.com/kubernetes/cloud-provider-azure>Azure</a></li>
<li><a href=https://github.com/kubernetes/cloud-provider-gcp>GCP</a></li>
<li><a href=https://github.com/kubernetes/cloud-provider-aws>AWS</a></li>
<li><a href=https://github.com/baidu/cloud-provider-baiducloud>BaiduCloud</a></li>
<li><a href=https://github.com/linode/linode-cloud-controller-manager>Linode</a></li>
</ul>
<h2 id=administração-de-cluster>Administração de Cluster</h2>
<p>Voce vai encontrar instruções completas para configurar e executar o CCM
<a href=/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager>aqui</a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ca8819042a505291540e831283da66df>5.3 - Controladores</h1>
<p>Em robótica e automação um <em>control loop</em>, ou em português <em>ciclo de controle</em>, é
um ciclo não terminado que regula o estado de um sistema.</p>
<p>Um exemplo de ciclo de controle é um termostato de uma sala.</p>
<p>Quando você define a temperatura, isso indica ao termostato
sobre o seu <em>estado desejado</em>. A temperatura ambiente real é o
<em>estado atual</em>. O termostato atua de forma a trazer o estado atual
mais perto do estado desejado, ligando ou desligando o equipamento.</p>
No Kubernetes, controladores são ciclos de controle que observam o estado do seu
<a class=glossary-tooltip title="Um conjunto de servidores de processamento, também chamados de nós, que executam aplicações containerizadas. Todo cluster possui ao menos um servidor de processamento (worker node)." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-cluster" target=_blank aria-label=cluster>cluster</a>, e então fazer ou requisitar
mudanças onde necessário.
Cada controlador tenta mover o estado atual do cluster mais perto do estado desejado.
<h2 id=padrão-controlador-controller-pattern>Padrão Controlador (Controller pattern)</h2>
<p>Um controlador rastreia pelo menos um tipo de recurso Kubernetes.
Estes <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects>objetos</a>
têm um campo <em>spec</em> que representa o <em>estado desejado</em>.
O(s) controlador(es) para aquele recurso são responsáveis por trazer o <em>estado atual</em>
mais perto do <em>estado desejado</em>.</p>
<p>O controlador pode executar uma ação ele próprio, ou,
o que é mais comum, no Kubernetes, o controlador envia uma mensagem para o
<a class=glossary-tooltip title="O componente da camada de gerenciamento que serve a API do Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label="API server">API server</a> (servidor de API) que tem
efeitos colaterais úteis. Você vai ver exemplos disto abaixo.</p>
<h3 id=controlador-via-api-server>Controlador via API server</h3>
<p>O controlador <a class=glossary-tooltip title="Uma tarefa finita ou em lotes que executa até finalizar." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job target=_blank aria-label=Job>Job</a> é um exemplo de um
controlador Kubernetes embutido. Controladores embutidos gerem estados através da
interação com o <em>cluster API server</em>.</p>
<p><em>Job</em> é um recurso do Kubernetes que é executado em um
<em><a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a></em>, ou talvez vários <em>Pods</em>, com o objetivo de
executar uma tarefa e depois parar.</p>
<p>(Uma vez <a href=/docs/concepts/scheduling/>agendado</a>, objetos <em>Pod</em> passam a fazer parte
do <em>estado desejado</em> para um kubelet.</p>
<p>Quando o controlador <em>Job</em> observa uma nova tarefa ele garante que,
algures no seu <em>cluster</em>, os kubelets num conjunto de nós (<em>Nodes</em>) estão correndo o número
correto de <em>Pods</em> para completar o trabalho.
O controlador <em>Job</em> não corre <em>Pods</em> ou <em>containers</em> ele próprio.
Em vez disso, o controlador <em>Job</em> informa o <em>API server</em> para criar ou remover <em>Pods</em>.
Outros componentes do plano de controle
(<a class=glossary-tooltip title="A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a>)
atuam na nova informação (existem novos <em>Pods</em> para serem agendados e executados),
e eventualmente o trabalho é feito.</p>
<p>Após ter criado um novo <em>Job</em>, o <em>estado desejado</em> é que esse Job seja completado.
O controlador <em>Job</em> faz com que o <em>estado atual</em> para esse <em>Job</em> esteja mais perto do seu
<em>estado desejado</em>: criando <em>Pods</em> que fazem o trabalho desejado para esse <em>Job</em> para que
o <em>Job</em> fique mais perto de ser completado.</p>
<p>Controladores também atualizam os objetos que os configuram.
Por exemplo: assim que o trabalho de um <em>Job</em> está completo,
o controlador <em>Job</em> atualiza esse objeto <em>Job</em> para o marcar como <code>Finished</code> (terminado).</p>
<p>(Isto é um pouco como alguns termostatos desligam uma luz para
indicar que a temperatura da sala está agora na temperatura que foi introduzida).</p>
<h3 id=controle-direto>Controle direto</h3>
<p>Em contraste com <em>Job</em>, alguns controladores necessitam de efetuar
mudanças fora do <em>cluster</em>.</p>
<p>Por exemplo, se usar um ciclo de controle para garantir que existem
<em><a class=glossary-tooltip title="Um Nó é uma máquina de trabalho no Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a></em> suficientes
no seu <em>cluster</em>, então esse controlador necessita de algo exterior ao
<em>cluster</em> atual para configurar novos <em>Nodes</em> quando necessário.</p>
<p>Controladores que interagem com estados externos encontram o seu estado desejado
a partir do <em>API server</em>, e então comunicam diretamente com o sistema externo para
trazer o <em>estado atual</em> mais próximo do desejado.</p>
<p>(Existe um controlador que escala horizontalmente nós no seu <em>cluster</em>.
Veja <a href=/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaling>Escalamento automático do cluster</a>)</p>
<h2 id=desired-vs-current>Estado desejado versus atual</h2>
<p>Kubernetes tem uma visão <em>cloud-native</em> de sistemas e é capaz de manipular
mudanças constantes.</p>
<p>O seu <em>cluster</em> pode mudar em qualquer momento à medida que as ações acontecem e
os ciclos de controle corrigem falhas automaticamente. Isto significa que,
potencialmente, o seu <em>cluster</em> nunca atinge um estado estável.</p>
<p>Enquanto os controladores no seu <em>cluster</em> estiverem rodando e forem capazes de
fazer alterações úteis, não importa se o estado é estável ou se é instável.</p>
<h2 id=design>Design</h2>
<p>Como um princípio do seu desenho, o Kubernetes usa muitos controladores onde cada
um gerencia um aspecto particular do estado do <em>cluster</em>. Comumente, um particular
ciclo de controle (controlador) usa uma espécie de recurso como o seu <em>estado desejado</em>,
e tem uma espécie diferente de recurso que o mesmo gere para garantir que esse <em>estado desejado</em>
é cumprido.</p>
<p>É útil que haja controladores simples em vez de um conjunto monolítico de ciclos de controle
que estão interligados. Controladores podem falhar, então o Kubernetes foi desenhado para
permitir isso.</p>
<p>Por exemplo: um controlador de <em>Jobs</em> rastreia objetos <em>Job</em> (para
descobrir novos trabalhos) e objetos <em>Pod</em> (para correr o <em>Jobs</em>, e então
ver quando o trabalho termina). Neste caso outra coisa cria os <em>Jobs</em>,
enquanto o controlador <em>Job</em> cria <em>Pods</em>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> <p>Podem existir vários controladores que criam ou atualizam a mesma espécie (kind) de objeto.
Atrás das cortinas, os controladores do Kubernetes garantem que eles apenas tomam
atenção aos recursos ligados aos seus recursos controladores.</p>
<p>Por exemplo, você pode ter <em>Deployments</em> e <em>Jobs</em>; ambos criam <em>Pods</em>.
O controlador de <em>Job</em> não apaga os <em>Pods</em> que o seu <em>Deployment</em> criou,
porque existe informação (<a class=glossary-tooltip title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a>)
que os controladores podem usar para diferenciar esses <em>Pods</em>.</p>
</div>
<h2 id=running-controllers>Formas de rodar controladores</h2>
<p>O Kubernetes vem com um conjunto de controladores embutidos que correm
dentro do <a class=glossary-tooltip title="Componente da camada de gerenciamento que executa os processos de controle." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>.
Estes controladores embutidos providenciam comportamentos centrais importantes.</p>
<p>O controlador <em>Deployment</em> e o controlador <em>Job</em> são exemplos de controladores
que veem como parte do próprio Kubernetes (controladores "embutidos").
O Kubernetes deixa você correr o plano de controle resiliente, para que se qualquer
um dos controladores embutidos falhar, outra parte do plano de controle assume
o trabalho.</p>
<p>Pode encontrar controladores fora do plano de controle, para extender o Kubernetes.
Ou, se quiser, pode escrever um novo controlador você mesmo.
Pode correr o seu próprio controlador como um conjunto de <em>Pods</em>,
ou externo ao Kubernetes. O que encaixa melhor vai depender no que esse
controlador faz em particular.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia mais sobre o <a href=/docs/concepts/#kubernetes-control-plane>plano de controle do Kubernetes</a></li>
<li>Descubra alguns dos <a href=/docs/concepts/#kubernetes-objects>objetos Kubernetes</a> básicos.</li>
<li>Aprenda mais sobre <a href=/docs/concepts/overview/kubernetes-api/>API do Kubernetes</a></li>
<li>Se pretender escrever o seu próprio controlador, veja <a href=/docs/concepts/extend-kubernetes/extend-cluster/#extension-patterns>Padrões de Extensão</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-a5f7383c83ab9eb9cd0e3c4c020b3ae6>6 - Contêineres</h1>
<div class=lead>Tecnologia para empacotar aplicações com suas dependências em tempo de execução</div>
<p>Cada contêiner executado é repetível; a padronização de ter
dependências incluídas significa que você obtém o mesmo comportamento onde quer que você execute.</p>
<p>Os contêineres separam os aplicativos da infraestrutura de <em>host</em> subjacente.
Isso torna a implantação mais fácil em diferentes ambientes de nuvem ou sistema operacional.</p>
<h2 id=imagem-de-contêiner>Imagem de contêiner</h2>
<p>Uma <a href=/docs/concepts/containers/images/>imagem de contêiner</a> é um pacote de software pronto para executar, contendo tudo que é preciso para executar uma aplicação:
o código e o agente de execução necessário, aplicação, bibliotecas do sistema e valores padrões para qualquer configuração essencial.</p>
<p>Por <em>design</em>, um contêiner é imutável: você não pode mudar o código de um contêiner que já está executando. Se você tem uma aplicação conteinerizada e quer fazer mudanças, você precisa construir uma nova imagem que inclui a mudança, e recriar o contêiner para iniciar a partir da imagem atualizada.</p>
<h2 id=agente-de-execução-de-contêiner>Agente de execução de contêiner</h2>
<p>O agente de execução (<em>runtime</em>) de contêiner é o software responsável por executar os contêineres.</p>
<p>O Kubernetes suporta diversos agentes de execução de contêineres: <a class=glossary-tooltip title="Docker is a software technology providing operating-system-level virtualization also known as containers." data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a>, <a class=glossary-tooltip title="Um agente de execução de contêiner com enfase em simplicidade, robustez e portabilidade" data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a>, <a class=glossary-tooltip title="Um agente de execução leve de contêineres criado especificamente para o Kubernetes" data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a>, e qualquer implementação do <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Kubernetes CRI (Container Runtime Interface)</a>.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li><a href=/docs/concepts/containers/images/>Imagens de contêineres</a></li>
<li><a href=/docs/concepts/workloads/pods/>Pods</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-16042b4652ad19e565c7263824029a43>6.1 - Imagens</h1>
<p>Uma imagem de contêiner representa dados binários que encapsulam uma aplicação e todas as suas dependências de software. As imagens de contêiner são pacotes de software executáveis que podem ser executados de forma autônoma e que fazem suposições muito bem definidas sobre seu agente de execução do ambiente.</p>
<p>Normalmente, você cria uma imagem de contêiner da sua aplicação e a envia para um registro antes de fazer referência a ela em um <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a></p>
<p>Esta página fornece um resumo sobre o conceito de imagem de contêiner.</p>
<h2 id=nomes-das-imagens>Nomes das imagens</h2>
<p>As imagens de contêiner geralmente recebem um nome como <code>pause</code>, <code>exemplo/meuconteiner</code>, ou <code>kube-apiserver</code>.
As imagens também podem incluir um hostname de algum registro; por exemplo: <code>exemplo.registro.ficticio/nomeimagem</code>,
e um possível número de porta; por exemplo: <code>exemplo.registro.ficticio:10443/nomeimagem</code>.</p>
<p>Se você não especificar um hostname de registro, o Kubernetes presumirá que você se refere ao registro público do Docker.</p>
<p>Após a parte do nome da imagem, você pode adicionar uma <em>tag</em> (como também usar com comandos como <code>docker</code> e<code> podman</code>).
As tags permitem identificar diferentes versões da mesma série de imagens.</p>
<p>Tags de imagem consistem em letras maiúsculas e minúsculas, dígitos, sublinhados (<code>_</code>),
pontos (<code>.</code>) e travessões (<code> -</code>).
Existem regras adicionais sobre onde você pode colocar o separador
caracteres (<code>_</code>,<code>-</code> e <code>.</code>) dentro de uma tag de imagem.
Se você não especificar uma tag, o Kubernetes presumirá que você se refere à tag <code>latest</code> (mais recente).</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Cuidado:</strong> <p>Você deve evitar usar a tag <code>latest</code> quando estiver realizando o deploy de contêineres em produção,
pois é mais difícil rastrear qual versão da imagem está sendo executada, além de tornar mais difícil o processo de reversão para uma versão funcional.</p>
<p>Em vez disso, especifique uma tag significativa, como <code>v1.42.0</code>.</p>
</div>
<h2 id=atualizando-imagens>Atualizando imagens</h2>
<p>A política padrão de pull é <code>IfNotPresent</code> a qual faz com que o
<a class=glossary-tooltip title="Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a> ignore
o processo de <em>pull</em> da imagem, caso a mesma já exista. Se você prefere sempre forçar o processo de <em>pull</em>,
você pode seguir uma das opções abaixo:</p>
<ul>
<li>defina a <code>imagePullPolicy</code> do contêiner para<code> Always</code>.</li>
<li>omita <code>imagePullPolicy</code> e use<code>: latest</code> como a tag para a imagem a ser usada.</li>
<li>omita o <code>imagePullPolicy</code> e a tag da imagem a ser usada.</li>
<li>habilite o <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>AlwaysPullImages</a> controlador de admissão.</li>
</ul>
<p>Quando <code>imagePullPolicy</code> é definido sem um valor específico, ele também é definido como<code> Always</code>.</p>
<h2 id=multiarquitetura-de-imagens-com-índice-de-imagens>Multiarquitetura de imagens com índice de imagens</h2>
<p>Além de fornecer o binário das imagens, um registro de contêiner também pode servir um <a href=https://github.com/opencontainers/image-spec/blob/master/image-index.md>índice de imagem do contêiner</a>. Um índice de imagem pode apontar para múltiplos <a href=https://github.com/opencontainers/image-spec/blob/master/manifest.md>manifestos da imagem</a> para versões específicas de arquitetura de um contêiner. A ideia é que você possa ter um nome para uma imagem (por exemplo: <code>pause</code>, <code>exemple/meuconteiner</code>, <code>kube-apiserver</code>) e permitir que diferentes sistemas busquem o binário da imagem correta para a arquitetura de máquina que estão usando.</p>
<p>O próprio Kubernetes normalmente nomeia as imagens de contêiner com o sufixo <code>-$(ARCH)</code>. Para retrocompatibilidade, gere as imagens mais antigas com sufixos. A ideia é gerar a imagem <code>pause</code> que tem o manifesto para todas as arquiteturas e <code>pause-amd64</code> que é retrocompatível com as configurações anteriores ou arquivos YAML que podem ter codificado as imagens com sufixos.</p>
<h2 id=usando-um-registro-privado>Usando um registro privado</h2>
<p>Os registros privados podem exigir chaves para acessar as imagens deles.
As credenciais podem ser fornecidas de várias maneiras:</p>
<ul>
<li>Configurando nós para autenticação em um registro privado
<ul>
<li>todos os pods podem ler qualquer registro privado configurado</li>
<li>requer configuração de nó pelo administrador do cluster</li>
</ul>
</li>
<li>Imagens pré-obtidas
<ul>
<li>todos os pods podem usar qualquer imagem armazenada em cache em um nó</li>
<li>requer acesso root a todos os nós para configurar</li>
</ul>
</li>
<li>Especificando ImagePullSecrets em um Pod
<ul>
<li>apenas pods que fornecem chaves próprias podem acessar o registro privado</li>
</ul>
</li>
<li>Extensões locais ou específicas do fornecedor
<ul>
<li>se estiver usando uma configuração de nó personalizado, você (ou seu provedor de nuvem) pode implementar seu mecanismo para autenticar o nó ao registro do contêiner.</li>
</ul>
</li>
</ul>
<p>Essas opções são explicadas com mais detalhes abaixo.</p>
<h3 id=configurando-nós-para-autenticação-em-um-registro-privado>Configurando nós para autenticação em um registro privado</h3>
<p>Se você executar o Docker em seus nós, poderá configurar o contêiner runtime do Docker
para autenticação em um registro de contêiner privado.</p>
<p>Essa abordagem é adequada se você puder controlar a configuração do nó.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> O Kubernetes padrão é compatível apenas com as seções <code>auths</code> e<code> HttpHeaders</code> na configuração do Docker.
Auxiliares de credencial do Docker (<code>credHelpers</code> ou <code>credsStore</code>) não são suportados.
</div>
<p>Docker armazena chaves de registros privados no arquivo <code>$HOME/.dockercfg</code> ou <code>$HOME/.docker/config.json</code>. Se você colocar o mesmo arquivo na lista de caminhos de pesquisa abaixo, o kubelet o usa como provedor de credenciais ao obter imagens.</p>
<ul>
<li><code>{--root-dir:-/var/lib/kubelet}/config.json</code></li>
<li><code>{cwd of kubelet}/config.json</code></li>
<li><code>${HOME}/.docker/config.json</code></li>
<li><code>/.docker/config.json</code></li>
<li><code>{--root-dir:-/var/lib/kubelet}/.dockercfg</code></li>
<li><code>{cwd of kubelet}/.dockercfg</code></li>
<li><code>${HOME}/.dockercfg</code></li>
<li><code>/.dockercfg</code></li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Você talvez tenha que definir <code>HOME=/root</code> explicitamente no ambiente do processo kubelet.
</div>
<p>Aqui estão as etapas recomendadas para configurar seus nós para usar um registro privado. Neste
exemplo, execute-os em seu desktop/laptop:</p>
<ol>
<li>Execute <code>docker login [servidor]</code> para cada conjunto de credenciais que deseja usar. Isso atualiza o <code>$HOME/.docker/config.json</code> em seu PC.</li>
<li>Visualize <code>$HOME/.docker/config.json</code> em um editor para garantir que contém apenas as credenciais que você deseja usar.</li>
<li>Obtenha uma lista de seus nós; por exemplo:
<ul>
<li>se você quiser os nomes: <code>nodes=$( kubectl get nodes -o jsonpath='{range.items[*].metadata}{.name} {end}' )</code></li>
<li>se você deseja obter os endereços IP: <code>nodes=$( kubectl get nodes -o jsonpath='{range .items[*].status.addresses[?(@.type=="ExternalIP")]}{.address} {end}' )</code></li>
</ul>
</li>
<li>Copie seu <code>.docker/config.json</code> local para uma das listas de caminhos de busca acima.
<ul>
<li>por exemplo, para testar isso: <code>for n in $nodes; do scp ~/.docker/config.json root@"$n":/var/lib/kubelet/config.json; done</code></li>
</ul>
</li>
</ol>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Para clusters de produção, use uma ferramenta de gerenciamento de configuração para que você possa aplicar esta
configuração em todos os nós que você precisar.
</div>
<p>Verifique se está funcionando criando um pod que usa uma imagem privada; por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f - <span style=color:#b44>&lt;&lt;EOF
</span><span style=color:#b44>apiVersion: v1
</span><span style=color:#b44>kind: Pod
</span><span style=color:#b44>metadata:
</span><span style=color:#b44>  name: private-image-test-1
</span><span style=color:#b44>spec:
</span><span style=color:#b44>  containers:
</span><span style=color:#b44>    - name: uses-private-image
</span><span style=color:#b44>      image: $PRIVATE_IMAGE_NAME
</span><span style=color:#b44>      imagePullPolicy: Always
</span><span style=color:#b44>      command: [ &#34;echo&#34;, &#34;SUCCESS&#34; ]
</span><span style=color:#b44>EOF</span>
</code></pre></div><pre><code>pod/private-image-test-1 created
</code></pre><p>Se tudo estiver funcionando, então, após algum tempo, você pode executar:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs private-image-test-1
</code></pre></div><p>e veja o resultado do comando:</p>
<pre><code>SUCCESS
</code></pre><p>Se você suspeitar que o comando falhou, você pode executar:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pods/private-image-test-1 | grep <span style=color:#b44>&#39;Failed&#39;</span>
</code></pre></div><p>Em caso de falha, a saída é semelhante a:</p>
<pre><code>  Fri, 26 Jun 2015 15:36:13 -0700    Fri, 26 Jun 2015 15:39:13 -0700    19    {kubelet node-i2hq}    spec.containers{uses-private-image}    failed        Failed to pull image &quot;user/privaterepo:v1&quot;: Error: image user/privaterepo:v1 not found
</code></pre><p>Você deve garantir que todos os nós no cluster tenham o mesmo <code>.docker/config.json</code>. Caso contrário, os pods serão executados com sucesso em alguns nós e falharão em outros. Por exemplo, se você usar o escalonamento automático de nós, cada modelo de instância precisa incluir o <code>.docker/config.json</code> ou montar um drive que o contenha.</p>
<p>Todos os pods terão premissão de leitura às imagens em qualquer registro privado, uma vez que
as chaves privadas do registro são adicionadas ao <code>.docker/config.json</code>.</p>
<h3 id=imagens-pré-obtidas>Imagens pré-obtidas</h3>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Essa abordagem é adequada se você puder controlar a configuração do nó. Isto
não funcionará de forma confiável se o seu provedor de nuvem for responsável pelo gerenciamento de nós e os substituir
automaticamente.
</div>
<p>Por padrão, o kubelet tenta realizar um "pull" para cada imagem do registro especificado.
No entanto, se a propriedade <code>imagePullPolicy</code> do contêiner for definida como<code> IfNotPresent</code> ou <code>Never</code>,
em seguida, uma imagem local é usada (preferencial ou exclusivamente, respectivamente).</p>
<p>Se você quiser usar imagens pré-obtidas como um substituto para a autenticação do registro,
você deve garantir que todos os nós no cluster tenham as mesmas imagens pré-obtidas.</p>
<p>Isso pode ser usado para pré-carregar certas imagens com o intuíto de aumentar a velocidade ou como uma alternativa para autenticação em um registro privado.</p>
<p>Todos os pods terão permissão de leitura a quaisquer imagens pré-obtidas.</p>
<h3 id=especificando-imagepullsecrets-em-um-pod>Especificando imagePullSecrets em um pod</h3>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Esta é a abordagem recomendada para executar contêineres com base em imagens
de registros privados.
</div>
<p>O Kubernetes oferece suporte à especificação de chaves de registro de imagem de contêiner em um pod.</p>
<h4 id=criando-um-segredo-com-docker-config>Criando um segredo com Docker config</h4>
<p>Execute o seguinte comando, substituindo as palavras em maiúsculas com os valores apropriados:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret docker-registry &lt;name&gt; --docker-server<span style=color:#666>=</span>DOCKER_REGISTRY_SERVER --docker-username<span style=color:#666>=</span>DOCKER_USER --docker-password<span style=color:#666>=</span>DOCKER_PASSWORD --docker-email<span style=color:#666>=</span>DOCKER_EMAIL
</code></pre></div><p>Se você já tem um arquivo de credenciais do Docker, em vez de usar o
comando acima, você pode importar o arquivo de credenciais como um Kubernetes
<a class=glossary-tooltip title="Armazena dados sensíveis, como senhas, tokens OAuth e chaves SSH." data-toggle=tooltip data-placement=top href=/pt-br/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a>.
<a href=/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials>Criar um segredo com base nas credenciais Docker existentes</a> explica como configurar isso.</p>
<p>Isso é particularmente útil se você estiver usando vários registros privados de contêineres, como <code>kubectl create secret docker-registry</code> cria um Segredo que
só funciona com um único registro privado.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Os pods só podem fazer referência a <em>pull secrets</em> de imagem em seu próprio namespace,
portanto, esse processo precisa ser feito uma vez por namespace.
</div>
<h4 id=referenciando-um-imagepullsecrets-em-um-pod>Referenciando um imagePullSecrets em um pod</h4>
<p>Agora, você pode criar pods que fazem referência a esse segredo adicionando uma seção <code>imagePullSecrets</code>
na definição de Pod.</p>
<p>Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF &gt; pod.yaml
</span><span style=color:#b44>apiVersion: v1
</span><span style=color:#b44>kind: Pod
</span><span style=color:#b44>metadata:
</span><span style=color:#b44>  name: foo
</span><span style=color:#b44>  namespace: awesomeapps
</span><span style=color:#b44>spec:
</span><span style=color:#b44>  containers:
</span><span style=color:#b44>    - name: foo
</span><span style=color:#b44>      image: janedoe/awesomeapp:v1
</span><span style=color:#b44>  imagePullSecrets:
</span><span style=color:#b44>    - name: myregistrykey
</span><span style=color:#b44>EOF</span>
cat <span style=color:#b44>&lt;&lt;EOF &gt;&gt; ./kustomization.yaml
</span><span style=color:#b44>resources:
</span><span style=color:#b44>- pod.yaml
</span><span style=color:#b44>EOF</span>
</code></pre></div><p>Isso precisa ser feito para cada pod que está usando um registro privado.</p>
<p>No entanto, a configuração deste campo pode ser automatizada definindo o imagePullSecrets
em um recurso de <a href=/docs/tasks/configure-pod-container/configure-service-account/>ServiceAccount</a>.</p>
<p>Verifique <a href=/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account>Adicionar ImagePullSecrets a uma conta de serviço</a> para obter instruções detalhadas.</p>
<p>Você pode usar isso em conjunto com um <code>.docker / config.json</code> por nó. As credenciais
serão mescladas.</p>
<h2 id=casos-de-uso>Casos de uso</h2>
<p>Existem várias soluções para configurar registros privados. Aqui estão alguns
casos de uso comuns e soluções sugeridas.</p>
<ol>
<li>Cluster executando apenas imagens não proprietárias (por exemplo, código aberto). Não há necessidade de ocultar imagens.
<ul>
<li>Use imagens públicas no Docker hub.
<ul>
<li>Nenhuma configuração necessária.</li>
<li>Alguns provedores de nuvem armazenam em cache ou espelham automaticamente imagens públicas, o que melhora a disponibilidade e reduz o tempo para extrair imagens.</li>
</ul>
</li>
</ul>
</li>
<li>Cluster executando algumas imagens proprietárias que devem ser ocultadas para quem está fora da empresa, mas
visível para todos os usuários do cluster.
<ul>
<li>Use um <a href=https://docs.docker.com/registry/>registro Docker</a> privado hospedado.
<ul>
<li>Pode ser hospedado no <a href=https://hub.docker.com/signup>Docker Hub</a> ou em outro lugar.</li>
<li>Configure manualmente .docker/config.json em cada nó conforme descrito acima.</li>
</ul>
</li>
<li>Ou execute um registro privado interno atrás de seu firewall com permissão de leitura.
<ul>
<li>Nenhuma configuração do Kubernetes é necessária.</li>
</ul>
</li>
<li>Use um serviço de registro de imagem de contêiner que controla o acesso à imagem
<ul>
<li>Funcionará melhor com o escalonamento automático do cluster do que com a configuração manual de nós.</li>
</ul>
</li>
<li>Ou, em um cluster onde alterar a configuração do nó é inconveniente, use <code>imagePullSecrets</code>.</li>
</ul>
</li>
<li>Cluster com imagens proprietárias, algumas das quais requerem controle de acesso mais rígido.
<ul>
<li>Certifique-se de que o <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>controlador de admissão AlwaysPullImages</a> está ativo. Caso contrário, todos os pods têm potencialmente acesso a todas as imagens.</li>
<li>Mova dados confidenciais para um recurso "secreto", em vez de empacotá-los em uma imagem.</li>
</ul>
</li>
<li>Um cluster multilocatário em que cada locatário precisa de seu próprio registro privado.
<ul>
<li>Certifique-se de que o <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>controlador de admissão AlwaysPullImages</a> está ativo. Caso contrário, todos os Pods de todos os locatários terão potencialmente acesso a todas as imagens.</li>
<li>Execute um registro privado com autorização necessária.</li>
<li>Gere credenciais de registro para cada locatário, coloque em segredo e preencha o segredo para cada namespace de locatário.</li>
<li>O locatário adiciona esse segredo a imagePullSecrets de cada namespace.</li>
</ul>
</li>
</ol>
<p>Se precisar de acesso a vários registros, você pode criar um segredo para cada registro.
O Kubelet mesclará qualquer <code>imagePullSecrets</code> em um único <code>.docker/config.json</code> virtual</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia a <a href=https://github.com/opencontainers/image-spec/blob/master/manifest.md>OCI Image Manifest Specification</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-643212488f778acf04bebed65ba34441>6.2 - Ambiente de Contêiner</h1>
<p>Essa página descreve os recursos disponíveis para contêineres no ambiente de contêiner.</p>
<h2 id=ambiente-de-contêiner>Ambiente de contêiner</h2>
<p>O ambiente de contêiner do Kubernetes fornece recursos importantes para contêineres:</p>
<ul>
<li>Um sistema de arquivos, que é a combinação de uma <a href=/docs/concepts/containers/images/>imagem</a> e um ou mais <a href=/docs/concepts/storage/volumes/>volumes</a>.</li>
<li>Informação sobre o contêiner propriamente.</li>
<li>Informação sobre outros objetos no cluster.</li>
</ul>
<h3 id=informação-de-contêiner>Informação de contêiner</h3>
<p>O <em>hostname</em> de um contêiner é o nome do Pod em que o contêiner está executando.
Isso é disponibilizado através do comando <code>hostname</code> ou da função <a href=https://man7.org/linux/man-pages/man2/gethostname.2.html><code>gethostname</code></a> chamada na libc.</p>
<p>O nome do Pod e o Namespace são expostos como variáveis de ambiente através de um mecanismo chamado <a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>downward API</a>.</p>
<p>Variáveis de ambiente definidas pelo usuário a partir da definição do Pod também são disponíveis para o contêiner, assim como qualquer variável de ambiente especificada estáticamente na imagem Docker.</p>
<h3 id=informação-do-cluster>Informação do cluster</h3>
<p>Uma lista de todos os serviços que estão executando quando um contêiner foi criado é disponibilizada para o contêiner como variáveis de ambiente.
Essas variáveis de ambiente são compatíveis com a funcionalidade <em>docker link</em> do Docker.</p>
<p>Para um serviço nomeado <em>foo</em> que mapeia para um contêiner nomeado <em>bar</em>, as seguintes variáveis são definidas:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>FOO_SERVICE_HOST</span><span style=color:#666>=</span>&lt;o host em que o serviço está executando&gt;
<span style=color:#b8860b>FOO_SERVICE_PORT</span><span style=color:#666>=</span>&lt;a porta em que o serviço está executando&gt;
</code></pre></div><p>Serviços possuem endereço IP dedicado e são disponibilizados para o contêiner via DNS,
se possuírem <a href=https://releases.k8s.io/v1.23.17/cluster/addons/dns/>DNS addon</a> habilitado.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Aprenda mais sobre <a href=/docs/concepts/containers/container-lifecycle-hooks/>hooks de ciclo de vida do contêiner</a>.</li>
<li>Obtenha experiência prática
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>anexando manipuladores a eventos de ciclo de vida do contêiner</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-a858027489648786a3b16264e451272b>6.3 - Classes de execução</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>
<p>Essa página descreve o recurso <em>RuntimeClass</em> e a seleção do mecanismo do agente de execução.</p>
<p>RuntimeClass é uma funcionalidade para selecionar as configurações do agente de execução do contêiner.
A configuração do agente de execução de contêineres é usada para executar os contêineres de um Pod.</p>
<h2 id=motivação>Motivação</h2>
<p>Você pode configurar um <em>RuntimeClass</em> diferente entre os diferentes Pods para prover
um equilíbrio entre performance versus segurança. Por exemplo, se parte de sua carga de
trabalho necessita de um alto nível de garantia de segurança da informação, você pode
optar em executar esses Pods em um agente de execução que usa virtualização de hardware.
Você então terá o benefício do isolamento extra de um agente de execução alternativo, ao
custo de uma latência adicional.</p>
<p>Você pode ainda usar um <em>RuntimeClass</em> para executar diferentes Pods com o mesmo agente
de execução de contêineres mas com diferentes configurações.</p>
<h2 id=configuração>Configuração</h2>
<ol>
<li>Configure a implementação do CRI nos nós (depende do agente de execução)</li>
<li>Crie o recurso RuntimeClass correspondente.</li>
</ol>
<h3 id=1-configure-a-implementação-do-cri-nos-nós>1. Configure a implementação do CRI nos nós</h3>
<p>As configurações disponíveis através do RuntimeClass sáo dependentes da implementação do
<em>Container Runtime Interface</em> (<a class=glossary-tooltip title="Uma API para agentes de execução de contêineres se integrarem com o kubelet" data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#container-runtime target=_blank aria-label="Container runtime interface (CRI)">Container runtime interface (CRI)</a>). Veja a documentação correspondente <a href=#configura%C3%A7%C3%A3o-do-cri>abaixo</a> para a
sua implementação CRI para verificar como configurar.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> RuntimeClass assume uma configuração homogênea de nós entre todo o cluster por padrão
(o que significa que todos os nós estão configurados do mesmo jeito referente aos agentes de
execução). Para suportar configurações heterogêneas, veja <a href=#associa%C3%A7%C3%A3o>Associação</a> abaixo.
</div>
<p>As configurações possuem um nome <code>handler</code> correspondente, referenciado pelo RuntimeClass.
Esse nome deve ser um valor DNS 1123 válido (letras, números e o carácter <code>-</code>).</p>
<h3 id=2-crie-o-recurso-runtimeclass-correspondente>2. Crie o recurso RuntimeClass correspondente</h3>
<p>As etapas de configuração no passo 1 devem todas estar associadas a um nome para o campo <code>handler</code>
que identifica a configuração. Para cada um, crie o objeto RuntimeClass correspondente.</p>
<p>O recurso RuntimeClass atualmente possui apenas 2 campos significativos: o nome do RuntimeClass
(<code>metadata.name</code>) e o agente (<code>handler</code>). A definição do objeto se parece conforme a seguir:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1 <span style=color:#bbb> </span><span style=color:#080;font-style:italic># RuntimeClass é definido no grupo de API node.k8s.io</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myclass <span style=color:#bbb> </span><span style=color:#080;font-style:italic># O nome que o RuntimeClass será chamado como</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># RuntimeClass é um recurso global, e não possui namespace.</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>myconfiguration <span style=color:#bbb> </span><span style=color:#080;font-style:italic># Nome da configuração CRI correspondente</span><span style=color:#bbb>
</span></code></pre></div><p>O nome de um objeto RuntimeClass deve ser um
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nome de subdomínio DNS</a> válido.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> É recomendado que operações de escrita no objeto RuntimeClass (criar/atualizar/patch/apagar)
sejam restritas a administradores do cluster. Isso geralmente é o padrão. Veja <a href=/docs/reference/access-authn-authz/authorization/>Visão Geral
de autorizações</a> para maiores detalhes.
</div>
<h2 id=uso>Uso</h2>
<p>Uma vez que as classes de execução estão configuradas no cluster, usar elas é relativamente
simples. Especifique um <code>runtimeClassName</code> na especificação do Pod. Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>myclass<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># ...</span><span style=color:#bbb>
</span></code></pre></div><p>Isso irá instruir o kubelet a usar o RuntimeClass nomeado acima (myclass) para esse Pod. Se
o nome do RuntimeClass não existir, ou o CRI não puder executar a solicitação, o Pod entrará na <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase>fase
final</a> <code>Failed</code>. Procure por um
<a href=/docs/tasks/debug-application-cluster/debug-application-introspection/>evento</a> correspondente
para uma mensagem de erro.</p>
<p>Se nenhum <code>runtimeClassName</code> for especificado, o RuntimeHandler padrão será utilizado, que é equivalente
ao comportamento quando a funcionalidade de RuntimeClass está desativada.</p>
<h3 id=configuração-do-cri>Configuração do CRI</h3>
<p>Para maiores detalhes de configuração dos agentes de execução CRI, veja <a href=/docs/setup/production-environment/container-runtimes/>instalação do CRI</a>.</p>
<h4 id=dockershim>dockershim</h4>
<p>O CRI dockershim embutido no Kubernetes não suporta outros agentes de execução.</p>
<h4 id=hahahugoshortcode-s4-hbhb><a class=glossary-tooltip title="Um agente de execução de contêiner com enfase em simplicidade, robustez e portabilidade" data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></h4>
<p>Agentes de execução são configurados através da configuração do containerd em
<code>/etc/containerd/config.toml</code>. Agentes válidos são configurados sob a seção de <code>runtimes</code>:</p>
<pre><code>[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.${HANDLER_NAME}]
</code></pre><p>Veja a documentação de configuração do containerd para maiores detalhes:
<a href=https://github.com/containerd/cri/blob/master/docs/config.md>https://github.com/containerd/cri/blob/master/docs/config.md</a></p>
<h4 id=hahahugoshortcode-s5-hbhb><a class=glossary-tooltip title="Um agente de execução leve de contêineres criado especificamente para o Kubernetes" data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></h4>
<p>Agentes de execução são configurados através da configuração do CRI-O em <code>/etc/crio/crio.conf</code>.
Agentes válidos são configurados na seção <a href=https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table>crio.runtime
table</a>:</p>
<pre><code>[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = &quot;${PATH_TO_BINARY}&quot;
</code></pre><p>Veja a <a href=https://raw.githubusercontent.com/cri-o/cri-o/9f11d1d/docs/crio.conf.5.md>documentação de configuração</a> do CRI-O para maiores detalhes.</p>
<h2 id=associação>Associação</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code>
</div>
<p>Ao especificar o campo <code>scheduling</code> para um RuntimeClass, você pode colocar limites e
garantir que os Pods executando dentro de uma RuntimeClass sejam associados a nós que
suportem eles. Se o <code>scheduling</code> não estiver configurado, assume-se que esse RuntimeClass
é suportado por todos os nós.</p>
<p>Para garantir que os Pods sejam executados em um nó que suporte um RuntimeClass específico,
aquele conjunto de nós deve possuir uma marca/label padrão que é selecionado pelo campo
<code>runtimeclass.scheduling.nodeSelector</code>. O nodeSelector do RuntimeClass é combinado com o
nodeSelector do Pod em tempo de admissão, obtendo a intersecção do conjunto de nós selecionado
por cada. Se existir um conflito, o pod será rejeitado.</p>
<p>Se os nós suportados possuírem marcação de restrição para prevenir outros Pods com uma
classe de execução diferente de executar no nó, você pode adicionar o campo <code>tolerations</code>
ao objeto RuntimeClass. Assim como com o <code>nodeSelector</code>, o <code>tolerations</code> é combinado com
o campo <code>tolerations</code> do Pod em tempo de admissão, efetivamente pegando a intersecção do
conjunto de nós aplicáveis para cada.</p>
<p>Para saber mais sobre a configuração de seleção de nós e tolerâncias, veja <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>Associando Pods a
Nós</a>.</p>
<h3 id=sobrecarga-de-pods>Sobrecarga de Pods</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>
<p>Você pode especificar os recursos extra que estão associados à execução de um Pod. Declarar esses
recursos extra permite ao cluster (incluindo o agendador/scheduler de pods) contabilizar por
esses recursos quando estiver decidindo sobre Pods e recursos. Para usar a contabilização
desses recursos extras, você deve estar com o <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
PodOverhead habilitado (ele já está habilitado por padrão).</p>
<p>Os recursos extras utilizados são especificados no objeto RuntimeClass através do campo <code>overhead</code>.
Ao usar esses campos, você especifica o uso extra de recursos necessários para executar
Pods utilizando-se desse Runtimeclass e assim contabilizar esses recursos para o Kubernetes.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md>RuntimeClass Design</a></li>
<li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling>RuntimeClass Scheduling Design</a></li>
<li>Leia mais sobre <a href=/docs/concepts/scheduling-eviction/pod-overhead/>Sobrecarga de Pods</a></li>
<li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20190226-pod-overhead.md>PodOverhead Feature Design</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-e6941d969d81540208a3e78bc56f43bc>6.4 - Hooks de Ciclo de Vida do Contêiner</h1>
<p>Essa página descreve como os contêineres gerenciados pelo <em>kubelet</em> podem usar a estrutura de <em>hook</em> de ciclo de vida do contêiner para executar código acionado por eventos durante seu ciclo de vida de gerenciamento.</p>
<h2 id=visão-geral>Visão Geral</h2>
<p>Análogo a muitas estruturas de linguagem de programação que tem <em>hooks</em> de ciclo de vida de componentes, como angular,
o Kubernetes fornece aos contêineres <em>hooks</em> de ciclo de vida.
Os <em>hooks</em> permitem que os contêineres estejam cientes dos eventos em seu ciclo de vida de gerenciamento
e executem código implementado em um manipulador quando o <em>hook</em> de ciclo de vida correspondente é executado.</p>
<h2 id=hooks-do-contêiner>Hooks do contêiner</h2>
<p>Existem dois <em>hooks</em> que são expostos para os contêiners:</p>
<p><code>PostStart</code></p>
<p>Este <em>hook</em> é executado imediatamente após um contêiner ser criado.
Entretanto, não há garantia que o <em>hook</em> será executado antes do ENTRYPOINT do contêiner.
Nenhum parâmetro é passado para o manipulador.</p>
<p><code>PreStop</code></p>
<p>Esse <em>hook</em> é chamado imediatamente antes de um contêiner ser encerrado devido a uma solicitação de API ou um gerenciamento de evento como liveness/startup probe failure, preemption, resource contention e outros.
Uma chamada ao <em>hook</em> <code>PreStop</code> falha se o contêiner já está em um estado finalizado ou concluído e o <em>hook</em> deve ser concluído antes que o sinal TERM seja enviado para parar o contêiner. A contagem regressiva do período de tolerância de término do Pod começa antes que o <em>hook</em> <code>PreStop</code> seja executado, portanto, independentemente do resultado do manipulador, o contêiner será encerrado dentro do período de tolerância de encerramento do Pod. Nenhum parâmetro é passado para o manipulador.</p>
<p>Uma descrição mais detalhada do comportamento de término pode ser encontrada em <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>Término de Pods</a>.</p>
<h3 id=implementações-de-manipulador-de-hook>Implementações de manipulador de hook</h3>
<p>Os contêineres podem acessar um <em>hook</em> implementando e registrando um manipulador para esse <em>hook</em>.
Existem dois tipos de manipuladores de <em>hooks</em> que podem ser implementados para contêineres:</p>
<ul>
<li>Exec - Executa um comando específico, como <code>pre-stop.sh</code>, dentro dos cgroups e Namespaces do contêiner.</li>
<li>HTTP - Executa uma requisição HTTP em um endpoint específico do contêiner.</li>
</ul>
<h3 id=execução-do-manipulador-de-hook>Execução do manipulador de hook</h3>
<p>Quando um <em>hook</em> de gerenciamento de ciclo de vida do contêiner é chamado, o sistema de gerenciamento do Kubernetes executa o manipulador de acordo com a ação do <em>hook</em>, <code>httpGet</code> e <code>tcpSocket</code> são executados pelo processo kubelet e <code>exec</code> é executado pelo contêiner.</p>
<p>As chamadas do manipulador do <em>hook</em> são síncronas no contexto do Pod que contém o contêiner.
Isso significa que para um <em>hook</em> <code>PostStart</code>, o ENTRYPOINT do contêiner e o <em>hook</em> disparam de forma assíncrona.
No entanto, se o <em>hook</em> demorar muito para ser executado ou travar, o contêiner não consegue atingir o estado <code>running</code>.</p>
<p>Os <em>hooks</em> <code>PreStop</code> não são executados de forma assíncrona a partir do sinal para parar o contêiner, o <em>hook</em> precisa finalizar a sua execução antes que o sinal TERM possa ser enviado.
Se um <em>hook</em> <code>PreStop</code> travar durante a execução, a fase do Pod será <code>Terminating</code> e permanecerá até que o Pod seja morto após seu <code>terminationGracePeriodSeconds</code> expirar. Esse período de tolerância se aplica ao tempo total necessário
para o <em>hook</em> <code>PreStop</code>executar e para o contêiner parar normalmente.
Se por exemplo, o <code>terminationGracePeriodSeconds</code> é 60, e o <em>hook</em> leva 55 segundos para ser concluído, e o contêiner leva 10 segundos para parar normalmente após receber o sinal, então o contêiner será morto antes que possa parar
normalmente, uma vez que o <code>terminationGracePeriodSeconds</code> é menor que o tempo total (55 + 10) que é necessário para que essas duas coisas aconteçam.</p>
<p>Se um <em>hook</em> <code>PostStart</code> ou <code>PreStop</code> falhar, ele mata o contêiner.</p>
<p>Os usuários devem tornar seus <em>hooks</em> o mais leve possíveis.
Há casos, no entanto, em que comandos de longa duração fazem sentido, como ao salvar o estado
antes de parar um contêiner.</p>
<h3 id=garantias-de-entrega-de-hooks>Garantias de entrega de <em>hooks</em></h3>
<p>A entrega do <em>hook</em> é destinada a acontecer <em>pelo menos uma vez</em>,
o que quer dizer que um <em>hook</em> pode ser chamado várias vezes para qualquer evento,
como para <code>PostStart</code> ou <code>PreStop</code>.
Depende da implementação do <em>hook</em> lidar com isso corretamente.</p>
<p>Geralmente, apenas entregas únicas são feitas.
Se, por exemplo, um receptor de <em>hook</em> HTTP estiver inativo e não puder receber tráfego,
não há tentativa de reenviar.
Em alguns casos raros, no entanto, pode ocorrer uma entrega dupla.
Por exemplo, se um kubelet reiniciar no meio do envio de um <em>hook</em>, o <em>hook</em> pode ser
reenviado depois que o kubelet voltar a funcionar.</p>
<h3 id=depurando-manipuladores-de-hooks>Depurando manipuladores de <em>hooks</em></h3>
<p>Os logs para um manipulador de <em>hook</em> não são expostos em eventos de Pod.
Se um manipulador falhar por algum motivo, ele transmitirá um evento.
Para <code>PostStart</code> é o evento <code>FailedPostStartHook</code> e para <code>PreStop</code> é o evento
<code>FailedPreStopHook</code>.
Você pode ver esses eventos executando <code>kubectl describe pod &lt;nome_do_pod></code>.
Aqui está um exemplo de saída de eventos da execução deste comando:</p>
<pre><code>Events:
  FirstSeen  LastSeen  Count  From                                                   SubObjectPath          Type      Reason               Message
  ---------  --------  -----  ----                                                   -------------          --------  ------               -------
  1m         1m        1      {default-scheduler }                                                          Normal    Scheduled            Successfully assigned test-1730497541-cq1d2 to gke-test-cluster-default-pool-a07e5d30-siqd
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulling              pulling image &quot;test:1.0&quot;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Created              Created container with docker id 5c6a256a2567; Security:[seccomp=unconfined]
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulled               Successfully pulled image &quot;test:1.0&quot;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Started              Started container with docker id 5c6a256a2567
  38s        38s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 5c6a256a2567: PostStart handler: Error executing in Docker Container: 1
  37s        37s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 8df9fdfd7054: PostStart handler: Error executing in Docker Container: 1
  38s        37s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}                         Warning   FailedSync           Error syncing pod, skipping: failed to &quot;StartContainer&quot; for &quot;main&quot; with RunContainerError: &quot;PostStart handler: Error executing in Docker Container: 1&quot;
  1m         22s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Warning   FailedPostStartHook
</code></pre><h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Saiba mais sobre o <a href=/docs/concepts/containers/container-environment/>Ambiente de contêiner</a>.</li>
<li>Obtenha experiência prática
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>anexando manipuladores a eventos de ciclo de vida do contêiner</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-0a0a7eca3e302a3c08f8c85e15d337fd>7 - Serviços, balanceamento de carga e conectividade</h1>
<div class=lead>Conceitos e recursos por trás da conectividade no Kubernetes.</div>
<p>A conectividade do Kubernetes trata quatro preocupações:</p>
<ul>
<li>Contêineres em um Pod se comunicam via interface <em>loopback</em>.</li>
<li>A conectividade do cluster provê a comunicação entre diferentes Pods.</li>
<li>O recurso de <em>Service</em> permite a você expor uma aplicação executando em um Pod,
de forma a ser alcançável de fora de seu cluster.</li>
<li>Você também pode usar os <em>Services</em> para publicar serviços de consumo interno do
seu cluster.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ded1daafdcd293023ee333728007ca61>7.1 - Políticas de rede</h1>
<p>Se você deseja controlar o fluxo do tráfego de rede no nível do endereço IP ou de portas TCP e UDP
(camadas OSI 3 e 4) então você deve considerar usar Políticas de rede (<code>NetworkPolicies</code>) do Kubernetes para aplicações
no seu cluster. <code>NetworkPolicy</code> é um objeto focado em aplicações/experiência do desenvolvedor
que permite especificar como é permitido a um <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=pod>pod</a>
comunicar-se com várias "entidades" de rede.</p>
<p>As entidades que um Pod pode se comunicar são identificadas através de uma combinação dos 3
identificadores à seguir:</p>
<ol>
<li>Outros pods que são permitidos (exceção: um pod não pode bloquear a si próprio)</li>
<li>Namespaces que são permitidos</li>
<li>Blocos de IP (exceção: o tráfego de e para o nó que um Pod está executando sempre é permitido,
independentemente do endereço IP do Pod ou do Nó)</li>
</ol>
<p>Quando definimos uma política de rede baseada em pod ou namespace, utiliza-se um <a class=glossary-tooltip title="Permite ao usuário filtrar uma lista de recursos com base em rótulos (labels)." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=selector>selector</a>
para especificar qual tráfego é permitido de e para o(s) Pod(s) que correspondem ao seletor.</p>
<p>Quando uma política de redes baseada em IP é criada, nós definimos a política baseada em blocos de IP (faixas CIDR).</p>
<h2 id=pré-requisitos>Pré requisitos</h2>
<p>As políticas de rede são implementadas pelo <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>plugin de redes</a>. Para usar
uma política de redes, você deve usar uma solução de redes que suporte o objeto <code>NetworkPolicy</code>.
A criação de um objeto <code>NetworkPolicy</code> sem um controlador que implemente essas regras não tem efeito.</p>
<h2 id=pods-isolados-e-não-isolados>Pods isolados e não isolados</h2>
<p>Por padrão, pods não são isolados; eles aceitam tráfego de qualquer origem.</p>
<p>Os pods tornam-se isolados ao existir uma <code>NetworkPolicy</code> que selecione eles. Uma vez que
exista qualquer <code>NetworkPolicy</code> no namespace selecionando um pod em específico, aquele pod
irá rejeitar qualquer conexão não permitida por qualquer <code>NetworkPolicy</code>. (Outros pod no mesmo
namespace que não são selecionados por nenhuma outra <code>NetworkPolicy</code> irão continuar aceitando
todo tráfego de rede.)</p>
<p>As políticas de rede não conflitam; elas são aditivas. Se qualquer política selecionar um pod,
o pod torna-se restrito ao que é permitido pela união das regras de entrada/saída de tráfego definidas
nas políticas. Assim, a ordem de avaliação não afeta o resultado da política.</p>
<p>Para o fluxo de rede entre dois pods ser permitido, tanto a política de saída no pod de origem
e a política de entrada no pod de destino devem permitir o tráfego. Se a política de saída na
origem, ou a política de entrada no destino negar o tráfego, o tráfego será bloqueado.</p>
<h2 id=networkpolicy-resource>O recurso NetworkPolicy</h2>
<p>Veja a referência <a href=/docs/reference/generated/kubernetes-api/v1.23/#networkpolicy-v1-networking-k8s-io>NetworkPolicy</a> para uma definição completa do recurso.</p>
<p>Uma <code>NetworkPolicy</code> de exemplo é similar ao abaixo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-network-policy<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>db<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>172.17.0.0</span>/16<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>except</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:#666>172.17.1.0</span>/24<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>project</span>:<span style=color:#bbb> </span>myproject<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>6379</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>to</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.0.0</span>/24<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>5978</span><span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Criar esse objeto no seu cluster não terá efeito a não ser que você escolha uma
solução de redes que suporte políticas de rede.
</div>
<p><strong>Campos obrigatórios</strong>: Assim como todas as outras configurações do Kubernetes, uma <code>NetworkPolicy</code>
necessita dos campos <code>apiVersion</code>, <code>kind</code> e <code>metadata</code>. Para maiores informações sobre
trabalhar com arquivos de configuração, veja
<a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>Configurando containeres usando ConfigMap</a>,
e <a href=/docs/concepts/overview/working-with-objects/object-management>Gerenciamento de objetos</a>.</p>
<p><strong>spec</strong>: A <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>spec</a> contém todas as informações necessárias
para definir uma política de redes em um namespace.</p>
<p><strong>podSelector</strong>: Cada <code>NetworkPolicy</code> inclui um <code>podSelector</code> que seleciona o grupo de pods
que a política se aplica. A política acima seleciona os pods com a <em>label</em> "role=db". Um <code>podSelector</code>
vazio seleciona todos os pods no namespace.</p>
<p><strong>policyTypes</strong>: Cada <code>NetworkPolicy</code> inclui uma lista de <code>policyTypes</code> que pode incluir <code>Ingress</code>,
<code>Egress</code> ou ambos. O campo <code>policyTypes</code> indica se a política se aplica ao tráfego de entrada
com destino aos pods selecionados, o tráfego de saída com origem dos pods selecionados ou ambos.
Se nenhum <code>policyType</code> for definido então por padrão o tipo <code>Ingress</code> será sempre utilizado, e o
tipo <code>Egress</code> será configurado apenas se o objeto contiver alguma regra de saída. (campo <code>egress</code> a seguir).</p>
<p><strong>ingress</strong>: Cada <code>NetworkPolicy</code> pode incluir uma lista de regras de entrada permitidas através do campo <code>ingress</code>.
Cada regra permite o tráfego que corresponde simultaneamente às sessões <code>from</code> (de) e <code>ports</code> (portas).
A política de exemplo acima contém uma regra simples, que corresponde ao tráfego em uma única porta,
de uma das três origens definidas, sendo a primeira definida via <code>ipBlock</code>, a segunda via <code>namespaceSelector</code> e
a terceira via <code>podSelector</code>.</p>
<p><strong>egress</strong>: Cada política pode incluir uma lista de regras de regras de saída permitidas através do campo <code>egress</code>.
Cada regra permite o tráfego que corresponde simultaneamente às sessões <code>to</code> (para) e <code>ports</code> (portas).
A política de exemplo acima contém uma regra simples, que corresponde ao tráfego destinado a uma
porta em qualquer destino pertencente à faixa de IPs em <code>10.0.0.0/24</code>.</p>
<p>Então a <code>NetworkPolicy</code> acima:</p>
<ol>
<li>
<p>Isola os pods no namespace "default" com a <em>label</em> "role=db" para ambos os tráfegos de entrada
e saída (se eles ainda não estavam isolados)</p>
</li>
<li>
<p>(Regras de entrada/ingress) permite conexões para todos os pods no namespace "default" com a <em>label</em> "role=db" na porta TCP 6379 de:</p>
<ul>
<li>qualquer pod no namespace "default" com a <em>label</em> "role=frontend"</li>
<li>qualquer pod em um namespace que tenha a <em>label</em> "project=myproject" (aqui cabe ressaltar que o namespace que deve ter a <em>label</em> e não os pods dentro desse namespace)</li>
<li>IPs dentro das faixas 172.17.0.0–172.17.0.255 e 172.17.2.0–172.17.255.255 (ex.:, toda 172.17.0.0/16 exceto 172.17.1.0/24)</li>
</ul>
</li>
<li>
<p>(Regras de saída/egress) permite conexões de qualquer pod no namespace "default" com a <em>label</em>
"role=db" para a faixa de destino 10.0.0.0/24 na porta TCP 5978.</p>
</li>
</ol>
<p>Veja o tutorial <a href=/docs/tasks/administer-cluster/declare-network-policy/>Declarando uma política de redes</a> para mais exemplos.</p>
<h2 id=comportamento-dos-seletores-to-e-from>Comportamento dos seletores <code>to</code> e <code>from</code></h2>
<p>Existem quatro tipos de seletores que podem ser especificados nas sessões <code>ingress.from</code> ou
<code>egress.to</code>:</p>
<p><strong>podSelector</strong>: Seleciona Pods no mesmo namespace que a política de rede foi criada, e que deve
ser permitido origens no tráfego de entrada ou destinos no tráfego de saída.</p>
<p><strong>namespaceSelector</strong>: Seleciona namespaces para o qual todos os Pods devem ser permitidos como
origens no caso de tráfego de entrada ou destino no tráfego de saída.</p>
<p><strong>namespaceSelector</strong> <em>e</em> <strong>podSelector</strong>: Uma entrada <code>to</code>/<code>from</code> única que permite especificar
ambos <code>namespaceSelector</code> e <code>podSelector</code> e seleciona um conjunto de Pods dentro de um namespace.
Seja cuidadoso em utilizar a sintaxe YAML correta; essa política:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>alice<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>client<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></code></pre></div><p>contém um único elemento <code>from</code> permitindo conexões de Pods com a label <code>role=client</code> em
namespaces com a <em>label</em> <code>user=alice</code>. Mas <em>essa</em> política:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>alice<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>client<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></code></pre></div><p>contém dois elementos no conjunto <code>from</code> e permite conexões de Pods no namespace local com
a <em>label</em> <code>role=client</code>, <em>OU</em> de qualquer outro Pod em qualquer outro namespace que tenha
a label <code>user=alice</code>.</p>
<p>Quando estiver em dúvida, utilize o comando <code>kubectl describe</code> para verificar como o
Kubernetes interpretou a política.</p>
<p><strong>ipBlock</strong>: Isso seleciona um conjunto particular de faixas de IP a serem permitidos como
origens no caso de entrada ou destinos no caso de saída. Devem ser considerados IPs externos
ao cluster, uma vez que os IPs dos Pods são efêmeros e imprevisíveis.</p>
<p>Os mecanismos de entrada e saída do cluster geralmente requerem que os IPs de origem ou destino
sejam reescritos. Em casos em que isso aconteça, não é definido se deve acontecer antes ou
depois do processamento da <code>NetworkPolicy</code> que corresponde a esse tráfego, e o comportamento
pode ser diferente para cada plugin de rede, provedor de nuvem, implementação de <code>Service</code>, etc.</p>
<p>No caso de tráfego de entrada, isso significa que em alguns casos você pode filtrar os pacotes
de entrada baseado no IP de origem atual, enquanto que em outros casos o IP de origem que
a <code>NetworkPolicy</code> atua pode ser o IP de um <code>LoadBalancer</code> ou do Nó em que o Pod está executando.</p>
<p>No caso de tráfego de saída, isso significa que conexões de Pods para <code>Services</code> que são reescritos
para IPs externos ao cluster podem ou não estar sujeitos a políticas baseadas no campo <code>ipBlock</code>.</p>
<h2 id=políticas-padrão>Políticas padrão</h2>
<p>Por padrão, se nenhuma política existir no namespace, então todo o tráfego de entrada e saída é
permitido de e para os pods nesse namespace. Os exemplos a seguir permitem a você mudar o
comportamento padrão nesse namespace.</p>
<h3 id=bloqueio-padrão-de-todo-tráfego-de-entrada>Bloqueio padrão de todo tráfego de entrada</h3>
<p>Você pode criar uma política padrão de isolamento para um namespace criando um objeto <code>NetworkPolicy</code>
que seleciona todos os pods mas não permite o tráfego de entrada para esses pods.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/service/networking/network-policy-default-deny-ingress.yaml download=service/networking/network-policy-default-deny-ingress.yaml><code>service/networking/network-policy-default-deny-ingress.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('service-networking-network-policy-default-deny-ingress-yaml')" title="Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard">
</img>
</div>
<div class=includecode id=service-networking-network-policy-default-deny-ingress-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-ingress<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Isso garante que mesmo pods que não são selecionados por nenhuma outra política de rede ainda
serão isolados. Essa política não muda o comportamento padrão de isolamento de tráfego de saída
nesse namespace.</p>
<h3 id=permitir-por-padrão-todo-tráfego-de-entrada>Permitir por padrão todo tráfego de entrada</h3>
<p>Se você deseja permitir todo o tráfego de todos os pods em um namespace (mesmo que políticas que
sejam adicionadas faça com que alguns pods sejam tratados como "isolados"), você pode criar
uma política que permite explicitamente todo o tráfego naquele namespace.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/service/networking/network-policy-allow-all-ingress.yaml download=service/networking/network-policy-allow-all-ingress.yaml><code>service/networking/network-policy-allow-all-ingress.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('service-networking-network-policy-allow-all-ingress-yaml')" title="Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard">
</img>
</div>
<div class=includecode id=service-networking-network-policy-allow-all-ingress-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>allow-all-ingress<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- {}<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<h3 id=bloqueio-padrão-de-todo-tráfego-de-saída>Bloqueio padrão de todo tráfego de saída</h3>
<p>Você pode criar uma política de isolamento de saída padrão para um namespace criando uma
política de redes que selecione todos os pods, mas não permita o tráfego de saída a partir
de nenhum desses pods.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/service/networking/network-policy-default-deny-egress.yaml download=service/networking/network-policy-default-deny-egress.yaml><code>service/networking/network-policy-default-deny-egress.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('service-networking-network-policy-default-deny-egress-yaml')" title="Copy service/networking/network-policy-default-deny-egress.yaml to clipboard">
</img>
</div>
<div class=includecode id=service-networking-network-policy-default-deny-egress-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-egress<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Isso garante que mesmo pods que não são selecionados por outra política de rede não seja permitido
tráfego de saída. Essa política não muda o comportamento padrão de tráfego de entrada.</p>
<h3 id=permitir-por-padrão-todo-tráfego-de-saída>Permitir por padrão todo tráfego de saída</h3>
<p>Caso você queira permitir todo o tráfego de todos os pods em um namespace (mesmo que políticas sejam
adicionadas e cause com que alguns pods sejam tratados como "isolados"), você pode criar uma
política explicita que permite todo o tráfego de saída no namespace.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/service/networking/network-policy-allow-all-egress.yaml download=service/networking/network-policy-allow-all-egress.yaml><code>service/networking/network-policy-allow-all-egress.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('service-networking-network-policy-allow-all-egress-yaml')" title="Copy service/networking/network-policy-allow-all-egress.yaml to clipboard">
</img>
</div>
<div class=includecode id=service-networking-network-policy-allow-all-egress-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>allow-all-egress<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- {}<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<h3 id=bloqueio-padrão-de-todo-tráfego-de-entrada-e-saída>Bloqueio padrão de todo tráfego de entrada e saída</h3>
<p>Você pode criar uma política padrão em um namespace que previne todo o tráfego de entrada
E saída criando a política a seguir no namespace.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/service/networking/network-policy-default-deny-all.yaml download=service/networking/network-policy-default-deny-all.yaml><code>service/networking/network-policy-default-deny-all.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('service-networking-network-policy-default-deny-all-yaml')" title="Copy service/networking/network-policy-default-deny-all.yaml to clipboard">
</img>
</div>
<div class=includecode id=service-networking-network-policy-default-deny-all-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-all<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Isso garante que mesmo pods que não são selecionados por nenhuma outra política de redes não
possuam permissão de tráfego de entrada ou saída.</p>
<h2 id=selecionando-uma-faixa-de-portas>Selecionando uma faixa de portas</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>
<p>Ao escrever uma política de redes, você pode selecionar uma faixa de portas ao invés de uma
porta única, utilizando-se do campo <code>endPort</code> conforme a seguir:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>multi-port-egress<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>db<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>to</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.0.0</span>/24<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>32000</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>endPort</span>:<span style=color:#bbb> </span><span style=color:#666>32768</span><span style=color:#bbb>
</span></code></pre></div><p>A regra acima permite a qualquer Pod com a <em>label</em> "role=db" no namespace <code>default</code> de se comunicar
com qualquer IP na faixa <code>10.0.0.0/24</code> através de protocolo TCP, desde que a porta de destino
esteja na faixa entre 32000 e 32768.</p>
<p>As seguintes restrições aplicam-se ao se utilizar esse campo:</p>
<ul>
<li>Por ser uma funcionalidade "alpha", ela é desativada por padrão. Para habilitar o campo <code>endPort</code>
no cluster, você (ou o seu administrador do cluster) deve habilitar o <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>NetworkPolicyEndPort</code> no <code>kube-apiserver</code> com a flag <code>--feature-gates=NetworkPolicyEndPort=true,...</code>.</li>
<li>O valor de <code>endPort</code> deve ser igual ou maior ao valor do campo <code>port</code>.</li>
<li>O campo <code>endPort</code> só pode ser definido se o campo <code>port</code> também for definido.</li>
<li>Ambos os campos <code>port</code> e <code>endPort</code> devem ser números.</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Seu cluster deve utilizar um plugin <a class=glossary-tooltip title="Plugins Container network interface (CNI) são um tipo de plugin de Rede em conformidade com a especificação appc/CNI." data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni target=_blank aria-label=CNI>CNI</a>
que suporte o campo <code>endPort</code> na especificação da política de redes.
</div>
<h2 id=selecionando-um-namespace-pelo-seu-nome>Selecionando um Namespace pelo seu nome</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes 1.21 [beta]</code>
</div>
<p>A camada de gerenciamento do Kubernetes configura uma <em>label</em> imutável <code>kubernetes.io/metadata.name</code> em
todos os namespaces, uma vez que o <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> esteja habilitado por padrão.
O valor dessa <em>label</em> é o nome do namespace.</p>
<p>Enquanto que um objeto <code>NetworkPolicy</code> não pode selecionar um namespace pelo seu nome através de
um campo específico, você pode utilizar essa <em>label</em> padrão para selecionar um namespace pelo seu nome.</p>
<h2 id=o-que-você-não-pode-fazer-com-networkpolicies-ao-menos-por-enquanto>O que você não pode fazer com <code>NetworkPolicies</code> (ao menos por enquanto!)</h2>
<p>Por enquanto no Kubernetes 1.27 as funcionalidades a seguir não existem
mas você pode conseguir implementar de forma alternativa utilizando componentes do Sistema Operacional
(como SELinux, OpenVSwitch, IPtables, etc) ou tecnologias da camada 7 OSI (Ingress controllers, implementações de service mesh) ou ainda <em>admission controllers</em>.
No caso do assunto "segurança de redes no Kubernetes" ser novo para você, vale notar que as
histórias de usuário a seguir ainda não podem ser implementadas:</p>
<ul>
<li>Forçar o tráfego interno do cluster passar por um gateway comum (pode ser implementado via service mesh ou outros proxies)</li>
<li>Qualquer coisa relacionada a TLS/mTLS (use um service mesh ou ingress controller para isso)</li>
<li>Políticas específicas a nível do nó kubernetes (você pode utilizar as notações de IP CIDR para isso, mas não pode selecionar nós Kubernetes por suas identidades)</li>
<li>Selecionar <code>Services</code> pelo seu nome (você pode, contudo, selecionar pods e namespaces por seus <a class=glossary-tooltip title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a> o que torna-se uma solução de contorno viável).</li>
<li>Criação ou gerenciamento</li>
<li>Políticas padrão que são aplicadas a todos os namespaces e pods (existem alguns plugins externos do Kubernetes e projetos que podem fazer isso, e a comunidade está trabalhando nessa especificação).</li>
<li>Ferramental de testes para validação de políticas de redes.</li>
<li>Possibilidade de logar eventos de segurança de redes (conexões bloqueadas, aceitas). Existem plugins CNI que conseguem fazer isso à parte.</li>
<li>Possibilidade de explicitamente negar políticas de rede (o modelo das <code>NetworkPolicies</code> são "negar por padrão e conforme a necessidade, deve-se adicionar regras que permitam o tráfego).</li>
<li>Bloquear o tráfego que venha da interface de loopback/localhost ou que venham do nó em que o Pod se encontre.</li>
</ul>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Veja o tutorial <a href=/docs/tasks/administer-cluster/declare-network-policy/>Declarando políticas de redes</a> para mais exemplos.</li>
<li>Veja mais <a href=https://github.com/ahmetb/kubernetes-network-policy-recipes>cenários comuns e exemplos</a> de políticas de redes.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-275bea454e1cf4c5adeca4058b5af988>8 - Configuração</h1>
</div>
<div class=td-content>
<h1 id=pg-ddef6fd0e47bb51c6f05e8e7fb11d2dd>8.1 - Melhores Práticas de Configuração</h1>
<p>Esse documento destaca e consolida as melhores práticas de configuração apresentadas em todo o guia de usuário,
na documentação de introdução e nos exemplos.</p>
<p>Este é um documento vivo. Se você pensar em algo que não está nesta lista, mas pode ser útil para outras pessoas,
não hesite em criar uma <em>issue</em> ou submeter um PR.</p>
<h2 id=dicas-gerais-de-configuração>Dicas Gerais de Configuração</h2>
<ul>
<li>
<p>Ao definir configurações, especifique a versão mais recente estável da API.</p>
</li>
<li>
<p>Os arquivos de configuração devem ser armazenados em um sistema de controle antes de serem enviados ao cluster.
Isso permite que você reverta rapidamente uma alteração de configuração, caso necessário. Isso também auxilia na recriação e restauração do cluster.</p>
</li>
<li>
<p>Escreva seus arquivos de configuração usando YAML ao invés de JSON. Embora esses formatos possam ser usados alternadamente em quase todos os cenários, YAML tende a ser mais amigável.</p>
</li>
<li>
<p>Agrupe objetos relacionados em um único arquivo sempre que fizer sentido. Geralmente, um arquivo é mais fácil de
gerenciar do que vários. Veja o <a href=https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml>guestbook-all-in-one.yaml</a> como exemplo dessa sintaxe.</p>
</li>
<li>
<p>Observe também que vários comandos <code>kubectl</code> podem ser chamados em um diretório. Por exemplo, você pode chamar
<code>kubectl apply</code> em um diretório de arquivos de configuração.</p>
</li>
<li>
<p>Não especifique valores padrões desnecessariamente: configurações simples e mínimas diminuem a possibilidade de erros.</p>
</li>
<li>
<p>Coloque descrições de objetos nas anotações para permitir uma melhor análise.</p>
</li>
</ul>
<h2 id=naked-pods-vs-replicasets-deployments-and-jobs>"Naked" Pods comparados a ReplicaSets, Deployments, e Jobs</h2>
<ul>
<li>
<p>Se você puder evitar, não use "naked" Pods (ou seja, se você puder evitar, pods não vinculados a um <a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a> ou <a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a>).
Os "naked" pods não serão reconfigurados em caso de falha de um nó.</p>
<p>Criar um Deployment, que cria um ReplicaSet para garantir que o número desejado de Pods esteja disponível e especifica uma estratégia para substituir os Pods (como <a href=/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment>RollingUpdate</a>), é quase sempre preferível do que criar Pods diretamente, exceto para alguns cenários explícitos de restartPolicy:Never. Um Job também pode ser apropriado.</p>
</li>
</ul>
<h2 id=services>Services</h2>
<ul>
<li>
<p>Crie o <a href=/docs/concepts/services-networking/service/>Service</a> antes de suas cargas de trabalho de backend correspondentes (Deployments ou ReplicaSets) e antes de quaisquer cargas de trabalho que precisem acessá-lo. Quando o
Kubernetes inicia um contêiner, ele fornece variáveis de ambiente apontando para todos os Services que estavam em execução quando o contêiner foi iniciado. Por exemplo, se um Service chamado <code>foo</code> existe, todos os contêineres vão
receber as seguintes variáveis em seu ambiente inicial:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>FOO_SERVICE_HOST</span><span style=color:#666>=</span>&lt;o host em que o Service está executando&gt;
<span style=color:#b8860b>FOO_SERVICE_PORT</span><span style=color:#666>=</span>&lt;a porta em que o Service está executando&gt;
</code></pre></div></li>
</ul>
<p><em>Isso implica em um requisito de pedido</em> - qualquer <code>Service</code> que um <code>Pod</code> quer acessar precisa ser criado antes do <code>Pod</code> em si, ou então as variáveis de ambiente não serão populadas. O DNS não possui essa restrição.</p>
<ul>
<li>
<p>Um <a href=/docs/concepts/cluster-administration/addons/>cluster add-on</a> opcional (embora fortemente recomendado) é um servidor DNS. O
servidor DNS monitora a API do Kubernetes buscando novos <code>Services</code> e cria um conjunto de DNS para cada um. Se o DNS foi habilitado em todo o cluster, então todos os <code>Pods</code> devem ser capazes de fazer a resolução de <code>Services</code> automaticamente.</p>
</li>
<li>
<p>Não especifique um <code>hostPort</code> para um Pod a menos que isso seja absolutamente necessário. Quando você vincula um Pod a um <code>hostPort</code>, isso limita o número de lugares em que o Pod pode ser agendado, porque cada
combinação de &lt;<code>hostIP</code>, <code>hostPort</code>, <code>protocol</code>> deve ser única. Se você não especificar o <code>hostIP</code> e <code>protocol</code> explicitamente, o Kubernetes vai usar <code>0.0.0.0</code> como o <code>hostIP</code> padrão e <code>TCP</code> como <code>protocol</code> padrão.</p>
<p>Se você precisa de acesso a porta apenas para fins de depuração, pode usar o <a href=/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls>apiserver proxy</a> ou o <a href=/docs/tasks/access-application-cluster/port-forward-access-application-cluster/><code>kubectl port-forward</code></a>.</p>
<p>Se você precisa expor explicitamente a porta de um Pod no nó, considere usar um Service do tipo <a href=/docs/concepts/services-networking/service/#nodeport>NodePort</a> antes de recorrer a <code>hostPort</code>.</p>
</li>
<li>
<p>Evite usar <code>hostNetwork</code> pelos mesmos motivos do <code>hostPort</code>.</p>
</li>
<li>
<p>Use <a href=/docs/concepts/services-networking/service/#headless-services>headless Services</a> (que tem um <code>ClusterIP</code> ou <code>None</code>) para descoberta de serviço quando você não precisar de um balanceador de carga <code>kube-proxy</code>.</p>
</li>
</ul>
<h2 id=usando-labels>Usando Labels</h2>
<ul>
<li>Defina e use <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> que identifiquem <em>atributos semânticos</em> da sua aplicação ou Deployment, como <code>{ app: myapp, tier: frontend, phase: test, deployment: v3 }</code>. Você pode usar essas labels para selecionar os Pods apropriados para outros recursos; por exemplo, um Service que seleciona todos os Pods <code>tier: frontend</code>, ou todos
os componentes de <code>app: myapp</code>. Veja o app <a href=https://github.com/kubernetes/examples/tree/master/guestbook/>guestbook</a> para exemplos dessa abordagem.</li>
</ul>
<p>Um Service pode ser feito para abranger vários Deployments, omitindo labels específicas de lançamento de seu seletor. Quando você
precisar atualizar um serviço em execução sem <em>downtime</em>, use um <a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a>.</p>
<p>Um estado desejado de um objeto é descrito por um Deployment, e se as alterações nesse <em>spec</em> forem <em>aplicadas</em> o controlador
do Deployment altera o estado real para o estado desejado em uma taxa controlada.</p>
<ul>
<li>
<p>Use as <a href=/docs/concepts/overview/working-with-objects/common-labels/>labels comuns do Kubernetes</a> para casos de uso comuns.
Essas labels padronizadas enriquecem os metadados de uma forma que permite que ferramentas, incluindo <code>kubectl</code> e a <a href=/docs/tasks/access-application-cluster/web-ui-dashboard>dashboard</a>, funcionem de uma forma interoperável.</p>
</li>
<li>
<p>Você pode manipular labels para depuração. Como os controladores do Kubernetes (como ReplicaSet) e Services se relacionam com os Pods usando seletor de labels, remover as labels relevantes de um Pod impedirá que ele seja considerado por um controlador ou que
seja atendido pelo tráfego de um Service. Se você remover as labels de um Pod existente, seu controlador criará um novo Pod para
substituí-lo. Essa é uma maneira útil de depurar um Pod anteriormente "ativo" em um ambiente de "quarentena". Para remover ou
alterar labels interativamente, use <a href=/docs/reference/generated/kubectl/kubectl-commands#label><code>kubectl label</code></a>.</p>
</li>
</ul>
<h2 id=imagens-de-contêiner>Imagens de Contêiner</h2>
<p>A <a href=/docs/concepts/containers/images/#updating-images>imagePullPolicy</a> e tag da imagem afetam quando o <a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a> tenta puxar a imagem especificada.</p>
<ul>
<li>
<p><code>imagePullPolicy: IfNotPresent</code>: a imagem é puxada apenas se ainda não estiver presente localmente.</p>
</li>
<li>
<p><code>imagePullPolicy: Always</code>: sempre que o kubelet inicia um contêiner, ele consulta o <em>registry</em> da imagem do contêiner para verificar o resumo de assinatura da imagem. Se o kubelet tiver uma imagem do contêiner com o mesmo resumo de assinatura
armazenado em cache localmente, o kubelet usará a imagem em cache, caso contrário, o kubelet baixa(<em>pulls</em>) a imagem com o resumo de assinatura resolvido, e usa essa imagem para iniciar o contêiner.</p>
</li>
<li>
<p><code>imagePullPolicy</code> é omitido se a tag da imagem é <code>:latest</code> ou se <code>imagePullPolicy</code> é omitido é automaticamente definido como <code>Always</code>. Observe que <em>não</em> será utilizado para <code>ifNotPresent</code>se o valor da tag mudar.</p>
</li>
<li>
<p><code>imagePullPolicy</code> é omitido se uma tag da imagem existe mas não <code>:latest</code>: <code>imagePullPolicy</code> é automaticamente definido como <code>ifNotPresent</code>. Observe que isto <em>não</em> será atualizado para <code>Always</code> se a tag for removida ou alterada para <code>:latest</code>.</p>
</li>
<li>
<p><code>imagePullPolicy: Never</code>: presume-se que a imagem exista localmente. Não é feita nenhuma tentativa de puxar a imagem.</p>
</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Para garantir que seu contêiner sempre use a mesma versão de uma imagem, você pode especificar seu <a href=https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier>resumo de assinatura</a>;
substitua <code>&lt;nome-da-imagem>:&lt;tag></code> por <code>&lt;nome-da-imagem>@&lt;hash></code> (por exemplo, <code>image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2</code>). Esse resumo de assinatura identifica exclusivamente uma versão
específica de uma imagem, então isso nunca vai ser atualizado pelo Kubernetes a menos que você mude o valor do resumo de assinatura da imagem.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Você deve evitar o uso da tag <code>:latest</code> em produção, pois é mais difícil rastrear qual versão da imagem está sendo executada e mais difícil reverter adequadamente.
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> A semântica de cache do provedor de imagem subjacente torna até mesmo <code>imagePullPolicy: Always</code> eficiente, contanto que o registro esteja acessível de forma confiável. Com o Docker, por exemplo, se a imagem já existe, a tentativa de baixar(pull) é rápida porque todas as camadas da imagem são armazenadas em cache e nenhum download de imagem é necessário.
</div>
<h2 id=usando-kubectl>Usando kubectl</h2>
<ul>
<li>
<p>Use <code>kubectl apply -f &lt;directory></code>. Isso procura por configurações do Kubernetes em todos os arquivos <code>.yaml</code>, <code>.yml</code> em <code>&lt;directory></code> e passa isso para <code>apply</code>.</p>
</li>
<li>
<p>Use <em>labels selectors</em> para operações <code>get</code> e <code>delete</code> em vez de nomes de objetos específicos. Consulte as seções sobre <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selectors</a>
e <a href=/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively>usando Labels efetivamente</a>.</p>
</li>
<li>
<p>Use <code>kubectl create deployment</code> e <code>kubectl expose</code> para criar rapidamente Deployments e Services de um único contêiner. Consulte <a href=/docs/tasks/access-application-cluster/service-access-application-cluster/>Use um Service para acessar uma aplicação em um cluster</a> para obter um exemplo.</p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-6b5ccadd699df0904e8e9917c5450c4b>8.2 - ConfigMaps</h1>
<p><p>Um ConfigMap é um objeto da API usado para armazenar dados não-confidenciais em pares chave-valor.
<a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> podem consumir ConfigMaps como variáveis de ambiente, argumentos de linha de comando ou como arquivos de configuração em um <a class=glossary-tooltip title="Um diretório contendo dados, accessível aos contêineres em um pod." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a>.</p></p>
<p>Um ConfigMap ajuda a desacoplar configurações vinculadas ao ambiente das <a class=glossary-tooltip title="Instância armazenada de um contêiner que contém o conjunto de softwares necessários para rodar uma aplicação." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-image" target=_blank aria-label="imagens de contêiner">imagens de contêiner</a>, de modo a tornar aplicações mais facilmente portáveis.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Cuidado:</strong> O ConfigMap não oferece confidencialidade ou encriptação.
Se os dados que você deseja armazenar são confidenciais, utilize
<a class=glossary-tooltip title="Armazena dados sensíveis, como senhas, tokens OAuth e chaves SSH." data-toggle=tooltip data-placement=top href=/pt-br/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a> ao invés de um ConfigMap,
ou utilize ferramentas adicionais (de terceiros) para manter seus dados privados.
</div>
<h2 id=motivação>Motivação</h2>
<p>Utilize um ConfigMap para manter a configuração separada do código da aplicação.</p>
<p>Por exemplo, imagine que você esteja desenvolvendo uma aplicação que pode ser executada
no seu computador local (para desenvolvimento) e na nuvem (para manipular tráfego real).
Você escreve código para ler a variável de ambiente chamada <code>DATABASE_HOST</code>.
No seu ambiente local, você configura essa variável com o valor <code>localhost</code>. Na nuvem, você
configura essa variável para referenciar um <a class=glossary-tooltip title="Uma forma abstrata de expor uma aplicação que está executando em um conjunto de Pods como um serviço de rede." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=serviço>serviço</a>
do Kubernetes que expõe o componente do banco de dados ao seu cluster.
Isto permite que você baixe uma imagem de contêiner que roda na nuvem e depure exatamente
o mesmo código localmente se necessário.</p>
<p>Um ConfigMap não foi planejado para conter grandes quantidades de dados. Os dados armazenados
em um ConfigMap não podem exceder 1 MiB. Se você precisa armazenar configurações que são maiores
que este limite, considere montar um volume ou utilizar um serviço separado de banco de dados
ou de arquivamento de dados.</p>
<h2 id=objeto-configmap>Objeto ConfigMap</h2>
<p>Um ConfigMap é um <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/>objeto</a>
da API que permite o armazenamento de configurações para consumo por outros objetos. Diferentemente
de outros objetos do Kubernetes que contém um campo <code>spec</code>, o ConfigMap contém os campos <code>data</code> e
<code>binaryData</code>. Estes campos aceitam pares chave-valor como valores. Ambos os campos <code>data</code> e <code>binaryData</code>
são opcionais. O campo <code>data</code> foi pensado para conter sequências de bytes UTF-8, enquanto o campo <code>binaryData</code>
foi planejado para conter dados binários em forma de strings codificadas em base64.</p>
<p>É obrigatório que o nome de um ConfigMap seja um
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>subdomínio DNS válido</a>.</p>
<p>Cada chave sob as seções <code>data</code> ou <code>binaryData</code> pode conter quaisquer caracteres alfanuméricos,
<code>-</code>, <code>_</code> e <code>.</code>. As chaves armazenadas na seção <code>data</code> não podem colidir com as chaves armazenadas
na seção <code>binaryData</code>.</p>
<p>A partir da versão v1.19 do Kubernetes, é possível adicionar o campo <code>immutable</code> a uma definição de ConfigMap
para criar um <a href=#configmap-immutable>ConfigMap imutável</a>.</p>
<h2 id=configmaps-e-pods>ConfigMaps e Pods</h2>
<p>Você pode escrever uma <code>spec</code> para um Pod que se refere a um ConfigMap e configurar o(s) contêiner(es)
neste Pod baseados em dados do ConfigMap. O Pod e o ConfigMap devem estar no mesmo
<a class=glossary-tooltip title="Uma abstração utilizada pelo Kubernetes para suportar múltiplos clusters virtuais no mesmo cluster físico." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> A <code>spec</code> de um <a class=glossary-tooltip title="Um pod gerenciado diretamente pelo daemon do kubelet em um nó específico." data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/static-pod/ target=_blank aria-label="Pod estático">Pod estático</a> não pode se referir a um
ConfigMap ou a quaisquer outros objetos da API.
</div>
<p>Exemplo de um ConfigMap que contém algumas chaves com valores avulsos e outras chaves com valores semelhantes
a fragmentos de arquivos de configuração:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># chaves com valores de propriedades; cada chave mapeia para um valor avulso</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>player_initial_lives</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;3&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ui_properties_file_name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># chaves semelhantes a fragmentos de arquivos</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>game.properties</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    enemy.types=aliens,monsters
</span><span style=color:#b44;font-style:italic>    player.maximum-lives=5</span><span style=color:#bbb>    
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>user-interface.properties</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    color.good=purple
</span><span style=color:#b44;font-style:italic>    color.bad=yellow
</span><span style=color:#b44;font-style:italic>    allow.textmode=true</span><span style=color:#bbb>    
</span></code></pre></div><p>Existem quatro formas diferentes para consumo de um ConfigMap na configuração de um
contêiner dentro de um Pod:</p>
<ol>
<li>Dentro de um comando de contêiner e seus argumentos.</li>
<li>Variáveis de ambiente para um contêiner.</li>
<li>Criando um arquivo em um volume somente leitura, para consumo pela aplicação.</li>
<li>Escrevendo código para execução dentro do Pod que utilize a API do Kubernetes para ler um ConfigMap.</li>
</ol>
<p>Os diferentes métodos de consumo oferecem diferentes formas de modelar os dados sendo consumidos.
Para os três primeiros métodos, o <a class=glossary-tooltip title="Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a> utiliza
os dados de um ConfigMap quando o(s) contêiner(es) do Pod são inicializados.</p>
<p>O quarto método envolve escrita de código para leitura do ConfigMap e dos seus dados. No entanto,
como a API do Kubernetes está sendo utilizada diretamente, a aplicação pode solicitar atualizações
sempre que o ConfigMap for alterado e reagir quando isso ocorre. Acessar a API do Kubernetes
diretamente também permite ler ConfigMaps em outros namespaces.</p>
<p>Exemplo de um Pod que utiliza valores do ConfigMap <code>game-demo</code> para configurar um Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>configmap-demo-pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>demo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>alpine<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;sleep&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;3600&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Define as variáveis de ambiente</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PLAYER_INITIAL_LIVES<span style=color:#bbb> </span><span style=color:#080;font-style:italic># Note que aqui a variável está definida em caixa alta,</span><span style=color:#bbb>
</span><span style=color:#bbb>                                     </span><span style=color:#080;font-style:italic># diferente da chave no ConfigMap.</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>configMapKeyRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo          <span style=color:#bbb> </span><span style=color:#080;font-style:italic># O ConfigMap de onde esse valor vem.</span><span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>player_initial_lives<span style=color:#bbb> </span><span style=color:#080;font-style:italic># A chave que deve ser buscada.</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>UI_PROPERTIES_FILE_NAME<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>configMapKeyRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>ui_properties_file_name<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/config&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Volumes são definidos no escopo do Pod, e os pontos de montagem são definidos</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># nos contêineres dentro dos pods.</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Informe o nome do ConfigMap que deseja montar.</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Uma lista de chaves do ConfigMap para serem criadas como arquivos.</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;game.properties&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;game.properties&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>ConfigMaps não diferenciam entre propriedades com valores simples ou valores complexos,
que ocupam várias linhas. O importante é a forma que Pods e outros objetos consomem tais valores.</p>
<p>Neste exemplo, definir um volume e montar ele dentro do contêiner <code>demo</code> no caminho <code>/config</code>
cria dois arquivos: <code>/config/game.properties</code> e <code>/config/user-interface.properties</code>, embora existam
quatro chaves distintas no ConfigMap. Isso se deve ao fato de que a definição do Pod contém uma lista
<code>items</code> na seção <code>volumes</code>.
Se a lista <code>items</code> for omitida, cada chave do ConfigMap torna-se um arquivo cujo nome é a sua chave
correspondente, e quatro arquivos serão criados.</p>
<h2 id=usando-configmaps>Usando ConfigMaps</h2>
<p>ConfigMaps podem ser montados como volumes de dados. ConfigMaps também podem ser utilizados
por outras partes do sistema sem serem diretamente expostos ao Pod. Por exemplo, ConfigMaps
podem conter dados que outras partes do sistema devem usar para configuração.</p>
<p>A forma mais comum de utilização de ConfigMaps é a configuração de contêineres executando em
Pods no mesmo namespace. Você também pode utilizar um ConfigMap separadamente.</p>
<p>Por exemplo, existem <a class=glossary-tooltip title="Recursos que estendem a funcionalidade do Kubernetes." data-toggle=tooltip data-placement=top href=/pt-br/docs/concepts/cluster-administration/addons/ target=_blank aria-label=complementos>complementos</a> ou
<a class=glossary-tooltip title="Um controlador especializado que gerencia um recurso personalizado." data-toggle=tooltip data-placement=top href=/pt-br/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label=operadores>operadores</a> que adaptam seus comportamentos
de acordo com dados de um ConfigMap.</p>
<h3 id=utilizando-configmaps-como-arquivos-em-um-pod>Utilizando ConfigMaps como arquivos em um Pod</h3>
<p>Para consumir um ConfigMap em um volume em um Pod:</p>
<ol>
<li>Crie um ConfigMap ou utilize um ConfigMap existente. Múltiplos Pods
podem referenciar o mesmo ConfigMap.</li>
<li>Modifique sua definição de Pod para adicionar um volume em
<code>.spec.volumes[]</code>. Escolha um nome qualquer para o seu volume, e
referencie o seu objeto ConfigMap no campo
<code>.spec.volumes[].configMap.name</code>.</li>
<li>Adicione um campo <code>.spec.containers[].volumeMounts[]</code> a cada um dos
contêineres que precisam do ConfigMap. Especifique
<code>.spec.containers[].volumeMounts[].readOnly = true</code> e informe no campo
<code>.spec.containers[].volumeMounts[].mountPath</code> um caminho de um diretório
não utilizado onde você deseja que este ConfigMap apareça.</li>
<li>Modifique sua imagem ou linha de comando de modo que o programa procure
por arquivos no diretório especificado no passo anterior. Cada chave no
campo <code>data</code> do ConfigMap será transformado em um nome de arquivo no
diretório especificado por <code>mountPath</code>.</li>
</ol>
<p>Exemplo de um Pod que monta um ConfigMap em um volume:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myconfigmap<span style=color:#bbb>
</span></code></pre></div><p>Cada ConfigMap que você deseja utilizar precisa ser referenciado em
<code>.spec.volumes</code>.</p>
<p>Se houver múltiplos contêineres no Pod, cada contêiner deve ter seu
próprio bloco <code>volumeMounts</code>, mas somente uma instância de <code>.spec.volumes</code>
é necessária por ConfigMap.</p>
<h3 id=configmaps-montados-são-atualizados-automaticamente>ConfigMaps montados são atualizados automaticamente</h3>
<p>Quando um ConfigMap que está sendo consumido em um volume é atualizado, as chaves projetadas são
eventualmente atualizadas também. O Kubelet checa se o ConfigMap montado está atualizado em cada
sincronização periódica.
No entanto, o kubelet utiliza o cache local para buscar o valor atual do ConfigMap.
O tipo de cache é configurável utilizando o campo <code>ConfigMapAndSecretChangeDetectionStrategy</code> na
<a href=/docs/reference/config-api/kubelet-config.v1beta1/>configuração do Kubelet (KubeletConfiguration)</a>.
Um ConfigMap pode ter sua propagação baseada em um <em>watch</em> (comportamento padrão), que é o sistema
de propagação de mudanças incrementais em objetos do Kubernetes; baseado em TTL (<em>time to live</em>,
ou tempo de expiração); ou redirecionando todas as requisições diretamente para o servidor da API.
Como resultado, o tempo decorrido total entre o momento em que o ConfigMap foi atualizado até o momento
quando as novas chaves são projetadas nos Pods pode ser tão longo quanto o tempo de sincronização
do kubelet somado ao tempo de propagação do cache, onde o tempo de propagação do cache depende do
tipo de cache escolhido: o tempo de propagação pode ser igual ao tempo de propagação do <em>watch</em>,
TTL do cache, ou zero, de acordo com cada um dos tipos de cache.</p>
<p>ConfigMaps que são consumidos como variáveis de ambiente não atualizam automaticamente e requerem uma
reinicialização do pod.</p>
<h2 id=configmap-immutable>ConfigMaps imutáveis</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>
<p>A funcionalidade <em>Secrets e ConfigMaps imutáveis</em> do Kubernetes fornece uma opção
para marcar Secrets e ConfigMaps individuais como imutáveis. Para clusters que utilizam
ConfigMaps extensivamente (ao menos centenas de milhares de mapeamentos únicos de
ConfigMaps para Pods), prevenir alterações dos seus dados traz as seguintes vantagens:</p>
<ul>
<li>protege de atualizações acidentais ou indesejadas que podem causar disrupção na execução
de aplicações</li>
<li>melhora o desempenho do cluster através do fechamento de <em>watches</em> de ConfigMaps marcados
como imutáveis, diminuindo significativamente a carga no kube-apiserver</li>
</ul>
<p>Essa funcionalidade é controlada pelo <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>ImmutableEphemeralVolumes</code>. É possível criar um ConfigMap imutável adicionando o campo
<code>immutable</code> e marcando seu valor com <code>true</code>.
Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>immutable</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></code></pre></div><p>Após um ConfigMap ser marcado como imutável, <em>não</em> é possível reverter a alteração, nem
alterar o conteúdo dos campos <code>data</code> ou <code>binaryData</code>. É possível apenas apagar e recriar
o ConfigMap. Como Pods existentes que consomem o ConfigMap em questão mantém um ponto de
montagem que continuará referenciando este objeto após a remoção, é recomendado recriar
estes pods.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia sobre <a href=/docs/concepts/configuration/secret/>Secrets</a> (em inglês).</li>
<li>Leia <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>Configure a Pod to Use a ConfigMap</a> (em inglês).</li>
<li>Leia <a href=https://12factor.net/>The Twelve-Factor App</a> (em inglês) para entender a motivação da separação de código
e configuração.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-e511ed821ada65d0053341dbd8ad2bb5>8.3 - Secrets</h1>
<p>Um Secret é um objeto que contém uma pequena quantidade de informação sensível,
como senhas, tokens ou chaves. Este tipo de informação poderia, em outras
circunstâncias, ser colocada diretamente em uma configuração de
<a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> ou em uma
<a class=glossary-tooltip title="Instância armazenada de um contêiner que contém o conjunto de softwares necessários para rodar uma aplicação." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-image" target=_blank aria-label="imagem de contêiner">imagem de contêiner</a>. O uso de
Secrets evita que você tenha de incluir dados confidenciais no seu código.</p>
<p>Secrets podem ser criados de forma independente dos Pods que os consomem. Isto
reduz o risco de que o Secret e seus dados sejam expostos durante o processo de
criação, visualização e edição ou atualização de Pods. O Kubernetes e as
aplicações que rodam no seu cluster podem também tomar outras precauções com
Secrets, como por exemplo evitar a escrita de dados confidenciais em local de
armazenamento persistente (não-volátil).</p>
<p>Secrets são semelhantes a
<a class=glossary-tooltip title="Um objeto da API usado para armazenar dados não-confidenciais em pares chave-valor. Pode ser consumido como variáveis de ambiente, argumentos de linha de comando, ou arquivos de configuração em um volume." data-toggle=tooltip data-placement=top href=/pt-br/docs/concepts/configuration/configmap target=_blank aria-label=ConfigMaps>ConfigMaps</a>, mas foram
especificamente projetados para conter dados confidenciais.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Cuidado:</strong> <p>Os Secrets do Kubernetes são, por padrão, gravados não-encriptados no sistema
de armazenamento de dados utilizado pelo servidor da API (etcd). Qualquer pessoa
com acesso à API ou ao etcd consegue obter ou modificar um Secret.
Além disso, qualquer pessoa que possui autorização para criar Pods em um namespace
consegue utilizar este privilégio para ler qualquer Secret naquele namespace. Isso
inclui acesso indireto, como por exemplo a permissão para criar Deployments.</p>
<p>Para utilizar Secrets de forma segura, siga pelo menos as instruções abaixo:</p>
<ol>
<li><a href=/docs/tasks/administer-cluster/encrypt-data/>Habilite encriptação em disco</a> para Secrets.</li>
<li>Habilite ou configure <a href=/docs/reference/access-authn-authz/authorization/>regras de RBAC</a>
que restrinjam o acesso de leitura a Secrets (incluindo acesso indireto).</li>
<li>Quando apropriado, utilize mecanismos como RBAC para limitar quais perfis e
usuários possuem permissão para criar novos Secrets ou substituir Secrets
existentes.</li>
</ol>
</div>
<h2 id=visão-geral-de-secrets>Visão Geral de Secrets</h2>
<p>Para utilizar um Secret, um Pod precisa referenciar o Secret.
Um Secret pode ser utilizado em um Pod de três maneiras diferentes:</p>
<ul>
<li>Como um <a href=#using-secrets-as-files-from-a-pod>arquivo</a> em um
<a class=glossary-tooltip title="Um diretório contendo dados, accessível aos contêineres em um pod." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a> montado em um ou mais de
seus contêineres.</li>
<li>Como uma <a href=#using-secrets-as-environment-variables>variável de ambiente</a> em um
contêiner.</li>
<li>Pelo <a href=#using-imagepullsecrets>kubelet ao baixar imagens de contêiner</a> para o
Pod.</li>
</ul>
<p>A camada de gerenciamento do Kubernetes também utiliza Secrets. Por exemplo,
os <a href=#bootstrap-token-secrets>Secrets de tokens de autoinicialização</a> são um
mecanismo que auxilia a automação do registro de nós.</p>
<p>O nome de um Secret deve ser um <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>subdomínio DNS válido</a>.
Você pode especificar o campo <code>data</code> e/ou o campo <code>stringData</code> na criação de um
arquivo de configuração de um Secret. Ambos os campos <code>data</code> e <code>stringData</code> são
opcionais. Os valores das chaves no campo <code>data</code> devem ser strings codificadas
no formato base64. Se a conversão para base64 não for desejável, você pode
optar por informar os dados no campo <code>stringData</code>, que aceita strings arbitrárias
como valores.</p>
<p>As chaves dos campos <code>data</code> e <code>stringData</code> devem consistir de caracteres
alfanuméricos, <code>-</code>, <code>_</code>, ou <code>.</code>. Todos os pares chave-valor no campo <code>stringData</code>
são internamente combinados com os dados do campo <code>data</code>. Se uma chave aparece
em ambos os campos, o valor informado no campo <code>stringData</code> toma a precedência.</p>
<h2 id=secret-types>Tipos de Secrets</h2>
<p>Ao criar um Secret, você pode especificar o seu tipo utilizando o campo <code>type</code>
do objeto Secret, ou algumas opções de linha de comando equivalentes no comando
<code>kubectl</code>, quando disponíveis. O campo <code>type</code> de um Secret é utilizado para
facilitar a manipulação programática de diferentes tipos de dados confidenciais.</p>
<p>O Kubernetes oferece vários tipos embutidos de Secret para casos de uso comuns.
Estes tipos variam em termos de validações efetuadas e limitações que o
Kubernetes impõe neles.</p>
<table>
<thead>
<tr>
<th>Tipo embutido</th>
<th>Caso de uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Opaque</code></td>
<td>dados arbitrários definidos pelo usuário</td>
</tr>
<tr>
<td><code>kubernetes.io/service-account-token</code></td>
<td>token de service account (conta de serviço)</td>
</tr>
<tr>
<td><code>kubernetes.io/dockercfg</code></td>
<td>arquivo <code>~/.dockercfg</code> serializado</td>
</tr>
<tr>
<td><code>kubernetes.io/dockerconfigjson</code></td>
<td>arquivo <code>~/.docker/config.json</code> serializado</td>
</tr>
<tr>
<td><code>kubernetes.io/basic-auth</code></td>
<td>credenciais para autenticação básica (basic auth)</td>
</tr>
<tr>
<td><code>kubernetes.io/ssh-auth</code></td>
<td>credenciais para autenticação SSH</td>
</tr>
<tr>
<td><code>kubernetes.io/tls</code></td>
<td>dados para um cliente ou servidor TLS</td>
</tr>
<tr>
<td><code>bootstrap.kubernetes.io/token</code></td>
<td>dados de token de autoinicialização</td>
</tr>
</tbody>
</table>
<p>Você pode definir e utilizar seu próprio tipo de Secret definindo o valor do
campo <code>type</code> como uma string não-nula em um objeto Secret. Uma string em branco
é tratada como o tipo <code>Opaque</code>. O Kubernetes não restringe nomes de tipos. No
entanto, quando tipos embutidos são utilizados, você precisa atender a todos os
requisitos daquele tipo.</p>
<h3 id=secrets-tipo-opaque>Secrets tipo Opaque</h3>
<p><code>Opaque</code> é o tipo predefinido de Secret quando o campo <code>type</code> não é informado
em um arquivo de configuração. Quando um Secret é criado usando o comando
<code>kubectl</code>, você deve usar o subcomando <code>generic</code> para indicar que um Secret é
do tipo <code>Opaque</code>. Por exemplo, o comando a seguir cria um Secret vazio do tipo
<code>Opaque</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret generic empty-secret
kubectl get secret empty-secret
</code></pre></div><p>O resultado será semelhante ao abaixo:</p>
<pre><code>NAME           TYPE     DATA   AGE
empty-secret   Opaque   0      2m6s
</code></pre><p>A coluna <code>DATA</code> demonstra a quantidade de dados armazenados no Secret. Neste
caso, <code>0</code> significa que este objeto Secret está vazio.</p>
<h3 id=secrets-de-token-de-service-account-conta-de-serviço>Secrets de token de service account (conta de serviço)</h3>
<p>Secrets do tipo <code>kubernetes.io/service-account-token</code> são utilizados para
armazenar um token que identifica uma service account (conta de serviço). Ao
utilizar este tipo de Secret, você deve garantir que a anotação
<code>kubernetes.io/service-account.name</code> contém um nome de uma service account
existente. Um controlador do Kubernetes preenche outros campos, como por exemplo
a anotação <code>kubernetes.io/service-account.uid</code> e a chave <code>token</code> no campo <code>data</code>
com o conteúdo do token.</p>
<p>O exemplo de configuração abaixo declara um Secret de token de service account:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-sa-sample<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/service-account-name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;sa-name&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/service-account-token<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Você pode incluir pares chave-valor adicionais, da mesma forma que faria com</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Secrets do tipo Opaque</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extra</span>:<span style=color:#bbb> </span>YmFyCg==<span style=color:#bbb>
</span></code></pre></div><p>Ao criar um <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, o Kubernetes
automaticamente cria um Secret de service account e automaticamente atualiza o
seu Pod para utilizar este Secret. O Secret de token de service account contém
credenciais para acessar a API.</p>
<p>A criação automática e o uso de credenciais de API podem ser desativados se
desejado. Porém, se tudo que você necessita é poder acessar o servidor da API
de forma segura, este é o processo recomendado.</p>
<p>Veja a documentação de
<a href=/docs/tasks/configure-pod-container/configure-service-account/>ServiceAccount</a>
para mais informações sobre o funcionamento de service accounts. Você pode
verificar também os campos <code>automountServiceAccountToken</code> e <code>serviceAccountName</code>
do <a href=/docs/reference/generated/kubernetes-api/v1.23/#pod-v1-core><code>Pod</code></a>
para mais informações sobre como referenciar service accounts em Pods.</p>
<h3 id=secrets-de-configuração-do-docker>Secrets de configuração do Docker</h3>
<p>Você pode utilizar um dos tipos abaixo para criar um Secret que armazena
credenciais para accesso a um registro de contêineres compatível com Docker
para busca de imagens:</p>
<ul>
<li><code>kubernetes.io/dockercfg</code></li>
<li><code>kubernetes.io/dockerconfigjson</code></li>
</ul>
<p>O tipo <code>kubernetes.io/dockercfg</code> é reservado para armazenamento de um arquivo
<code>~/.dockercfg</code> serializado. Este arquivo é o formato legado para configuração
do utilitário de linha de comando do Docker. Ao utilizar este tipo de Secret,
é preciso garantir que o campo <code>data</code> contém uma chave <code>.dockercfg</code> cujo valor
é o conteúdo do arquivo <code>~/.dockercfg</code> codificado no formato base64.</p>
<p>O tipo <code>kubernetes.io/dockerconfigjson</code> foi projetado para armazenamento de um
conteúdo JSON serializado que obedece às mesmas regras de formato que o arquivo
<code>~/.docker/config.json</code>. Este arquivo é um formato mais moderno para o conteúdo
do arquivo <code>~/.dockercfg</code>. Ao utilizar este tipo de Secret, o conteúdo do campo
<code>data</code> deve conter uma chave <code>.dockerconfigjson</code> em que o conteúdo do arquivo
<code>~/.docker/config.json</code> é fornecido codificado no formato base64.</p>
<p>Um exemplo de um Secret do tipo <code>kubernetes.io/dockercfg</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-dockercfg<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/dockercfg<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>.dockercfg</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span><span style=color:#b44>&#34;&lt;base64 encoded ~/.dockercfg file&gt;&#34;</span><span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Se você não desejar fazer a codificação em formato base64, você pode utilizar o
campo <code>stringData</code> como alternativa.
</div>
<p>Ao criar estes tipos de Secret utilizando um manifesto (arquivo YAML), o servidor
da API verifica se a chave esperada existe no campo <code>data</code> e se o valor fornecido
pode ser interpretado como um conteúdo JSON válido. O servidor da API não verifica
se o conteúdo informado é realmente um arquivo de configuração do Docker.</p>
<p>Quando você não tem um arquivo de configuração do Docker, ou quer utilizar o
comando <code>kubectl</code> para criar um Secret de registro de contêineres compatível
com o Docker, você pode executar:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret docker-registry secret-tiger-docker <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --docker-username<span style=color:#666>=</span>tiger <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --docker-password<span style=color:#666>=</span>pass113 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --docker-email<span style=color:#666>=</span>tiger@acme.com <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --docker-server<span style=color:#666>=</span>my-registry.example:5000
</code></pre></div><p>Esse comando cria um secret do tipo <code>kubernetes.io/dockerconfigjson</code>, cujo
conteúdo é semelhante ao exemplo abaixo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
    <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
    <span style=color:green;font-weight:700>&#34;data&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;.dockerconfigjson&#34;</span>: <span style=color:#b44>&#34;eyJhdXRocyI6eyJteS1yZWdpc3RyeTo1MDAwIjp7InVzZXJuYW1lIjoidGlnZXIiLCJwYXNzd29yZCI6InBhc3MxMTMiLCJlbWFpbCI6InRpZ2VyQGFjbWUuY29tIiwiYXV0aCI6ImRHbG5aWEk2Y0dGemN6RXhNdz09In19fQ==&#34;</span>
    },
    <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Secret&#34;</span>,
    <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;creationTimestamp&#34;</span>: <span style=color:#b44>&#34;2021-07-01T07:30:59Z&#34;</span>,
        <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;secret-tiger-docker&#34;</span>,
        <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>,
        <span style=color:green;font-weight:700>&#34;resourceVersion&#34;</span>: <span style=color:#b44>&#34;566718&#34;</span>,
        <span style=color:green;font-weight:700>&#34;uid&#34;</span>: <span style=color:#b44>&#34;e15c1d7b-9071-4100-8681-f3a7a2ce89ca&#34;</span>
    },
    <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;kubernetes.io/dockerconfigjson&#34;</span>
}
</code></pre></div><p>Se você extrair o conteúdo da chave <code>.dockerconfigjson</code>, presente no campo
<code>data</code>, e decodificá-lo do formato base64, você irá obter o objeto JSON abaixo,
que é uma configuração válida do Docker criada automaticamente:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;auths&#34;</span>:{
    <span style=color:green;font-weight:700>&#34;my-registry:5000&#34;</span>:{
      <span style=color:green;font-weight:700>&#34;username&#34;</span>:<span style=color:#b44>&#34;tiger&#34;</span>,
      <span style=color:green;font-weight:700>&#34;password&#34;</span>:<span style=color:#b44>&#34;pass113&#34;</span>,
      <span style=color:green;font-weight:700>&#34;email&#34;</span>:<span style=color:#b44>&#34;tiger@acme.com&#34;</span>,
      <span style=color:green;font-weight:700>&#34;auth&#34;</span>:<span style=color:#b44>&#34;dGlnZXI6cGFzczExMw==&#34;</span>
    }
  }
}
</code></pre></div><h3 id=secret-de-autenticação-básica>Secret de autenticação básica</h3>
<p>O tipo <code>kubernetes.io/basic-auth</code> é fornecido para armazenar credenciais
necessárias para autenticação básica. Ao utilizar este tipo de Secret, o campo
<code>data</code> do Secret deve conter as duas chaves abaixo:</p>
<ul>
<li><code>username</code>: o usuário utilizado para autenticação;</li>
<li><code>password</code>: a senha ou token para autenticação.</li>
</ul>
<p>Ambos os valores para estas duas chaves são textos codificados em formato base64.
Você pode fornecer os valores como texto simples utilizando o campo <code>stringData</code>
na criação do Secret.</p>
<p>O arquivo YAML abaixo é um exemplo de configuração para um Secret de autenticação
básica:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-basic-auth<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/basic-auth<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>stringData</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>admin<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>password</span>:<span style=color:#bbb> </span>t0p-Secret<span style=color:#bbb>
</span></code></pre></div><p>O tipo de autenticação básica é fornecido unicamente por conveniência. Você pode
criar um Secret do tipo <code>Opaque</code> utilizado para autenticação básica. No entanto,
utilizar o tipo embutido de Secret auxilia a unificação dos formatos das suas
credenciais. O tipo embutido também fornece verificação de presença das chaves
requeridas pelo servidor da API.</p>
<h3 id=secret-de-autenticação-ssh>Secret de autenticação SSH</h3>
<p>O tipo embutido <code>kubernetes.io/ssh-auth</code> é fornecido para armazenamento de dados
utilizados em autenticação SSH. Ao utilizar este tipo de Secret, você deve
especificar um par de chave-valor <code>ssh-privatekey</code> no campo <code>data</code> ou no campo
<code>stringData</code> com a credencial SSH a ser utilizada.</p>
<p>O YAML abaixo é um exemplo de configuração para um Secret de autenticação SSH:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-ssh-auth<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/ssh-auth<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># os dados estão abreviados neste exemplo</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ssh-privatekey</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>     </span><span style=color:#bbb>     </span>MIIEpQIBAAKCAQEAulqb/Y ...<span style=color:#bbb>
</span></code></pre></div><p>O Secret de autenticação SSH é fornecido apenas para a conveniência do usuário.
Você pode criar um Secret do tipo <code>Opaque</code> para credentials utilizadas para
autenticação SSH. No entanto, a utilização do tipo embutido auxilia na
unificação dos formatos das suas credenciais e o servidor da API fornece
verificação dos campos requeridos em uma configuração de Secret.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Cuidado:</strong> Chaves privadas SSH não estabelecem, por si só, uma comunicação confiável
entre um cliente SSH e um servidor. Uma forma secundária de estabelecer
confiança é necessária para mitigar ataques "machine-in-the-middle", como
por exemplo um arquivo <code>known_hosts</code> adicionado a um ConfigMap.
</div>
<h3 id=secrets-tls>Secrets TLS</h3>
<p>O Kubernetes fornece o tipo embutido de Secret <code>kubernetes.io/tls</code> para
armazenamento de um certificado e sua chave associada que são tipicamente
utilizados para TLS. Estes dados são utilizados primariamente para a
finalização TLS do recurso Ingress, mas podem ser utilizados com outros
recursos ou diretamente por uma carga de trabalho. Ao utilizar este tipo de
Secret, as chaves <code>tls.key</code> e <code>tls.crt</code> devem ser informadas no campo <code>data</code>
(ou <code>stringData</code>) da configuração do Secret, embora o servidor da API não
valide o conteúdo de cada uma destas chaves.</p>
<p>O YAML a seguir tem um exemplo de configuração para um Secret TLS:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-tls<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/tls<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># os dados estão abreviados neste exemplo</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tls.crt</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>MIIC2DCCAcCgAwIBAgIBATANBgkqh ...<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tls.key</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...<span style=color:#bbb>
</span></code></pre></div><p>O tipo TLS é fornecido para a conveniência do usuário. Você pode criar um
Secret do tipo <code>Opaque</code> para credenciais utilizadas para o servidor e/ou
cliente TLS. No entanto, a utilização do tipo embutido auxilia a manter a
consistência dos formatos de Secret no seu projeto; o servidor da API
valida se os campos requeridos estão presentes na configuração do Secret.</p>
<p>Ao criar um Secret TLS utilizando a ferramenta de linha de comando <code>kubectl</code>,
você pode utilizar o subcomando <code>tls</code> conforme demonstrado no exemplo abaixo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret tls my-tls-secret <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --cert<span style=color:#666>=</span>path/to/cert/file  <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --key<span style=color:#666>=</span>path/to/key/file
</code></pre></div><p>O par de chaves pública/privada deve ser criado separadamente. O certificado
de chave pública a ser utilizado no argumento <code>--cert</code> deve ser codificado em
formato .PEM (formato DER codificado em texto base64) e deve corresponder à
chave privada fornecida no argumento <code>--key</code>.
A chave privada deve estar no formato de chave privada PEM não-encriptado. Em
ambos os casos, as linhas inicial e final do formato PEM (por exemplo,
<code>--------BEGIN CERTIFICATE-----</code> e <code>-------END CERTIFICATE----</code> para um
certificado) <em>não</em> são incluídas.</p>
<h3 id=bootstrap-token-secrets>Secret de token de autoinicialização</h3>
<p>Um Secret de token de autoinicialização pode ser criado especificando o tipo de
um Secret explicitamente com o valor <code>bootstrap.kubernetes.io/token</code>. Este tipo
de Secret é projetado para tokens utilizados durante o processo de inicialização
de nós. Este tipo de Secret armazena tokens utilizados para assinar ConfigMaps
conhecidos.</p>
<p>Um Secret de token de autoinicialização é normalmente criado no namespace
<code>kube-system</code> e nomeado na forma <code>bootstrap-token-&lt;id-do-token></code>, onde
<code>&lt;id-do-token></code> é um texto com 6 caracteres contendo a identificação do token.</p>
<p>No formato de manifesto do Kubernetes, um Secret de token de autoinicialização
se assemelha ao exemplo abaixo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>bootstrap-token-5emitj<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>bootstrap.kubernetes.io/token<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>auth-extra-groups</span>:<span style=color:#bbb> </span>c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>expiration</span>:<span style=color:#bbb> </span>MjAyMC0wOS0xM1QwNDozOToxMFo=<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>token-id</span>:<span style=color:#bbb> </span>NWVtaXRq<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>token-secret</span>:<span style=color:#bbb> </span>a3E0Z2lodnN6emduMXAwcg==<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>usage-bootstrap-authentication</span>:<span style=color:#bbb> </span>dHJ1ZQ==<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>usage-bootstrap-signing</span>:<span style=color:#bbb> </span>dHJ1ZQ==<span style=color:#bbb>
</span></code></pre></div><p>Um Secret do tipo token de autoinicialização possui as seguintes chaves no campo
<code>data</code>:</p>
<ul>
<li><code>token-id</code>: Uma string com 6 caracteres aleatórios como identificador do
token. Requerido.</li>
<li><code>token-secret</code>: Uma string de 16 caracteres aleatórios como o conteúdo do
token. Requerido.</li>
<li><code>description</code>: Uma string contendo uma descrição do propósito para o qual este
token é utilizado. Opcional.</li>
<li><code>expiration</code>: Um horário absoluto UTC no formato RFC3339 especificando quando
o token deve expirar. Opcional.</li>
<li><code>usage-bootstrap-&lt;usage></code>: Um conjunto de flags booleanas indicando outros
usos para este token de autoinicialização.</li>
<li><code>auth-extra-groups</code>: Uma lista separada por vírgulas de nomes de grupos que
serão autenticados adicionalmente, além do grupo <code>system:bootstrappers</code>.</li>
</ul>
<p>O YAML acima pode parecer confuso, já que os valores estão todos codificados em
formato base64. Você pode criar o mesmo Secret utilizando este YAML:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Observe como o Secret é nomeado</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>bootstrap-token-5emitj<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Um Secret de token de inicialização geralmente fica armazenado no namespace</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># kube-system</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>bootstrap.kubernetes.io/token<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>stringData</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>auth-extra-groups</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;system:bootstrappers:kubeadm:default-node-token&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>expiration</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2020-09-13T04:39:10Z&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Esta identificação de token é utilizada no nome</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>token-id</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5emitj&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>token-secret</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kq4gihvszzgn1p0r&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Este token pode ser utilizado para autenticação.</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>usage-bootstrap-authentication</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># e pode ser utilizado para assinaturas</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>usage-bootstrap-signing</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></code></pre></div><h2 id=criando-um-secret>Criando um Secret</h2>
<p>Há várias formas diferentes de criar um Secret:</p>
<ul>
<li><a href=/pt-br/docs/tasks/configmap-secret/managing-secret-using-kubectl/>criar um Secret utilizando o comando <code>kubectl</code></a></li>
<li><a href=/pt-br/docs/tasks/configmap-secret/managing-secret-using-config-file/>criar um Secret a partir de um arquivo de configuração</a></li>
<li><a href=/pt-br/docs/tasks/configmap-secret/managing-secret-using-kustomize/>criar um Secret utilizando a ferramenta kustomize</a></li>
</ul>
<h2 id=editando-um-secret>Editando um Secret</h2>
<p>Um Secret existente no cluster pode ser editado com o seguinte comando:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit secrets mysecret
</code></pre></div><p>Este comando abrirá o editor padrão configurado e permitirá a modificação dos
valores codificados em base64 no campo <code>data</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#080;font-style:italic># Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># and an empty file will abort the edit. If an error occurs while saving this file will be</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># reopened with the relevant failures.</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>YWRtaW4=<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>password</span>:<span style=color:#bbb> </span>MWYyZDFlMmU2N2Rm<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubectl.kubernetes.io/last-applied-configuration</span>:<span style=color:#bbb> </span>{<span style=color:#bbb> </span>... }<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>creationTimestamp</span>:<span style=color:#bbb> </span>2016-01-22T18:41:56Z<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resourceVersion</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;164619&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>uid</span>:<span style=color:#bbb> </span>cfee02d6-c137-11e5-8d73-42010af00002<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Opaque<span style=color:#bbb>
</span></code></pre></div><h2 id=utilizando-secrets>Utilizando Secrets</h2>
<p>Secrets podem ser montados como volumes de dados ou expostos como
<a class=glossary-tooltip title="Variáveis de ambiente de contêineres são pares nome=valor que trazem informações úteis para os contêineres rodando dentro de um Pod." data-toggle=tooltip data-placement=top href=/pt-br/docs/concepts/containers/container-environment/ target=_blank aria-label="variáveis de ambiente">variáveis de ambiente</a>
para serem utilizados num container de um Pod. Secrets também podem ser
utilizados por outras partes do sistema, sem serem diretamente expostos ao Pod.
Por exemplo, Secrets podem conter credenciais que outras partes do sistema devem
utilizar para interagir com sistemas externos no lugar do usuário.</p>
<h3 id=using-secrets-as-files-from-a-pod>Utilizando Secrets como arquivos em um Pod</h3>
<p>Para consumir um Secret em um volume em um Pod:</p>
<ol>
<li>Crie um Secret ou utilize um previamente existente. Múltiplos Pods podem
referenciar o mesmo secret.</li>
<li>Modifique sua definição de Pod para adicionar um volume na lista
<code>.spec.volumes[]</code>. Escolha um nome qualquer para o seu volume e adicione um
campo <code>.spec.volumes[].secret.secretName</code> com o mesmo valor do seu objeto
Secret.</li>
<li>Adicione um ponto de montagem de volume à lista
<code>.spec.containers[].volumeMounts[]</code> de cada contêiner que requer o Secret.
Especifique <code>.spec.containers[].volumeMounts[].readOnly = true</code> e especifique o
valor do campo <code>.spec.containers[].volumeMounts[].mountPath</code> com o nome de um
diretório não utilizado onde você deseja que os Secrets apareçam.</li>
<li>Modifique sua imagem ou linha de comando de modo que o programa procure por
arquivos naquele diretório. Cada chave no campo <code>data</code> se torna um nome de
arquivo no diretório especificado em <code>mountPath</code>.</li>
</ol>
<p>Este é um exemplo de Pod que monta um Secret em um volume:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></code></pre></div><p>Cada Secret que você deseja utilizar deve ser referenciado na lista
<code>.spec.volumes</code>.</p>
<p>Se existirem múltiplos contêineres em um Pod, cada um dos contêineres necessitará
seu próprio bloco <code>volumeMounts</code>, mas somente um volume na lista <code>.spec.volumes</code>
é necessário por Secret.</p>
<p>Você pode armazenar vários arquivos em um Secret ou utilizar vários Secrets
distintos, o que for mais conveniente.</p>
<h4 id=projeção-de-chaves-de-secrets-a-caminhos-específicos>Projeção de chaves de Secrets a caminhos específicos</h4>
<p>Você pode também controlar os caminhos dentro do volume onde as chaves do Secret
são projetadas. Você pode utilizar o campo <code>.spec.volumes[].secret.items</code> para
mudar o caminho de destino de cada chave:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span></code></pre></div><p>Neste caso:</p>
<ul>
<li>O valor da chave <code>username</code> é armazenado no arquivo
<code>/etc/foo/my-group/my-username</code> ao invés de <code>/etc/foo/username</code>.</li>
<li>O valor da chave <code>password</code> não é projetado no sistema de arquivos.</li>
</ul>
<p>Se <code>.spec.volumes[].secret.items</code> for utilizado, somente chaves especificadas
na lista <code>items</code> são projetadas. Para consumir todas as chaves do Secret, deve
haver um item para cada chave no campo <code>items</code>. Todas as chaves listadas precisam
existir no Secret correspondente. Caso contrário, o volume não é criado.</p>
<h4 id=permissões-de-arquivos-de-secret>Permissões de arquivos de Secret</h4>
<p>Você pode trocar os bits de permissão de uma chave avulsa de Secret.
Se nenhuma permissão for especificada, <code>0644</code> é utilizado por padrão.
Você pode também especificar uma permissão padrão para o volume inteiro de
Secret e sobrescrever esta permissão por chave, se necessário.</p>
<p>Por exemplo, você pode especificar uma permissão padrão da seguinte maneira:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>defaultMode</span>:<span style=color:#bbb> </span><span style=color:#666>0400</span><span style=color:#bbb>
</span></code></pre></div><p>Dessa forma, o Secret será montado em <code>/etc/foo</code> e todos os arquivos criados
no volume terão a permissão <code>0400</code>.</p>
<p>Note que a especificação JSON não suporta notação octal. Neste caso, utilize o
valor 256 para permissões equivalentes a 0400. Se você utilizar YAML ao invés
de JSON para o Pod, você pode utilizar notação octal para especificar permissões
de uma forma mais natural.</p>
<p>Perceba que se você acessar o Pod com <code>kubectl exec</code>, você precisará seguir o
vínculo simbólico para encontrar a permissão esperada. Por exemplo,</p>
<p>Verifique as permissões do arquivo de Secret no pod.</p>
<pre><code>kubectl exec mypod -it sh

cd /etc/foo
ls -l
</code></pre><p>O resultado é semelhante ao abaixo:</p>
<pre><code>total 0
lrwxrwxrwx 1 root root 15 May 18 00:18 password -&gt; ..data/password
lrwxrwxrwx 1 root root 15 May 18 00:18 username -&gt; ..data/username
</code></pre><p>Siga o vínculo simbólico para encontrar a permissão correta do arquivo.</p>
<pre><code>cd /etc/foo/..data
ls -l
</code></pre><p>O resultado é semelhante ao abaixo:</p>
<pre><code>total 8
-r-------- 1 root root 12 May 18 00:18 password
-r-------- 1 root root  5 May 18 00:18 username
</code></pre><p>Você pode também utilizar mapeamento, como no exemplo anterior, e especificar
permissões diferentes para arquivos diferentes conforme abaixo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>mode</span>:<span style=color:#bbb> </span><span style=color:#666>0777</span><span style=color:#bbb>
</span></code></pre></div><p>Neste caso, o arquivo resultante em <code>/etc/foo/my-group/my-username</code> terá as
permissões <code>0777</code>. Se você utilizar JSON, devido às limitações do formato,
você precisará informar as permissões em base decimal, ou o valor <code>511</code> neste
exemplo.</p>
<p>Note que os valores de permissões podem ser exibidos em formato decimal se você
ler essa informação posteriormente.</p>
<h4 id=consumindo-valores-de-secrets-em-volumes>Consumindo valores de Secrets em volumes</h4>
<p>Dentro do contêiner que monta um volume de Secret, as chaves deste Secret
aparecem como arquivos e os valores dos Secrets são decodificados do formato
base64 e armazenados dentro destes arquivos. Ao executar comandos dentro do
contêiner do exemplo anterior, obteremos os seguintes resultados:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ls /etc/foo
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>username
password
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat /etc/foo/username
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>admin
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat /etc/foo/password
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>1f2d1e2e67df
</code></pre><p>A aplicação rodando dentro do contêiner é responsável pela leitura dos Secrets
dentro dos arquivos.</p>
<h4 id=secrets-montados-são-atualizados-automaticamente>Secrets montados são atualizados automaticamente</h4>
<p>Quando um Secret que está sendo consumido a partir de um volume é atualizado, as
chaves projetadas são atualizadas após algum tempo também. O kubelet verifica
se o Secret montado está atualizado a cada sincronização periódica. No entanto,
o kubelet utiliza seu cache local para buscar o valor corrente de um Secret. O
tipo do cache é configurável utilizando o campo <code>ConfigMapAndSecretChangeDetectionStrategy</code>
na estrutura <a href=/docs/reference/config-api/kubelet-config.v1beta1/>KubeletConfiguration</a>.
Um Secret pode ser propagado através de um <em>watch</em> (comportamento padrão), que
é o sistema de propagação de mudanças incrementais em objetos do Kubernetes;
baseado em TTL (<em>time to live</em>, ou tempo de expiração); ou redirecionando todas
as requisições diretamente para o servidor da API.</p>
<p>Como resultado, o tempo decorrido total entre o momento em que o Secret foi
atualizado até o momento em que as novas chaves são projetadas nos Pods pode
ser tão longo quanto o tempo de sincronização do kubelet somado ao tempo de
propagação do cache, onde o tempo de propagação do cache depende do tipo de
cache escolhido: o tempo de propagação pode ser igual ao tempo de propagação
do <em>watch</em>, TTL do cache, ou zero, de acordo com cada um dos tipos de cache.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Um contêiner que utiliza Secrets através de um ponto de montagem com a
propriedade
<a href=/docs/concepts/storage/volumes#using-subpath>subPath</a> não recebe atualizações
deste Secret.
</div>
<h3 id=using-secrets-as-environment-variables>Utilizando Secrets como variáveis de ambiente</h3>
<p>Para utilizar um secret em uma <a class=glossary-tooltip title="Variáveis de ambiente de contêineres são pares nome=valor que trazem informações úteis para os contêineres rodando dentro de um Pod." data-toggle=tooltip data-placement=top href=/pt-br/docs/concepts/containers/container-environment/ target=_blank aria-label="variável de ambiente">variável de ambiente</a>
em um Pod:</p>
<ol>
<li>Crie um Secret ou utilize um já existente. Múltiplos Pods podem referenciar o
mesmo Secret.</li>
<li>Modifique a definição de cada contêiner do Pod em que desejar consumir o
Secret, adicionando uma variável de ambiente para cada uma das chaves que deseja
consumir.
A variável de ambiente que consumir o valor da chave em questão deverá popular o
nome do Secret e a sua chave correspondente no campo
<code>env[].valueFrom.secretKeyRef</code>.</li>
<li>Modifique sua imagem de contêiner ou linha de comando de forma que o programa
busque os valores nas variáveis de ambiente especificadas.</li>
</ol>
<p>Este é um exemplo de um Pod que utiliza Secrets em variáveis de ambiente:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-env-pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mycontainer<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>SECRET_USERNAME<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>secretKeyRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>SECRET_PASSWORD<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>secretKeyRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>password<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></code></pre></div><h4 id=consumindo-valores-de-secret-em-variáveis-de-ambiente>Consumindo valores de Secret em variáveis de ambiente</h4>
<p>Dentro de um contêiner que consome um Secret em variáveis de ambiente, a chave
do Secret aparece como uma variável de ambiente comum, contendo os dados do
Secret decodificados do formato base64. Ao executar comandos no contêiner do
exemplo anterior, obteremos os resultados abaixo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>echo</span> <span style=color:#b8860b>$SECRET_USERNAME</span>
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>admin
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>echo</span> <span style=color:#b8860b>$SECRET_PASSWORD</span>
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>1f2d1e2e67df
</code></pre><h4 id=variáveis-de-ambiente-não-são-atualizadas-após-uma-atualização-no-secret>Variáveis de ambiente não são atualizadas após uma atualização no Secret</h4>
<p>Se um contêiner já consome um Secret em uma variável de ambiente, uma atualização
dos valores do Secret não será refletida no contêiner a menos que o contêiner
seja reiniciado.
Existem ferramentas de terceiros que oferecem reinicializações automáticas
quando Secrets são atualizados.</p>
<h2 id=secret-immutable>Secrets imutáveis</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>
<p>A funcionalidade do Kubernetes <em>Secrets e ConfigMaps imutáveis</em> fornece uma
opção para marcar Secrets e ConfigMaps individuais como imutáveis. Em clusters
que fazem uso extensivo de Secrets (pelo menos dezenas de milhares de montagens
únicas de Secrets em Pods), prevenir alterações aos dados dos Secrets traz as
seguintes vantagens:</p>
<ul>
<li>protege você de alterações acidentais ou indesejadas que poderiam provocar
disrupções na execução de aplicações;</li>
<li>melhora a performance do seu cluster através da redução significativa de carga
no kube-apiserver, devido ao fechamento de <em>watches</em> de Secrets marcados como
imutáveis.</li>
</ul>
<p>Esta funcionalidade é controlada pelo
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>ImmutableEphemeralVolumes</code>, que está habilitado por padrão desde a versão
v1.19. Você pode criar um Secret imutável adicionando o campo <code>immutable</code> com
o valor <code>true</code>. Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>immutable</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Uma vez que um Secret ou ConfigMap seja marcado como imutável, <em>não</em> é mais
possível reverter esta mudança, nem alterar os conteúdos do campo <code>data</code>. Você
pode somente apagar e recriar o Secret. Pods existentes mantém um ponto de
montagem referenciando o Secret removido - é recomendado recriar tais Pods.
</div>
<h3 id=using-imagepullsecrets>Usando <code>imagePullSecrets</code></h3>
<p>O campo <code>imagePullSecrets</code> é uma lista de referências para Secrets no mesmo
namespace. Você pode utilizar a lista <code>imagePullSecrets</code> para enviar Secrets
que contém uma senha para acesso a um registro de contêineres do Docker (ou
outros registros de contêineres) ao kubelet. O kubelet utiliza essa informação
para baixar uma imagem privada no lugar do seu Pod.
Veja a <a href=/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core>API PodSpec</a>
para maiores detalhes sobre o campo <code>imagePullSecrets</code>.</p>
<h4 id=especificando-imagepullsecrets-manualmente>Especificando <code>imagePullSecrets</code> manualmente</h4>
<p>Você pode ler sobre como especificar <code>imagePullSecrets</code> em um Pod na
<a href=/pt-br/docs/concepts/containers/images/#especificando-imagepullsecrets-em-um-pod>documentação de imagens de contêiner</a>.</p>
<h3 id=configurando-imagepullsecrets-para-serem-vinculados-automaticamente>Configurando <code>imagePullSecrets</code> para serem vinculados automaticamente</h3>
<p>Você pode criar manualmente <code>imagePullSecrets</code> e referenciá-los em uma
ServiceAccount. Quaisquer Pods criados com esta ServiceAccount, especificada
explicitamente ou por padrão, têm o campo <code>imagePullSecrets</code> populado com os
mesmos valores existentes na service account.
Veja <a href=/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account>adicionando <code>imagePullSecrets</code> a uma service account</a>
para uma explicação detalhada do processo.</p>
<h2 id=detalhes>Detalhes</h2>
<h3 id=restrições>Restrições</h3>
<p>Referências a Secrets em volumes são validadas para garantir que o objeto
especificado realmente existe e é um objeto do tipo Secret. Portanto, um Secret
precisa ser criado antes de quaisquer Pods que dependam deste.</p>
<p>Objetos Secret residem em um <a class=glossary-tooltip title="Uma abstração utilizada pelo Kubernetes para suportar múltiplos clusters virtuais no mesmo cluster físico." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>.
Secrets podem ser referenciados somente por Pods no mesmo namespace.</p>
<p>Secrets individuais são limitados ao tamanho de 1MiB. Esta limitação ter por
objetivo desencorajar a criação de Secrets muito grandes que poderiam exaurir
a memória do servidor da API e do kubelet. No entanto, a criação de muitos
Secrets pequenos também pode exaurir a memória. Limites mais completos de uso
de memória em função de Secrets é uma funcionalidade prevista para o futuro.</p>
<p>O kubelet suporta apenas o uso de Secrets em Pods onde os Secrets são obtidos
do servidor da API. Isso inclui quaisquer Pods criados usando o comando
<code>kubectl</code>, ou indiretamente através de um controlador de replicação, mas não
inclui Pods criados como resultado das flags <code>--manifest-url</code> e <code>--config</code> do
kubelet, ou a sua API REST (estas são formas incomuns de criar um Pod).
A <code>spec</code> de um <a class=glossary-tooltip title="Um pod gerenciado diretamente pelo daemon do kubelet em um nó específico." data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/static-pod/ target=_blank aria-label="Pod estático">Pod estático</a>
não pode se referir a um Secret ou a qualquer outro objeto da API.</p>
<p>Secrets precisam ser criados antes de serem consumidos em Pods como variáveis de
ambiente, exceto quando são marcados como opcionais. Referências a Secrets que
não existem provocam falhas na inicialização do Pod.</p>
<p>Referências (campo <code>secretKeyRef</code>) a chaves que não existem em um Secret nomeado
provocam falhas na inicialização do Pod.</p>
<p>Secrets utilizados para popular variáveis de ambiente através do campo <code>envFrom</code>
que contém chaves inválidas para utilização como nome de uma variável de ambiente
terão tais chaves ignoradas. O Pod inicializará normalmente. Porém, um evento
será gerado com a razão <code>InvalidVariableNames</code> e a mensagem gerada conterá a lista
de chaves inválidas que foram ignoradas. O exemplo abaixo demonstra um Pod que se
refere ao Secret default/mysecret, contendo duas chaves inválidas: <code>1badkey</code> e
<code>2alsobad</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get events
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>LASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON
0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.
</code></pre><h3 id=interações-do-ciclo-de-vida-entre-secrets-e-pods>Interações do ciclo de vida entre Secrets e Pods</h3>
<p>Quando um Pod é criado através de chamadas à API do Kubernetes, não há validação
da existência de um Secret referenciado. Uma vez que um Pod seja agendado, o
kubelet tentará buscar o valor do Secret. Se o Secret não puder ser encontrado
porque não existe ou porque houve uma falha de comunicação temporária entre o
kubelet e o servidor da API, o kubelet fará novas tentativas periodicamente.
O kubelet irá gerar um evento sobre o Pod, explicando a razão pela qual o Pod
ainda não foi inicializado. Uma vez que o Secret tenha sido encontrado, o
kubelet irá criar e montar um volume contendo este Secret. Nenhum dos contêineres
do Pod irá iniciar até que todos os volumes estejam montados.</p>
<h2 id=casos-de-uso>Casos de uso</h2>
<h3 id=caso-de-uso-como-variáveis-de-ambiente-em-um-contêiner>Caso de uso: Como variáveis de ambiente em um contêiner</h3>
<p>Crie um manifesto de Secret</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Opaque<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>USER_NAME</span>:<span style=color:#bbb> </span>YWRtaW4=<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>PASSWORD</span>:<span style=color:#bbb> </span>MWYyZDFlMmU2N2Rm<span style=color:#bbb>
</span></code></pre></div><p>Crie o Secret no seu cluster:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f mysecret.yaml
</code></pre></div><p>Utilize <code>envFrom</code> para definir todos os dados do Secret como variáveis de
ambiente do contêiner. Cada chave do Secret se torna o nome de uma variável de
ambiente no Pod.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-test-pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/busybox<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;/bin/sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;env&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>envFrom</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>secretRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></code></pre></div><h3 id=caso-de-uso-pod-com-chaves-ssh>Caso de uso: Pod com chaves SSH</h3>
<p>Crie um Secret contendo chaves SSH:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret generic ssh-key-secret --from-file<span style=color:#666>=</span>ssh-privatekey<span style=color:#666>=</span>/path/to/.ssh/id_rsa --from-file<span style=color:#666>=</span>ssh-publickey<span style=color:#666>=</span>/path/to/.ssh/id_rsa.pub
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>secret &quot;ssh-key-secret&quot; created
</code></pre><p>Você também pode criar um manifesto <code>kustomization.yaml</code> com um campo
<code>secretGenerator</code> contendo chaves SSH.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Cuidado:</strong> Analise cuidadosamente antes de enviar suas próprias chaves SSH: outros usuários
do cluster podem ter acesso a este Secret. Utilize uma service account que você
deseje que seja acessível a todos os usuários com os quais você compartilha o
cluster do Kubernetes em questão. Desse modo, você pode revogar esta service
account caso os usuários sejam comprometidos.
</div>
<p>Agora você pode criar um Pod que referencia o Secret com a chave SSH e consome-o
em um volume:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-test-pod<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-test<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>ssh-key-secret<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ssh-test-container<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mySshImage<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Ao rodar o comando do contêiner, as partes da chave estarão disponíveis em:</p>
<pre><code>/etc/secret-volume/ssh-publickey
/etc/secret-volume/ssh-privatekey
</code></pre><p>O contêiner então pode utilizar os dados do secret para estabelecer uma conexão
SSH.</p>
<h3 id=caso-de-uso-pods-com-credenciais-de-ambientes-de-produção-ou-testes>Caso de uso: Pods com credenciais de ambientes de produção ou testes</h3>
<p>Este exemplo ilustra um Pod que consome um Secret contendo credenciais de um
ambiente de produção e outro Pod que consome um Secret contendo credenciais de
um ambiente de testes.</p>
<p>Você pode criar um manifesto <code>kustomization.yaml</code> com um <code>secretGenerator</code> ou
rodar <code>kubectl create secret</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret generic prod-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>produser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span>Y4nys7f11
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>secret &quot;prod-db-secret&quot; created
</code></pre><p>Você pode também criar um Secret com credenciais para o ambiente de testes.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret generic test-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>testuser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span>iluvtests
</code></pre></div><p>O resultado é semelhante a:</p>
<pre><code>secret &quot;test-db-secret&quot; created
</code></pre><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> <p>Caracteres especiais como <code>$</code>, <code>\</code>, <code>*</code>, <code>+</code> e <code>!</code> serão interpretados pelo seu
<a href=https://pt.wikipedia.org/wiki/Shell_(computa%C3%A7%C3%A3o)>shell</a> e precisam de
sequências de escape. Na maioria dos shells, a forma mais fácil de gerar sequências
de escape para suas senhas é escrevê-las entre aspas simples (<code>'</code>). Por exemplo,
se a sua senha for <code>S!B\*d$zDsb=</code>, você deve executar o comando da seguinte
forma:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret generic dev-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>devuser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span><span style=color:#b44>&#39;S!B\*d$zDsb=&#39;</span>
</code></pre></div><p>Não é necessário gerar sequências de escape para caracteres especiais em arquivos
(utilizados com a opção <code>--from-file</code>).</p>
</div>
<p>Agora, crie os Pods:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF &gt; pod.yaml
</span><span style=color:#b44>apiVersion: v1
</span><span style=color:#b44>kind: List
</span><span style=color:#b44>items:
</span><span style=color:#b44>- kind: Pod
</span><span style=color:#b44>  apiVersion: v1
</span><span style=color:#b44>  metadata:
</span><span style=color:#b44>    name: prod-db-client-pod
</span><span style=color:#b44>    labels:
</span><span style=color:#b44>      name: prod-db-client
</span><span style=color:#b44>  spec:
</span><span style=color:#b44>    volumes:
</span><span style=color:#b44>    - name: secret-volume
</span><span style=color:#b44>      secret:
</span><span style=color:#b44>        secretName: prod-db-secret
</span><span style=color:#b44>    containers:
</span><span style=color:#b44>    - name: db-client-container
</span><span style=color:#b44>      image: myClientImage
</span><span style=color:#b44>      volumeMounts:
</span><span style=color:#b44>      - name: secret-volume
</span><span style=color:#b44>        readOnly: true
</span><span style=color:#b44>        mountPath: &#34;/etc/secret-volume&#34;
</span><span style=color:#b44>- kind: Pod
</span><span style=color:#b44>  apiVersion: v1
</span><span style=color:#b44>  metadata:
</span><span style=color:#b44>    name: test-db-client-pod
</span><span style=color:#b44>    labels:
</span><span style=color:#b44>      name: test-db-client
</span><span style=color:#b44>  spec:
</span><span style=color:#b44>    volumes:
</span><span style=color:#b44>    - name: secret-volume
</span><span style=color:#b44>      secret:
</span><span style=color:#b44>        secretName: test-db-secret
</span><span style=color:#b44>    containers:
</span><span style=color:#b44>    - name: db-client-container
</span><span style=color:#b44>      image: myClientImage
</span><span style=color:#b44>      volumeMounts:
</span><span style=color:#b44>      - name: secret-volume
</span><span style=color:#b44>        readOnly: true
</span><span style=color:#b44>        mountPath: &#34;/etc/secret-volume&#34;
</span><span style=color:#b44>EOF</span>
</code></pre></div><p>Adicione os Pods a um manifesto <code>kustomization.yaml</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF &gt;&gt; kustomization.yaml
</span><span style=color:#b44>resources:
</span><span style=color:#b44>- pod.yaml
</span><span style=color:#b44>EOF</span>
</code></pre></div><p>Crie todos estes objetos no servidor da API rodando o comando:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -k .
</code></pre></div><p>Ambos os contêineres terão os seguintes arquivos presentes nos seus sistemas de
arquivos, com valores para cada um dos ambientes dos contêineres:</p>
<pre><code>/etc/secret-volume/username
/etc/secret-volume/password
</code></pre><p>Observe como as <code>spec</code>s para cada um dos Pods diverge somente em um campo. Isso
facilita a criação de Pods com capacidades diferentes a partir de um template
mais genérico.</p>
<p>Você pode simplificar ainda mais a definição básica do Pod através da utilização
de duas service accounts diferentes:</p>
<ol>
<li><code>prod-user</code> com o Secret <code>prod-db-secret</code></li>
<li><code>test-user</code> com o Secret <code>test-db-secret</code></li>
</ol>
<p>A especificação do Pod é reduzida para:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prod-db-client-pod<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prod-db-client<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceAccount</span>:<span style=color:#bbb> </span>prod-db-client<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>db-client-container<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>myClientImage<span style=color:#bbb>
</span></code></pre></div><h3 id=caso-de-uso-dotfiles-em-um-volume-de-secret>Caso de uso: <em>dotfiles</em> em um volume de Secret</h3>
<p>Você pode fazer com que seus dados fiquem "ocultos" definindo uma chave que se
inicia com um ponto (<code>.</code>). Este tipo de chave representa um <em>dotfile</em>, ou
arquivo "oculto". Por exemplo, quando o Secret abaixo é montado em um volume,
<code>secret-volume</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dotfile-secret<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>.secret-file</span>:<span style=color:#bbb> </span>dmFsdWUtMg0KDQo=<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-dotfiles-pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>dotfile-secret<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dotfile-test-container<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ls<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;-l&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Este volume irá conter um único arquivo, chamado <code>.secret-file</code>, e o contêiner
<code>dotfile-test-container</code> terá este arquivo presente no caminho
<code>/etc/secret-volume/.secret-file</code>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Arquivos com nomes iniciados por um caractere de ponto são ocultos do resultado
do comando <code>ls -l</code>. Você precisa utilizar <code>ls -la</code> para vê-los ao listar o
conteúdo de um diretório.
</div>
<h3 id=use-case-secret-visible-to-one-container-in-a-pod>Caso de uso: Secret visível somente em um dos contêineres de um pod</h3>
<p>Suponha que um programa necessita manipular requisições HTTP, executar regras
de negócio complexas e então assinar mensagens com HMAC. Devido à natureza
complexa da aplicação, pode haver um <em>exploit</em> despercebido que lê arquivos
remotos no servidor e que poderia expor a chave privada para um invasor.</p>
<p>Esta aplicação poderia ser dividida em dois processos, separados em dois
contêineres distintos: um contêiner de <em>front-end</em>, que manipula as interações
com o usuário e a lógica de negócio, mas não consegue ver a chave privada; e
um contêiner assinador, que vê a chave privada e responde a requisições simples
de assinatura do <em>front-end</em> (por exemplo, através de rede local).</p>
<p>Com essa abordagem particionada, um invasor agora precisa forçar o servidor de
aplicação a rodar comandos arbitrários, o que é mais difícil de ser feito do que
apenas ler um arquivo presente no disco.</p>
<h2 id=melhores-práticas>Melhores práticas</h2>
<h3 id=clientes-que-utilizam-a-api-de-secrets>Clientes que utilizam a API de Secrets</h3>
<p>Ao instalar aplicações que interajam com a API de Secrets, você deve limitar o
acesso utilizando <a href=/docs/reference/access-authn-authz/authorization/>políticas de autorização</a>
como <a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>.</p>
<p>Secrets frequentemente contém valores com um espectro de importância, muitos dos
quais podem causar escalações dentro do Kubernetes (por exemplo, tokens de service
account) e de sistemas externos. Mesmo que um aplicativo individual possa
avaliar o poder do Secret com o qual espera interagir, outras aplicações dentro
do mesmo namespace podem tornar estas suposições inválidas.</p>
<p>Por estas razões, as requisições <code>watch</code> (observar) e <code>list</code> (listar) de
Secrets dentro de um namespace são permissões extremamente poderosas e devem
ser evitadas, pois a listagem de Secrets permite a clientes inspecionar os
valores de todos os Secrets presentes naquele namespace. A habilidade de listar
e observar todos os Secrets em um cluster deve ser reservada somente para os
componentes mais privilegiados, que fazem parte do nível de aplicações de sistema.</p>
<p>Aplicações que necessitam acessar a API de Secret devem realizar uma requisição
<code>get</code> nos Secrets que precisam. Isto permite que administradores restrinjam o
acesso a todos os Secrets, enquanto
<a href=/docs/reference/access-authn-authz/rbac/#referring-to-resources>utilizam uma lista de autorização a instâncias individuais</a>
que a aplicação precise.</p>
<p>Para melhor desempenho em uma requisição <code>get</code> repetitiva, clientes podem criar
objetos que referenciam o Secret e então utilizar a requisição <code>watch</code> neste
novo objeto, requisitando o Secret novamente quando a referência mudar.
Além disso, uma <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/bulk_watch.md>API de "observação em lotes"</a>
para permitir a clientes observar recursos individuais também foi proposta e
provavelmente estará disponível em versões futuras do Kubernetes.</p>
<h2 id=propriedades-de-segurança>Propriedades de segurança</h2>
<h3 id=proteções>Proteções</h3>
<p>Como Secrets podem ser criados de forma independente de Pods que os utilizam,
há menos risco de um Secret ser exposto durante o fluxo de trabalho de criação,
visualização, e edição de Pods. O sistema pode também tomar precauções adicionais
com Secrets, como por exemplo evitar que sejam escritos em disco quando possível.</p>
<p>Um Secret só é enviado para um nó se um Pod naquele nó requerê-lo. O kubelet
armazena o Secret num sistema de arquivos <code>tmpfs</code>, de forma a evitar que o Secret
seja escrito em armazenamento persistente. Uma vez que o Pod que depende do
Secret é removido, o kubelet apaga sua cópia local do Secret também.</p>
<p>Secrets de vários Pods diferentes podem existir no mesmo nó. No entanto, somente
os Secrets que um Pod requerer estão potencialmente visíveis em seus contêineres.
Portanto, um Pod não tem acesso aos Secrets de outro Pod.</p>
<p>Um Pod pode conter vários contêineres. Porém, cada contêiner em um Pod precisa
requerer o volume de Secret nos seus <code>volumeMounts</code> para que este fique visível
dentro do contêiner. Esta característica pode ser utilizada para construir
<a href=#use-case-secret-visible-to-one-container-in-a-pod>partições de segurança ao nível do Pod</a>.</p>
<p>Na maioria das distribuições do Kubernetes, a comunicação entre usuários e o
servidor da API e entre servidor da API e os kubelets é protegida por SSL/TLS.
Secrets são protegidos quando transmitidos através destes canais.</p>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.13 [beta]</code>
</div>
<p>Você pode habilitar <a href=/docs/tasks/administer-cluster/encrypt-data/>encriptação em disco</a>
em dados de Secret para evitar que estes sejam armazenados em texto plano no
<a class=glossary-tooltip title="Armazenamento do tipo Chave-Valor consistente e em alta-disponibilidade usado como repositório de apoio do Kubernetes para todos os dados do cluster." data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>.</p>
<h3 id=riscos>Riscos</h3>
<ul>
<li>No servidor da API, os dados de Secret são armazenados no
<a class=glossary-tooltip title="Armazenamento do tipo Chave-Valor consistente e em alta-disponibilidade usado como repositório de apoio do Kubernetes para todos os dados do cluster." data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>; portanto:
<ul>
<li>Administradores devem habilitar encriptação em disco para dados do cluster
(requer Kubernetes v1.13 ou posterior).</li>
<li>Administradores devem limitar o acesso ao etcd somente para usuários
administradores.</li>
<li>Administradores podem desejar apagar definitivamente ou destruir discos
previamente utilizados pelo etcd que não estiverem mais em uso.</li>
<li>Ao executar o etcd em um cluster, administradores devem garantir o uso de
SSL/TLS para conexões ponto-a-ponto do etcd.</li>
</ul>
</li>
<li>Se você configurar um Secret utilizando um arquivo de manifesto (JSON ou
YAML) que contém os dados do Secret codificados como base64, compartilhar
este arquivo ou salvá-lo num sistema de controle de versão de código-fonte
compromete este Secret. Codificação base64 <em>não</em> é um método de encriptação
e deve ser considerada idêntica a texto plano.</li>
<li>Aplicações ainda precisam proteger o valor do Secret após lê-lo de um volume,
como por exemplo não escrever seu valor em logs ou enviá-lo para um sistema
não-confiável.</li>
<li>Um usuário que consegue criar um Pod que utiliza um Secret também consegue
ler o valor daquele Secret. Mesmo que o servidor da API possua políticas para
impedir que aquele usuário leia o valor do Secret, o usuário poderia criar um
Pod que expõe o Secret.</li>
</ul>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Aprenda a <a href=/pt-br/docs/tasks/configmap-secret/managing-secret-using-kubectl/>gerenciar Secrets utilizando <code>kubectl</code></a></li>
<li>Aprenda a <a href=/pt-br/docs/tasks/configmap-secret/managing-secret-using-config-file/>gerenciar Secrets utilizando arquivos de configuração</a></li>
<li>Aprenda a <a href=/pt-br/docs/tasks/configmap-secret/managing-secret-using-kustomize/>gerenciar Secrets utilizando kustomize</a></li>
<li>Leia a <a href=/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/>documentação de referência da API</a> de <code>Secrets</code></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-436057b96151ecb8a4a9a9f456b5d0fc>8.4 - Gerenciamento de recursos em Pods e contêineres</h1>
<p>Ao criar a especificação de um <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, você pode
opcionalmente especificar quanto de cada recurso um <a class=glossary-tooltip title="Uma imagem executável leve e portável que contém software e todas as suas dependências." data-toggle=tooltip data-placement=top href=/docs/concepts/containers/ target=_blank aria-label=contêiner>contêiner</a>
precisa. Os recursos mais comuns a serem especificados são CPU e memória (RAM);
há outros recursos que podem ser especificados.</p>
<p>Quando você especifica o <em>requerimento</em> de recursos em um Pod, o
<a class=glossary-tooltip title="Componente da camada de gerenciamento que observa os pods recém-criados sem nenhum nó atribuído, e seleciona um nó para executá-los." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a> utiliza
esta informação para decidir a qual nó o Pod será atribuído. Quando você
especifica um <em>limite</em> de recurso para um contêiner, o kubelet garante o
cumprimento de tais limites, de modo que o contêiner em execução não consiga
utilizar uma quantidade de tal recurso além do limite especificado. O kubelet
também reserva pelo menos o <em>requerimento</em> daquele recurso de sistema
especificamente para que este contêiner utilize.</p>
<h2 id=requerimentos-e-limites>Requerimentos e limites</h2>
<p>Se o nó em que um Pod está rodando tem o suficiente de um recurso específico
disponível, é possível (e permitido) a um contêiner utilizar mais do que o seu
<code>request</code> para aquele recurso especifica. No entanto, não é permitido a um
contêiner consumir mais do que o seu <code>limit</code> para um recurso.</p>
<p>Por exemplo, se você especificar um requerimento de <code>memory</code> de 256 MiB para um
contêiner, e aquele contêiner está em um Pod atribuído a um nó com 8GiB de
memória, sem outros Pods, então este contêiner pode tentar consumir mais memória
RAM.</p>
<p>Se você especificar um limite de <code>memory</code> de 4GiB para aquele contêiner, o
kubelet (e o
<a class=glossary-tooltip title="O agente de execução de contêiner é o software responsável por executar os contêineres." data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label="agente de execução de contêiner">agente de execução de contêiner</a>)
vão garantir o cumprimento do limite. O agente de execução impede que o contêiner
utilize mais de um recurso do que seu limite configurado. Por exemplo, quando
um processo no contêiner tenta consumir mais que o limite permitido de memória,
o núcleo do sistema encerra o processo que tentou efetuar a alocação de memória
com um erro de memória esgotada (<em>out of memory (OOM) error</em>).</p>
<p>Limites podem ser implementados de forma reativa (o sistema intervém quando
uma violação ocorre) ou por garantia (o sistema previne o contêiner de exceder
o limite). Diferentes agentes de execução implementam as mesmas restrições de
maneiras diferentes.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Se um contêiner especifica seu próprio limite de memória, mas não especifica seu
requerimento de memória, o Kubernetes automaticamente cria um requerimento de
memória com o mesmo valor do limite. A mesma regra vale para o limite de CPU:
quando não há requerimento de CPU, o Kubernetes automaticamente cria um
requerimento de CPU idêntico ao limite.
</div>
<h2 id=tipos-de-recursos>Tipos de recursos</h2>
<p><em>CPU</em> e <em>memória</em> são <em>tipos de recursos</em>. Um tipo de recurso possui uma unidade
básica. CPU representa processamento computacional e é especificada em unidades
de <a href=#meaning-of-cpu>CPU do Kubernetes</a>.
Memória é especificada em bytes. Em cargas de trabalho Linux, você pode
especificar o recurso <em>huge pages</em>. <em>Huge pages</em> são uma funcionalidade
específica do Linux que permite ao núcleo do sistema operacional alocar
blocos de memória muito maiores que o tamanho de página de memória padrão.</p>
<p>Por exemplo, em um sistema onde o tamanho da página de memória padrão é de 4 KiB,
você pode especificar um limite <code>hugepages-2Mi: 80Mi</code>. Se o contêiner tentar
alocar mais de 40 <em>huge pages</em> de 2 MiB cada, ou um total de 80 MiB, essa
alocação irá falhar.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Você não pode superdimensionar (ou solicitar acima do limite físico) recursos do
tipo <code>hugepages-*</code>.
O recurso <code>hugepages-*</code> difere dos recursos <code>memory</code> e <code>cpu</code> neste aspecto.
</div>
<p>CPU e memória são chamados coletivamente de <em>recursos computacionais</em>, ou apenas
<em>recursos</em>. Recursos computacionais são quantidades mensuráveis que podem ser
requisitadas, alocadas, e consumidas. Estes recursos diferem dos
<a href=/docs/concepts/overview/kubernetes-api/>recursos de API</a>. Recursos de API,
como Pods e <a href=/docs/concepts/services-networking/service/>Services</a> são objetos
que podem ser lidos e modificados através do servidor da API do Kubernetes.</p>
<h2 id=requerimentos-de-recursos-e-limites-de-pod-e-contêiner>Requerimentos de recursos e limites de Pod e contêiner</h2>
<p>Para cada contêiner, você pode especificar limites e requerimentos de recursos,
incluindo os seguintes recursos:</p>
<ul>
<li><code>spec.containers[].resources.limits.cpu</code></li>
<li><code>spec.containers[].resources.limits.memory</code></li>
<li><code>spec.containers[].resources.limits.hugepages-&lt;size></code></li>
<li><code>spec.containers[].resources.requests.cpu</code></li>
<li><code>spec.containers[].resources.requests.memory</code></li>
<li><code>spec.containers[].resources.requests.hugepages-&lt;size></code></li>
</ul>
<p>Embora você possa especificar apenas requerimentos e limites para contêineres
individuais, é útil também pensar sobre os requerimentos e limites gerais de um
Pod.
Para um recurso em particular, um <em>requerimento ou limite de recurso de um Pod</em>
é a soma de todos os valores dos requerimentos ou limites de um recurso daquele
tipo, especificados em cada um dos contêineres daquele Pod.</p>
<h2 id=unidades-de-recursos-no-kubernetes>Unidades de recursos no Kubernetes</h2>
<h3 id=meaning-of-cpu>Unidades de recurso de CPU</h3>
<p>Limites e requerimentos de recursos de CPU são mensurados em unidades de <em>cpu</em>.
No Kubernetes, uma unidade de CPU é equivalente a <strong>um núcleo físico de CPU</strong>,
ou <strong>um núcleo virtual</strong>, dependendo se o nó é uma máquina física ou uma máquina
virtual rodando em uma máquina física.</p>
<p>Requerimentos fracionários são permitidos. Quando você define um contêiner cujo
valor do campo <code>spec.containers[].resources.requests.cpu</code> é <code>0.5</code>, você está
solicitando metade da quantidade de CPU que teria sido solicitada caso o valor
fosse <code>1.0</code>.
No caso de unidades de recurso de CPU, a expressão de
<a href=/docs/reference/kubernetes-api/common-definitions/quantity/>quantidade</a> <code>0.1</code>
é equivalente à expressão <code>100m</code>, que pode ser lida como "cem milicpus", ou
"cem milinúcleos". "Milicpu" ou "milinúcleo" equivalem à milésima parte de um
núcleo ou CPU, de modo que "100m" equivalem a 10% do tempo computacional de um
processador.</p>
<p>Recursos de CPU são sempre especificados como uma quantidade absoluta de recurso,
nunca como uma quantidade relativa. Por exemplo, <code>500m</code> de CPU representam
grosseiramente a mesma quantidade de poder computacional, independentemente do
contêiner rodar em uma máquina com processador de núcleo único, de dois núcleos
ou de 48 núcleos.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> O Kubernetes não permite que você especifique recursos de CPU com uma precisão
maior que <code>1m</code>. Devido a isso, é útil especificar unidades de CPU menores do que
<code>1.0</code> ou <code>1000m</code> utilizando a notação de milicpu. Por exemplo, <code>5m</code> ao invés de
<code>0.005</code>.
</div>
<h3 id=meaning-of-memory>Unidades de recurso de memória</h3>
<p>Limites e requerimentos de <code>memory</code> são medidos em bytes. Você pode expressar
memória como um número inteiro ou como um número de ponto fixo, utilizando um
destes sufixos de
<a href=/docs/reference/kubernetes-api/common-definitions/quantity/>quantidade</a>:
E, P, T, G, M, k. Você também pode utilizar os equivalentes de potência de dois:
Ei, Pi, Ti, Gi, Mi, Ki. Por exemplo, as quantidades abaixo representam, a grosso
modo, o mesmo valor:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>128974848, 129e6, 129M, 128974848000m, 123Mi
</code></pre></div><p>Tome cuidado com os sufixos. Se você solicitar <code>400m</code> de memória, esta
quantidade estará de fato requerendo o equivalente a 0,4 byte de memória. A
intenção da pessoa que fez esta requisição provavelmente era solictar 400
mebibytes (<code>400Mi</code>) ou 400 megabytes (<code>400M</code>).</p>
<h2 id=example-1>Exemplo de recursos de contêiner</h2>
<p>O Pod seguinte tem dois contêineres. Ambos os contêineres têm um requerimento de
0,25 CPU e 64 MiB (ou 2<sup>26</sup> bytes) de memória. Cada contêiner tem um
limite de 0,5 CPU e 128 MiB de memória. Você pode dizer que o Pod tem um
requerimento de 0,5 CPU e 128 MiB de memória, e um limite de 1 CPU e 256 MiB de
memória.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/app:v4<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;64Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;128Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>log-aggregator<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/log-aggregator:v6<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;64Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;128Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></code></pre></div><h2 id=como-pods-com-requerimentos-de-recursos-são-agendados>Como Pods com requerimentos de recursos são agendados</h2>
<p>Quando você cria um Pod, o escalonador do Kubernetes seleciona um nó para que o
Pod rode. Cada nó possui uma capacidade máxima para cada um dos tipos de recurso:
a quantidade de CPU e memória que o nó pode fornecer aos Pods. O escalonador
garante que, para cada tipo de recurso, a soma dos requerimentos de recursos dos
contêineres agendados seja menor que a capacidade do nó.
Note que, embora o consumo de memória ou CPU real nos nós seja muito baixo, o
escalonador ainda irá se recusar a agendar um Pod em um nó se a verificação de
capacidade falhar. Isso protege contra a falta de um recurso em um nó quando o
consumo de recursos aumenta com o passar do tempo, como por exemplo durante o
pico diário de requisições a um serviço.</p>
<h2 id=how-pods-with-resource-limits-are-run>Como o Kubernetes aplica requisições e limites de recursos</h2>
<p>Quando o kubelet inicia um contêiner como parte de um Pod, o kubelet envia as
requisições e limites de memória e de CPU ao agente de execução de contêiner.</p>
<p>No Linux, o agente de execução de contêiner normalmente configura os
<a class=glossary-tooltip title="Um grupo de processos do Linux com isolamento de recursos opcional, contagem e limites." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-cgroup" target=_blank aria-label=cgroups>cgroups</a> que aplicam e garantem
os limites que você definiu.</p>
<ul>
<li>O limite de CPU determina um teto de quanto tempo de CPU o contêiner pode
utilizar. A cada intervalo de agendamento, o núcleo do sistema operacional do
Linux verifica se este limite foi excedido; se este for o caso, o núcleo
aguarda antes de permitir que aquele cgroup continue sua execução.</li>
<li>O requerimento de CPU normalmente define um método de balanceamento. Se vários
contêineres diferentes (cgroups) querem rodar em um sistema disputado, cargas
de trabalho com requerimentos maiores de CPU têm mais tempo de CPU alocado
para si do que cargas de trabalho com pequenos requerimentos.</li>
<li>O requerimento de memória é usado principalmente durante o agendamento de um
Pod. Em um nó que utiliza cgroups v2, o agente de execução de contêiner pode
utilizar o requerimento de memória como uma dica para definir valores para
<code>memory.min</code> e <code>memory.low</code>.</li>
<li>O limite de memória define um limite de memória para aquele cgroup. Se o
contêiner tenta alocar mais memória que aquele limite, o subsistema
<em>out-of-memory</em> do núcleo do sistema operacional Linux é ativado e,
normalmente, intervém encerrando um dos processos do contêiner que tentou
alocar mais memória. Se o processo em questão for o PID 1 do contêiner, e o
contêiner estiver marcado como reinicializável, então o Kubernetes irá
reiniciar o contêiner.</li>
<li>O limite de memória para um Pod ou contêiner é também aplicado a páginas em
volumes armazenados em memória, como um <code>emptyDir</code>. O kubelet considera
sistemas de arquivos <code>tmpfs</code> em volumes do tipo <code>emptyDir</code> como uso de memória
em um contêiner, ao invés de armazenamento efêmero local.</li>
</ul>
<p>Se um contêiner exceder seu requerimento de memória e o nó em que esse contêiner
está rodando ficar com pouca memória no total, é provável que o Pod a que este
contêiner pertence seja <a class=glossary-tooltip title="Processo de encerramento de um ou mais Pods em Nós" data-toggle=tooltip data-placement=top href=/pt-br/docs/concepts/scheduling-eviction/ target=_blank aria-label=removido>removido</a>.</p>
<p>A um contêiner pode ou não ser permitido exceder seu limite de CPU por períodos
de tempo estendidos. No entanto, agentes de execução de contêiner não encerram
Pods por uso excessivo de CPU.</p>
<p>A fim de determinar se um contêiner não pode ser agendado ou está sendo
encerrado devido a limites de recursos, consulte a seção de
<a href=#troubleshooting>solução de problemas</a>.</p>
<h3 id=monitorando-utilização-de-recursos-computacionais-e-de-memória>Monitorando utilização de recursos computacionais e de memória</h3>
<p>O kubelet relata a utilização de recursos de um Pod como parte do
<a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status><code>status</code></a>
do Pod.</p>
<p>Se ferramentas opcionais para
<a href=/docs/tasks/debug-application-cluster/resource-usage-monitoring/>monitoramento de recursos</a>
estiverem disponíveis em seu cluster, a utilização de recursos de um Pod pode
ser verificada diretamente através de
<a href=/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api>API de métricas</a>
ou através das suas ferramentas de monitoramento</p>
<h2 id=armazenamento-efêmero-local>Armazenamento efêmero local</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code>
</div>
<p>Nós possuem armazenamento efêmero local, através de dispositivos de escrita
conectados localmente ou através de RAM. "Efêmero" significa que não há garantia
de longo termo com relação a durabilidade.</p>
<p>Pods utilizam armazenamento local efêmero para dados temporários, cache e logs.
O kubelet pode fornecer armazenamento temporário a Pods que utilizam
armazenamento local efêmero para montar <a class=glossary-tooltip title="Um diretório contendo dados, accessível aos contêineres em um pod." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volumes>volumes</a>
do tipo <a href=/docs/concepts/storage/volumes/#emptydir><code>emptyDir</code></a> em contêineres.</p>
<p>O kubelet também utiliza este tipo de armazenamento para
<a href=/pt-br/docs/concepts/cluster-administration/logging/#logs-no-n%C3%ADvel-do-n%C3%B3>logs de contêineres a nível de nó</a>,
imagens de contêiner e camadas graváveis de contêineres em execução.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Cuidado:</strong> Se um nó falhar, os dados em seu armazenamento efêmero podem ser perdidos.
Suas aplicações não devem ter expectativas de cumprimento de SLAs de desempenho
(como quantidade de operações de entrada e saída de disco por segundo (IOPS),
por exemplo) pelo armazenamento local efêmero.
</div>
<p>Com esta funcionalidade em fase beta, o Kubernetes permite que você rastreie,
reserve e limite quanto armazenamento local efêmero um Pod pode consumir.</p>
<h3 id=configurations-for-local-ephemeral-storage>Configurações para armazenamento local efêmero</h3>
<p>O Kubernetes suporta duas formas de configuração para o armazenamento local
efêmero em um nó:</p>
<ul class="nav nav-tabs" id=local-storage-configurations role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#local-storage-configurations-0 role=tab aria-controls=local-storage-configurations-0 aria-selected=true>Sistema de arquivos único</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#local-storage-configurations-1 role=tab aria-controls=local-storage-configurations-1>Dois sistemas de arquivos</a></li></ul>
<div class=tab-content id=local-storage-configurations><div id=local-storage-configurations-0 class="tab-pane show active" role=tabpanel aria-labelledby=local-storage-configurations-0>
<p><p>Nesta configuração, você armazena todos os tipos diferentes de dados locais
efêmeros (volumes do tipo <code>emptyDir</code>, camadas graváveis, imagens de contêiner,
logs) em um sistema de arquivos único. A forma mais efetiva de configurar o
kubelet é dedicar este sistema de arquivos aos dados do Kubernetes (kubelet).</p>
<p>O kubelet também escreve
<a href=/pt-br/docs/concepts/cluster-administration/logging/#logs-no-n%C3%ADvel-do-n%C3%B3>logs de contêiner a nível de nó</a>
e trata estes logs de maneira semelhante ao armazenamento efêmero local.</p>
<p>O kubelet escreve logs em arquivos dentro do seu diretório de log configurado
(<code>/var/log</code> por padrão) e possui um diretório base para outros dados armazenados
localmente (<code>/var/lib/kubelet</code> por padrão).</p>
<p>Normalmente, ambos os diretórios <code>/var/lib/kubelet</code> e <code>/var/log</code> encontram-se no
sistema de arquivos raiz, e o kubelet é projetado com este desenho em mente.</p>
<p>Seu nó pode ter tantos outros sistemas de arquivos não utilizados pelo Kubernetes
quantos você desejar.</p>
</div>
<div id=local-storage-configurations-1 class=tab-pane role=tabpanel aria-labelledby=local-storage-configurations-1>
<p><p>Você tem um sistema de arquivos no nó que você utiliza para dados efêmeros que
vêm de Pods em execução: logs e volumes do tipo <code>emptyDir</code>. Você pode utilizar
este sistema de arquivos para outros dados (por exemplo, logs de sistema não
relacionados ao Kubernetes); este sistema de arquivos pode até mesmo ser o
sistema de arquivos raiz.</p>
<p>O kubelet também escreve
<a href=/pt-br/docs/concepts/cluster-administration/logging/#logs-no-n%C3%ADvel-do-n%C3%B3>logs de contêiner a nível de nó</a>
no primeiro sistema de arquivos e os trata de forma semelhante ao armazenamento
local efêmero.</p>
<p>Você também tem um segundo sistema de arquivos, separado, conectado a um
dispositivo lógico de armazenamento distinto. Nesta configuração, o diretório
que você configurou o kubelet para armazenar as camadas de imagens de contêiner
e as camadas graváveis de contêineres em execução estará neste segundo sistema
de arquivos.</p>
<p>O primeiro sistema de arquivos não armazena nenhuma camada de imagens de
contêiner ou camada gravável.</p>
<p>Seu nó pode ter tantos outros sistemas de arquivos não utilizados pelo Kubernetes
quantos você desejar.</p>
</div></div>
<p>O kubelet consegue medir quanto armazenamento local está sendo utilizado. O
kubelet faz isso desde que:</p>
<ul>
<li>o <a href=/docs/reference/command-line-tools-reference/feature-gates/><em>feature gate</em></a>
<code>LocalStorageCapacityIsolation</code> esteja habilitado (a funcionalidade está
ligada por padrão), e</li>
<li>você tenha configurado o nó utilizando uma das configurações suportadas para
o armazenamento local efêmero.</li>
</ul>
<p>Se você tiver uma configuração diferente, o kubelet não irá aplicar limites de
recursos para o armazenamento local efêmero.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> O kubelet rastreia volumes <code>emptyDir</code> que utilizem o sistema de arquivos <code>tmpfs</code>
como uso de memória de contêiner, ao invés de consumo de armazenamento local
efêmero.
</div>
<h3 id=configurando-requerimentos-e-limites-para-armazenamento-local-efêmero>Configurando requerimentos e limites para armazenamento local efêmero</h3>
<p>Você pode especificar o recurso <code>ephemeral-storage</code> para gerenciar o
armazenamento local efêmero. Cada contêiner de um Pod pode especificar um dos
valores abaixo, ou ambos:</p>
<ul>
<li><code>spec.containers[].resources.limits.ephemeral-storage</code></li>
<li><code>spec.containers[].resources.requests.ephemeral-storage</code></li>
</ul>
<p>Limites e requerimentos de <code>ephemeral-storage</code> são medidos em quantidades de
bytes. Você pode expressar armazenamento como um inteiro ou como um valor de
ponto fixo utilizando um dos seguintes sufixos: E, P, T, G, M, k. Você pode
também utilizar os equivalentes de potência de dois: Ei, Pi, Ti, Gi, Mi, Ki.
Por exemplo, as quantidades abaixo representam grosseiramente o mesmo valor:</p>
<ul>
<li><code>128974848</code></li>
<li><code>129e6</code></li>
<li><code>129M</code></li>
<li><code>123Mi</code></li>
</ul>
<p>No exemplo a seguir, o Pod tem dois contêineres. Cada contêiner tem um
requerimento de 2GiB de armazenamento efêmero local. Cada contêiner tem um
limite de 4GiB de armazenamento efêmero local. Portanto, o Pod tem um
requerimento de 4GiB e um limite de 8GiB de armazenamento efêmero local.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/app:v4<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/tmp&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>log-aggregator<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/log-aggregator:v6<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/tmp&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></code></pre></div><h3 id=como-pods-com-requerimentos-de-ephemeral-storage-são-agendados>Como Pods com requerimentos de <code>ephemeral-storage</code> são agendados</h3>
<p>Quando você cria um Pod, o Kubernetes seleciona um nó para o Pod rodar. Cada nó
tem uma quantidade máxima de armazenamento efêmero local que pode ser fornecida
aos Pods. Para mais informações, consulte
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable><em>Node Allocatable</em></a>.</p>
<p>O escalonador garante que a soma dos requerimentos de recursos dos contêineres
agendados é menor que a capacidade do nó.</p>
<h3 id=resource-emphemeralstorage-consumption>Gerenciamento do consumo do armazenamento efêmero</h3>
<p>Se o kubelet estiver gerenciando armazenamento local efêmero como um recurso,
o kubelet irá medir o consumo de armazenamento em:</p>
<ul>
<li>volumes <code>emptyDir</code>, com exceção dos volumes do tipo <code>tmpfs</code></li>
<li>diretórios que armazenem logs a nível de nó</li>
<li>camadas de contêiner graváveis</li>
</ul>
<p>Se um Pod estiver utilizando mais armazenamento efêmero do que o permitido, o
kubelet irá gerar um sinal de remoção para aquele Pod.</p>
<p>Para isolamento a nível de contêiner, se o consumo de armazenamento de um
contêiner em camadas graváveis e logs exceder seu limite de armazenamento, o
kubelet irá marcar o Pod para remoção.</p>
<p>Para isolamento a nível de Pod, o kubelet calcula um limite de armazenamento
total para um Pod somando os limites de cada contêiner naquele Pod. Neste caso,
se a soma do consumo de armazenamento efêmero local de todas os contêineres e
também dos volumes <code>emptyDir</code> de um Pod exceder o limite de armazenamento total
do Pod, então o kubelet marca o Pod para remoção.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Cuidado:</strong> <p>Se o kubelet não estiver medindo armazenamento efêmero local, um Pod que exeder
seu limite de armazenamento local não será removido por exceder os limites de
recurso de armazenamento local.</p>
<p>No entanto, se o espaço de um sistema de arquivos para camadas de contêiner
graváveis, logs a nível de nó, ou volumes <code>emptyDir</code> ficar reduzido, o nó irá
marcar a si próprio com um <a class=glossary-tooltip title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=_taint_><em>taint</em></a>
indicando que está com armazenamento local reduzido, e esse <em>taint</em> dispara a
remoção de Pods que não toleram o <em>taint</em> em questão.</p>
<p>Veja as <a href=#configurations-for-local-ephemeral-storage>configurações</a> suportadas
para armazenamento efêmero local.</p>
</div>
<p>O kubelet suporta formas diferentes de medir o uso de armazenamento dos Pods:</p>
<ul class="nav nav-tabs" id=resource-emphemeralstorage-measurement role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#resource-emphemeralstorage-measurement-0 role=tab aria-controls=resource-emphemeralstorage-measurement-0 aria-selected=true>Varredura periódica</a></li>
<li class=nav-item><a data-toggle=tab class=nav-link href=#resource-emphemeralstorage-measurement-1 role=tab aria-controls=resource-emphemeralstorage-measurement-1>Quota de projeto do sistema de arquivos</a></li></ul>
<div class=tab-content id=resource-emphemeralstorage-measurement><div id=resource-emphemeralstorage-measurement-0 class="tab-pane show active" role=tabpanel aria-labelledby=resource-emphemeralstorage-measurement-0>
<p><p>O kubelet executa verificações agendadas, em intervalos regulares, que varrem
cada volume do tipo <code>emptyDir</code>, diretório de log de contêiner, e camada gravável
de contêiner.</p>
<p>A varredura mede quanto espaço está sendo utilizado.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> <p>Neste modo, o kubelet não rastreia descritores de arquivos abertos para arquivos
removidos.</p>
<p>Se você (ou um contêiner) criar um arquivo dentro de um volume <code>emptyDir</code>, um
processo ou usuário abrir tal arquivo, e você apagar o arquivo enquanto ele
ainda estiver aberto, o nó de índice para o arquivo apagado será mantido até que
o arquivo seja fechado novamente. O kubelet, no entanto, não computa este espaço
como espaço em uso.</p>
</div>
</div>
<div id=resource-emphemeralstorage-measurement-1 class=tab-pane role=tabpanel aria-labelledby=resource-emphemeralstorage-measurement-1>
<p><p>Quotas de projeto são uma funcionalidade a nível de sistema operacional para
gerenciamento de uso do armazenamento em sistemas de arquivos. Com o Kubernetes,
você pode habilitar quotas de projeto para o monitoramento de armazenamento em
uso. Tenha certeza que o sistema de arquivos do nó que esteja sendo utilizado em
volumes do tipo <code>emptyDir</code> possui suporte a quotas de projeto. Por exemplo,
os sistemas de arquivos XFS e ext4fs oferecem suporte a quotas de projeto.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Quotas de projeto permitem o monitoramento do uso de armazenamento, mas não
garantem limites.
</div>
<p>O Kubernetes utiliza IDs de projeto iniciando em <code>1048576</code>. Os IDs em uso estão
registrados nos diretórios <code>/etc/projects</code> e <code>/etc/projid</code>. Se os IDs de projeto
nestes intervalos forem utilizados para outros propósitos no sistema, estes IDs
de projeto deverão estar registrados nos diretórios especificados acima para que
o Kubernetes não os tente utilizar.</p>
<p>Quotas fornecem melhor desempenho e mais precisão do que varredura de diretórios.
Quando um diretório é atribuído a um projeto, todos os arquivos criados no
diretório são também criados no projeto, e o núcleo do sistema pode simplesmente
manter controle de quantos blocos estão em uso por arquivos daquele projeto. Se
um arquivo é criado e apagado, mas possui um descritor de arquivo aberto, ele
continua a consumir espaço. O rastreio de quotas registra este espaço de forma
precisa, enquanto varreduras de diretório ignoram o uso de espaço de
armazenamento por arquivos apagados.</p>
<p>Se você deseja utilizar quotas de projeto, você deve:</p>
<ul>
<li>
<p>Habilitar o <a href=/docs/reference/command-line-tools-reference/feature-gates/><em>feature gate</em></a>
<code>LocalStorageCapacityIsolationFSQuotaMonitoring=true</code> utilizando o campo
<code>featureGates</code> na <a href=/docs/reference/config-api/kubelet-config.v1beta1/>configuração do kubelet</a>
ou a opção de linha de comando <code>--feature-gates</code>.</p>
</li>
<li>
<p>Garantir que o sistema de arquivos raiz (ou o sistema de arquivos opcional de
tempo de execução) tem quotas de projeto habilitadas. Todos os sistemas de
arquivos XFS suportam quotas de projeto. Em sistemas de arquivos ext4, você
precisa habilitar a funcionalidade de rastreio de quotas de projeto enquanto
o sistema de arquivos ainda não está montado.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Para sistema de arquivos ext4, com o volume /dev/block-device não montado</span>
sudo tune2fs -O project -Q prjquota /dev/block-device
</code></pre></div></li>
<li>
<p>Garanta que o sistema de arquivos raiz (ou sistema de arquivos opcional de
tempo de execução) esteja montado com quotas de projeto habilitadas. Em ambos
os sistemas XFS e ext4fs, a opção de montagem é chamada <code>prjquota</code>.</p>
</li>
</ul>
</div></div>
<h2 id=recursos-estendidos>Recursos estendidos</h2>
<p>Recursos estendidos são nomes de recursos absolutos fora do domínio
<code>kubernetes.io</code>. Estes recursos permitem a operadores de cluster anunciar e a
usuários consumir recursos que não são embutidos pelo Kubernetes.</p>
<p>Dois passos são necessários para a utilização de recursos estendidos.
Primeiramente, o operador do cluster deve anunciar um recurso estendido. Em
segundo lugar, os usuários devem solicitar o recurso estendido em Pods.</p>
<h3 id=gerenciando-recursos-estendidos>Gerenciando recursos estendidos</h3>
<h4 id=recursos-estendidos-a-nível-de-nó>Recursos estendidos a nível de nó</h4>
<p>Recursos estendidos a nível de nó são recursos ligados ao nó.</p>
<h5 id=recursos-gerenciados-por-dispositivos-conectados>Recursos gerenciados por dispositivos conectados</h5>
<p>Veja <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>Device Plugin</a>
para mais informações sobre como anunciar recursos gerenciados por dispositivos
conectados em cada nó.</p>
<h5 id=outros-recursos>Outros recursos</h5>
<p>A fim de anunciar um novo recurso estendido a nível de nó, o operador do cluster
pode enviar uma requisição HTTP com o método <code>PATCH</code> para o servidor da API do
Kubernetes para especificar a quantidade disponível em um nó no cluster, através
do campo <code>status.capacity</code>. Após a realização desta operação, o campo
<code>status.capacity</code> do nó irá conter um novo recurso. O campo <code>status.allocatable</code>
é atualizado automaticamente pelo kubelet, de forma assíncrona, com o novo
recurso.</p>
<p>Como o escalonador utiliza o valor do campo <code>status.allocatable</code> do nó ao
verificar a saúde do Pod, o escalonador somente considerará o novo valor do
campo após esta atualização assíncrona. Pode haver um pequeno atraso entre a
atualização da capacidade do nó com um novo recurso e o momento em que o
primeiro Pod que requer o recurso poderá ser agendado naquele nó.</p>
<p><strong>Exemplo</strong>:</p>
<p>Este exemplo demonstra como utilizar a ferramenta <code>curl</code> para criar uma
requisição HTTP que anuncia cinco recursos "example.com/foo" no nó <code>k8s-node-1</code>,
cujo nó da camada de gerenciamento é <code>k8s-master</code>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl --header <span style=color:#b44>&#34;Content-Type: application/json-patch+json&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --request PATCH <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --data <span style=color:#b44>&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1foo&#34;, &#34;value&#34;: &#34;5&#34;}]&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
</code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Na requisição anterior, a notação <code>~1</code> é a codificação do caractere <code>/</code> no campo
<code>path</code> para a operação de atualização. O valor do campo <code>path</code> em JSON-Patch é
interpretado como um JSON-Pointer. Para maiores detalhes, veja
<a href=https://tools.ietf.org/html/rfc6901#section-3>a seção 3 da IETF RFC 6901</a>.
</div>
<h4 id=recursos-estendidos-a-nível-de-cluster>Recursos estendidos a nível de cluster</h4>
<p>Recursos estendidos a nível de cluster não são vinculados aos nós. Estes
recursos são normalmente gerenciados por extensões do escalonador, que manipulam
o consumo e as quotas de recursos.</p>
<p>Você pode especificar os recursos estendidos que são manipulados por extensões
do escalonador nas <a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>configurações do kube-scheduler</a>.</p>
<p><strong>Exemplo</strong>:</p>
<p>A configuração abaixo para uma política do escalonador indica que o recurso
estendido a nível de cluster "example.com/foo" é manipulado pelas extensões do
escalonador.</p>
<ul>
<li>O escalonador envia um Pod para a extensão do escalonador somente se o Pod
solicitar "example.com/foo".</li>
<li>O campo <code>ignoredByScheduler</code> especifica que o escalonador não verifica o
recurso "example.com/foo" em seu predicado <code>PodFitsResources</code>.</li>
</ul>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Policy&#34;</span>,
  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
  <span style=color:green;font-weight:700>&#34;extenders&#34;</span>: [
    {
      <span style=color:green;font-weight:700>&#34;urlPrefix&#34;</span>:<span style=color:#b44>&#34;&lt;extender-endpoint&gt;&#34;</span>,
      <span style=color:green;font-weight:700>&#34;bindVerb&#34;</span>: <span style=color:#b44>&#34;bind&#34;</span>,
      <span style=color:green;font-weight:700>&#34;managedResources&#34;</span>: [
        {
          <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;example.com/foo&#34;</span>,
          <span style=color:green;font-weight:700>&#34;ignoredByScheduler&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>
        }
      ]
    }
  ]
}
</code></pre></div><h3 id=consumindo-recursos-estendidos>Consumindo recursos estendidos</h3>
<p>Usuários podem consumir recursos estendidos em especificações de Pods como CPU
e memória. O escalonador controla a contagem de recursos de modo que a
quantidade alocada simultaneamente a Pods não seja maior que a quantidade
disponível.</p>
<p>O servidor da API limita as quantidades de recursos estendidos a números inteiros.
Exemplos de quantidades <em>válidas</em> são <code>3</code>, <code>3000m</code> e <code>3Ki</code>. Exemplos de
quantidades <em>inválidas</em> são <code>0.5</code> e <code>1500m</code>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Recursos estendidos substituem os Recursos Inteiros Opacos.
Usuários podem escolher qualquer prefixo de nome de domínio, com exceção do
domínio <code>kubernetes.io</code>, que é reservado.
</div>
<p>Para consumir um recurso estendido em um Pod, inclua o nome do recurso como uma
chave no mapa <code>spec.containers[].resources.limits</code> na especificação do contêiner.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Recursos estendidos não podem ser superdimensionados. Portanto, <code>request</code> e
<code>limit</code> devem ser iguais se ambos estiverem presentes na especificação de um
contêiner.
</div>
<p>Um Pod só é agendado se todos os seus requerimentos de recursos forem
satisfeitos, incluindo CPU, memória e quaisquer recursos estendidos. O Pod
permanece no estado <code>PENDING</code> enquanto seus requerimentos de recursos não puderem
ser satisfeitos.</p>
<p><strong>Exemplo</strong>:</p>
<p>O Pod abaixo requisita duas CPUs e um "example.com/foo" (um recurso estendido).</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-container<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>myimage<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/foo</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/foo</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></code></pre></div><h2 id=limitação-de-pid>Limitação de PID</h2>
<p>Limites de ID de processo (PID) permitem à configuração de um kubelet limitar o
número de PIDs que um dado Pod pode consumir. Consulte
<a href=/docs/concepts/policy/pid-limiting/>PID Limiting</a> para mais informações.</p>
<h2 id=troubleshooting>Solução de problemas</h2>
<h3 id=meus-pods-estão-pendentes-com-um-evento-failedscheduling>Meus pods estão pendentes com um evento <code>FailedScheduling</code></h3>
<p>Se o escalonador não conseguir encontrar nenhum nó que atenda aos requisitos de
recursos do Pod, este Pod permanecerá não-agendado até que um local destino
possa ser encontrado. Um <a href=/docs/reference/kubernetes-api/cluster-resources/event-v1/>Evento</a>
é produzido cada vez que o escalonador falhar em encontrar um local para agendar
o Pod. Você pode utilizar o utilitário <code>kubectl</code> para ver os eventos de um Pod.
Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pod frontend | grep -A <span style=color:#666>9999999999</span> Events
</code></pre></div><pre><code>Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu
</code></pre><p>No exemplo acima, o Pod de nome "frontend" não pôde ser agendado devido à nenhum
nó possuir CPU suficiente para suprir seu requerimento de CPU. Mensagens de erro
semelhantes a essa podem sugerir falha devido a falta de memória
(<code>PodExceedsFreeMemory</code>). De maneira geral, se um Pod estiver pendente com uma
mensagem deste tipo, há diversas possibilidades de solução a serem tentadas:</p>
<ul>
<li>Adicione mais nós ao cluster.</li>
<li>Encerre Pods desnecessários para liberar espaço para Pods pendentes.</li>
<li>Verifique se o Pod não é maior que todos os nós. Por exemplo, se todos os nós
têm uma capacidade de <code>cpu: 1</code>, um Pod que requisita <code>cpu: 1.1</code> nunca será
agendado.</li>
<li>Verifique se os nós não possuem <em>taints</em>. Se a maioria dos seus nós possuem
<em>taints</em>, e o novo Pod não tolera tal <em>taint</em>, o escalonador somente considera
agendar o Pod nos nós que não possuem aquele <em>taint</em>.</li>
</ul>
<p>Você pode verificar capacidades de nós e quantidades alocadas com o comando
<code>kubectl describe nodes</code>. Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe nodes e2e-test-node-pool-4lw4
</code></pre></div><pre><code>Name:            e2e-test-node-pool-4lw4
[ ... linhas abreviadas para simplificação ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... linhas abreviadas para simplificação ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
</code></pre><p>No exemplo anterior, você pode verificar que se um Pod requisitar mais que 1,120
CPUs ou mais que 6,23Gi de memória, tal Pod não caberá neste nó.</p>
<p>Ao verificar a seção "Pods", você pode observar quais Pods estão consumindo
espaço neste nó.</p>
<p>A quantidade de recursos disponível aos Pods é menor que a capacidade do nó, pois
daemons do sistema utilizam uma parcela dos recursos disponíveis. Dentro da API
do Kubernetes, cada nó tem um campo <code>.status.allocatable</code>
(consulte <a href=/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus>NodeStatus</a>
para mais detalhes).</p>
<p>O campo <code>.status.allocatable</code> descreve a quantidade de recursos que está
disponível a Pods naquele nó (por exemplo: 15 CPUs virtuais e 7538 MiB de
memória). Para mais informações sobre recursos alocáveis do nó no Kubernetes,
veja <a href=/docs/tasks/administer-cluster/reserve-compute-resources/>Reserve Compute Resources for System Daemons</a>.</p>
<p>Você pode configurar <a href=/docs/concepts/policy/resource-quotas/>quotas de recursos</a>
para limitar a quantidade total de recursos que um namespace pode consumir.
O Kubernetes garante quotas para objetos em um namespace específico quando há
uma <code>ResourceQuota</code> naquele namespace. Por exemplo, se você atribuir namespaces
específicos a times diferentes, você pode adicionar <code>ResourceQuota</code>s nestes
namespaces. Criar quotas de recursos ajuda a evitar que um time utilize tanto de
um recurso que chegue a afetar outros times utilizando o mesmo cluster.</p>
<p>Você deve também considerar o nível de acesso fornecido aos usuários de qualquer
namespace: acesso <strong>completo</strong> para escrita permite a alguém com este acesso
remover <strong>qualquer</strong> recurso, incluindo uma configuração de <code>ResourceQuota</code>.</p>
<h3 id=meu-contêiner-foi-terminado>Meu contêiner foi terminado</h3>
<p>Seu contêiner pode ser terminado se faltar recursos para que este rode. Para
verificar se um contêiner está sendo terminado por chegar no limite de algum
recurso, utilize o comando <code>kubectl describe pod</code> no Pod em questão:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pod simmemleak-hra99
</code></pre></div><p>A saída será semelhante a:</p>
<pre><code>Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak:latest
    Limits:
      cpu:          100m
      memory:       50Mi
    State:          Running
      Started:      Tue, 07 Jul 2019 12:54:41 -0700
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Fri, 07 Jul 2019 12:54:30 -0700
      Finished:     Fri, 07 Jul 2019 12:54:33 -0700
    Ready:          False
    Restart Count:  5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image &quot;saadali/simmemleak:latest&quot; already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod
</code></pre><p>No exemplo acima, o campo <code>Restart Count: 5</code> indica que o contêiner <code>simmemleak</code>
deste Pod foi terminado e reiniciado cinco vezes até o momento. A razão
<code>OOMKilled</code> demonstra que o contêiner tentou consumir mais memória do que o seu
limite.</p>
<p>O próximo passo neste cenário seria vasculhar e depurar o código da aplicação,
procurando por vazamentos de memória. Se você determinar que a aplicação está se
comportando conforme o esperado, considere aumentar o limite (e possivelmente
o requerimento) de memória para aquele contêiner.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Pratique <a href=/docs/tasks/configure-pod-container/assign-memory-resource/>a criação de requerimentos de recursos de memória em contêineres e Pods</a>.</li>
<li>Pratique <a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>a criação de requerimentos de CPU em contêineres and Pods</a>.</li>
<li>Leia como a referência da API define um <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container>contêiner</a>
e seus <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources>requerimentos de recursos</a>.</li>
<li>Leia sobre <a href=https://xfs.org/index.php/XFS_FAQ#Q:_Quota:_Do_quotas_work_on_XFS.3F>quotas de projeto</a> no XFS.</li>
<li>Leia mais sobre a <a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>referência de configuração do kube-scheduler (v1beta3)</a>.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ab6d20f33ad930a67ee7ef57bff6c75e>8.5 - Organizando o acesso ao cluster usando arquivos kubeconfig</h1>
<p>Utilize arquivos kubeconfig para organizar informações sobre clusters, usuários, namespaces e mecanismos de autenticação. A ferramenta de linha de comando <code>kubectl</code> faz uso dos arquivos kubeconfig para encontrar as informações necessárias para escolher e se comunicar com o serviço de API de um cluster.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Um arquivo que é utilizado para configurar o acesso aos clusters é chamado de <em>kubeconfig</em>. Esta á uma forma genérica de referenciamento para um arquivo de configuração desta natureza. Isso não significa que existe um arquivo com o nome <code>kubeconfig</code>.
</div>
<p>Por padrão, o <code>kubectl</code> procura por um arquivo de nome <code>config</code> no diretório <code>$HOME/.kube</code></p>
<p>Você pode especificar outros arquivos kubeconfig através da variável de ambiente <code>KUBECONFIG</code> ou adicionando a opção <a href=/docs/reference/generated/kubectl/kubectl/><code>--kubeconfig</code></a>.</p>
<p>Para maiores detalhes na criação e especificação de um kubeconfig, veja o passo a passo em <a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters>Configurar Acesso para Múltiplos Clusters</a>.</p>
<h2 id=suportando-múltiplos-clusters-usuários-e-mecanismos-de-autenticação>Suportando múltiplos clusters, usuários e mecanismos de autenticação</h2>
<p>Imagine que você possua inúmeros clusters, e seus usuários e componentes se autenticam de várias formas. Por exemplo:</p>
<ul>
<li>Um kubelet ativo pode se autenticar utilizando certificados</li>
<li>Um usuário pode se autenticar através de tokens</li>
<li>Administradores podem possuir conjuntos de certificados os quais provém acesso aos usuários de forma individual.</li>
</ul>
<p>Através de arquivos kubeconfig, você pode organizar os seus clusters, usuários, e namespaces. Você também pode definir contextos para uma fácil troca entre clusters e namespaces.</p>
<h2 id=contexto>Contexto</h2>
<p>Um elemento de <em>contexto</em> em um kubeconfig é utilizado para agrupar parâmetros de acesso em um nome conveniente. Cada contexto possui três parâmetros: cluster, namespace, e usuário.</p>
<p>Por padrão, a ferramenta de linha de comando <code>kubectl</code> utiliza os parâmetros do <em>contexto atual</em> para se comunicar com o cluster.</p>
<p>Para escolher o contexto atual:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config use-context
</code></pre></div><h2 id=a-variável-de-ambiente-kubeconfig>A variável de ambiente KUBECONFIG</h2>
<p>A variável de ambiente <code>KUBECONFIG</code> possui uma lista dos arquivos kubeconfig. Para Linux e Mac, esta lista é delimitada por vírgula. No Windows, a lista é delimitada por ponto e vírgula. A variável de ambiente <code>KUBECONFIG</code> não é um requisito obrigatório - caso ela não exista o <code>kubectl</code> utilizará o arquivo kubeconfig padrão localizado no caminho <code>$HOME/.kube/config</code>.</p>
<p>Se a variável de ambiente <code>KUBECONFIG</code> existir, o <code>kubectl</code> utilizará uma configuração que é o resultado da combinação dos arquivos listados na variável de ambiente <code>KUBECONFIG</code>.</p>
<h2 id=combinando-arquivos-kubeconfig>Combinando arquivos kubeconfig</h2>
<p>Para inspecionar a sua configuração atual, execute o seguinte comando:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl config view
</code></pre></div><p>Como descrito anteriormente, a saída poderá ser resultado de um único arquivo kubeconfig, ou poderá ser o resultado da junção de vários arquivos kubeconfig.</p>
<p>Aqui estão as regras que o <code>kubectl</code> utiliza quando realiza a combinação de arquivos kubeconfig:</p>
<ol>
<li>
<p>Se o argumento <code>--kubeconfig</code> está definido, apenas o arquivo especificado será utilizado. Apenas uma instância desta flag é permitida.</p>
<p>Caso contrário, se a variável de ambiente <code>KUBECONFIG</code> estiver definida, esta deverá ser utilizada como uma lista de arquivos a serem combinados, seguindo o fluxo a seguir:</p>
<ul>
<li>Ignorar arquivos vazios.</li>
<li>Produzir erros para aquivos cujo conteúdo não for possível desserializar.</li>
<li>O primeiro arquivo que definir um valor ou mapear uma chave determinada, será o escolhido.</li>
<li>Nunca modificar um valor ou mapear uma chave.
Exemplo: Preservar o contexto do primeiro arquivo que definir <code>current-context</code>.
Exemplo: Se dois arquivos especificarem um <code>red-user</code>, use apenas os valores do primeiro <code>red-user</code>. Mesmo se um segundo arquivo possuir entradas não conflitantes sobre a mesma entrada <code>red-user</code>, estas deverão ser descartadas.</li>
</ul>
<p>Para um exemplo de definição da variável de ambiente <code>KUBECONFIG</code> veja <a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable>Definido a variável de ambiente KUBECONFIG</a>.</p>
<p>Caso contrário, utilize o arquivo kubeconfig padrão encontrado no diretório <code>$HOME/.kube/config</code>, sem qualquer tipo de combinação.</p>
</li>
<li>
<p>Determine o contexto a ser utilizado baseado no primeiro padrão encontrado, nesta ordem:</p>
<ol>
<li>Usar o conteúdo da flag <code>--context</code> caso ela existir.</li>
<li>Usar o <code>current-context</code> a partir da combinação dos arquivos kubeconfig.</li>
</ol>
<p>Um contexto vazio é permitido neste momento.</p>
</li>
<li>
<p>Determinar o cluster e o usuário. Neste ponto, poderá ou não existir um contexto.
Determinar o cluster e o usuário no primeiro padrão encontrado de acordo com a ordem à seguir. Este procedimento deverá executado duas vezes: uma para definir o usuário a outra para definir o cluster.</p>
<ol>
<li>Utilizar a flag caso ela existir: <code>--user</code> ou <code>--cluster</code>.</li>
<li>Se o contexto não estiver vazio, utilizar o cluster ou usuário deste contexto.</li>
</ol>
<p>O usuário e o cluster poderão estar vazios neste ponto.</p>
</li>
<li>
<p>Determinar as informações do cluster atual a serem utilizadas. Neste ponto, poderá ou não existir informações de um cluster.</p>
<p>Construir cada peça de informação do cluster baseado nas opções à seguir; a primeira ocorrência encontrada será a opção vencedora:</p>
<ol>
<li>Usar as flags de linha de comando caso existirem: <code>--server</code>, <code>--certificate-authority</code>, <code>--insecure-skip-tls-verify</code>.</li>
<li>Se algum atributo do cluster existir a partir da combinação de kubeconfigs, estes deverão ser utilizados.</li>
<li>Se não existir informação de localização do servidor falhar.</li>
</ol>
</li>
<li>
<p>Determinar a informação atual de usuário a ser utilizada. Construir a informação de usuário utilizando as mesmas regras utilizadas para o caso de informações de cluster, exceto para a regra de técnica de autenticação que deverá ser única por usuário:</p>
<ol>
<li>Usar as flags, caso existirem: <code>--client-certificate</code>, <code>--client-key</code>, <code>--username</code>, <code>--password</code>, <code>--token</code>.</li>
<li>Usar os campos <code>user</code> resultado da combinação de arquivos kubeconfig.</li>
<li>Se existirem duas técnicas conflitantes, falhar.</li>
</ol>
</li>
<li>
<p>Para qualquer informação que ainda estiver ausente, utilizar os valores padrão e potencialmente solicitar informações de autenticação a partir do prompt de comando.</p>
</li>
</ol>
<h2 id=referências-de-arquivos>Referências de arquivos</h2>
<p>Arquivos e caminhos referenciados em um arquivo kubeconfig são relativos à localização do arquivo kubeconfig.</p>
<p>Referências de arquivos na linha de comando são relativas ao diretório de trabalho vigente.</p>
<p>No arquivo <code>$HOME/.kube/config</code>, caminhos relativos são armazenados de forma relativa, e caminhos absolutos são armazenados de forma absoluta.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li><a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters/>Configurar Accesso para Multiplos Clusters</a></li>
<li><a href=/docs/reference/generated/kubectl/kubectl-commands#config><code>kubectl config</code></a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-712cb3c03ff14a39e5a83a6d9b71d203>9 - Segurança</h1>
</div>
<div class=td-content>
<h1 id=pg-04eeb110d75afc8acb2cf7a3db743985>9.1 - Visão Geral da Segurança Cloud Native</h1>
<p>Esta visão geral define um modelo para pensar sobre a segurança em Kubernetes no contexto da Segurança em Cloud Native.</p>
<div class="alert alert-danger warning callout" role=alert>
<strong>Aviso:</strong> Este modelo de segurança no contêiner fornece sugestões, não prova políticas de segurança da informação.
</div>
<h2 id=os-4c-da-segurança-cloud-native>Os 4C da Segurança Cloud Native</h2>
<p>Você pode pensar na segurança em camadas. Os 4C da segurança Cloud Native são a Cloud,
Clusters, Contêineres e Código.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Esta abordagem em camadas aumenta a <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defesa em profundidade</a>
para segurança, que é amplamente considerada como uma boa prática de segurança para software de sistemas.
</div>
<figure>
<img src=/images/docs/4c.png> <figcaption>
<h4>Os 4C da Segurança Cloud Native</h4>
</figcaption>
</figure>
<p>Cada camada do modelo de segurança Cloud Native é construída sobre a próxima camada mais externa.
A camada de código se beneficia de uma base forte (Cloud, Cluster, Contêiner) de camadas seguras.
Você não pode proteger contra padrões ruins de segurança nas camadas de base através de
segurança no nível do Código.</p>
<h2 id=cloud>Cloud</h2>
<p>De muitas maneiras, a Cloud (ou servidores co-localizados, ou o datacenter corporativo) é a
<a href=https://en.wikipedia.org/wiki/Trusted_computing_base>base de computação confiável</a>
de um cluster Kubernetes. Se a camada de Cloud é vulnerável (ou
configurado de alguma maneira vulnerável), então não há garantia de que os componentes construídos
em cima desta base estejam seguros. Cada provedor de Cloud faz recomendações de segurança
para executar as cargas de trabalho com segurança nos ambientes.</p>
<h3 id=segurança-no-provedor-da-cloud>Segurança no provedor da Cloud</h3>
<p>Se você estiver executando um cluster Kubernetes em seu próprio hardware ou em um provedor de nuvem diferente,
consulte sua documentação para melhores práticas de segurança.
Aqui estão os links para as documentações de segurança dos provedores mais populares de nuvem:</p>
<table><caption style=display:none>Cloud provider security</caption>
<thead>
<tr>
<th>Provedor IaaS</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alibaba Cloud</td>
<td><a href=https://www.alibabacloud.com/trust-center>https://www.alibabacloud.com/trust-center</a></td>
</tr>
<tr>
<td>Amazon Web Services</td>
<td><a href=https://aws.amazon.com/security/>https://aws.amazon.com/security/</a></td>
</tr>
<tr>
<td>Google Cloud Platform</td>
<td><a href=https://cloud.google.com/security/>https://cloud.google.com/security/</a></td>
</tr>
<tr>
<td>IBM Cloud</td>
<td><a href=https://www.ibm.com/cloud/security>https://www.ibm.com/cloud/security</a></td>
</tr>
<tr>
<td>Microsoft Azure</td>
<td><a href=https://docs.microsoft.com/en-us/azure/security/azure-security>https://docs.microsoft.com/en-us/azure/security/azure-security</a></td>
</tr>
<tr>
<td>Oracle Cloud Infrastructure</td>
<td><a href=https://www.oracle.com/security/>https://www.oracle.com/security/</a></td>
</tr>
<tr>
<td>VMWare VSphere</td>
<td><a href=https://www.vmware.com/security/hardening-guides.html>https://www.vmware.com/security/hardening-guides.html</a></td>
</tr>
</tbody>
</table>
<h3 id=infrastructure-security>Segurança de Infraestrutura</h3>
<p>Sugestões para proteger sua infraestrutura em um cluster Kubernetes:</p>
<table><caption style=display:none>Infrastructure security</caption>
<thead>
<tr>
<th>Área de Interesse para Infraestrutura Kubernetes</th>
<th>Recomendação</th>
</tr>
</thead>
<tbody>
<tr>
<td>Acesso de rede ao servidor API (Control plane)</td>
<td>Todo o acesso ao control plane do Kubernetes publicamente na Internet não é permitido e é controlado por listas de controle de acesso à rede restritas ao conjunto de endereços IP necessários para administrar o cluster.</td>
</tr>
<tr>
<td>Acesso de rede aos Nós (nodes)</td>
<td>Os nós devem ser configurados para <em>só</em> aceitar conexões (por meio de listas de controle de acesso à rede) do control plane nas portas especificadas e aceitar conexões para serviços no Kubernetes do tipo NodePort e LoadBalancer. Se possível, esses nós não devem ser expostos inteiramente na Internet pública.</td>
</tr>
<tr>
<td>Acesso do Kubernetes à API do provedor de Cloud</td>
<td>Cada provedor de nuvem precisa conceder um conjunto diferente de permissões para o control plane e nós do Kubernetes. É melhor fornecer ao cluster permissão de acesso ao provedor de nuvem que segue o <a href=https://en.wikipedia.org/wiki/Principle_of_least_privilege>princípio do menor privilégio</a> para os recursos que ele precisa administrar. A <a href=https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles>documentação do Kops</a> fornece informações sobre as políticas e roles do IAM.</td>
</tr>
<tr>
<td>Acesso ao etcd</td>
<td>O acesso ao etcd (o armazenamento de dados do Kubernetes) deve ser limitado apenas ao control plane. Dependendo de sua configuração, você deve tentar usar etcd sobre TLS. Mais informações podem ser encontradas na <a href=https://github.com/etcd-io/etcd/tree/master/Documentation>documentação do etcd</a>.</td>
</tr>
<tr>
<td>Encriptação etcd</td>
<td>Sempre que possível, é uma boa prática encriptar todas as unidades de armazenamento, mas como o etcd mantém o estado de todo o cluster (incluindo os Secrets), seu disco deve ser criptografado.</td>
</tr>
</tbody>
</table>
<h2 id=cluster>Cluster</h2>
<p>Existem duas áreas de preocupação para proteger o Kubernetes:</p>
<ul>
<li>Protegendo os componentes do cluster que são configuráveis.</li>
<li>Protegendo as aplicações que correm no cluster.</li>
</ul>
<h3 id=cluster-components>Componentes do Cluster</h3>
<p>Se você deseja proteger seu cluster de acesso acidental ou malicioso e adotar
boas práticas de informação, leia e siga os conselhos sobre
<a href=/docs/tasks/administer-cluster/securing-a-cluster/>protegendo seu cluster</a>.</p>
<h3 id=cluster-applications>Componentes no cluster (sua aplicação)</h3>
<p>Dependendo da superfície de ataque de sua aplicação, você pode querer se concentrar em
tópicos específicos de segurança. Por exemplo: se você estiver executando um serviço (Serviço A) que é crítico
numa cadeia de outros recursos e outra carga de trabalho separada (Serviço B) que é
vulnerável a um ataque de exaustão de recursos e, por consequência, o risco de comprometer o Serviço A
é alto se você não limitar os recursos do Serviço B. A tabela a seguir lista
áreas de atenção na segurança e recomendações para proteger cargas de trabalho em execução no Kubernetes:</p>
<table>
<thead>
<tr>
<th>Área de interesse para a segurança do Workload</th>
<th>Recomendação</th>
</tr>
</thead>
<tbody>
<tr>
<td>Autorização RBAC (acesso à API Kubernetes)</td>
<td><a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a></td>
</tr>
<tr>
<td>Autenticação</td>
<td><a href=https://kubernetes.io/docs/concepts/security/controlling-access/>https://kubernetes.io/docs/concepts/security/controlling-access/</a></td>
</tr>
<tr>
<td>Gerenciamento de segredos na aplicação (e encriptando-os no etcd em repouso)</td>
<td><a href=https://kubernetes.io/docs/concepts/configuration/secret/>https://kubernetes.io/docs/concepts/configuration/secret/</a> <br> <a href=https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</a></td>
</tr>
<tr>
<td>Políticas de segurança do Pod</td>
<td><a href=https://kubernetes.io/docs/concepts/policy/pod-security-policy/>https://kubernetes.io/docs/concepts/policy/pod-security-policy/</a></td>
</tr>
<tr>
<td>Qualidade de serviço (e gerenciamento de recursos de cluster)</td>
<td><a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/>https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/</a></td>
</tr>
<tr>
<td>Políticas de Rede</td>
<td><a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>https://kubernetes.io/docs/concepts/services-networking/network-policies/</a></td>
</tr>
<tr>
<td>TLS para Kubernetes Ingress</td>
<td><a href=https://kubernetes.io/docs/concepts/services-networking/ingress/#tls>https://kubernetes.io/docs/concepts/services-networking/ingress/#tls</a></td>
</tr>
</tbody>
</table>
<h2 id=contêiner>Contêiner</h2>
<p>A segurança do contêiner está fora do escopo deste guia. Aqui estão recomendações gerais e
links para explorar este tópico:</p>
<table>
<thead>
<tr>
<th>Área de Interesse para Contêineres</th>
<th>Recomendação</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scanners de Vulnerabilidade de Contêiner e Segurança de Dependência de SO</td>
<td>Como parte da etapa de construção de imagem, você deve usar algum scanner em seus contêineres em busca de vulnerabilidades.</td>
</tr>
<tr>
<td>Assinatura Imagem e Enforcement</td>
<td>Assinatura de imagens de contêineres para manter um sistema de confiança para o conteúdo de seus contêineres.</td>
</tr>
<tr>
<td>Proibir Usuários Privilegiados</td>
<td>Ao construir contêineres, consulte a documentação para criar usuários dentro dos contêineres que tenham o menor nível de privilégio no sistema operacional necessário para cumprir o objetivo do contêiner.</td>
</tr>
<tr>
<td>Use o Contêiner em Runtime com Isolamento mais Forte</td>
<td>Selecione <a href=/docs/concepts/containers/runtime-class/>classes de contêiner runtime</a> com o provedor de isolamento mais forte.</td>
</tr>
</tbody>
</table>
<h2 id=código>Código</h2>
<p>O código da aplicação é uma das principais superfícies de ataque sobre a qual você tem maior controle.
Embora a proteção do código do aplicativo esteja fora do tópico de segurança do Kubernetes, aqui
são recomendações para proteger o código do aplicativo:</p>
<h3 id=segurança-de-código>Segurança de código</h3>
<table><caption style=display:none>Code security</caption>
<thead>
<tr>
<th>Área de Atenção para o Código</th>
<th>Recomendação</th>
</tr>
</thead>
<tbody>
<tr>
<td>Acesso só através de TLS</td>
<td>Se seu código precisar se comunicar por TCP, execute um handshake TLS com o cliente antecipadamente. Com exceção de alguns casos, encripte tudo em trânsito. Indo um passo adiante, é uma boa ideia encriptar o tráfego de rede entre os serviços. Isso pode ser feito por meio de um processo conhecido como mutual ou <a href=https://en.wikipedia.org/wiki/Mutual_authentication>mTLS</a>, que realiza uma verificação bilateral da comunicação mediante os certificados nos serviços.</td>
</tr>
<tr>
<td>Limitando intervalos de porta de comunicação</td>
<td>Essa recomendação pode ser um pouco autoexplicativa, mas, sempre que possível, você só deve expor as portas em seu serviço que são absolutamente essenciais para a comunicação ou coleta de métricas.</td>
</tr>
<tr>
<td>Segurança na Dependência de Terceiros</td>
<td>É uma boa prática verificar regularmente as bibliotecas de terceiros de sua aplicação em busca de vulnerabilidades de segurança. Cada linguagem de programação possui uma ferramenta para realizar essa verificação automaticamente.</td>
</tr>
<tr>
<td>Análise de Código Estático</td>
<td>A maioria das linguagens fornece uma maneira para analisar um extrato do código referente a quaisquer práticas de codificação potencialmente inseguras. Sempre que possível, você deve automatizar verificações usando ferramentas que podem verificar as bases de código em busca de erros de segurança comuns. Algumas das ferramentas podem ser encontradas em <a href=https://owasp.org/www-community/Source_Code_Analysis_Tools>OWASP Source Code Analysis Tools</a>.</td>
</tr>
<tr>
<td>Ataques de sondagem dinâmica</td>
<td>Existem algumas ferramentas automatizadas que você pode executar contra seu serviço para tentar alguns dos ataques mais conhecidos. Isso inclui injeção de SQL, CSRF e XSS. Uma das ferramentas de análise dinâmica mais populares é o <a href=https://owasp.org/www-project-zap/>OWASP Zed Attack proxy</a>.</td>
</tr>
</tbody>
</table>
<h2 id=próximos-passos>Próximos passos</h2>
<p>Saiba mais sobre os tópicos de segurança do Kubernetes:</p>
<ul>
<li><a href=/docs/concepts/security/pod-security-standards/>Padrões de segurança do Pod</a></li>
<li><a href=/docs/concepts/services-networking/network-policies/>Políticas de rede para Pods</a></li>
<li><a href=/docs/concepts/security/controlling-access>Controle de acesso à API Kubernetes</a></li>
<li><a href=/docs/tasks/administer-cluster/securing-a-cluster/>Protegendo seu cluster</a></li>
<li><a href=/docs/tasks/tls/managing-tls-in-a-cluster/>Criptografia de dados em trânsito</a> for the control plane</li>
<li><a href=/docs/tasks/administer-cluster/encrypt-data/>Criptografia de dados em repouso</a></li>
<li><a href=/docs/concepts/configuration/secret/>Secrets no Kubernetes</a></li>
<li><a href=/docs/concepts/containers/runtime-class>Runtime class</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c21d05f31057c5bcd2ebdd01f4e62a0e>10 - Escalonamento</h1>
<div class=lead>No Kubernetes, agendamento refere-se a garantia de que os pods correspondam aos nós para que o kubelet possa executá-los. Remoção é o processo de falha proativa de um ou mais pods em nós com falta de recursos.</div>
</div>
<div class=td-content>
<h1 id=pg-ede4960b56a3529ee0bfe7c8fe2d09a5>10.1 - Taints e Tolerâncias</h1>
<p><a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity><em>Afinidade de nó</em></a>
é uma propriedade dos <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> que os <em>associa</em> a um conjunto de <a class=glossary-tooltip title="Um Nó é uma máquina de trabalho no Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nós>nós</a> (seja como uma preferência ou uma exigência). <em>Taints</em> são o oposto -- eles permitem que um nó repudie um conjunto de pods.</p>
<p><em>Tolerâncias</em> são aplicadas em pods e permitem, mas não exigem, que os pods sejam alocados em nós com <em>taints</em> correspondentes.</p>
<p>Taints e tolerâncias trabalham juntos para garantir que pods não sejam alocados em nós inapropriados. Um ou mais taints são aplicados em um nó; isso define que o nó não deve aceitar nenhum pod que não tolera essas taints.</p>
<h2 id=conceitos>Conceitos</h2>
<p>Você adiciona um taint a um nó utilizando <a href=/docs/reference/generated/kubectl/kubectl-commands#taint>kubectl taint</a>.
Por exemplo,</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</code></pre></div><p>define um taint no nó <code>node1</code>. O taint tem a chave <code>key1</code>, valor <code>value1</code> e o efeito <code>NoSchedule</code>.
Isso significa que nenhum pod conseguirá ser executado no nó <code>node1</code> a menos que possua uma tolerância correspondente.</p>
<p>Para remover o taint adicionado pelo comando acima, você pode executar:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule-
</code></pre></div><p>Você especifica uma tolerância para um pod na <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1#PodSpec>especificação do Pod</a>. Ambas as seguintes tolerâncias "correspondem" ao taint criado pelo <code>kubectl taint</code> acima, e assim um pod com qualquer uma delas poderia ser executado no <code>node1</code>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Aqui está um exemplo de um pod que utiliza tolerâncias:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/pods/pod-with-toleration.yaml download=pods/pod-with-toleration.yaml><code>pods/pod-with-toleration.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-with-toleration-yaml')" title="Copy pods/pod-with-toleration.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-pod-with-toleration-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-key&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>O valor padrão de <code>operator</code> é <code>Equal</code>.</p>
<p>Uma tolerância "casa" um taint se as chaves e efeitos são os mesmos, e:</p>
<ul>
<li>o valor de <code>operator</code> é <code>Exists</code> (no caso nenhum <code>value</code> deve ser especificado), ou</li>
<li>o valor de <code>operator</code> é <code>Equal</code> e os valores de <code>value</code> são iguais.</li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> <p>Existem dois casos especiais:</p>
<p>Uma <code>key</code> vazia com o operador <code>Exists</code> "casa" todas as chaves, valores e efeitos, o que significa que o pod irá tolerar tudo.</p>
<p>Um <code>effect</code> vazio "casa" todos os efeitos com a chave <code>key1</code>.</p>
</div>
<p>O exemplo acima usou <code>effect</code> de <code>NoSchedule</code>. De forma alternativa, você pode usar <code>effect</code> de <code>PreferNoSchedule</code>.
Nesse efeito, o sistema <em>tentará</em> evitar que o pod seja alocado ao nó caso ele não tolere os taints definidos, contudo a alocação não será evitada de forma obrigatória. Pode-se dizer que o <code>PreferNoSchedule</code> é uma versão permissiva do <code>NoSchedule</code>. O terceiro tipo de <code>effect</code> é o <code>NoExecute</code> que será descrito posteriormente.</p>
<p>Você pode colocar múltiplos taints no mesmo nó e múltiplas tolerâncias no mesmo pod.
O jeito que o Kubernetes processa múltiplos taints e tolerâncias é como um filtro: começa com todos os taints de um nó, em seguida ignora aqueles para os quais o pod tem uma tolerância relacionada; os taints restantes que não foram ignorados indicam o efeito no pod. Mais especificamente,</p>
<ul>
<li>se existe pelo menos um taint não tolerado com o efeito <code>NoSchedule</code>, o Kubernetes não alocará o pod naquele nó</li>
<li>se existe um taint não tolerado com o efeito <code>NoSchedule</code>, mas existe pelo menos um taint não tolerado com o efeito <code>PreferNoSchedule</code>, o Kubernetes <em>tentará</em> não alocar o pod no nó</li>
<li>se existe pelo menos um taint não tolerado com o efeito <code>NoExecute</code>, o pod será expulso do nó (caso já esteja em execução) e não será alocado ao nó (caso ainda não esteja em execução).</li>
</ul>
<p>Por exemplo, imagine que você tem um nó com os seguintes taints</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoExecute
kubectl taint nodes node1 <span style=color:#b8860b>key2</span><span style=color:#666>=</span>value2:NoSchedule
</code></pre></div><p>E um pod com duas tolerâncias:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Nesse caso, o pod não será alocado ao nó porque não possui uma tolerância para o terceiro taint. Porém, se ele já estiver rodando no nó quando o taint foi adicionado, não será afetado e continuará rodando, tendo em vista que o terceiro taint é o único não tolerado pelo pod.</p>
<p>Normalmente, se um taint com o efeito <code>NoExecute</code> é adicionado a um nó, qualquer pod que não o tolere será expulso imediatamente e pods que o toleram nunca serão expulsos. Contudo, uma tolerância com efeito <code>NoExecute</code> pode especificar de forma opcional o campo <code>tolerationSeconds</code>, que determina quanto tempo o pod continuará alocado ao nó depois que o taint é adicionado. Por exemplo,</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></code></pre></div><p>significa que se esse pod está sendo executado e um taint correspondente é adicionado ao nó, o pod irá continuar rodando neste nó por 3600 segundos e depois será expulso. Se o taint for removido antes desse tempo acabar, o pod não será expulso.</p>
<h2 id=exemplos-de-casos-de-uso>Exemplos de Casos de Uso</h2>
<p>Taints e tolerâncias são um modo flexível de conduzir pods para <em>fora</em> dos nós ou expulsar pods que não deveriam estar sendo executados. Alguns casos de uso são</p>
<ul>
<li>
<p><strong>Nós Dedicados</strong>: Se você quiser dedicar um conjunto de nós para uso exclusivo de um conjunto específico de usuários, poderá adicionar um taint nesses nós. (digamos, <code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>) e em seguida adicionar uma tolerância correspondente para seus pods (isso seria feito mais facilmente com a escrita de um <a href=/docs/reference/access-authn-authz/admission-controllers/>controlador de admissão</a> customizado).
Os pods com tolerância terão sua execução permitida nos nós com taints (dedicados), assim como em qualquer outro nó no cluster. Se você quiser dedicar nós a esses pods <em>e garantir</em> que eles usem <em>apenas</em> os nós dedicados, precisará adicionar uma label similar ao taint para o mesmo conjunto de nós (por exemplo, <code>dedicated=groupName</code>), e o controle de admissão deverá adicionar uma afinidade de nó para exigir que os pods podem ser executados apenas nos nós definidos com a label <code>dedicated=groupName</code>.</p>
</li>
<li>
<p><strong>Nós com hardware especial</strong>: Em um cluster no qual um pequeno grupo de nós possui hardware especializado (por exemplo, GPUs), é desejável manter pods que não necessitem desse tipo de hardware fora desses nós, dessa forma o recurso estará disponível para pods que precisem do hardware especializado. Isso pode ser feito aplicando taints nos nós com o hardware especializado (por exemplo, <code>kubectl taint nodes nodename special=true:NoSchedule</code> or <code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>) e aplicando uma tolerância correspondente nos pods que usam o hardware especial. Assim como no caso de uso de nós dedicados, é provavelmente mais fácil aplicar as tolerâncias utilizando um <a href=/docs/reference/access-authn-authz/admission-controllers/>controlador de admissão</a>.
Por exemplo, é recomendado usar <a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>Extended Resources</a> para representar hardware especial, adicione um taint ao seus nós de hardware especializado com o nome do recurso estendido e execute o controle de admissão <a href=/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration>ExtendedResourceToleration</a>. Agora, tendo em vista que os nós estão marcados com um taint, nenhum pod sem a tolerância será executado neles. Porém, quando você submete um pod que requisita o recurso estendido, o controlador de admissão <code>ExtendedResourceToleration</code> irá adicionar automaticamente as tolerâncias necessárias ao pod que irá, por sua vez, ser alocado no nó com hardware especial. Isso garantirá que esses nós de hardware especial serão dedicados para os pods que requisitarem tal recurso e você não precisará adicionar manualmente as tolerâncias aos seus pods.</p>
</li>
<li>
<p><strong>Expulsões baseadas em Taint</strong>: Um comportamento de expulsão configurada por pod quando problemas existem em um nó, o qual será descrito na próxima seção.</p>
</li>
</ul>
<h2 id=expulsões-baseadas-em-taint>Expulsões baseadas em Taint</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>
<p>O efeito de taint <code>NoExecute</code>, mencionado acima, afeta pods que já estão rodando no nó da seguinte forma</p>
<ul>
<li>pods que não toleram o taint são expulsos imediatamente</li>
<li>pods que toleram o taint sem especificar <code>tolerationSeconds</code> em sua especificação de tolerância, ficam alocados para sempre</li>
<li>pods que toleram o taint com um <code>tolerationSeconds</code> especificado, permanecem alocados pela quantidade de tempo definida</li>
</ul>
<p>O controlador de nó automaticamente adiciona um taint ao Nó quando certas condições se tornam verdadeiras. Os seguintes taints são embutidos:</p>
<ul>
<li><code>node.kubernetes.io/not-ready</code>: Nó não está pronto. Isso corresponde ao NodeCondition <code>Ready</code> com o valor "<code>False</code>".</li>
<li><code>node.kubernetes.io/unreachable</code>: Nó é inalcançável a partir do controlador de nó. Isso corresponde ao NodeCondition <code>Ready</code> com o valor "<code>Unknown</code>".</li>
<li><code>node.kubernetes.io/memory-pressure</code>: Nó possui pressão de memória.</li>
<li><code>node.kubernetes.io/disk-pressure</code>: Nó possui pressão de disco.</li>
<li><code>node.kubernetes.io/pid-pressure</code>: Nó possui pressão de PID.</li>
<li><code>node.kubernetes.io/network-unavailable</code>: A rede do nó está indisponível.</li>
<li><code>node.kubernetes.io/unschedulable</code>: Nó não é alocável.</li>
<li><code>node.cloudprovider.kubernetes.io/uninitialized</code>: Quando o kubelet é iniciado com um provedor de nuvem "externo", esse taint é adicionado ao nó para que ele seja marcado como não utilizável. Após o controlador do cloud-controller-manager inicializar o nó, o kubelet remove esse taint.</li>
</ul>
<p>No caso de um nó estar prestes a ser expulso, o controlador de nó ou kubelet adicionam os taints relevantes com o efeito <code>NoExecute</code>. Se a condição de falha retorna ao normal, o kubelet ou controlador de nó podem remover esses taints.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> A camada de gerenciamento limita a taxa de adição de novos taints aos nós. Esse limite gerencia o número de expulsões que são disparadas quando muitos nós se tornam inalcançáveis ao mesmo tempo (por exemplo: se ocorre uma falha na rede).
</div>
<p>Você pode especificar <code>tolerationSeconds</code> em um Pod para definir quanto tempo ele ficará alocado em um nó que está falhando ou está sem resposta.</p>
<p>Por exemplo, você talvez queira manter uma aplicação com vários estados salvos localmente alocado em um nó por um longo período na ocorrência de uma divisão na rede, esperando que essa divisão se recuperará e assim a expulsão do pod pode ser evitada.
A tolerância que você define para esse Pod poderia ficar assim:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;node.kubernetes.io/unreachable&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>6000</span><span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> <p>O Kubernetes automaticamente adiciona uma tolerância para <code>node.kubernetes.io/not-ready</code> e <code>node.kubernetes.io/unreachable</code> com <code>tolerationSeconds=300</code>, a menos que você, ou um controlador, defina essas tolerâncias explicitamente.</p>
<p>Essas tolerâncias adicionadas automaticamente significam que Pods podem continuar alocados aos Nós por 5 minutos após um desses problemas ser detectado.</p>
</div>
<p>Pods do tipo <a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> são criados com tolerâncias <code>NoExecute</code> sem a propriedade <code>tolerationSeconds</code> para os seguintes taints:</p>
<ul>
<li><code>node.kubernetes.io/unreachable</code></li>
<li><code>node.kubernetes.io/not-ready</code></li>
</ul>
<p>Isso garante que esses pods do DaemonSet nunca sejam expulsos por conta desses problemas.</p>
<h2 id=taints-por-condições-de-nó>Taints por condições de nó</h2>
<p>A camada de gerenciamento, usando o <a class=glossary-tooltip title="Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a> do nó, cria taints automaticamente com o efeito <code>NoSchedule</code> para <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions>condições de nó</a>.</p>
<p>O agendador verifica taints, não condições de nó, quando realiza suas decisões de agendamento. Isso garante que as condições de nó não afetem diretamente o agendamento.
Por exemplo, se a condição de nó <code>DiskPressure</code> está ativa, a camada de gerenciamento adiciona o taint <code>node.kubernetes.io/disk-pressure</code> e não aloca novos pods no nó afetado. Se a condição <code>MemoryPressure</code> está ativa, a camada de gerenciamento adiciona o taint <code>node.kubernetes.io/memory-pressure</code>.</p>
<p>Você pode ignorar condições de nó para pods recém-criados adicionando tolerâncias correspondentes. A camada de controle também adiciona a tolerância <code>node.kubernetes.io/memory-pressure</code> em pods que possuem uma <a class=glossary-tooltip title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-qos-class" target=_blank aria-label="classe de QoS">classe de QoS</a> diferente de <code>BestEffort</code>. Isso ocorre porque o Kubernetes trata pods nas classes de QoS <code>Guaranteed</code> ou <code>Burstable</code> (até mesmo pods sem requisitos de memória definidos) como se fossem capazes de lidar com pressão de memória, enquanto novos pods com <code>BestEffort</code> não são alocados no nó afetado.</p>
<p>O controlador DaemonSet adiciona automaticamente as seguintes tolerâncias de <code>NoSchedule</code> para todos os daemons, prevenindo que DaemonSets quebrem.</p>
<ul>
<li><code>node.kubernetes.io/memory-pressure</code></li>
<li><code>node.kubernetes.io/disk-pressure</code></li>
<li><code>node.kubernetes.io/pid-pressure</code> (1.14 ou superior)</li>
<li><code>node.kubernetes.io/unschedulable</code> (1.10 ou superior)</li>
<li><code>node.kubernetes.io/network-unavailable</code> (<em>somente rede do host</em>)</li>
</ul>
<p>Adicionando essas tolerâncias garante retro compatibilidade. Você também pode adicionar tolerâncias de forma arbitrária aos DaemonSets.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia sobre <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a> e como você pode configurá-la</li>
<li>Leia sobre <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-598f36d691ab197f9d995784574b0a12>10.2 - Escalonador do Kubernetes</h1>
<p>No Kubernetes, <em>escalonamento</em> refere-se a garantir que os <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>
sejam correspondidos aos <a class=glossary-tooltip title="Um Nó é uma máquina de trabalho no Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a> para que o
<a class=glossary-tooltip title="Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=Kubelet>Kubelet</a> possa executá-los.</p>
<h2 id=escalonamento>Visão geral do Escalonamento</h2>
<p>Um escalonador observa Pods recém-criados que não possuem um Node atribuído.
Para cada Pod que o escalonador descobre, ele se torna responsável por
encontrar o melhor Node para execução do Pod. O escalonador chega a essa decisão de alocação levando em consideração os princípios de programação descritos abaixo.</p>
<p>Se você quiser entender por que os Pods são alocados em um Node específico
ou se planeja implementar um escalonador personalizado, esta página ajudará você a
aprender sobre escalonamento.</p>
<h2 id=kube-scheduler>kube-scheduler</h2>
<p><a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
é o escalonador padrão do Kubernetes e é executado como parte do
<a class=glossary-tooltip title="A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a>.
O kube-scheduler é projetado para que, se você quiser e precisar, possa
escrever seu próprio componente de escalonamento e usá-lo.</p>
<p>Para cada Pod recém-criado ou outros Pods não escalonados, o kube-scheduler
seleciona um Node ideal para execução. No entanto, todos os contêineres nos Pods
têm requisitos diferentes de recursos e cada Pod também possui requisitos diferentes.
Portanto, os Nodes existentes precisam ser filtrados de acordo com os requisitos de
escalonamento específicos.</p>
<p>Em um cluster, Nodes que atendem aos requisitos de escalonamento para um Pod
são chamados de Nodes <em>viáveis</em>. Se nenhum dos Nodes for adequado, o Pod
permanece não escalonado até que o escalonador possa alocá-lo.</p>
<p>O escalonador encontra Nodes viáveis para um Pod e, em seguida, executa um conjunto de
funções para pontuar os Nodes viáveis e escolhe um Node com a maior
pontuação entre os possíveis para executar o Pod. O escalonador então notifica
o servidor da API sobre essa decisão em um processo chamado <em>binding</em>.</p>
<p>Fatores que precisam ser levados em consideração para decisões de escalonamento incluem
requisitos individuais e coletivos de recursos,
restrições de hardware / software / política, especificações de afinidade e anti-afinidade,
localidade de dados, interferência entre cargas de trabalho e assim por diante.</p>
<h3 id=implementação-kube-scheduler>Seleção do Node no kube-scheduler</h3>
<p>O kube-scheduler seleciona um Node para o Pod em uma operação que consiste em duas etapas:</p>
<ol>
<li>Filtragem</li>
<li>Pontuação</li>
</ol>
<p>A etapa de <em>filtragem</em> localiza o conjunto de Nodes onde é possível
alocar o Pod. Por exemplo, o filtro PodFitsResources verifica se um Node
candidato possui recursos disponíveis suficientes para atender às solicitações
de recursos específicas de um Pod. Após esta etapa, a lista de Nodes contém
quaisquer Nodes adequados; frequentemente, haverá mais de um. Se a lista estiver vazia,
esse Pod (ainda) não é escalonável.</p>
<p>Na etapa de <em>pontuação</em>, o escalonador classifica os Nodes restantes para escolher
o mais adequado. O escalonador atribui uma pontuação a cada Node
que sobreviveu à filtragem, baseando essa pontuação nas regras de pontuação ativa.</p>
<p>Por fim, o kube-scheduler atribui o Pod ao Node com a classificação mais alta.
Se houver mais de um Node com pontuações iguais, o kube-scheduler seleciona
um deles aleatoriamente.</p>
<p>Existem duas maneiras suportadas de configurar o comportamento de filtragem e pontuação
do escalonador:</p>
<ol>
<li>
<p><a href=/docs/reference/scheduling/policies>Políticas de Escalonamento</a> permitem configurar <em>Predicados</em> para filtragem e <em>Prioridades</em> para pontuação.</p>
</li>
<li>
<p><a href=/docs/reference/scheduling/profiles>Perfis de Escalonamento</a> permitem configurar Plugins que implementam diferentes estágios de escalonamento, incluindo: <code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code>, e outros. Você também pode configurar o kube-scheduler para executar diferentes perfis.</p>
</li>
</ol>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia sobre <a href=/docs/concepts/scheduling/scheduler-perf-tuning/>ajuste de desempenho do escalonador</a></li>
<li>Leia sobre <a href=/docs/concepts/workloads/pods/pod-topology-spread-constraints/>restrições de propagação da topologia de pod</a></li>
<li>Leia a <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>documentação de referência</a> para o kube-scheduler</li>
<li>Aprenda como <a href=/docs/tasks/administer-cluster/configure-multiple-schedulers/>configurar vários escalonadores</a></li>
<li>Aprenda sobre <a href=/docs/tasks/administer-cluster/topology-manager/>políticas de gerenciamento de topologia</a></li>
<li>Aprenda sobre <a href=/docs/concepts/configuration/pod-overhead/>Pod Overhead</a></li>
<li>Saiba mais sobre o agendamento de pods que usam volumes em:
<ul>
<li><a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>Suporte de topologia de volume</a></li>
<li><a href=/docs/concepts/storage/storage-capacity/>Rastreamento de capacidade de armazenamento</a></li>
<li><a href=/docs/concepts/storage/storage-limits/>Limites de volumes específicos do nó</a></li>
</ul>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-da22fe2278df236f71efbe672f392677>10.3 - Sobrecarga de Pod</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>
<p>Quando você executa um Pod num nó, o próprio Pod usa uma quantidade de recursos do sistema. Estes
recursos são adicionais aos recursos necessários para executar o(s) contêiner(s) dentro do Pod.
Sobrecarga de Pod, do inglês <em>Pod Overhead</em>, é uma funcionalidade que serve para contabilizar os recursos consumidos pela
infraestrutura do Pod para além das solicitações e limites do contêiner.</p>
<p>No Kubernetes, a sobrecarga de Pods é definido no tempo de
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks>admissão</a>
de acordo com a sobrecarga associada à
<a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a> do Pod.</p>
<p>Quando é ativada a Sobrecarga de Pod, a sobrecarga é considerada adicionalmente à soma das
solicitações de recursos do contêiner ao agendar um Pod. Semelhantemente, o <em>kubelet</em>
incluirá a sobrecarga do Pod ao dimensionar o cgroup do Pod e ao
executar a classificação de prioridade de migração do Pod em caso de <em>drain</em> do Node.</p>
<h2 id=set-up>Habilitando a Sobrecarga de Pod</h2>
<p>Terá de garantir que o <a href=/docs/reference/command-line-tools-reference/feature-gates/>Feature Gate</a>
<code>PodOverhead</code> esteja ativo (está ativo por padrão a partir da versão 1.18)
em todo o cluster, e uma <code>RuntimeClass</code> utilizada que defina o campo <code>overhead</code>.</p>
<h2 id=exemplo-de-uso>Exemplo de uso</h2>
<p>Para usar a funcionalidade PodOverhead, é necessário uma RuntimeClass que define o campo <code>overhead</code>.
Por exemplo, poderia usar a definição da RuntimeClass abaixo com um agente de execução de contêiner virtualizado
que use cerca de 120MiB por Pod para a máquina virtual e o sistema operacional convidado:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>overhead</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podFixed</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;120Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>As cargas de trabalho que são criadas e que especificam o manipulador RuntimeClass <code>kata-fc</code> irão
usar a sobrecarga de memória e cpu em conta para os cálculos da quota de recursos, agendamento de nós,
assim como dimensionamento do cgroup do Pod.</p>
<p>Considere executar a seguinte carga de trabalho de exemplo, test-pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>stdin</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tty</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>1500m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></code></pre></div><p>No tempo de admissão o <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/>controlador de admissão</a> RuntimeClass
atualiza o <em>PodSpec</em> da carga de trabalho de forma a incluir o <code>overhead</code> como descrito na RuntimeClass. Se o <em>PodSpec</em> já tiver este campo definido
o Pod será rejeitado. No exemplo dado, como apenas o nome do RuntimeClass é especificado, o controlador de admissão muda o Pod de forma a
incluir um <code>overhead</code>.</p>
<p>Depois do controlador de admissão RuntimeClass, pode verificar o <em>PodSpec</em> atualizado:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.overhead}&#39;</span>
</code></pre></div><p>A saída é:</p>
<pre><code>map[cpu:250m memory:120Mi]
</code></pre><p>Se for definido um <em>ResourceQuota</em>, a soma das requisições dos contêineres assim como o campo <code>overhead</code> são contados.</p>
<p>Quando o kube-scheduler está decidindo que nó deve executar um novo Pod, o agendador considera o <code>overhead</code> do pod,
assim como a soma de pedidos aos contêineres para esse <em>Pod</em>. Para este exemplo, o agendador adiciona as requisições e a sobrecarga, depois procura um nó com 2.25 CPU e 320 MiB de memória disponível.</p>
<p>Assim que um Pod é agendado a um nó, o kubelet nesse nó cria um novo <a class=glossary-tooltip title="Um grupo de processos do Linux com isolamento de recursos opcional, contagem e limites." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-cgroup" target=_blank aria-label=cgroup>cgroup</a>
para o Pod. É dentro deste Pod que o agente de execução de contêiners subjacente vai criar contêineres.</p>
<p>Se o recurso tiver um limite definido para cada contêiner (<em>QoS</em> garantida ou <em>Burstrable QoS</em> com limites definidos),
o kubelet definirá um limite superior para o cgroup do Pod associado a esse recurso (cpu.cfs_quota_us para CPU
e memory.limit_in_bytes de memória). Este limite superior é baseado na soma dos limites do contêiner mais o <code>overhead</code>
definido no <em>PodSpec</em>.</p>
<p>Para CPU, se o Pod for QoS garantida ou <em>Burstrable QoS</em>, o kubelet vai definir <code>cpu.shares</code> baseado na soma dos
pedidos ao contêiner mais o <code>overhead</code> definido no <em>PodSpec</em>.</p>
<p>Olhando para o nosso exemplo, verifique as requisições ao contêiner para a carga de trabalho:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.containers[*].resources.limits}&#39;</span>
</code></pre></div><p>O total de requisições ao contêiner são 2000m CPU e 200MiB de memória:</p>
<pre><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><p>Verifique isto comparado ao que é observado pelo nó:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl describe node | grep test-pod -B2
</code></pre></div><p>A saída mostra que 2250m CPU e 320MiB de memória são solicitados, que inclui <em>PodOverhead</em>:</p>
<pre><code>  Namespace                   Name                CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------                   ----                ------------  ----------   ---------------  -------------  ---
  default                     test-pod            2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><h2 id=verificar-os-limites-cgroup-do-pod>Verificar os limites cgroup do Pod</h2>
<p>Verifique os cgroups de memória do Pod no nó onde a carga de trabalho está em execução. No seguinte exemplo, <a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md><code>crictl</code></a>
é usado no nó, que fornece uma CLI para agentes de execução compatíveis com CRI. Isto é um
exemplo avançado para mostrar o comportamento do <em>PodOverhead</em>, e não é esperado que os usuários precisem verificar
cgroups diretamente no nó.</p>
<p>Primeiro, no nó em particular, determine o identificador do Pod:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Execute no nó onde o Pod está agendado</span>
<span style=color:#b8860b>POD_ID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>sudo crictl pods --name test-pod -q<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</code></pre></div><p>A partir disto, pode determinar o caminho do cgroup para o <em>Pod</em>:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Execute no nó onde o Pod está agendado</span>
sudo crictl inspectp -o<span style=color:#666>=</span>json <span style=color:#b8860b>$POD_ID</span> | grep cgroupsPath
</code></pre></div><p>O caminho do cgroup resultante inclui o contêiner <code>pause</code> do Pod. O cgroup no nível do Pod está um diretório acima.</p>
<pre><code>        &quot;cgroupsPath&quot;: &quot;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a&quot;
</code></pre><p>Neste caso especifico, o caminho do cgroup do Pod é <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>. Verifique a configuração cgroup de nível do Pod para a memória:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Execute no nó onde o Pod está agendado</span>
<span style=color:#080;font-style:italic># Mude também o nome do cgroup para combinar com o cgroup alocado ao Pod.</span>
 cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</code></pre></div><p>Isto é 320 MiB, como esperado:</p>
<pre><code>335544320
</code></pre><h3 id=observabilidade>Observabilidade</h3>
<p>Uma métrica <code>kube_pod_overhead</code> está disponível em <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a>
para ajudar a identificar quando o <em>PodOverhead</em> está sendo utilizado e para ajudar a observar a estabilidade das cargas de trabalho
em execução com uma sobrecarga (<em>Overhead</em>) definida. Esta funcionalidade não está disponível na versão 1.9 do kube-state-metrics,
mas é esperado em uma próxima versão. Os usuários necessitarão entretanto construir o kube-state-metrics a partir do código fonte.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li><a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a></li>
<li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20190226-pod-overhead.md>PodOverhead Design</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-285a3785fd3d20f437c28d87ca4dadca>11 - Administração de Cluster</h1>
<div class=lead>Detalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.</div>
<p>A visão geral da administração do cluster é para qualquer pessoa que crie ou administre um cluster do Kubernetes.
É pressuposto alguma familiaridade com os <a href=/docs/concepts>conceitos</a> principais do Kubernetes.</p>
<h2 id=planejando-um-cluster>Planejando um cluster</h2>
<p>Consulte os guias em <a href=/docs/setup>Configuração</a> para exemplos de como planejar, instalar e configurar clusters Kubernetes. As soluções listadas neste artigo são chamadas de <em>distros</em>.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Nem todas as distros são mantidas ativamente. Escolha distros que foram testadas com uma versão recente do Kubernetes.
</div>
<p>Antes de escolher um guia, aqui estão algumas considerações:</p>
<ul>
<li>Você quer experimentar o Kubernetes em seu computador ou deseja criar um cluster de vários nós com alta disponibilidade? Escolha as distros mais adequadas ás suas necessidades.</li>
<li>Você vai usar um <strong>cluster Kubernetes gerenciado</strong> , como o <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>, ou <strong>vai hospedar seu próprio cluster</strong>?</li>
<li>Seu cluster será <strong>local</strong>, ou <strong>na nuvem (IaaS)</strong>? O Kubernetes não oferece suporte direto a clusters híbridos. Em vez disso, você pode configurar vários clusters.</li>
<li><strong>Se você estiver configurando o Kubernetes local</strong>, leve em consideração qual <a href=/docs/concepts/cluster-Administration/networking>modelo de rede</a> se encaixa melhor.</li>
<li>Você vai executar o Kubernetes em um hardware <strong>bare metal</strong> ou em <strong>máquinas virtuais? (VMs)</strong>?</li>
<li>Você <strong>deseja apenas executar um cluster</strong> ou espera <strong>participar ativamente do desenvolvimento do código do projeto Kubernetes</strong>? Se for a segunda opção,
escolha uma distro desenvolvida ativamente. Algumas distros usam apenas versão binária, mas oferecem uma maior variedade de opções.</li>
<li>Familiarize-se com os <a href=/docs/concepts/overview/components/>componentes</a> necessários para executar um cluster.</li>
</ul>
<h2 id=gerenciando-um-cluster>Gerenciando um cluster</h2>
<ul>
<li>Aprenda como <a href=/docs/concepts/architecture/nodes/>gerenciar nós</a>.</li>
<li>Aprenda a configurar e <a href=/docs/concepts/policy/resource-quotas/>gerenciar a quota de recursos</a> para clusters compartilhados.</li>
</ul>
<h2 id=protegendo-um-cluster>Protegendo um cluster</h2>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/certificates/>Gerar Certificados</a> descreve os passos para gerar certificados usando diferentes cadeias de ferramentas.</p>
</li>
<li>
<p><a href=/docs/concepts/containers/container-environment/>Ambiente de Contêineres do Kubernetes</a> descreve o ambiente para contêineres gerenciados pelo kubelet em um nó Kubernetes.</p>
</li>
<li>
<p><a href=/docs/concepts/security/controlling-access>Controle de Acesso a API do Kubernetes</a> descreve como o Kubernetes implementa o controle de acesso para sua própria API.</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/authentication/>Autenticação</a> explica a autenticação no Kubernetes, incluindo as várias opções de autenticação.</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/authorization/>Autorização</a> é separado da autenticação e controla como as chamadas HTTP são tratadas.</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/admission-controllers/>Usando Controladores de Admissão</a> explica plugins que interceptam requisições para o servidor da API Kubernetes após
a autenticação e autorização.</p>
</li>
<li>
<p><a href=/docs/tasks/administer-cluster/sysctl-cluster/>usando Sysctl em um Cluster Kubernetes</a> descreve a um administrador como usar a ferramenta de linha de comando <code>sysctl</code> para
definir os parâmetros do kernel.</p>
</li>
<li>
<p><a href=/docs/tasks/debug-application-cluster/audit/>Auditoria</a> descreve como interagir com <em>logs</em> de auditoria do Kubernetes.</p>
</li>
</ul>
<h3 id=protegendo-o-kubelet>Protegendo o kubelet</h3>
<ul>
<li><a href=/docs/concepts/architecture/control-plane-node-communication/>Comunicação Control Plane-Nó</a></li>
<li><a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>TLS bootstrapping</a></li>
<li><a href=/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/>Autenticação/autorização do kubelet</a></li>
</ul>
<h2 id=serviços-opcionais-para-o-cluster>Serviços Opcionais para o Cluster</h2>
<ul>
<li>
<p><a href=/docs/concepts/services-networking/dns-pod-service/>Integração com DNS</a> descreve como resolver um nome DNS diretamente para um serviço Kubernetes.</p>
</li>
<li>
<p><a href=/docs/concepts/cluster-administration/logging/>Registro e Monitoramento da Atividade do Cluster</a> explica como funciona o <em>logging</em> no Kubernetes e como implementá-lo.</p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-fb494ea3b1874bd753dcd11c3f35c2dc>11.1 - Visão Geral da Administração de Cluster</h1>
<p>A visão geral da administração de cluster é para qualquer um criando ou administrando um cluster Kubernetes. Assume-se que você tenha alguma familiaridade com os <a href=/docs/concepts/>conceitos</a> centrais do Kubernetes.</p>
<h2 id=planejando-um-cluster>Planejando um cluster</h2>
<p>Veja os guias em <a href=/docs/setup/>Setup</a> para exemplos de como planejar, iniciar e configurar clusters Kubernetes. As soluções listadas neste artigo são chamadas <em>distros</em>.</p>
<p>Antes de escolher um guia, aqui estão algumas considerações.</p>
<ul>
<li>
<p>Você quer experimentar o Kubernetes no seu computador, ou você quer construir um cluster de alta disponibilidade e multi-nós? Escolha as distros mais adequadas às suas necessidades.</p>
</li>
<li>
<p><strong>Se você esta projetando para alta-disponibilidade</strong>, saiba mais sobre configuração <a href=/docs/concepts/cluster-administration/federation/>clusters em múltiplas zonas</a>.</p>
</li>
<li>
<p>Você usará <strong>um cluster Kubernetes hospedado</strong>, como <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>, ou <strong>hospedará seu próprio cluster</strong>?</p>
</li>
<li>
<p>Seu cluster será <strong>on-premises</strong>, ou <strong>in the cloud (IaaS)</strong>? Kubernetes não suporta diretamente clusters híbridos. Em vez disso, você pode configurar vários clusters.</p>
</li>
<li>
<p><strong>Se você estiver configurando um Kubernetes on-premisess</strong>, considere qual <a href=/docs/concepts/cluster-administration/networking/>modelo de rede</a> melhor se adequa.</p>
</li>
<li>
<p>Você estará executando o Kubernetes em hardware <strong>"bare metal"</strong> ou em <strong>máquinas virtuais (VMs)</strong>?</p>
</li>
<li>
<p>Você <strong>quer apenas rodar um cluster</strong>, ou você espera fazer <strong>desenvolvimento ativo do código de projeto do Kubernetes</strong>? Se for a segunda opção, escolha uma distro mais ativa. Algumas distros fornecem apenas binários, mas oferecem uma maior variedade de opções.</p>
</li>
<li>
<p>Familiarize-se com os <a href=/docs/admin/cluster-components/>componentes</a> necessários para rodar um cluster.</p>
</li>
</ul>
<p>Nota: Nem todas as distros são ativamente mantidas. Escolha as distros que foram testadas com uma versão recente do Kubernetes.</p>
<h2 id=gerenciando-um-cluster>Gerenciando um cluster</h2>
<ul>
<li>
<p><a href=/docs/tasks/administer-cluster/cluster-management/>Gerenciando um cluster</a> descreve vários tópicos relacionados ao ciclo de vida de um cluster: criando um novo cluster, atualizando o nó mestre e os nós de trabalho do cluster, executando manutenção de nó (por exemplo, atualizações de kernel) e atualizando a versão da API do Kubernetes de um cluster em execução.</p>
</li>
<li>
<p>Aprender como <a href=/docs/concepts/nodes/node/>gerenciar um nó</a>.</p>
</li>
<li>
<p>Aprender como configurar e gerenciar o <a href=/docs/concepts/policy/resource-quotas/>recurso de quota</a> para um cluster compartilhado.</p>
</li>
</ul>
<h2 id=protegendo-um-cluster>Protegendo um cluster</h2>
<ul>
<li>
<p><a href=/docs/concepts/cluster-administration/certificates/>Certificados</a> descreve as etapas para gerar certificados usando diferentes ferramentas.</p>
</li>
<li>
<p><a href=/docs/concepts/containers/container-environment-variables/>Ambiente de Container Kubernetes</a> descreve o ambiente para contêineres gerenciados pelo Kubelet em um nó do Kubernetes.</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/controlling-access/>Controlando Acesso a API Kubernetes API</a> descreve como configurar
a permissão para usuários e contas de serviço.</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/authentication/>Autenticando</a> explica a autenticação no Kubernetes, incluindo as várias opções de autenticação.</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/authorization/>Autorização</a> é separada da autenticação e controla como as chamadas HTTP são tratadas.</p>
</li>
<li>
<p><a href=/docs/reference/access-authn-authz/admission-controllers/>Usando Controladores de Admissão</a> explica plug-ins que interceptam solicitações ao servidor da API do Kubernetes após autenticação e autorização.</p>
</li>
<li>
<p><a href=/docs/concepts/cluster-administration/sysctl-cluster/>Usando Sysctls em um Cluster Kubernetes</a> descreve a um administrador como usar a ferramenta de linha de comando <code>sysctl</code> para definir os parâmetros do kernel.</p>
</li>
<li>
<p><a href=/docs/tasks/debug-application-cluster/audit/>Auditando</a>
descreve como interagir com os logs de auditoria do Kubernetes.</p>
</li>
</ul>
<h3 id=protegendo-o-kubelet>Protegendo o kubelet</h3>
<ul>
<li><a href=/docs/concepts/architecture/master-node-communication/>Comunicação Master-Node </a></li>
<li><a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>TLS bootstrapping</a></li>
<li><a href=/docs/admin/kubelet-authentication-authorization/>Autenticação/Autorização Kubelet</a></li>
</ul>
<h2 id=serviços-opcionais-do-cluster>Serviços Opcionais do Cluster</h2>
<ul>
<li>
<p><a href=/docs/concepts/services-networking/dns-pod-service/>Integração DNS</a> descreve como resolver um nome DNS diretamente para um serviço do Kubernetes.</p>
</li>
<li>
<p><a href=/docs/concepts/cluster-administration/logging/>Logando e monitorando a atividade de cluster</a> explica como o log funciona no Kubernetes e como implementá-lo.</p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2bf9a93ab5ba014fb6ff70b22c29d432>11.2 - Certificates</h1>
<p>Ao usar um client para autenticação de certificado, você pode gerar certificados
manualmente através <code>easyrsa</code>, <code>openssl</code> ou <code>cfssl</code>.</p>
<h3 id=easyrsa>easyrsa</h3>
<p><strong>easyrsa</strong> pode gerar manualmente certificados para o seu cluster.</p>
<ol>
<li>
<p>Baixe, descompacte e inicialize a versão corrigida do easyrsa3.</p>
<pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz
tar xzf easy-rsa.tar.gz
cd easy-rsa-master/easyrsa3
./easyrsa init-pki
</code></pre>
</li>
<li>
<p>Gerar o CA. (<code>--batch</code> set automatic mode. <code>--req-cn</code> default CN to use.)</p>
<pre><code>./easyrsa --batch &quot;--req-cn=${MASTER_IP}@`date +%s`&quot; build-ca nopass
</code></pre>
</li>
<li>
<p>Gere o certificado e a chave do servidor.
O argumento <code>--subject-alt-name</code> define os possíveis IPs e nomes (DNS) que o servidor de API usará para ser acessado. O <code>MASTER_CLUSTER_IP</code> é geralmente o primeiro IP do serviço CIDR que é especificado como argumento em <code>--service-cluster-ip-range</code> para o servidor de API e o componente gerenciador do controlador. O argumento <code>--days</code> é usado para definir o número de dias após o qual o certificado expira.
O exemplo abaixo também assume que você está usando <code>cluster.local</code> como DNS de domínio padrão</p>
<pre><code>./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;\
&quot;IP:${MASTER_CLUSTER_IP},&quot;\
&quot;DNS:kubernetes,&quot;\
&quot;DNS:kubernetes.default,&quot;\
&quot;DNS:kubernetes.default.svc,&quot;\
&quot;DNS:kubernetes.default.svc.cluster,&quot;\
&quot;DNS:kubernetes.default.svc.cluster.local&quot; \
--days=10000 \
build-server-full server nopass
</code></pre>
</li>
<li>
<p>Copie <code>pki/ca.crt</code>, <code>pki/issued/server.crt</code>, e <code>pki/private/server.key</code> para o seu diretório.</p>
</li>
<li>
<p>Preencha e adicione os seguintes parâmetros aos parâmetros de inicialização do servidor de API:</p>
<pre><code>--client-ca-file=/yourdirectory/ca.crt
--tls-cert-file=/yourdirectory/server.crt
--tls-private-key-file=/yourdirectory/server.key
</code></pre>
</li>
</ol>
<h3 id=openssl>openssl</h3>
<p><strong>openssl</strong> pode gerar manualmente certificados para o seu cluster.</p>
<ol>
<li>
<p>Gere um ca.key com 2048bit:</p>
<pre><code>openssl genrsa -out ca.key 2048
</code></pre>
</li>
<li>
<p>De acordo com o ca.key, gere um ca.crt (use -days para definir o tempo efetivo do certificado):</p>
<pre><code> openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt
</code></pre>
</li>
<li>
<p>Gere um server.key com 2048bit:</p>
<pre><code>openssl genrsa -out server.key 2048
</code></pre>
</li>
<li>
<p>Crie um arquivo de configuração para gerar uma solicitação de assinatura de certificado (CSR - Certificate Signing Request). Certifique-se de substituir os valores marcados com colchetes angulares (por exemplo, <code>&lt;MASTER_IP></code>) com valores reais antes de salvá-lo em um arquivo (por exemplo, <code>csr.conf</code>). Note que o valor para o <code>MASTER_CLUSTER_IP</code> é o IP do cluster de serviços para o Servidor de API, conforme descrito na subseção anterior. O exemplo abaixo também assume que você está usando <code>cluster.local</code> como DNS de domínio padrão</p>
<pre><code>[ req ]
default_bits = 2048
prompt = no
default_md = sha256
req_extensions = req_ext
distinguished_name = dn

[ dn ]
C = &lt;country&gt;
ST = &lt;state&gt;
L = &lt;city&gt;
O = &lt;organization&gt;
OU = &lt;organization unit&gt;
CN = &lt;MASTER_IP&gt;

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
IP.1 = &lt;MASTER_IP&gt;
IP.2 = &lt;MASTER_CLUSTER_IP&gt;

[ v3_ext ]
authorityKeyIdentifier=keyid,issuer:always
basicConstraints=CA:FALSE
keyUsage=keyEncipherment,dataEncipherment
extendedKeyUsage=serverAuth,clientAuth
subjectAltName=@alt_names
</code></pre>
</li>
<li>
<p>Gere a solicitação de assinatura de certificado com base no arquivo de configuração:</p>
<pre><code>openssl req -new -key server.key -out server.csr -config csr.conf
</code></pre>
</li>
<li>
<p>Gere o certificado do servidor usando o ca.key, ca.crt e server.csr:</p>
<pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out server.crt -days 10000 \
-extensions v3_ext -extfile csr.conf
</code></pre>
</li>
<li>
<p>Veja o certificado:</p>
<pre><code>openssl x509  -noout -text -in ./server.crt
</code></pre>
</li>
</ol>
<p>Por fim, adicione os mesmos parâmetros nos parâmetros iniciais do Servidor de API.</p>
<h3 id=cfssl>cfssl</h3>
<p><strong>cfssl</strong> é outra ferramenta para geração de certificados.</p>
<ol>
<li>
<p>Baixe, descompacte e prepare as ferramentas de linha de comando, conforme mostrado abaixo. Observe que você pode precisar adaptar os comandos de exemplo abaixo com base na arquitetura do hardware e versão cfssl que você está usando.</p>
<pre><code>curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o cfssl
chmod +x cfssl
curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o cfssljson
chmod +x cfssljson
curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o cfssl-certinfo
chmod +x cfssl-certinfo
</code></pre>
</li>
<li>
<p>Crie um diretório para conter os artefatos e inicializar o cfssl:</p>
<pre><code>mkdir cert
cd cert
../cfssl print-defaults config &gt; config.json
../cfssl print-defaults csr &gt; csr.json
</code></pre>
</li>
<li>
<p>Crie um arquivo de configuração JSON para gerar o arquivo CA, por exemplo, <code>ca-config.json</code>:</p>
<pre><code>{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre>
</li>
<li>
<p>Crie um arquivo de configuração JSON para o CA - solicitação de assinatura de certificado (CSR - Certificate Signing Request), por exemplo, <code>ca-csr.json</code>. Certifique-se de substituir os valores marcados com colchetes angulares por valores reais que você deseja usar.</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;:[{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
</li>
<li>
<p>Gere a chave CA (<code>ca-key.pem</code>) e o certificado (<code>ca.pem</code>):</p>
<pre><code>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</code></pre>
</li>
<li>
<p>Crie um arquivo de configuração JSON para gerar chaves e certificados para o Servidor de API, por exemplo, <code>server-csr.json</code>. Certifique-se de substituir os valores entre colchetes angulares por valores reais que você deseja usar. O <code>MASTER_CLUSTER_IP</code> é o IP do serviço do cluster para o servidor da API, conforme descrito na subseção anterior. O exemplo abaixo também assume que você está usando <code>cluster.local</code> como DNS de domínio padrão</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;&lt;MASTER_IP&gt;&quot;,
    &quot;&lt;MASTER_CLUSTER_IP&gt;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
</li>
<li>
<p>Gere a chave e o certificado para o Servidor de API, que são, por padrão, salvos nos arquivos <code>server-key.pem</code> e<code> server.pem</code> respectivamente:</p>
<pre><code>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
--config=ca-config.json -profile=kubernetes \
server-csr.json | ../cfssljson -bare server
</code></pre>
</li>
</ol>
<h2 id=distribuindo-certificado-ca-auto-assinado>Distribuindo Certificado CA auto assinado</h2>
<p>Um nó cliente pode se recusar a reconhecer o certificado CA self-signed como válido.
Para uma implementação de não produção ou para uma instalação que roda atrás de um firewall, você pode distribuir certificados auto-assinados para todos os clientes e atualizar a lista de certificados válidos.</p>
<p>Em cada cliente, execute as seguintes operações:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
sudo update-ca-certificates
</code></pre></div><pre><code>Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><h2 id=api-de-certificados>API de certificados</h2>
<p>Você pode usar a API <code>certificates.k8s.io</code> para provisionar
certificados x509 a serem usados ​​para autenticação conforme documentado
<a href=/docs/tasks/tls/managing-tls-in-a-cluster>aqui</a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-d649067a69d8d5c7e71564b42b96909e>11.3 - Conectividade do Cluster</h1>
<p>Conectividade é uma parte central do Kubernetes, mas pode ser desafiador
entender exatamente como é o seu funcionamento esperado. Existem 4 problemas
distintos em conectividade que devem ser tratados:</p>
<ol>
<li>Comunicações contêiner-para-contêiner altamente acopladas: Isso é resolvido
por <a class=glossary-tooltip title="O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> e comunicações através do <code>localhost</code>.</li>
<li>Comunicações pod-para-pod: Esse é o foco primário desse documento.</li>
<li>Comunicações pod-para-serviço (<em>service</em>): Isso é tratado em <a href=/docs/concepts/services-networking/service/>Services</a>.</li>
<li>Comunicações Externas-para-serviços: Isso é tratado em <a href=/docs/concepts/services-networking/service/>services</a>.</li>
</ol>
<p>Kubernetes é basicamente o compartilhamento de máquinas entre aplicações. Tradicionalmente,
compartilhar máquinas requer a garantia de que duas aplicações não tentem utilizar
as mesmas portas. Coordenar a alocação de portas entre múltiplos desenvolvedores é
muito dificil de fazer em escala e expõe os usuários a problemas em nível do cluster e
fora de seu controle.</p>
<p>A alocação dinâmica de portas traz uma série de complicações para o sistema - toda
aplicação deve obter suas portas através de flags de configuração, os servidores de API
devem saber como inserir números dinämicos de portas nos blocos de configuração, serviços
precisam saber como buscar um ao outro, etc. Ao invés de lidar com isso, o Kubernetes
faz de uma maneira diferente.</p>
<h2 id=o-modelo-de-conectividade-e-rede-do-kubernetes>O modelo de conectividade e rede do Kubernetes</h2>
<p>Todo <code>Pod</code> obtém seu próprio endereço IP. Isso significa que vocë não precisa
criar links explícitos entre os <code>Pods</code> e vocë quase nunca terá que lidar com o
mapeamento de portas de contêineres para portas do host. Isso cria um modelo simples,
retro-compatível onde os <code>Pods</code> podem ser tratados muito mais como VMs ou hosts
físicos da perspectiva de alocação de portas, nomes, descobrimento de serviços
(<em>service discovery</em>), balanceamento de carga, configuração de aplicações e migrações.</p>
<p>O Kubernetes impõe os seguintes requisitos fundamentais para qualquer implementação de
rede (exceto qualquer política de segmentação intencional):</p>
<ul>
<li>pods em um nó podem se comunicar com todos os pods em todos os nós sem usar <em>NAT</em>.</li>
<li>agentes em um nó (por exemplo o kubelet ou um serviço local) podem se comunicar com
todos os Pods naquele nó.</li>
</ul>
<p>Nota: Para as plataformas que suportam <code>Pods</code> executando na rede do host (como o Linux):</p>
<ul>
<li>pods alocados na rede do host de um nó podem se comunicar com todos os pods
em todos os nós sem <em>NAT</em>.</li>
</ul>
<p>Esse modelo não só é menos complexo, mas é principalmente compatível com o
desejo do Kubernetes de permitir a portabilidade com baixo esforço de aplicações
de VMs para contêineres. Se a sua aplicação executava anteriormente em uma VM, sua VM
possuía um IP e podia se comunicar com outras VMs no seu projeto. Esse é o mesmo
modelo básico.</p>
<p>Os endereços de IP no Kubernetes existem no escopo do <code>Pod</code> - contêineres em um <code>Pod</code>
compartilham o mesmo <em>network namespace</em> - incluíndo seu endereço de IP e MAC.
Isso significa que contêineres que compõem um <code>Pod</code> podem se comunicar entre eles
através do endereço <code>localhost</code> e respectivas portas. Isso também significa que
contêineres em um mesmo <code>Pod</code> devem coordenar a alocação e uso de portas, o que não
difere do modelo de processos rodando dentro de uma mesma VM. Isso é chamado de
modelo "IP-por-pod".</p>
<p>Como isso é implementado é um detalhe do agente de execução de contêiner em uso.</p>
<p>É possível solicitar uma porta no nó que será encaminhada para seu <code>Pod</code> (chamado
de <em>portas do host</em>), mas isso é uma operação muito específica. Como esse encaminhamento
é implementado é um detalhe do agente de execução do contêiner. O <code>Pod</code> mesmo
desconhece a existência ou não de portas do host.</p>
<h2 id=como-implementar-o-modelo-de-conectividade-do-kubernetes>Como implementar o modelo de conectividade do Kubernetes</h2>
<p>Existe um número de formas de implementar esse modelo de conectividade. Esse
documento não é um estudo exaustivo desses vários métodos, mas pode servir como
uma introdução de várias tecnologias e serve como um ponto de início.</p>
<p>A conectividade no Kubernetes é fornecida através de plugins de
<a class=glossary-tooltip title="Plugins Container network interface (CNI) são um tipo de plugin de Rede em conformidade com a especificação appc/CNI." data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni target=_blank aria-label=CNIs>CNIs</a></p>
<p>As seguintes opções estão organizadas alfabeticamente e não implicam preferência por
qualquer solução.</p>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Nota:</strong>
Esta seção tem links para projetos de terceiros que fornecem a funcionalidade exigida pelo Kubernetes. Os autores do projeto Kubernetes não são responsáveis por esses projetos. Esta página obedece as <a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>diretrizes de conteúdo do site CNCF</a>, listando os itens em ordem alfabética. Para adicionar um projeto a esta lista, leia o <a href=/docs/contribute/style/content-guide/#third-party-content>guia de conteúdo</a> antes de enviar sua alteração.</div>
<h3 id=antrea>Antrea</h3>
<p>O projeto <a href=https://github.com/vmware-tanzu/antrea>Antrea</a> é uma solução de
conectividade para Kubernetes que pretende ser nativa. Ela utiliza o Open vSwitch
na camada de conectividade de dados. O Open vSwitch é um switch virtual de alta
performance e programável que suporta Linux e Windows. O Open vSwitch permite
ao Antrea implementar políticas de rede do Kubernetes (<em>NetworkPolicies</em>) de
uma forma muito performática e eficiente.</p>
<p>Graças à característica programável do Open vSwitch, o Antrea consegue implementar
uma série de funcionalidades de rede e segurança.</p>
<h3 id=aws-vpc-cni-para-kubernetes>AWS VPC CNI para Kubernetes</h3>
<p>O <a href=https://github.com/aws/amazon-vpc-cni-k8s>AWS VPC CNI</a> oferece conectividade
com o AWS Virtual Private Cloud (VPC) para clusters Kubernetes. Esse plugin oferece
alta performance e disponibilidade e baixa latência. Adicionalmente, usuários podem
aplicar as melhores práticas de conectividade e segurança existentes no AWS VPC
para a construção de clusters Kubernetes. Isso inclui possibilidade de usar o
<em>VPC flow logs</em>, políticas de roteamento da VPC e grupos de segurança para isolamento
de tráfego.</p>
<p>O uso desse plugin permite aos Pods no Kubernetes ter o mesmo endereço de IP dentro do
pod como se eles estivessem dentro da rede do VPC. O CNI (Container Network Interface)
aloca um <em>Elastic Networking Interface</em> (ENI) para cada nó do Kubernetes e usa uma
faixa de endereços IP secundário de cada ENI para os Pods no nó. O CNI inclui
controles para pré alocação dos ENIs e endereços IP para um início mais rápido dos
pods e permite clusters com até 2,000 nós.</p>
<p>Adicionalmente, esse CNI pode ser utilizado junto com o <a href=https://docs.aws.amazon.com/eks/latest/userguide/calico.html>Calico</a>
para a criação de políticas de rede (<em>NetworkPolicies</em>). O projeto AWS VPC CNI
tem código fonte aberto com a <a href=https://github.com/aws/amazon-vpc-cni-k8s>documentação no Github</a>.</p>
<h3 id=azure-cni-para-o-kubernetes>Azure CNI para o Kubernetes</h3>
<p><a href=https://docs.microsoft.com/en-us/azure/virtual-network/container-networking-overview>Azure CNI</a> é um
plugin de <a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>código fonte aberto</a>
que integra os Pods do Kubernetes com uma rede virtual da Azure (também conhecida como VNet)
provendo performance de rede similar à de máquinas virtuais no ambiente. Os Pods
podem se comunicar com outras VNets e com ambientes <em>on-premises</em> com o uso de
funcionalidades da Azure, e também podem ter clientes com origem dessas redes.
Os Pods podem acessar serviços da Azure, como armazenamento e SQL, que são
protegidos por <em>Service Endpoints</em> e <em>Private Link</em>. Você pode utilizar as políticas
de segurança e roteamento para filtrar o tráfico do Pod. O plugin associa IPs da VNet
para os Pods utilizando um pool de IPs secundário pré-configurado na interface de rede
do nó Kubernetes.</p>
<p>O Azure CNI está disponível nativamente no <a href=https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni>Azure Kubernetes Service (AKS)</a>.</p>
<h3 id=calico>Calico</h3>
<p><a href=https://docs.projectcalico.org/>Calico</a> é uma solução de conectividade e
segurança para contêineres, máquinas virtuais e serviços nativos em hosts. O
Calico suporta múltiplas camadas de conectividade/dados, como por exemplo:
uma camada Linux eBPF nativa, uma camada de conectividade baseada em conceitos
padrão do Linux e uma camada baseada no HNS do Windows. O calico provê uma
camada completa de conectividade e rede, mas também pode ser usado em conjunto com
<a href=https://docs.projectcalico.org/networking/determine-best-networking#calico-compatible-cni-plugins-and-cloud-provider-integrations>CNIs de provedores de nuvem</a>
para permitir a criação de políticas de rede.</p>
<h3 id=cilium>Cilium</h3>
<p><a href=https://github.com/cilium/cilium>Cilium</a> é um software de código fonte aberto
para prover conectividade e segurança entre contêineres de aplicação. O Cilium
pode lidar com tráfego na camada de aplicação (ex. HTTP) e pode forçar políticas
de rede nas camadas L3-L7 usando um modelo de segurança baseado em identidade e
desacoplado do endereçamento de redes, podendo inclusive ser utilizado com outros
plugins CNI.</p>
<h3 id=flannel>Flannel</h3>
<p><a href=https://github.com/coreos/flannel#flannel>Flannel</a> é uma camada muito simples
de conectividade que satisfaz os requisitos do Kubernetes. Muitas pessoas
reportaram sucesso em utilizar o Flannel com o Kubernetes.</p>
<h3 id=google-compute-engine-gce>Google Compute Engine (GCE)</h3>
<p>Para os scripts de configuração do Google Compute Engine, <a href=https://cloud.google.com/vpc/docs/routes>roteamento
avançado</a> é usado para associar
para cada VM uma sub-rede (o padrão é <code>/24</code> - 254 IPs). Qualquer tráfico direcionado
para aquela sub-rede será roteado diretamente para a VM pela rede do GCE. Isso é
adicional ao IP principal associado à VM, que é mascarado para o acesso à Internet.
Uma <em>brige</em> Linux (chamada <code>cbr0</code>) é configurada para existir naquela sub-rede, e é
configurada no docker através da opção <code>--bridge</code>.</p>
<p>O Docker é iniciado com:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>DOCKER_OPTS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--bridge=cbr0 --iptables=false --ip-masq=false&#34;</span>
</code></pre></div><p>Essa <em>bridge</em> é criada pelo Kubelet (controlada pela opção <code>--network-plugin=kubenet</code>)
de acordo com a informação <code>.spec.podCIDR</code> do Nó.</p>
<p>O Docker irá agora alocar IPs do bloco <code>cbr-cidr</code>. Contêineres podem alcançar
outros contêineres e nós através da interface <code>cbr0</code>. Esses IPs são todos roteáveis
dentro da rede do projeto do GCE.</p>
<p>O GCE mesmo não sabe nada sobre esses IPs, então não irá mascará-los quando tentarem
se comunicar com a internet. Para permitir isso uma regra de IPTables é utilizada para
mascarar o tráfego para IPs fora da rede do projeto do GCE (no exemplo abaixo, 10.0.0.0/8):</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>iptables -t nat -A POSTROUTING ! -d 10.0.0.0/8 -o eth0 -j MASQUERADE
</code></pre></div><p>Por fim, o encaminhamento de IP deve ser habilitado no Kernel de forma a processar
os pacotes vindos dos contêineres:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sysctl net.ipv4.ip_forward<span style=color:#666>=</span><span style=color:#666>1</span>
</code></pre></div><p>O resultado disso tudo é que <code>Pods</code> agora podem alcançar outros <code>Pods</code> e podem também
se comunicar com a Internet.</p>
<h3 id=kube-router>Kube-router</h3>
<p><a href=https://github.com/cloudnativelabs/kube-router>Kube-router</a> é uma solução construída
que visa prover alta performance e simplicidade operacional. Kube-router provê um
proxy de serviços baseado no <a href=https://www.linuxvirtualserver.org/software/ipvs.html>LVS/IPVS</a>,
uma solução de comunicação pod-para-pod baseada em encaminhamento de pacotes Linux e sem camadas
adicionais, e funcionalidade de políticas de redes baseadas no IPTables/IPSet.</p>
<h3 id=redes-l2-e-bridges-linux>Redes L2 e bridges Linux</h3>
<p>Se você tem uma rede L2 "burra", como um switch em um ambiente "bare-metal",
você deve conseguir fazer algo similar ao ambiente GCE explicado acima.
Note que essas instruções foram testadas casualmente - parece funcionar, mas
não foi propriamente testado. Se você conseguir usar essa técnica e aperfeiçoar
o processo, por favor nos avise!!</p>
<p>Siga a parte <em>"With Linux Bridge devices"</em> desse
<a href=https://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/>tutorial super bacana</a> do
Lars Kellogg-Stedman.</p>
<h3 id=multus>Multus (Plugin multi redes)</h3>
<p>Multus é um plugin Multi CNI para
suportar a funcionalidade multi redes do Kubernetes usando objetos baseados em <a class=glossary-tooltip title="Código customizado que define um recurso a ser adicionado ao seu servidor de API Kubernetes sem a necessidade de construir um servidor customizado." data-toggle=tooltip data-placement=top href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ target=_blank aria-label=CRDs>CRDs</a>.</p>
<p>Multus suporta todos os <a href=https://github.com/containernetworking/plugins>plugins referência</a> (ex. <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>Flannel</a>,
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp>DHCP</a>,
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan>Macvlan</a>)
que implementam a especificação de CNI e plugins de terceiros
(ex. <a href=https://github.com/projectcalico/cni-plugin>Calico</a>, <a href=https://github.com/weaveworks/weave>Weave</a>,
<a href=https://github.com/cilium/cilium>Cilium</a>, <a href=https://github.com/contiv/netplugin>Contiv</a>).
Adicionalmente, Multus suporta cargas de trabalho no Kubernetes que necessitem de funcionalidades como
<a href=https://github.com/hustcat/sriov-cni>SRIOV</a>, <a href=https://github.com/Intel-Corp/sriov-cni>DPDK</a>,
<a href=https://github.com/intel/vhost-user-net-plugin>OVS-DPDK & VPP</a>.</p>
<h3 id=ovn-open-virtual-networking>OVN (Open Virtual Networking)</h3>
<p>OVN é uma solução de virtualização de redes de código aberto desenvolvido pela
comunidade Open vSwitch. Permite a criação de switches lógicos, roteadores lógicos,
listas de acesso, balanceadores de carga e mais, para construir diferences topologias
de redes virtuais. Esse projeto possui um plugin específico para o Kubernetes e a
documentação em <a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<p>Design inicial do modelo de conectividade do Kubernetes e alguns planos futuros
estão descritos com maiores detalhes no
<a href=https://git.k8s.io/community/contributors/design-proposals/network/networking.md>documento de design de redes</a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c4b1e87a84441f8a90699a345ce48d68>11.4 - Arquitetura de Log</h1>
<p>Os logs de aplicativos e sistemas podem ajudá-lo a entender o que está acontecendo dentro do seu cluster. Os logs são particularmente úteis para depurar problemas e monitorar a atividade do cluster. A maioria das aplicações modernas possui algum tipo de mecanismo de logs; como tal, a maioria dos mecanismos de contêineres também é projetada para suportar algum tipo de log. O método de log mais fácil e abrangente para aplicações em contêiner é gravar nos fluxos de saída e erro padrão.</p>
<p>No entanto, a funcionalidade nativa fornecida por um mecanismo de contêiner ou tempo de execução geralmente não é suficiente para uma solução completa de log. Por exemplo, se um contêiner travar, um pod for despejado ou um nó morrer, geralmente você ainda desejará acessar os logs do aplicativo. Dessa forma, os logs devem ter armazenamento e ciclo de vida separados, independentemente de nós, pods ou contêineres. Este conceito é chamado <em>cluster-level-logging</em>. O log no nível de cluster requer um back-end separado para armazenar, analisar e consultar logs. O kubernetes não fornece uma solução de armazenamento nativa para dados de log, mas você pode integrar muitas soluções de log existentes no cluster do Kubernetes.</p>
<p>As arquiteturas de log no nível de cluster são descritas no pressuposto de que um back-end de log esteja presente dentro ou fora do cluster. Se você não estiver interessado em ter o log no nível do cluster, ainda poderá encontrar a descrição de como os logs são armazenados e manipulados no nó para serem úteis.</p>
<h2 id=log-básico-no-kubernentes>Log básico no Kubernentes</h2>
<p>Nesta seção, você pode ver um exemplo de log básico no Kubernetes que gera dados para o fluxo de saída padrão(standard output stream). Esta demostração usa uma <a href=/examples/debug/counter-pod.yaml>especificação de pod</a> com um contêiner que grava algum texto na saída padrão uma vez por segundo.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/debug/counter-pod.yaml download=debug/counter-pod.yaml><code>debug/counter-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('debug-counter-pod-yaml')" title="Copy debug/counter-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=debug-counter-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c,<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:#b44>&#39;i=0; while true; do echo &#34;$i: $(date)&#34;; i=$((i+1)); sleep 1; done&#39;</span>]<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Para executar este pod, use o seguinte comando:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</code></pre></div><p>A saída será:</p>
<pre><code>pod/counter created
</code></pre><p>Para buscar os logs, use o comando <code>kubectl logs</code>, da seguinte maneira:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs counter
</code></pre></div><p>A saída será:</p>
<pre><code>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><p>Você pode usar <code>kubectl logs</code> para recuperar logs de uma instanciação anterior de um contêiner com o sinalizador <code>--previous</code>, caso o contêiner tenha falhado. Se o seu pod tiver vários contêineres, você deverá especificar quais logs do contêiner você deseja acessar anexando um nome de contêiner ao comando. Veja a <a href=/docs/reference/generated/kubectl/kubectl-commands#logs>documentação do <code>kubectl logs</code></a> para mais destalhes.</p>
<h2 id=logs-no-nível-do-nó>Logs no nível do Nó</h2>
<p><img src=/images/docs/user-guide/logging/logging-node-level.png alt="Log no nível do nó"></p>
<p>Tudo o que um aplicativo em contêiner grava no <code>stdout</code> e <code>stderr</code> é tratado e redirecionado para algum lugar por dentro do mecanismo de contêiner. Por exemplo, o mecanismo de contêiner do Docker redireciona esses dois fluxos para <a href=https://docs.docker.com/engine/admin/logging/overview>um driver de log</a>, configurado no Kubernetes para gravar em um arquivo no formato json.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> O driver de log json do Docker trata cada linha como uma mensagem separada. Ao usar o driver de log do Docker, não há suporte direto para mensagens de várias linhas. Você precisa lidar com mensagens de várias linhas no nível do agente de log ou superior.
</div>
<p>Por padrão, se um contêiner reiniciar, o kubelet manterá um contêiner terminado com seus logs. Se um pod for despejado do nó, todos os contêineres correspondentes também serão despejados, juntamente com seus logs.</p>
<p>Uma consideração importante no log no nível do nó está implementado a rotação de log, para que os logs não consumam todo o armazenamento disponível no nó. Atualmente, o Kubernentes não é responsável pela rotação de logs, mas uma ferramenta de deployment deve configurar uma solução para resolver isso.
Por exemplo, nos clusters do Kubernetes, implementados pelo script <code>kube-up.sh</code>, existe uma ferramenta <a href=https://linux.die.net/man/8/logrotate><code>logrotate</code></a> configurada para executar a cada hora. Você pode configurar um tempo de execução do contêiner para girar os logs do aplicativo automaticamente, por exemplo, usando o <code>log-opt</code> do Docker.
No script <code>kube-up.sh</code>, a última abordagem é usada para imagem COS no GCP, e a anterior é usada em qualquer outro ambiente. Nos dois casos por padrão, a rotação é configurada para ocorrer quando o arquivo de log exceder 10MB.</p>
<p>Como exemplo, você pode encontrar informações detalhadas sobre como o <code>kube-up.sh</code> define o log da imagem COS no GCP no <a href=https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh>script</a> correspondente.</p>
<p>Quando você executa <a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code></a> como no exemplo de log básico acima, o kubelet no nó lida com a solicitação e lê diretamente do arquivo de log, retornando o conteúdo na resposta.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> Atualmente, se algum sistema externo executou a rotação, apenas o conteúdo do arquivo de log mais recente estará disponível através de <code>kubectl logs</code>. Por exemplo, se houver um arquivo de 10MB, o <code>logrotate</code> executa a rotação e existem dois arquivos, um com 10MB de tamanho e um vazio, o <code>kubectl logs</code> retornará uma resposta vazia.
</div>
<h3 id=logs-de-componentes-do-sistema>Logs de componentes do sistema</h3>
<p>Existem dois tipos de componentes do sistema: aqueles que são executados em um contêiner e aqueles que não são executados em um contêiner. Por exemplo:</p>
<ul>
<li>O scheduler Kubernetes e o kube-proxy são executados em um contêiner.</li>
<li>O tempo de execução do kubelet e do contêiner, por exemplo, Docker, não é executado em contêineres.</li>
</ul>
<p>Nas máquinas com systemd, o tempo de execução do kubelet e do container é gravado no journald. Se systemd não estiver presente, eles gravam em arquivos <code>.log</code> no diretório <code>/var/log</code>.
Os componentes do sistema dentro dos contêineres sempre gravam no diretório <code>/var/log</code>, ignorando o mecanismo de log padrão. Eles usam a biblioteca de logs <a href=https://github.com/kubernetes/klog>klog</a>. Você pode encontrar as convenções para a gravidade do log desses componentes nos <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>documentos de desenvolvimento sobre log</a>.</p>
<p>Da mesma forma que os logs de contêiner, os logs de componentes do sistema no diretório <code>/var/log</code> devem ser rotacionados. Nos clusters do Kubernetes criados pelo script <code>kube-up.sh</code>, esses logs são configurados para serem rotacionados pela ferramenta <code>logrotate</code> diariamente ou quando o tamanho exceder 100MB.</p>
<h2 id=arquiteturas-de-log-no-nível-de-cluster>Arquiteturas de log no nível de cluster</h2>
<p>Embora o Kubernetes não forneça uma solução nativa para o log em nível de cluster, há várias abordagens comuns que você pode considerar. Aqui estão algumas opções:</p>
<ul>
<li>Use um agente de log no nível do nó que seja executado em todos os nós.</li>
<li>Inclua um contêiner sidecar dedicado para efetuar logging em um pod de aplicativo.</li>
<li>Envie logs diretamente para um back-end de dentro de um aplicativo.</li>
</ul>
<h3 id=usando-um-agente-de-log-de-nó>Usando um agente de log de nó</h3>
<p><img src=/images/docs/user-guide/logging/logging-with-node-agent.png alt="Usando um agente de log no nível do nó"></p>
<p>Você pode implementar o log em nível de cluster incluindo um <em>agente de log em nível de nó</em> em cada nó. O agente de log é uma ferramenta dedicada que expõe logs ou envia logs para um back-end. Geralmente, o agente de log é um contêiner que tem acesso a um diretório com arquivos de log de todos os contêineres de aplicativos nesse nó.</p>
<p>Como o agente de log deve ser executado em todos os nós, é comum implementá-lo como uma réplica do DaemonSet, um pod de manifesto ou um processo nativo dedicado no nó. No entanto, as duas últimas abordagens são obsoletas e altamente desencorajadas.</p>
<p>O uso de um agente de log no nível do nó é a abordagem mais comum e incentivada para um cluster Kubernetes, porque ele cria apenas um agente por nó e não requer alterações nos aplicativos em execução no nó. No entanto, o log no nível do nó <em>funciona apenas para a saída padrão dos aplicativos e o erro padrão</em>.</p>
<p>O Kubernetes não especifica um agente de log, mas dois agentes de log opcionais são fornecidos com a versão Kubernetes: <a href=/docs/user-guide/logging/stackdriver>Stackdriver Logging</a> para uso com o Google Cloud Platform e <a href=/docs/user-guide/logging/elasticsearch>Elasticsearch</a>. Você pode encontrar mais informações e instruções nos documentos dedicados. Ambos usam <a href=http://www.fluentd.org/>fluentd</a> com configuração customizada como um agente no nó.</p>
<h3 id=usando-um-contêiner-sidecar-com-o-agente-de-log>Usando um contêiner sidecar com o agente de log</h3>
<p>Você pode usar um contêiner sidecar de uma das seguintes maneiras:</p>
<ul>
<li>O container sidecar transmite os logs do aplicativo para seu próprio <code>stdout</code>.</li>
<li>O contêiner do sidecar executa um agente de log, configurado para selecionar logs de um contêiner de aplicativo.</li>
</ul>
<h4 id=streaming-sidecar-conteiner>Streaming sidecar conteiner</h4>
<p><img src=/images/docs/user-guide/logging/logging-with-streaming-sidecar.png alt="Conteiner sidecar com um streaming container"></p>
<p>Fazendo com que seus contêineres de sidecar fluam para seus próprios <code>stdout</code> e <code>stderr</code>, você pode tirar proveito do kubelet e do agente de log que já executam em cada nó. Os contêineres sidecar lêem logs de um arquivo, socket ou journald. Cada contêiner sidecar individual imprime o log em seu próprio <code>stdout</code> ou <code>stderr</code> stream.</p>
<p>Essa abordagem permite separar vários fluxos de logs de diferentes partes do seu aplicativo, algumas das quais podem não ter suporte para gravar em <code>stdout</code> ou <code>stderr</code>. A lógica por trás do redirecionamento de logs é mínima, portanto dificilmente representa uma sobrecarga significativa. Além disso, como <code>stdout</code> e <code>stderr</code> são manipulados pelo kubelet, você pode usar ferramentas internas como o <code>kubectl logs</code>.</p>
<p>Considere o seguinte exemplo. Um pod executa um único contêiner e grava em dois arquivos de log diferentes, usando dois formatos diferentes. Aqui está um arquivo de configuração para o Pod:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/admin/logging/two-files-counter-pod.yaml download=admin/logging/two-files-counter-pod.yaml><code>admin/logging/two-files-counter-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-logging-two-files-counter-pod-yaml')" title="Copy admin/logging/two-files-counter-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-logging-two-files-counter-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>      i=0;
</span><span style=color:#b44;font-style:italic>      while true;
</span><span style=color:#b44;font-style:italic>      do
</span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span><span style=color:#b44;font-style:italic>        sleep 1;
</span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Seria uma bagunça ter entradas de log de diferentes formatos no mesmo fluxo de logs, mesmo se você conseguisse redirecionar os dois componentes para o fluxo <code>stdout</code> do contêiner. Em vez disso, você pode introduzir dois contêineres sidecar. Cada contêiner sidecar pode direcionar um arquivo de log específico de um volume compartilhado e depois redirecionar os logs para seu próprio fluxo <code>stdout</code>.</p>
<p>Aqui está um arquivo de configuração para um pod que possui dois contêineres sidecar:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml download=admin/logging/two-files-counter-pod-streaming-sidecar.yaml><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-logging-two-files-counter-pod-streaming-sidecar-yaml')" title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-logging-two-files-counter-pod-streaming-sidecar-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>      i=0;
</span><span style=color:#b44;font-style:italic>      while true;
</span><span style=color:#b44;font-style:italic>      do
</span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span><span style=color:#b44;font-style:italic>        sleep 1;
</span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-1<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -f /var/log/1.log&#39;]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-2<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -f /var/log/2.log&#39;]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Agora, quando você executa este pod, é possível acessar cada fluxo de log separadamente, executando os seguintes comandos:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs counter count-log-1
</code></pre></div><pre><code>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs counter count-log-2
</code></pre></div><pre><code>Mon Jan  1 00:00:00 UTC 2001 INFO 0
Mon Jan  1 00:00:01 UTC 2001 INFO 1
Mon Jan  1 00:00:02 UTC 2001 INFO 2
...
</code></pre><p>O agente no nível do nó instalado em seu cluster coleta esses fluxos de logs automaticamente sem nenhuma configuração adicional. Se desejar, você pode configurar o agente para analisar as linhas de log, dependendo do contêiner de origem.</p>
<p>Observe que, apesar do baixo uso da CPU e da memória (ordem de alguns milicores por CPU e ordem de vários megabytes de memória), gravar logs em um arquivo e depois transmiti-los para o <code>stdout</code> pode duplicar o uso do disco. Se você tem um aplicativo que grava em um único arquivo, geralmente é melhor definir <code>/dev/stdout</code> como destino, em vez de implementar a abordagem de contêiner de transmissão no sidecar.</p>
<p>Os contêineres sidecar também podem ser usados para rotacionar arquivos de log que não podem ser rotacionados pelo próprio aplicativo. Um exemplo dessa abordagem é um pequeno contêiner executando <code>logrotate</code> periodicamente.
No entanto, é recomendável usar o <code>stdout</code> e o <code>stderr</code> diretamente e deixar as políticas de rotação e retenção no kubelet.</p>
<h4 id=contêiner-sidecar-com-um-agente-de-log>Contêiner sidecar com um agente de log</h4>
<p><img src=/images/docs/user-guide/logging/logging-with-sidecar-agent.png alt="Contêiner sidecar com um agente de log"></p>
<p>Se o agente de log no nível do nó não for flexível o suficiente para sua situação, você poderá criar um contêiner secundário com um agente de log separado que você configurou especificamente para executar com seu aplicativo.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> O uso de um agente de log em um contêiner sidecar pode levar a um consumo significativo de recursos. Além disso, você não poderá acessar esses logs usando o comando <code>kubectl logs</code>, porque eles não são controlados pelo kubelet.
</div>
<p>Como exemplo, você pode usar o <a href=/docs/tasks/debug-application-cluster/logging-stackdriver/>Stackdriver</a>, que usa fluentd como um agente de log. Aqui estão dois arquivos de configuração que você pode usar para implementar essa abordagem. O primeiro arquivo contém um <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a> para configurar o fluentd.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/admin/logging/fluentd-sidecar-config.yaml download=admin/logging/fluentd-sidecar-config.yaml><code>admin/logging/fluentd-sidecar-config.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-logging-fluentd-sidecar-config-yaml')" title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-logging-fluentd-sidecar-config-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fluentd.conf</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span><span style=color:#b44;font-style:italic>      type tail
</span><span style=color:#b44;font-style:italic>      format none
</span><span style=color:#b44;font-style:italic>      path /var/log/1.log
</span><span style=color:#b44;font-style:italic>      pos_file /var/log/1.log.pos
</span><span style=color:#b44;font-style:italic>      tag count.format1
</span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span><span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span><span style=color:#b44;font-style:italic>      type tail
</span><span style=color:#b44;font-style:italic>      format none
</span><span style=color:#b44;font-style:italic>      path /var/log/2.log
</span><span style=color:#b44;font-style:italic>      pos_file /var/log/2.log.pos
</span><span style=color:#b44;font-style:italic>      tag count.format2
</span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span><span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    &lt;match **&gt;
</span><span style=color:#b44;font-style:italic>      type google_cloud
</span><span style=color:#b44;font-style:italic>    &lt;/match&gt;</span><span style=color:#bbb>    
</span></code></pre></div>
</div>
</div>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> A configuração do fluentd está além do escopo deste artigo. Para obter informações sobre como configurar o fluentd, consulte a <a href=http://docs.fluentd.org/>documentação oficial do fluentd</a>.
</div>
<p>O segundo arquivo descreve um pod que possui um contêiner sidecar rodando fluentemente.
O pod monta um volume onde o fluentd pode coletar seus dados de configuração.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/pt-br/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml download=admin/logging/two-files-counter-pod-agent-sidecar.yaml><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('admin-logging-two-files-counter-pod-agent-sidecar-yaml')" title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard">
</img>
</div>
<div class=includecode id=admin-logging-two-files-counter-pod-agent-sidecar-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>      i=0;
</span><span style=color:#b44;font-style:italic>      while true;
</span><span style=color:#b44;font-style:italic>      do
</span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span><span style=color:#b44;font-style:italic>        sleep 1;
</span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-agent<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/fluentd-gcp:1.30<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>FLUENTD_ARGS<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>-c /etc/fluentd-config/fluentd.conf<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/fluentd-config<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>Depois de algum tempo, você pode encontrar mensagens de log na interface do Stackdriver.</p>
<p>Lembre-se de que este é apenas um exemplo e você pode realmente substituir o fluentd por qualquer agente de log, lendo de qualquer fonte dentro de um contêiner de aplicativo.</p>
<h3 id=expondo-logs-diretamente-do-aplicativo>Expondo logs diretamente do aplicativo</h3>
<p><img src=/images/docs/user-guide/logging/logging-from-application.png alt="Expondo logs diretamente do aplicativo"></p>
<p>Você pode implementar o log no nível do cluster, expondo ou enviando logs diretamente de todos os aplicativos; no entanto, a implementação desse mecanismo de log está fora do escopo do Kubernetes.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-5cc31ecfba86467f8884856412cfb6b2>11.5 - Logs de Sistema</h1>
<p>Logs de componentes do sistema armazenam eventos que acontecem no cluster, sendo muito úteis para depuração. Seus níveis de detalhe podem ser ajustados para mais ou para menos. Podendo se ater, por exemplo, a mostrar apenas os erros que ocorrem no componente, ou chegando a mostrar cada passo de um evento. (Como acessos HTTP, mudanças no estado dos pods, ações dos controllers, ou decisões do scheduler).</p>
<h2 id=klog>Klog</h2>
<p><a href=https://github.com/kubernetes/klog>Klog</a> é a biblioteca de logs do Kubernetes. Responsável por gerar as mensagens de log para os componentes do sistema.</p>
<p>Para mais informações acerca da sua configuração, veja a documentação da <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/>ferramenta de linha de comando</a>.</p>
<p>Um exemplo do formato padrão dos logs da biblioteca:</p>
<pre><code>I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
</code></pre><h3 id=logs-estruturados>Logs Estruturados</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code>
</div>
<div class="alert alert-danger warning callout" role=alert>
<strong>Aviso:</strong> <p>A migração pro formato de logs estruturados é um processo em andamento. Nem todos os logs estão dessa forma na versão atual. Sendo assim, para realizar o processamento de arquivos de log, você também precisa lidar com logs não estruturados.</p>
<p>A formatação e serialização dos logs ainda estão sujeitas a alterações.</p>
</div>
<p>A estruturação dos logs trás uma estrutura uniforme para as mensagens de log, permitindo a extração programática de informações. Logs estruturados podem ser armazenados e processados com menos esforço e custo. Esse formato é totalmente retrocompatível e é habilitado por padrão.</p>
<p>Formato dos logs estruturados:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=color:#b44>&lt;klog header&gt; &#34;&lt;message&gt;&#34; &lt;key1&gt;</span><span style=color:#666>=</span><span style=color:#b44>&#34;&lt;value1&gt;&#34; &lt;key2&gt;=&#34;&lt;value2&gt;&#34; ...</span>
</code></pre></div><p>Exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=color:#b44>I1025 00:15:15.525108       1 controller_utils.go:116] &#34;Pod status updated&#34; pod</span><span style=color:#666>=</span><span style=color:#b44>&#34;kube-system/kubedns&#34; status=&#34;ready&#34;</span>
</code></pre></div><h3 id=logs-em-formato-json>Logs em formato JSON</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code>
</div>
<div class="alert alert-danger warning callout" role=alert>
<strong>Aviso:</strong> <p>Algumas opções da biblioteca klog ainda não funcionam com os logs em formato JSON. Para ver uma lista completa de quais são estas, veja a documentação da <a href=/docs/reference/command-line-tools-reference/>ferramenta de linha de comando</a>.</p>
<p>Nem todos os logs estarão garantidamente em formato JSON (como por exemplo durante o início de processos). Sendo assim, se você pretende realizar o processamento dos logs, seu código deverá saber tratar também linhas que não são JSON.</p>
<p>O nome dos campos e a serialização JSON ainda estão sujeitos a mudanças.</p>
</div>
<p>A opção <code>--logging-format=json</code> muda o formato dos logs, do formato padrão da klog para JSON. Abaixo segue um exemplo de um log em formato JSON (identado):</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
   <span style=color:green;font-weight:700>&#34;ts&#34;</span>: <span style=color:#666>1580306777.04728</span>,
   <span style=color:green;font-weight:700>&#34;v&#34;</span>: <span style=color:#666>4</span>,
   <span style=color:green;font-weight:700>&#34;msg&#34;</span>: <span style=color:#b44>&#34;Pod status updated&#34;</span>,
   <span style=color:green;font-weight:700>&#34;pod&#34;</span>:{
      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;nginx-1&#34;</span>,
      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
   },
   <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;ready&#34;</span>
}
</code></pre></div><p>Chaves com significados especiais:</p>
<ul>
<li><code>ts</code> - Data e hora no formato Unix (obrigatório, float)</li>
<li><code>v</code> - Nível de detalhe (obrigatório, int, padrão 0)</li>
<li><code>err</code> - Mensagem de erro (opcional, string)</li>
<li><code>msg</code> - Mensagem (obrigatório, string)</li>
</ul>
<p>Lista dos componentes que suportam o formato JSON atualmente:</p>
<ul>
<li><a class=glossary-tooltip title="Componente da camada de gerenciamento que executa os processos de controle." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li>
<li><a class=glossary-tooltip title="O componente da camada de gerenciamento que serve a API do Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li>
<li><a class=glossary-tooltip title="Componente da camada de gerenciamento que observa os pods recém-criados sem nenhum nó atribuído, e seleciona um nó para executá-los." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li>
<li><a class=glossary-tooltip title="Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a></li>
</ul>
<h3 id=limpeza-dos-logs>Limpeza dos Logs</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>
<div class="alert alert-danger warning callout" role=alert>
<strong>Aviso:</strong> A funcionalidade de limpeza dos logs pode causar impactos significativos na performance, sendo portanto contraindicada em produção.
</div>
<p>A opção <code>--experimental-logging-sanitization</code> habilita o filtro de limpeza dos logs.
Quando habilitado, esse filtro inspeciona todos os argumentos dos logs, procurando por campos contendo dados sensíveis (como senhas, chaves e tokens). Tais campos não serão expostos nas mensagens de log.</p>
<p>Lista dos componentes que suportam a limpeza de logs atualmente:</p>
<ul>
<li><a class=glossary-tooltip title="Componente da camada de gerenciamento que executa os processos de controle." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li>
<li><a class=glossary-tooltip title="O componente da camada de gerenciamento que serve a API do Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li>
<li><a class=glossary-tooltip title="Componente da camada de gerenciamento que observa os pods recém-criados sem nenhum nó atribuído, e seleciona um nó para executá-los." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li>
<li><a class=glossary-tooltip title="Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a></li>
</ul>
<div class="alert alert-info note callout" role=alert>
<strong>Nota:</strong> O filtro de limpeza dos logs não impede a exposição de dados sensíveis nos logs das aplicações em execução.
</div>
<h3 id=nível-de-detalhe-dos-logs>Nível de detalhe dos logs</h3>
<p>A opção <code>-v</code> controla o nível de detalhe dos logs. Um valor maior aumenta o número de eventos registrados, começando a registrar também os eventos menos importantes. Similarmente, um valor menor restringe os logs apenas aos eventos mais importantes. O valor padrão 0 registra apenas eventos críticos.</p>
<h3 id=localização-dos-logs>Localização dos Logs</h3>
<p>Existem dois tipos de componentes do sistema: aqueles que são executados em um contêiner e aqueles que não são. Por exemplo:</p>
<ul>
<li>O <a href=https://kubernetes.io/pt-br/docs/concepts/overview/components/#kube-scheduler>Kubernetes scheduler</a> e o <a href=https://kubernetes.io/pt-br/docs/concepts/overview/components/#kube-proxy>kube-proxy</a> são executados em um contêiner.</li>
<li>O <a href=https://kubernetes.io/pt-br/docs/concepts/overview/components/#kubelet>kubelet</a> e os <a href=https://kubernetes.io/pt-br/docs/concepts/overview/components/#container-runtime>agentes de execução</a>, como o Docker por exemplo, não são executados em contêineres.</li>
</ul>
<p>Em máquinas com systemd, o kubelet e os agentes de execução gravam os logs no journald.
Em outros casos, eles escrevem os logs em arquivos <code>.log</code> no diretório <code>/var/log</code>.
Já os componentes executados dentro de contêineres, sempre irão escrever os logs em arquivos <code>.log</code>
no diretório <code>/var/log</code>, ignorando o mecanismo padrão de log.</p>
<p>De forma similar aos logs de contêiner, os logs de componentes do sistema no diretório <code>/var/log</code> devem ser rotacionados.
Nos clusters Kubernetes criados com o script <code>kube-up.sh</code>, a rotação dos logs é configurada pela ferramenta <code>logrotate</code>. Essa ferramenta rotaciona os logs diariamente
ou quando o tamanho do arquivo excede 100MB.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia sobre <a href=/pt-br/docs/concepts/cluster-administration/logging/>Arquitetura de Logs do Kubernetes</a></li>
<li>Leia sobre <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging>Logs Estruturados</a></li>
<li>Leia sobre <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>Convenções sobre os níveis de logs</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-cbfd3654996eae9fcdef009f70fa83f0>11.6 - Métricas para componentes do sistema Kubernetes</h1>
<p>Métricas dos componentes do sistema podem dar uma visão melhor do que acontece internamente. Métricas são particularmente úteis para construir <em>dashboards</em> e alertas.</p>
<p>Componentes do Kubernetes emitem métricas no <a href=https://prometheus.io/docs/instrumenting/exposition_formats/>formato Prometheus</a>. Esse formato é um texto simples estruturado, projetado para que pessoas e máquinas possam lê-lo.</p>
<h2 id=métricas-no-kubernetes>Métricas no Kubernetes</h2>
<p>Na maioria dos casos, as métricas estão disponíveis no <em>endpoint</em> <code>/metrics</code> do servidor HTTP. Para componentes que não expõem o <em>endpoint</em> por padrão, ele pode ser ativado usando a <em>flag</em> <code>--bind-address</code>.</p>
<p>Exemplos desses componentes:</p>
<ul>
<li><a class=glossary-tooltip title="Componente da camada de gerenciamento que executa os processos de controle." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li>
<li><a class=glossary-tooltip title="kube-proxy é um proxy de rede executado em cada nó do cluster." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a></li>
<li><a class=glossary-tooltip title="O componente da camada de gerenciamento que serve a API do Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li>
<li><a class=glossary-tooltip title="Componente da camada de gerenciamento que observa os pods recém-criados sem nenhum nó atribuído, e seleciona um nó para executá-los." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li>
<li><a class=glossary-tooltip title="Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a></li>
</ul>
<p>Em um ambiente de produção, você pode querer configurar o <a href=https://prometheus.io/>Servidor Prometheus</a> ou algum outro coletor de métricas e disponibilizá-las em algum tipo de banco de dados temporais.</p>
<p>Observe que o <a class=glossary-tooltip title="Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a> também expõe métricas nos <em>endpoints</em> <code>/metrics/cadvisor</code>, <code>/metrics/resource</code> e <code>/metrics/probes</code>. Essas métricas não possuem o mesmo ciclo de vida.</p>
<p>Se o seu <em>cluster</em> usa <a class=glossary-tooltip title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a>, ler as métricas requer autorização por meio de um usuário, grupo ou <em>ServiceAccount</em> com um <em>ClusterRole</em> que conceda o acesso ao <code>/metrics</code>.</p>
<p>Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prometheus<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/metrics&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- get<span style=color:#bbb>
</span></code></pre></div><h2 id=ciclo-de-vida-da-métrica>Ciclo de vida da métrica</h2>
<p>Métrica alfa → Métrica estável → Métrica ultrapassada → Métrica oculta → Métrica excluída</p>
<p>A métrica alfa não tem garantias de estabilidade. Essas métricas podem ser modificadas ou deletadas a qualquer momento.</p>
<p>Métricas estáveis possuem a garantia de que não serão alteradas. Isso significa:</p>
<ul>
<li>Uma métrica estável sem uma assinatura ultrapassada não será deletada ou renomeada</li>
<li>O tipo de uma métrica estável não será modificado</li>
</ul>
<p>As métricas ultrapassadas estão programadas para exclusão, mas ainda estão disponíveis para uso.
Essas métricas incluem uma anotação sobre a versão em que se tornarão ultrapassadas.</p>
<p>Por exemplo:</p>
<ul>
<li>
<p>Antes de se tornar ultrapassado</p>
<pre><code># HELP some_counter isso conta coisas
# TYPE some_counter contador
some_counter 0
</code></pre></li>
<li>
<p>Depois de se tornar ultrapassado</p>
<pre><code># HELP some_counter (obsoleto desde 1.15.0) isso conta coisas
# TYPE some_counter contador
some_counter 0
</code></pre></li>
</ul>
<p>Métricas ocultas não são mais publicadas para extração, mas ainda estão disponíveis para uso. Para usar uma métrica oculta, por favor consulte a seção <a href=#mostrar-m%C3%A9tricas-ocultas>mostrar métricas ocultas</a>.</p>
<p>Métricas excluídas não estão mais disponíveis e não podem mais ser usadas.</p>
<h2 id=mostrar-métricas-ocultas>Mostrar métricas ocultas</h2>
<p>Como descrito anteriormente, administradores podem habilitar métricas ocultas por meio de uma <em>flag</em> de linha de comando em um binário específico. Isso pode ser usado como uma saída de emergência para os administradores caso percam a migração das métricas ultrapassadas na última versão.</p>
<p>A <em>flag</em> <code>show-hidden-metrics-for-version</code> usa uma versão para a qual você deseja mostrar métricas ultrapassadas nessa versão. A versão é expressada como x.y, onde x é a versão principal e y a versão secundária. A versão de <em>patch</em> não é necessária mesmo que uma métrica possa ser descontinuada em uma versão de <em>patch</em>, o motivo é que a política de descontinuação de métricas é executada na versão secundária.</p>
<p>A <em>flag</em> só pode usar a versão secundária anterior como seu valor. Todas as métricas ocultas no anterior serão emitidas se os administradores definirem a versão anterior como <code>show-hidden-metrics-for-version</code>. A versão muito antiga não é permitida porque viola a política de métricas ultrapassadas.</p>
<p>Utilize a métrica <code>A</code> como exemplo, assumindo que <code>A</code> está obsoleto em 1.n. De acordo com a política de métricas ultrapassadas, podemos chegar à seguinte conclusão:</p>
<ul>
<li>Na versão <code>1.n</code>, a métrica está ultrapassada, e pode ser emitida por padrão.</li>
<li>Na versão <code>1.n+1</code>, a métrica está oculta por padrão e pode ser emitida via linha de comando <code>show-hidden-metrics-for-version=1.n</code>.</li>
<li>Na versão <code>1.n+2</code>, a métrica deve ser removida do código fonte. Não há mais <em>escape hatch</em>.</li>
</ul>
<p>Se você está atualizando da versão <code>1.12</code> para <code>1.13</code>, mas ainda depende da métrica <code>A</code> ultrapassada em <code>1.12</code>, você deve definir métricas ocultas via linha de comando: <code>--show-hidden-metrics=1.12</code> e lembre-se de remover essa dependência de métrica antes de atualizar para <code>1.14</code>.</p>
<h2 id=desativar-métricas-do-accelerator>Desativar métricas do <em>accelerator</em></h2>
<p>O kubelet coleta métricas do <em>accelerator</em> por meio do cAdvisor. Para coletar essas métricas, para <em>accelerator</em> como as GPUs NVIDIA, o kubelet mantinha uma alça aberta no driver. Isso significava que, para realizar alterações na infraestrutura (por exemplo, atualizar o <em>driver</em>), um administrador do <em>cluster</em> precisa interromper o agente kubelet.</p>
<p>A responsabilidade de colear métricas do <em>accelerator</em> agora pertence ao fornecedor, e não ao kubelet. Os fornecedores devem providenciar um contêiner que colete métricas e as exponha ao serviço de métricas (por exemplo, Prometheus).</p>
<p>O <a href=/docs/reference/command-line-tools-reference/feature-gates/><code>DisableAcceleratorUsageMetrics</code> <em>feature gate</em></a> desabilita as métricas coletadas pelo kubelet, com uma <a href=https://github.com/kubernetes/enhancements/tree/411e51027db842355bd489691af897afc1a41a5e/keps/sig-node/1867-disable-accelerator-usage-metrics#graduation-criteria><em>timeline</em> para habilitar esse recurso por padrão</a>.</p>
<h2 id=métricas-de-componentes>Métricas de componentes</h2>
<h3 id=métricas-do-kube-controller-manager>Métricas do <em>kube-controller-manager</em></h3>
<p>As métricas do <em>controller manager</em> fornecem informações importantes sobre o desempenho e a integridade do <em>controller manager</em>.
Essas métricas incluem métricas comuns do agente de execução da linguagem Go, tais como a quantidade de <em>go_routine</em> e métricas específicas do <em>controller</em>, como latência de requisições etcd ou latência da <em>API</em> dos provedores de serviços de nuvem (AWS, GCE, OpenStack), que podem ser usadas para medir a integridade de um <em>cluster</em>.</p>
<p>A partir do Kubernetes 1.7, métricas detalhadas de provedores de serviços de nuvem estão disponíveis para operações de armazenamento para o GCE, AWS, Vsphere e OpenStack.
Essas métricas podem ser usadas para monitorar a integridade das operações de volumes persistentes.</p>
<p>Por exemplo, para o GCE as seguintes métricas são chamadas:</p>
<pre><code>cloudprovider_gce_api_request_duration_seconds { request = &quot;instance_list&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;disk_insert&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;disk_delete&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;attach_disk&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;detach_disk&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;list_disk&quot;}
</code></pre><h3 id=métricas-do-kube-scheduler>Métricas do <em>kube-scheduler</em></h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code>
</div>
<p>O <em>scheduler</em> expõe métricas opcionais que relatam os recursos solicitados e os limites desejados de todos os <em>pods</em> em execução. Essas métricas podem ser usadas para criar <em>dashboards</em> de planejamento de capacidade, avaliar os limites de agendamentos atuais ou históricos, identificar rapidamente cargas de trabalho que não podem ser agendadas devido à falta de recursos e comparar o uso atual com a solicitação do <em>pod</em>.</p>
<p>O <em>kube-scheduler</em> identifica as requisições de <a href=/docs/concepts/configuration/manage-resources-containers/>recursos e limites</a> configurado para cada <em>Pod</em>; quando uma requisição ou limite é diferente de zero o <em>kube-scheduler</em> relata uma <em>timeseries</em> de métricas. Essa <em>timeseries</em> é etiquetada por:</p>
<ul>
<li><em>namespace</em></li>
<li>nome do <em>pod</em></li>
<li>o nó onde o <em>pod</em> está programado ou uma <em>string</em> vazia caso ainda não esteja programado</li>
<li>prioridade</li>
<li>o <em>scheduler</em> atribuído para esse <em>pod</em></li>
<li>o nome do recurso (por exemplo, <code>cpu</code>)</li>
<li>a unidade do recurso, se conhecida (por exemplo, <code>cores</code>)</li>
</ul>
<p>Uma vez que o <em>pod</em> alcança um estado de conclusão (sua <code>restartPolicy</code> está como <code>Never</code> ou <code>onFailure</code> e está na fase de <code>Succeeded</code> ou <code>Failed</code>, ou foi deletado e todos os contêineres tem um estado de terminado), a série não é mais relatada já que o <em>scheduler</em> agora está livre para agendar a execução de outros <em>pods</em>. As duas métricas são chamadas de <code>kube_pod_resource_request</code> e <code>kube_pod_resource_limit</code>.</p>
<p>As métricas são expostas no <em>endpoint</em> HTTP <code>/metrics/resources</code> e requerem a mesma autorização que o <em>endpoint</em> <code>/metrics</code> no <em>scheduler</em>. Você deve usar a <em>flag</em> <code>--show-hidden-metrics-for-version=1.20</code> para expor essas métricas de estabilidade alfa.</p>
<h2 id=desativando-métricas>Desativando métricas</h2>
<p>Você pode desativar explicitamente as métricas via linha de comando utilizando a <em>flag</em> <code>--disabled-metrics</code>. Isso pode ser desejado se, por exemplo, uma métrica estiver causando um problema de desempenho. A entrada é uma lista de métricas desabilitadas (ou seja, <code>--disabled-metrics=metric1,metric2</code>).</p>
<h2 id=aplicação-de-cardinalidade-de-métrica>Aplicação de cardinalidade de métrica</h2>
<p>As métricas com dimensões sem limites podem causar problemas de memória nos componentes que elas instrumentam. Para limitar a utilização de recursos você pode usar a opção de linha de comando <code>--allow-label-value</code> para dinamicamente configurar uma lista de permissões de valores de <em>label</em> para uma métrica.</p>
<p>No estágio alfa, a <em>flag</em> pode receber apenas uma série de mapeamentos como lista de permissões de <em>labels</em> para uma métrica.
Cada mapeamento tem o formato <code>&lt;metric_name>,&lt;label_name>=&lt;allowed_labels></code> onde <code>&lt;allowed_labels></code> é uma lista separada por vírgulas de nomes aceitáveis para a <em>label</em>.</p>
<p>O formato geral se parece com:
<code>--allow-label-value &lt;metric_name>,&lt;label_name>='&lt;allow_value1>, &lt;allow_value2>...', &lt;metric_name2>,&lt;label_name>='&lt;allow_value1>, &lt;allow_value2>...', ...</code>.</p>
<p>Por exemplo:
<code>--allow-label-value number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday'</code></p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Leia sobre o <a href=https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format>formato de texto do Prometheus</a> para métricas</li>
<li>Veja a lista de <a href=https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml>métricas estáveis ​​do Kubernetes</a></li>
<li>Leia sobre a <a href=/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior>Política de suspensão de uso do Kubernetes</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2e05a56491965ae320c2662590b2ca18>11.7 - Configurando o Garbage Collection do kubelet</h1>
<p>O Garbage collection(Coleta de lixo) é uma função útil do kubelet que limpa imagens e contêineres não utilizados. O kubelet executará o garbage collection para contêineres a cada minuto e para imagens a cada cinco minutos.</p>
<p>Ferramentas externas de garbage collection não são recomendadas, pois podem potencialmente interromper o comportamento do kubelet removendo os contêineres que existem.</p>
<h2 id=coleta-de-imagens>Coleta de imagens</h2>
<p>O Kubernetes gerencia o ciclo de vida de todas as imagens através do imageManager, com a cooperação do cadvisor.</p>
<p>A política para o garbage collection de imagens leva dois fatores em consideração:
<code>HighThresholdPercent</code> e <code>LowThresholdPercent</code>. Uso do disco acima do limite acionará o garbage collection. O garbage collection excluirá as imagens que foram menos usadas recentemente até que o nível fique abaixo do limite.</p>
<h2 id=coleta-de-container>Coleta de container</h2>
<p>A política para o garbage collection de contêineres considera três variáveis definidas pelo usuário. <code>MinAge</code> é a idade mínima em que um contêiner pode ser coletado. <code>MaxPerPodContainer</code> é o número máximo de contêineres mortos que todo par de pod (UID, container name) pode ter. <code>MaxContainers</code> é o número máximo de contêineres mortos totais. Essas variáveis podem ser desabilitadas individualmente, definindo <code>MinAge</code> como zero e definindo <code>MaxPerPodContainer</code> e <code>MaxContainers</code> respectivamente para menor que zero.</p>
<p>O Kubelet atuará em contêineres não identificados, excluídos ou fora dos limites definidos pelos sinalizadores mencionados. Os contêineres mais antigos geralmente serão removidos primeiro. <code>MaxPerPodContainer</code> e <code>MaxContainer</code> podem potencialmente conflitar entre si em situações em que a retenção do número máximo de contêineres por pod (<code>MaxPerPodContainer</code>) estaria fora do intervalo permitido de contêineres globais mortos (<code>MaxContainers</code>). O <code>MaxPerPodContainer</code> seria ajustado nesta situação: O pior cenário seria fazer o downgrade do <code>MaxPerPodContainer</code> para 1 e remover os contêineres mais antigos. Além disso, os contêineres pertencentes a pods que foram excluídos são removidos assim que se tornem mais antigos que <code>MinAge</code>.</p>
<p>Os contêineres que não são gerenciados pelo kubelet não estão sujeitos ao garbage collection de contêiner.</p>
<h2 id=configurações-do-usuário>Configurações do usuário</h2>
<p>Os usuários podem ajustar os seguintes limites para ajustar o garbage collection da imagem com os seguintes sinalizadores do kubelet:</p>
<ol>
<li><code>image-gh-high-threshold</code>, a porcentagem de uso de disco que aciona o garbage collection da imagem. O padrão é 85%.</li>
<li><code>image-gc-low-threshold</code>, a porcentagem de uso de disco com o qual o garbage collection da imagem tenta liberar. O padrão é 80%.</li>
</ol>
<p>Também permitimos que os usuários personalizem a política do garbagem collection através dos seguintes sinalizadores do kubelet:</p>
<ol>
<li><code>minimum-container-ttl-duration</code>, idade mínima para um contêiner finalizado antes de ser colectado. O padrão é 0 minuto, o que significa que todo contêiner finalizado será coletado como lixo.</li>
<li><code>maximum-dead-containers-per-container</code>, número máximo de instâncias antigas a serem retidas por contêiner. O padrão é 1.</li>
<li><code>maximum-dead-containers</code>, número máximo de instâncias antigas de contêineres para retenção global. O padrão é -1, o que significa que não há limite global.</li>
</ol>
<p>Os contêineres podem ser potencialmente coletados como lixo antes que sua utilidade expire. Esses contêineres podem conter logs e outros dados que podem ser úteis para solucionar problemas. Um valor suficientemente grande para <code>maximum-dead-containers-per-container</code> é altamente recomendado para permitir que pelo menos 1 contêiner morto seja retido por contêiner esperado. Um valor maior para <code>maximum-dead-containers</code> também é recomendados por um motivo semelhante.
Consulte <a href=https://github.com/kubernetes/kubernetes/issues/13287>esta issue</a> para obter mais detalhes.</p>
<h2 id=descontinuado>Descontinuado</h2>
<p>Alguns recursos do Garbage Collection neste documento serão substituídos pelo kubelet eviction no futuro.</p>
<p>Incluindo:</p>
<table>
<thead>
<tr>
<th>Flag Existente</th>
<th>Nova Flag</th>
<th>Fundamentação</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--image-gc-high-threshold</code></td>
<td><code>--eviction-hard</code> ou <code>--eviction-soft</code></td>
<td>os sinais existentes de despejo podem acionar o garbage collection da imagem</td>
</tr>
<tr>
<td><code>--image-gc-low-threshold</code></td>
<td><code>--eviction-minimum-reclaim</code></td>
<td>recuperações de despejo atinge o mesmo comportamento</td>
</tr>
<tr>
<td><code>--maximum-dead-containers</code></td>
<td></td>
<td>descontinuado quando os logs antigos forem armazenados fora do contexto do contêiner</td>
</tr>
<tr>
<td><code>--maximum-dead-containers-per-container</code></td>
<td></td>
<td>descontinuado quando os logs antigos forem armazenados fora do contexto do contêiner</td>
</tr>
<tr>
<td><code>--minimum-container-ttl-duration</code></td>
<td></td>
<td>descontinuado quando os logs antigos forem armazenados fora do contexto do contêiner</td>
</tr>
<tr>
<td><code>--low-diskspace-threshold-mb</code></td>
<td><code>--eviction-hard</code> ou <code>eviction-soft</code></td>
<td>O despejo generaliza os limites do disco para outros recursos</td>
</tr>
<tr>
<td><code>--outofdisk-transition-frequency</code></td>
<td><code>--eviction-pressure-transition-period</code></td>
<td>O despejo generaliza a transição da pressão do disco para outros recursos</td>
</tr>
</tbody>
</table>
<h2 id=próximos-passos>Próximos passos</h2>
<p>Consulte <a href=/docs/tasks/administer-cluster/out-of-resource/>Configurando a Manipulação de Recursos Insuficientes</a> para mais detalhes.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-08e94e6a480e0d6b2de72d84a1b97617>11.8 - Proxies no Kubernetes</h1>
<p>Esta página descreve o uso de proxies com Kubernetes.</p>
<h2 id=proxies>Proxies</h2>
<p>Existem vários tipos diferentes de proxies que você pode encontrar usando Kubernetes:</p>
<ol>
<li>O <a href=/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api>kubectl proxy</a>:</li>
</ol>
<p>Quando o kubectl proxy é utilizado ocorre o seguinte:
- executa na máquina do usuário ou em um pod
- redireciona/encapsula conexões direcionadas ao localhost para o servidor de API
- a comunicação entre o cliente e o o proxy usa HTTP
- a comunicação entre o proxy e o servidor de API usa HTTPS
- o proxy localiza o servidor de API do cluster
- o proxy adiciona os cabeçalhos de comunicação.</p>
<ol>
<li>
<p>O <a href=/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>apiserver proxy</a>:</p>
<ul>
<li>é um bastion server, construído no servidor de API</li>
<li>conecta um usuário fora do cluster com os IPs do cluster que não podem ser acessados de outra forma</li>
<li>executa dentro do processo do servidor de API</li>
<li>cliente para proxy usa HTTPS (ou HTTP se o servidor de API for configurado)</li>
<li>proxy para o destino pode usar HTTP ou HTTPS conforme escolhido pelo proxy usando as informações disponíveis</li>
<li>pode ser usado para acessar um Nó, Pod ou serviço</li>
<li>faz balanceamento de carga quando usado para acessar um Service.</li>
</ul>
</li>
<li>
<p>O <a href=/docs/concepts/services-networking/service/#ips-and-vips>kube proxy</a>:</p>
<ul>
<li>executa em todos os Nós</li>
<li>atua como proxy para UDP, TCP e SCTP</li>
<li>não aceita HTTP</li>
<li>provém balanceamento de carga</li>
<li>apenas é usado para acessar serviços.</li>
</ul>
</li>
<li>
<p>Um Proxy/Balanceador de carga na frente de servidores de API(s):</p>
<ul>
<li>a existência e a implementação de tal elemento varia de cluster para cluster, por exemplo nginx</li>
<li>fica entre todos os clientes e um ou mais serviços</li>
<li>atua como balanceador de carga se existe mais de um servidor de API.</li>
</ul>
</li>
<li>
<p>Balanceadores de carga da nuvem em serviços externos:</p>
<ul>
<li>são fornecidos por algum provedor de nuvem (e.x AWS ELB, Google Cloud Load Balancer)</li>
<li>são criados automaticamente quando o serviço de Kubernetes tem o tipo <code>LoadBalancer</code></li>
<li>geralmente suportam apenas UDP/TCP</li>
<li>O suporte ao SCTP fica por conta da implementação do balanceador de carga da provedora de nuvem</li>
<li>a implementação varia de acordo com o provedor de cloud.</li>
</ul>
</li>
</ol>
<p>Os usuários de Kubernetes geralmente não precisam se preocupar com outras coisas além dos dois primeiros tipos. O
administrador do cluster tipicamente garante que os últimos tipos serão configurados corretamente.</p>
<h2 id=redirecionamento-de-requisições>Redirecionamento de requisições</h2>
<p>Os proxies substituíram as capacidades de redirecionamento. O redirecionamento foi depreciado.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-85d633ae590aa20ec024f1b7af1d74fc>11.9 - Instalando Complementos</h1>
<div class="alert alert-secondary callout third-party-content" role=alert><strong>Nota:</strong>
Esta seção tem links para projetos de terceiros que fornecem a funcionalidade exigida pelo Kubernetes. Os autores do projeto Kubernetes não são responsáveis por esses projetos. Esta página obedece as <a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>diretrizes de conteúdo do site CNCF</a>, listando os itens em ordem alfabética. Para adicionar um projeto a esta lista, leia o <a href=/docs/contribute/style/content-guide/#third-party-content>guia de conteúdo</a> antes de enviar sua alteração.</div>
<p>Complementos estendem as funcionalidades do Kubernetes.</p>
<p>Esta página lista alguns dos complementos disponíveis e links com suas respectivas instruções de instalação.</p>
<h2 id=rede-e-política-de-rede>Rede e Política de Rede</h2>
<ul>
<li><a href=https://www.github.com/noironetworks/aci-containers>ACI</a> fornece rede integrada de contêineres e segurança de rede com a Cisco ACI.</li>
<li><a href=https://antrea.io/>Antrea</a> opera nas camadas 3 e 4 do modelo de rede OSI para fornecer serviços de rede e de segurança para o Kubernetes, aproveitando o Open vSwitch como camada de dados de rede.</li>
<li><a href=https://docs.projectcalico.org/latest/introduction/>Calico</a> é um provedor de serviços de rede e de políticas de rede. Este complemento suporta um conjunto flexível de opções de rede, de modo a permitir a escolha da opção mais eficiente para um dado caso de uso, incluindo redes <em>overlay</em> (sobrepostas) e não-<em>overlay</em>, com ou sem o uso do protocolo BGP. Calico usa o mesmo mecanismo para aplicar políticas de rede a hosts, pods, e aplicações na camada de <em>service mesh</em> (quando Istio e Envoy estão instalados).</li>
<li><a href=https://github.com/tigera/canal/tree/master/k8s-install>Canal</a> une Flannel e Calico, fornecendo rede e política de rede.</li>
<li><a href=https://github.com/cilium/cilium>Cilium</a> é um plug-in de rede de camada 3 e de políticas de rede que pode aplicar políticas HTTP/API/camada 7 de forma transparente. Tanto o modo de roteamento quanto o de sobreposição/encapsulamento são suportados. Este plug-in também consegue operar no topo de outros plug-ins CNI.</li>
<li><a href=https://github.com/Huawei-PaaS/CNI-Genie>CNI-Genie</a> permite que o Kubernetes se conecte facilmente a uma variedade de plug-ins CNI, como Calico, Canal, Flannel, Romana ou Weave.</li>
<li><a href=http://contiv.github.io>Contiv</a> oferece serviços de rede configuráveis para diferentes casos de uso (camada 3 nativa usando BGP, <em>overlay</em> (sobreposição) usando vxlan, camada 2 clássica e Cisco-SDN/ACI) e também um <em>framework</em> rico de políticas de rede. O projeto Contiv é totalmente <a href=http://github.com/contiv>open source</a>. O <a href=http://github.com/contiv/install>instalador</a> fornece opções de instalação com ou sem kubeadm.</li>
<li><a href=http://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a> é uma plataforma open source baseada no <a href=https://tungsten.io>Tungsten Fabric</a> que oferece virtualização de rede multi-nuvem e gerenciamento de políticas de rede. O Contrail e o Tungsten Fabric são integrados a sistemas de orquestração de contêineres, como Kubernetes, OpenShift, OpenStack e Mesos, e fornecem modos de isolamento para cargas de trabalho executando em máquinas virtuais, contêineres/pods e servidores físicos.</li>
<li><a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>Flannel</a> é um provedor de redes <em>overlay</em> (sobrepostas) que pode ser usado com o Kubernetes.</li>
<li><a href=https://github.com/ZTE/Knitter/>Knitter</a> é um plug-in para suporte de múltiplas interfaces de rede em Pods do Kubernetes.</li>
<li>Multus é um plugin para suporte a várias interfaces de rede em Pods no Kubernetes. Este plug-in pode agir como um "meta-plug-in", ou um plug-in CNI que se comunica com múltiplos outros plug-ins CNI (por exemplo, Calico, Cilium, Contiv, Flannel), além das cargas de trabalho baseadas em SRIOV, DPDK, OVS-DPDK e VPP no Kubernetes.</li>
<li><a href=https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf>NSX-T</a> Container Plug-in (NCP) fornece integração entre o VMware NSX-T e sistemas de orquestração de contêineres como o Kubernetes. Além disso, oferece também integração entre o NSX-T e as plataformas CaaS/PaaS baseadas em contêiner, como o Pivotal Container Service (PKS) e o OpenShift.</li>
<li><a href=https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst>Nuage</a> é uma plataforma de rede definida por software que fornece serviços de rede baseados em políticas entre os Pods do Kubernetes e os ambientes não-Kubernetes, com visibilidade e monitoramento de segurança.</li>
<li><a href=https://github.com/ovn-org/ovn-kubernetes/>OVN-Kubernetes</a> é um provedor de rede para o Kubernetes baseado no <a href=https://github.com/ovn-org/ovn/>OVN (Open Virtual Network)</a>, uma implementação de redes virtuais que surgiu através do projeto Open vSwitch (OVS). O OVN-Kubernetes fornece uma implementação de rede baseada em <em>overlay</em> (sobreposição) para o Kubernetes, incluindo uma implementação baseada em OVS para serviços de balanceamento de carga e políticas de rede.</li>
<li><a href=https://github.com/opnfv/ovn4nfv-k8s-plugin>OVN4NFV-K8S-Plugin</a> é um plug-in controlador CNI baseado no OVN (Open Virtual Network) que fornece serviços de rede <em>cloud native</em>, como <em>Service Function Chaining</em> (SFC), redes <em>overlay</em> (sobrepostas) OVN múltiplas, criação dinâmica de subredes, criação dinâmica de redes virtuais, provedor de rede VLAN e provedor de rede direto, e é plugável a outros plug-ins multi-rede. Ideal para cargas de trabalho que utilizam computação de borda <em>cloud native</em> em redes multi-cluster.</li>
<li><a href=http://romana.io>Romana</a> é uma solução de rede de camada 3 para redes de pods que também suporta a <a href=/docs/concepts/services-networking/network-policies/>API NetworkPolicy</a>. Detalhes da instalação do complemento Kubeadm disponíveis <a href=https://github.com/romana/romana/tree/master/containerize>aqui</a>.</li>
<li><a href=https://www.weave.works/docs/net/latest/kube-addon/>Weave Net</a> fornece rede e política de rede, funciona em ambos os lados de uma partição de rede e não requer um banco de dados externo.</li>
</ul>
<h2 id=descoberta-de-serviço>Descoberta de Serviço</h2>
<ul>
<li><a href=https://coredns.io>CoreDNS</a> é um servidor DNS flexível e extensível que pode ser <a href=https://github.com/coredns/deployment/tree/master/kubernetes>instalado</a> como o serviço de DNS dentro do cluster para ser utilizado por pods.</li>
</ul>
<h2 id=visualização-amp-controle>Visualização & Controle</h2>
<ul>
<li><a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>Dashboard</a> é uma interface web para gestão do Kubernetes.</li>
<li><a href=https://www.weave.works/documentation/scope-latest-installing/#k8s>Weave Scope</a> é uma ferramenta gráfica para visualizar contêineres, pods, serviços, entre outros objetos do cluster. Pode ser utilizado com uma <a href=https://cloud.weave.works/>conta Weave Cloud</a>. Como alternativa, é possível hospedar a interface do usuário por conta própria.</li>
</ul>
<h2 id=infraestrutura>Infraestrutura</h2>
<ul>
<li><a href=https://kubevirt.io/user-guide/#/installation/installation>KubeVirt</a> é um complemento para executar máquinas virtuais no Kubernetes. É geralmente executado em clusters em máquina física.</li>
</ul>
<h2 id=complementos-legados>Complementos Legados</h2>
<p>Existem vários outros complementos documentados no diretório <a href=https://git.k8s.io/kubernetes/cluster/addons>cluster/addons</a> que não são mais utilizados.</p>
<p>Projetos bem mantidos devem ser listados aqui. PRs são bem-vindos!</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-7e0d97616b15e2c383c6a0a96ec442cb>12 - Extendendo o Kubernetes</h1>
</div>
<div class=td-content>
<h1 id=pg-0af41d3bd7c785621b58b7564793396a>12.1 - Extendendo a API do Kubernetes</h1>
</div>
<div class=td-content>
<h1 id=pg-1ea4977c0ebf97569bf54a477faa7fa5>12.1.1 - Extendendo a API do Kubernetes com a camada de agregação</h1>
<p>A camada de agregação permite ao Kubernetes ser estendido com APIs adicionais,
para além do que é oferecido pelas APIs centrais do Kubernetes.
As APIs adicionais podem ser soluções prontas tal como o
<a href=/docs/concepts/extend-kubernetes/service-catalog/>catálogo de serviços</a>,
ou APIs que você mesmo desenvolva.</p>
<p>A camada de agregação é diferente dos <a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Recursos Personalizados</a>,
que são uma forma de fazer o <a class=glossary-tooltip title="O componente da camada de gerenciamento que serve a API do Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a>
reconhecer novas espécies de objetos.</p>
<h2 id=camada-de-agregação>Camada de agregação</h2>
<p>A camada de agregação executa em processo com o kube-apiserver.
Até que um recurso de extensão seja registado, a camada de agregação
não fará nada. Para registar uma API, terá de adicionar um objeto <em>APIService</em>
que irá "reclamar" o caminho URL na API do Kubernetes. Nesta altura, a camada
de agregação procurará qualquer coisa enviada para esse caminho da API
(e.g. <code>/apis/myextension.mycompany.io/v1/…</code>) para o <em>APIService</em> registado.</p>
<p>A maneira mais comum de implementar o <em>APIService</em> é executar uma
<em>extensão do servidor API</em> em <em>Pods</em> que executam no seu cluster.
Se estiver a usar o servidor de extensão da API para gerir recursos
no seu cluster, o servidor de extensão da API (também escrito como "extension-apiserver")
é tipicamente emparelhado com um ou mais <a class=glossary-tooltip title="Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controladores>controladores</a>.
A biblioteca apiserver-builder providencia um esqueleto para ambos
os servidores de extensão da API e controladores associados.</p>
<h3 id=latência-da-resposta>Latência da resposta</h3>
<p>Servidores de extensão de APIs devem ter baixa latência de rede de e para o kube-apiserver.
Pedidos de descoberta são necessários que façam a ida e volta do kube-apiserver em 5
segundos ou menos.</p>
<p>Se o seu servidor de extensão da API não puder cumprir com o requisito de latência,
considere fazer alterações que permitam atingi-lo. Pode também definir
<a href=/docs/reference/command-line-tools-reference/feature-gates/>portal de funcionalidade</a> <code>EnableAggregatedDiscoveryTimeout=false</code> no kube-apiserver para desativar
a restrição de intervalo. Esta portal de funcionalidade deprecado será removido
num lançamento futuro.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Para pôr o agregador a funcionar no seu ambiente, <a href=/docs/tasks/access-kubernetes-api/configure-aggregation-layer/>configure a camada de agregação</a>.</li>
<li>De seguida, <a href=/docs/tasks/access-kubernetes-api/setup-extension-api-server/>configura um api-server de extensão</a> para funcionar com a camada de agregação.</li>
<li>Também, aprenda como pode <a href=/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/>estender a API do Kubernetes através do use de Definições de Recursos Personalizados</a>.</li>
<li>Leia a especificação do <a href=/docs/reference/generated/kubernetes-api/v1.23/#apiservice-v1-apiregistration-k8s-io>APIService</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c8937cdc9df96f3328becf04f8211292>12.2 - Extensões de Computação, armazenamento e redes</h1>
</div>
<div class=td-content>
<h1 id=pg-1ac2260db9ecccbf0303a899bc27ce6d>12.2.1 - Plugins de rede</h1>
<p>Plugins de redes no Kubernetes podem ser dos seguintes tipos:</p>
<ul>
<li>Plugins CNI: Aderentes à especificação <a href=https://github.com/containernetworking/cni>Container Network Interface</a> (CNI), desenhados para interoperabilidade.
<ul>
<li>Kubernetes usa a versão <a href=https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md>v0.4.0</a> da especificação CNI.</li>
</ul>
</li>
<li>Plugin kubenet: Implementa o <code>cbr0</code> básico usando os plugins CNI <code>bridge</code> e <code>host-local</code></li>
</ul>
<h2 id=instalação>Instalação</h2>
<p>O kubelet possui um plugin único padrão, e um plugin padrão comum para todo o cluster.
Ele verifica o plugin quando inicia, se lembra o que encontrou, e executa o plugin selecionado
em momentos oportunos dentro do ciclo de vida de um Pod (isso é verdadeiro apenas com o Docker,
uma vez que o CRI gerencia seus próprios plugins de CNI). Existem dois parâmetros de linha de comando
no Kubelet para se ter em mente quando usando plugins:</p>
<ul>
<li><code>cni-bin-dir</code>: O Kubelet verifica esse diretório por plugins na inicialização</li>
<li><code>network-plugin</code>: O plugin de rede que deve ser utilizado do diretório configurado em
<code>cni-bin-dir</code>. Deve ser igual ao nome configurado por um plugin no diretório de plugins.
Para plugins de CNI, isso equivale ao valor <code>cni</code>.</li>
</ul>
<h2 id=requisitos-de-plugins-de-rede>Requisitos de plugins de Rede</h2>
<p>Além de prover a <a href=https://github.com/kubernetes/kubernetes/tree/v1.23.17/pkg/kubelet/dockershim/network/plugins.go>interface <code>NetworkPlugin</code></a>
para configuração da rede do pod, o plugin pode necessitar de suporte específico ao
kube-proxy.
O proxy iptables obviamente depende do iptables, e o plugin deve garantir que o
tráfego do contêiner esteja disponível para o iptables. Por exemplo, se o plugin
conecta os contêineres à <em>Linux bridge</em>, o plugin deve configurar a diretiva de
<em>sysctl</em> <code>net/bridge/bridge-nf-call-iptables</code> com o valor <code>1</code> para garantir que o
proxy iptables opere normalmente. Se o plugin não faz uso da <em>Linux Bridge</em> (mas outro
mecanismo, como Open vSwitch) ele deve garantir que o tráfego do contêiner é roteado
apropriadamente para o proxy.</p>
<p>Por padrão, se nenhum plugin de rede é configurado no kubelet, o plugin <code>noop</code> é utilizado,
que configura <code>net/bridge/bridge-nf-call-iptables=1</code> para garantir que configurações simples
(como Docker com <em>bridge Linux</em>) operem corretamente com o proxy iptables.</p>
<h3 id=cni>CNI</h3>
<p>O plugin de CNI é selecionado utilizando-se da opção <code>--network-plugin=cni</code> no início do Kubeket.
O Kubelet lê um arquivo do diretório especificado em <code>--cni-conf-dir</code> (padrão <code>/etc/cni/net.d</code>)
e usa a configuração de CNI desse arquivo para configurar a rede de cada Pod. O arquivo de
configuração do CNI deve usar a <a href=https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration>especificação de CNI</a>,
e qualquer plugin referenciado nesse arquivo deve estar presente no diretório
<code>--cni-bin-dir</code> (padrão <code>/opt/cni/bin</code>).</p>
<p>Se existirem múltiplos arquivos de configuração no diretório, o kubelet usa o arquivo de
configuração que vier primeiro pelo nome, em ordem alfabética.</p>
<p>Adicionalmente ao plugin de CNI especificado no arquivo de configuração, o Kubernetes requer
o plugin CNI padrão <a href=https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go><code>lo</code></a> ao menos na versão 0.2.0.</p>
<h4 id=suporte-a-hostport>Suporte a hostPort</h4>
<p>O plugin de redes CNI suporta <code>hostPort</code>. Você pode utilizar o plugin oficial
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap>portmap</a>
ou usar seu próprio plugin com a funcionalidade de <em>portMapping</em>.</p>
<p>Caso você deseje habilitar o suporte a <code>hostPort</code>, você deve especificar
<code>portMappings capability</code> no seu <code>cni-conf-dir</code>.
Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;k8s-pod-network&#34;</span>,
  <span style=color:green;font-weight:700>&#34;cniVersion&#34;</span>: <span style=color:#b44>&#34;0.3.0&#34;</span>,
  <span style=color:green;font-weight:700>&#34;plugins&#34;</span>: [
    {
      <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;calico&#34;</span>,
      <span style=color:green;font-weight:700>&#34;log_level&#34;</span>: <span style=color:#b44>&#34;info&#34;</span>,
      <span style=color:green;font-weight:700>&#34;datastore_type&#34;</span>: <span style=color:#b44>&#34;kubernetes&#34;</span>,
      <span style=color:green;font-weight:700>&#34;nodename&#34;</span>: <span style=color:#b44>&#34;127.0.0.1&#34;</span>,
      <span style=color:green;font-weight:700>&#34;ipam&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;host-local&#34;</span>,
        <span style=color:green;font-weight:700>&#34;subnet&#34;</span>: <span style=color:#b44>&#34;usePodCidr&#34;</span>
      },
      <span style=color:green;font-weight:700>&#34;policy&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;k8s&#34;</span>
      },
      <span style=color:green;font-weight:700>&#34;kubernetes&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;kubeconfig&#34;</span>: <span style=color:#b44>&#34;/etc/cni/net.d/calico-kubeconfig&#34;</span>
      }
    },
    {
      <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;portmap&#34;</span>,
      <span style=color:green;font-weight:700>&#34;capabilities&#34;</span>: {<span style=color:green;font-weight:700>&#34;portMappings&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>}
    }
  ]
}
</code></pre></div><h4 id=suporte-a-controle-de-banda>Suporte a controle de banda</h4>
<p><strong>Funcionalidade experimental</strong></p>
<p>O plugin de rede CNI também suporta o controle de banda de entrada e saída.
Você pode utilizar o plugin oficial <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth>bandwidth</a>
desenvolvido ou usar seu próprio plugin de controle de banda.</p>
<p>Se você habilitar o suporte ao controle de banda, você deve adicionar o plugin <code>bandwidth</code>
no seu arquivo de configuração de CNI (padrão <code>/etc/cni/net.d</code>) e garantir que o programa
exista no diretório de binários do CNI (padrão <code>/opt/cni/bin</code>).</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;k8s-pod-network&#34;</span>,
  <span style=color:green;font-weight:700>&#34;cniVersion&#34;</span>: <span style=color:#b44>&#34;0.3.0&#34;</span>,
  <span style=color:green;font-weight:700>&#34;plugins&#34;</span>: [
    {
      <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;calico&#34;</span>,
      <span style=color:green;font-weight:700>&#34;log_level&#34;</span>: <span style=color:#b44>&#34;info&#34;</span>,
      <span style=color:green;font-weight:700>&#34;datastore_type&#34;</span>: <span style=color:#b44>&#34;kubernetes&#34;</span>,
      <span style=color:green;font-weight:700>&#34;nodename&#34;</span>: <span style=color:#b44>&#34;127.0.0.1&#34;</span>,
      <span style=color:green;font-weight:700>&#34;ipam&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;host-local&#34;</span>,
        <span style=color:green;font-weight:700>&#34;subnet&#34;</span>: <span style=color:#b44>&#34;usePodCidr&#34;</span>
      },
      <span style=color:green;font-weight:700>&#34;policy&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;k8s&#34;</span>
      },
      <span style=color:green;font-weight:700>&#34;kubernetes&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;kubeconfig&#34;</span>: <span style=color:#b44>&#34;/etc/cni/net.d/calico-kubeconfig&#34;</span>
      }
    },
    {
      <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;bandwidth&#34;</span>,
      <span style=color:green;font-weight:700>&#34;capabilities&#34;</span>: {<span style=color:green;font-weight:700>&#34;bandwidth&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>}
    }
  ]
}
</code></pre></div><p>Agora você pode adicionar as anotações <code>kubernetes.io/ingress-bandwidth</code> e
<code>kubernetes.io/egress-bandwidth</code> em seu pod.
Por exemplo:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/ingress-bandwidth</span>:<span style=color:#bbb> </span>1M<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/egress-bandwidth</span>:<span style=color:#bbb> </span>1M<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></code></pre></div><h3 id=kubenet>kubenet</h3>
<p>Kubenet é um plugin de rede muito simples, existente apenas no Linux. Ele não
implementa funcionalidades mais avançadas, como rede entre nós ou políticas de rede.
Ele é geralmente utilizado junto a um provedor de nuvem que configura as regras de
roteamento para comunicação entre os nós, ou em ambientes com apenas um nó.</p>
<p>O Kubenet cria uma <em>interface bridge</em> no Linux chamada <code>cbr0</code> e cria um par <em>veth</em>
para cada um dos pods com o host como a outra ponta desse par, conectado à <code>cbr0</code>.
Na interface no lado do Pod um endereço IP é alocado de uma faixa associada ao nó,
sendo parte de alguma configuração no nó ou pelo controller-manager. Na interface <code>cbr0</code>
é associado o MTU equivalente ao menor MTU de uma interface de rede do host.</p>
<p>Esse plugin possui alguns requisitos:</p>
<ul>
<li>Os plugins CNI padrão <code>bridge</code>, <code>lo</code> e <code>host-local</code> são obrigatórios, ao menos na
versão 0.2.0. O Kubenet buscará inicialmente esses plugins no diretório <code>/opt/cni/bin</code>.
Especifique a opção <code>cni-bin-dir</code> no kubelet para fornecer um diretório adicional
de busca. O primeiro local equivalente será o utilizado.</li>
<li>O kubelet deve ser executado com a opção <code>--network-plugin=kubenet</code> para habilitar esse plugin.</li>
<li>O Kubelet deve ainda ser executado com a opção <code>--non-masquerade-cidr=&lt;clusterCidr></code> para
garantir que o tráfego de IPs para fora dessa faixa seja mascarado.</li>
<li>O nó deve possuir uma subrede associada, através da opção <code>--pod-cidr</code> configurada
na inicialização do kubelet, ou as opções <code>--allocate-node-cidrs=true --cluster-cidr=&lt;cidr></code>
utilizadas na inicialização do <em>controller-manager</em>.</li>
</ul>
<h3 id=customizando-o-mtu-com-kubenet>Customizando o MTU (com kubenet)</h3>
<p>O MTU deve sempre ser configurado corretamente para obter-se a melhor performance de
rede. Os plugins de rede geralmente tentam detectar uma configuração correta de MTU,
porém algumas vezes a lógica não irá resultar em uma configuração adequada. Por exemplo,
se a <em>Docker bridge</em> ou alguma outra interface possuir um MTU pequeno, o kubenet irá
selecionar aquela MTU. Ou caso você esteja utilizando encapsulamento IPSEC, o MTU deve
ser reduzido, e esse cálculo não faz parte do escopo da maioria dos plugins de rede.</p>
<p>Sempre que necessário, você pode configurar explicitamente o MTU com a opção <code>network-plugin-mtu</code>
no kubelet. Por exemplo, na AWS o MTU da <code>eth0</code> geralmente é 9001 então você deve
especificar <code>--network-plugin-mtu=9001</code>. Se você estiver usando IPSEC você deve reduzir
o MTU para permitir o encapsulamento excedente; por exemplo: <code>--network-plugin-mtu=8773</code>.</p>
<p>Essa opção faz parte do plugin de rede. Atualmente <strong>apenas o kubenet suporta a configuração
<code>network-plugin-mtu</code></strong>.</p>
<h2 id=resumo-de-uso>Resumo de uso</h2>
<ul>
<li><code>--network-plugin=cni</code> especifica que devemos usar o plugin de redes <code>cni</code> com os
binários do plugin localizados em <code>--cni-bin-dir</code> (padrão <code>/opt/cni/bin</code>) e as
configurações do plugin localizadas em <code>--cni-conf-dir</code> (default <code>/etc/cni/net.d</code>).</li>
<li><code>--network-plugin=kubenet</code> especifica que iremos usar o plugin de rede <code>kubenet</code>
com os plugins CNI <code>bridge</code>, <code>lo</code> e <code>host-local</code> localizados em <code>/opt/cni/bin</code> ou <code>cni-bin-dir</code>.</li>
<li><code>--network-plugin-mtu=9001</code> especifica o MTU a ser utilizado, atualmente apenas em uso
pelo plugin de rede <code>kubenet</code></li>
</ul>
<h2 id=próximos-passos>Próximos passos</h2>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-3131452556176159fb269593c1a52012>12.3 - Padrão Operador</h1>
<p>Operadores são extensões de software para o Kubernetes que
fazem uso de <a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/><em>recursos personalizados</em></a>
para gerir aplicações e os seus componentes. Operadores seguem os<br>
princípios do Kubernetes, notavelmente o <a href=/docs/concepts/#kubernetes-control-plane>ciclo de controle</a>.</p>
<h2 id=motivação>Motivação</h2>
<p>O padrão Operador tem como objetivo capturar o principal objetivo de um operador
humano que gere um serviço ou um conjunto de serviços. Operadores humanos
responsáveis por aplicações e serviços específicos têm um conhecimento
profundo da forma como o sistema é suposto se comportar, como é instalado
e como deve reagir na ocorrência de problemas.</p>
<p>As pessoas que executam cargas de trabalho no Kubernetes habitualmente gostam
de usar automação para cuidar de tarefas repetitivas. O padrão Operador captura
a forma como pode escrever código para automatizar uma tarefa para além do que
o Kubernetes fornece.</p>
<h2 id=operadores-no-kubernetes>Operadores no Kubernetes</h2>
<p>O Kubernetes é desenhado para automação. <em>Out of the box</em>, você tem bastante
automação embutida no núcleo do Kubernetes. Pode usar
o Kubernetes para automatizar instalações e executar cargas de trabalho,
e pode ainda automatizar a forma como o Kubernetes faz isso.</p>
<p>O conceito de <a class=glossary-tooltip title="Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a> no
Kubernetes permite a extensão do comportamento sem modificar o código do próprio
Kubernetes.
Operadores são clientes da API do Kubernetes que atuam como controladores para
um dado <a href=/docs/concepts/api-extension/custom-resources/><em>Custom Resource</em></a></p>
<h2 id=exemplo>Exemplo de um Operador</h2>
<p>Algumas das coisas que um operador pode ser usado para automatizar incluem:</p>
<ul>
<li>instalar uma aplicação a pedido</li>
<li>obter e restaurar backups do estado dessa aplicação</li>
<li>manipular atualizações do código da aplicação juntamente com alterações
como esquemas de base de dados ou definições de configuração extra</li>
<li>publicar um <em>Service</em> para aplicações que não suportam a APIs do Kubernetes
para as descobrir</li>
<li>simular uma falha em todo ou parte do cluster de forma a testar a resiliência</li>
<li>escolher um lider para uma aplicação distribuída sem um processo
de eleição de membro interno</li>
</ul>
<p>Como deve um Operador parecer em mais detalhe? Aqui está um exemplo em mais
detalhe:</p>
<ol>
<li>Um recurso personalizado (<em>custom resource</em>) chamado SampleDB, que você pode
configurar para dentro do <em>cluster</em>.</li>
<li>Um <em>Deployment</em> que garante que um <em>Pod</em> está a executar que contém a
parte controlador do operador.</li>
<li>Uma imagem do <em>container</em> do código do operador.</li>
<li>Código do controlador que consulta o plano de controle para descobrir quais
recursos <em>SampleDB</em> estão configurados.</li>
<li>O núcleo do Operador é o código para informar ao servidor da API (<em>API server</em>) como fazer
a realidade coincidir com os recursos configurados.
<ul>
<li>Se você adicionar um novo <em>SampleDB</em>, o operador configurará <em>PersistentVolumeClaims</em>
para fornecer armazenamento de base de dados durável, um <em>StatefulSet</em> para executar <em>SampleDB</em> e
um <em>Job</em> para lidar com a configuração inicial.</li>
<li>Se você apagá-lo, o Operador tira um <em>snapshot</em> e então garante que
o <em>StatefulSet</em> e <em>Volumes</em> também são removidos.</li>
</ul>
</li>
<li>O operador também gere backups regulares da base de dados. Para cada recurso <em>SampleDB</em>,
o operador determina quando deve criar um <em>Pod</em> que possa se conectar
à base de dados e faça backups. Esses <em>Pods</em> dependeriam de um <em>ConfigMap</em>
e / ou um <em>Secret</em> que possui detalhes e credenciais de conexão com à base de dados.</li>
<li>Como o Operador tem como objetivo fornecer automação robusta para o recurso
que gere, haveria código de suporte adicional. Para este exemplo,
O código verifica se a base de dados está a executar uma versão antiga e, se estiver,
cria objetos <em>Job</em> que o atualizam para si.</li>
</ol>
<h2 id=instalar-operadores>Instalar Operadores</h2>
<p>A forma mais comum de instalar um Operador é a de adicionar a
definição personalizada de recurso (<em>Custom Resource Definition</em>) e
o seu Controlador associado ao seu cluster.
O Controlador vai normalmente executar fora do
<a class=glossary-tooltip title="A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres." data-toggle=tooltip data-placement=top href="/pt-br/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="plano de controle">plano de controle</a>,
como você faria com qualquer aplicação containerizada.
Por exemplo, você pode executar o controlador no seu cluster como um <em>Deployment</em>.</p>
<h2 id=usando-um-operador>Usando um Operador</h2>
<p>Uma vez que você tenha um Operador instalado, usaria-o adicionando, modificando
ou apagando a espécie de recurso que o Operador usa. Seguindo o exemplo acima,
você configuraria um <em>Deployment</em> para o próprio Operador, e depois:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get SampleDB                   <span style=color:#080;font-style:italic># encontra a base de dados configurada</span>

kubectl edit SampleDB/example-database <span style=color:#080;font-style:italic># mudar manualmente algumas definições</span>
</code></pre></div><p>…e é isso! O Operador vai tomar conta de aplicar
as mudanças assim como manter o serviço existente em boa forma.</p>
<h2 id=escrevendo-operador>Escrevendo o seu próprio Operador</h2>
<p>Se não existir no ecosistema um Operador que implementa
o comportamento que pretende, pode codificar o seu próprio.
<a href=#qual-%C3%A9-o-pr%C3%B3ximo>Qual é o próximo</a> você vai encontrar
alguns <em>links</em> para bibliotecas e ferramentas que pode usar
para escrever o seu próprio Operador <em>cloud native</em>.</p>
<p>Pode também implementar um Operador (isto é, um Controlador) usando qualquer linguagem / <em>runtime</em>
que pode atuar como um <a href=/docs/reference/using-api/client-libraries/>cliente da API do Kubernetes</a>.</p>
<h2 id=próximos-passos>Próximos passos</h2>
<ul>
<li>Aprenda mais sobre <a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Recursos Personalizados</a></li>
<li>Encontre operadores prontos em <a href=https://operatorhub.io/>OperatorHub.io</a> para o seu caso de uso</li>
<li>Use ferramentes existentes para escrever os seus Operadores:
<ul>
<li>usando <a href=https://kudo.dev/>KUDO</a> (Kubernetes Universal Declarative Operator)</li>
<li>usando <a href=https://book.kubebuilder.io/>kubebuilder</a></li>
<li>usando <a href=https://metacontroller.github.io/metacontroller/intro.html>Metacontroller</a> juntamente com WebHooks que
implementa você mesmo</li>
<li>usando o <a href=https://operatorframework.io/>Operator Framework</a></li>
</ul>
</li>
<li><a href=https://operatorhub.io/>Publique</a> o seu operador para que outras pessoas o possam usar</li>
<li>Leia o <a href=https://coreos.com/blog/introducing-operators.html>artigo original da CoreOS</a> que introduz o padrão Operador</li>
<li>Leia um <a href=https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-building-kubernetes-operators-and-stateful-apps>artigo</a> da Google Cloud sobre as melhores práticas para contruir Operadores</li>
</ul>
</div>
</main>
</div>
</div>
<footer class=d-print-none>
<div class=footer__links>
<nav>
<a class=text-white href=/pt-br/docs/home/>Home</a>
<a class=text-white href=/pt-br/blog/>Blog</a>
<a class=text-white href=/pt-br/partners/>Parceiros</a>
<a class=text-white href=/pt-br/community/>Comunidade</a>
<a class=text-white href=/pt-br/case-studies/>Casos de estudo</a>
</nav>
</div>
<div class=container-fluid>
<div class=row>
<div class="col-6 col-sm-2 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list">
<a class=text-white target=_blank href=https://discuss.kubernetes.io>
<i class="fa fa-envelope"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter>
<a class=text-white target=_blank href=https://twitter.com/kubernetesio>
<i class="fab fa-twitter"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar>
<a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
<i class="fas fa-calendar-alt"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube>
<a class=text-white target=_blank href=https://youtube.com/kubernetescommunity>
<i class="fab fa-youtube"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank href=https://slack.k8s.io>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute>
<a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide>
<i class="fas fa-edit"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow">
<a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes>
<i class="fab fa-stack-overflow"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-8 text-center order-sm-2">
<small class=text-white>&copy; 2023 Os autores do Kubernetes | Documentação Distribuída sob <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small>
<br>
<small class=text-white>Copyright &copy; 2023 A Fundação Linux &reg;. Todos os direitos reservados. A Linux Foundation tem marcas registradas e usa marcas registradas. Para uma lista de marcas registradas da The Linux Foundation, por favor, veja nossa <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Página de uso de marca registrada</a></small>
<br>
<small class=text-white>ICP license: 京ICP备17074266号-3</small>
</div>
</div>
</div>
</footer>
</div>
<script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script>
<script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script>
<script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script>
</body>
</html>