<!doctype html><html lang=en class=no-js>
<head>
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JPP6RFM2BP"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JPP6RFM2BP')</script>
<link rel=alternate hreflang=zh href=https://kubernetes.io/zh/docs/concepts/workloads/pods/>
<link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/workloads/pods/>
<link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/workloads/pods/>
<link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/workloads/pods/>
<link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/workloads/pods/>
<link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/workloads/pods/>
<link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/workloads/pods/>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.87.0">
<link rel=canonical type=text/html href=https://kubernetes.io/docs/concepts/workloads/pods/>
<link rel="shortcut icon" type=image/png href=/images/favicon.png>
<link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=manifest href=/manifest.webmanifest>
<link rel=apple-touch-icon href=/images/kubernetes-192x192.png>
<title>Pods | Kubernetes</title><meta property="og:title" content="Pods">
<meta property="og:description" content="Production-Grade Container Orchestration">
<meta property="og:type" content="website">
<meta property="og:url" content="https://kubernetes.io/docs/concepts/workloads/pods/"><meta property="og:site_name" content="Kubernetes">
<meta itemprop=name content="Pods">
<meta itemprop=description content="Production-Grade Container Orchestration"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Pods">
<meta name=twitter:description content="Production-Grade Container Orchestration">
<link href=/scss/main.css rel=stylesheet>
<script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script>
<meta name=theme-color content="#326ce5">
<link rel=stylesheet href=/css/feature-states.css>
<meta name=description content="Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.
A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific &#34;logical host&#34;: it contains one or more application containers which are relatively tightly coupled.">
<meta property="og:description" content="Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.
A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific &#34;logical host&#34;: it contains one or more application containers which are relatively tightly coupled.">
<meta name=twitter:description content="Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.
A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific &#34;logical host&#34;: it contains one or more application containers which are relatively tightly coupled.">
<meta property="og:url" content="https://kubernetes.io/docs/concepts/workloads/pods/">
<meta property="og:title" content="Pods">
<meta name=twitter:title content="Pods">
<meta name=twitter:image content="https://kubernetes.io/images/favicon.png">
<meta name=twitter:image:alt content="Kubernetes">
<meta property="og:image" content="/images/kubernetes-horizontal-color.png">
<meta property="og:type" content="article">
<script src=/js/script.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary>
<a class=navbar-brand href=/></a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-2 mb-lg-0">
<a class="nav-link active" href=/docs/>Documentation</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/blog/>Kubernetes Blog</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/training/>Training</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/partners/>Partners</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/community/>Community</a>
</li>
<li class="nav-item mr-2 mb-lg-0">
<a class=nav-link href=/case-studies/>Case Studies</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/concepts/workloads/pods/>v1.27</a>
<a class=dropdown-item href=https://v1-26.docs.kubernetes.io/docs/concepts/workloads/pods/>v1.26</a>
<a class=dropdown-item href=https://v1-25.docs.kubernetes.io/docs/concepts/workloads/pods/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/concepts/workloads/pods/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/concepts/workloads/pods/>v1.23</a>
</div>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
English
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/zh/docs/concepts/workloads/pods/>中文 Chinese</a>
<a class=dropdown-item href=/ko/docs/concepts/workloads/pods/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/concepts/workloads/pods/>日本語 Japanese</a>
<a class=dropdown-item href=/fr/docs/concepts/workloads/pods/>Français</a>
<a class=dropdown-item href=/de/docs/concepts/workloads/pods/>Deutsch</a>
<a class=dropdown-item href=/es/docs/concepts/workloads/pods/>Español</a>
<a class=dropdown-item href=/id/docs/concepts/workloads/pods/>Bahasa Indonesia</a>
</div>
</li>
</ul>
</div>
<button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.
</p><p>
<a href=/docs/concepts/workloads/pods/>Return to the regular view of this page</a>.
</p>
</div>
<h1 class=title>Pods</h1>
<ul>
<li>1: <a href=#pg-c3c2b9cf30915ec9d46c147201da3332>Pod Lifecycle</a></li>
<li>2: <a href=#pg-1ccbd4eeded6ab138d98b59175bd557e>Init Containers</a></li>
<li>3: <a href=#pg-c8d62295ca703fdcef1aaf89fb4c916a>Pod Topology Spread Constraints</a></li>
<li>4: <a href=#pg-4aaf43c715cd764bc8ed4436f3537e68>Disruptions</a></li>
<li>5: <a href=#pg-53a1005011e1bda2ce81819aad7c8b32>Ephemeral Containers</a></li>
</ul>
<div class=content>
<p><em>Pods</em> are the smallest deployable units of computing that you can create and manage in Kubernetes.</p>
<p>A <em>Pod</em> (as in a pod of whales or pea pod) is a group of one or more
<a class=glossary-tooltip title="A lightweight and portable executable image that contains software and all of its dependencies." data-toggle=tooltip data-placement=top href=/docs/concepts/containers/ target=_blank aria-label=containers>containers</a>, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and
co-scheduled, and run in a shared context. A Pod models an
application-specific "logical host": it contains one or more application
containers which are relatively tightly coupled.
In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.</p>
<p>As well as application containers, a Pod can contain
<a href=/docs/concepts/workloads/pods/init-containers/>init containers</a> that run
during Pod startup. You can also inject
<a href=/docs/concepts/workloads/pods/ephemeral-containers/>ephemeral containers</a>
for debugging if your cluster offers this.</p>
<h2 id=what-is-a-pod>What is a Pod?</h2>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> While Kubernetes supports more
<a class=glossary-tooltip title="The container runtime is the software that is responsible for running containers." data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label="container runtimes">container runtimes</a>
than just Docker, <a href=https://www.docker.com/>Docker</a> is the most commonly known
runtime, and it helps to describe Pods using some terminology from Docker.
</div>
<p>The shared context of a Pod is a set of Linux namespaces, cgroups, and
potentially other facets of isolation - the same things that isolate a Docker
container. Within a Pod's context, the individual applications may have
further sub-isolations applied.</p>
<p>In terms of Docker concepts, a Pod is similar to a group of Docker containers
with shared namespaces and shared filesystem volumes.</p>
<h2 id=using-pods>Using Pods</h2>
<p>The following is an example of a Pod which consists of a container running the image <code>nginx:1.14.2</code>.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/pods/simple-pod.yaml download=pods/simple-pod.yaml><code>pods/simple-pod.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-simple-pod-yaml')" title="Copy pods/simple-pod.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-simple-pod-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></code></pre></div>
</div>
</div>
<p>To create the Pod shown above, run the following command:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
</code></pre></div><p>Pods are generally not created directly and are created using workload resources.
See <a href=#working-with-pods>Working with Pods</a> for more information on how Pods are used
with workload resources.</p>
<h3 id=workload-resources-for-managing-pods>Workload resources for managing pods</h3>
<p>Usually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as <a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a> or <a class=glossary-tooltip title="A finite or batch task that runs to completion." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a>.
If your Pods need to track state, consider the
<a class=glossary-tooltip title="Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a> resource.</p>
<p>Pods in a Kubernetes cluster are used in two main ways:</p>
<ul>
<li>
<p><strong>Pods that run a single container</strong>. The "one-container-per-Pod" model is the
most common Kubernetes use case; in this case, you can think of a Pod as a
wrapper around a single container; Kubernetes manages Pods rather than managing
the containers directly.</p>
</li>
<li>
<p><strong>Pods that run multiple containers that need to work together</strong>. A Pod can
encapsulate an application composed of multiple co-located containers that are
tightly coupled and need to share resources. These co-located containers
form a single cohesive unit of service—for example, one container serving data
stored in a shared volume to the public, while a separate <em>sidecar</em> container
refreshes or updates those files.
The Pod wraps these containers, storage resources, and an ephemeral network
identity together as a single unit.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Grouping multiple co-located and co-managed containers in a single Pod is a
relatively advanced use case. You should use this pattern only in specific
instances in which your containers are tightly coupled.
</div>
</li>
</ul>
<p>Each Pod is meant to run a single instance of a given application. If you want to
scale your application horizontally (to provide more overall resources by running
more instances), you should use multiple Pods, one for each instance. In
Kubernetes, this is typically referred to as <em>replication</em>.
Replicated Pods are usually created and managed as a group by a workload resource
and its <a class=glossary-tooltip title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>.</p>
<p>See <a href=#pods-and-controllers>Pods and controllers</a> for more information on how
Kubernetes uses workload resources, and their controllers, to implement application
scaling and auto-healing.</p>
<h3 id=how-pods-manage-multiple-containers>How Pods manage multiple containers</h3>
<p>Pods are designed to support multiple cooperating processes (as containers) that form
a cohesive unit of service. The containers in a Pod are automatically co-located and
co-scheduled on the same physical or virtual machine in the cluster. The containers
can share resources and dependencies, communicate with one another, and coordinate
when and how they are terminated.</p>
<p>For example, you might have a container that
acts as a web server for files in a shared volume, and a separate "sidecar" container
that updates those files from a remote source, as in the following diagram:</p>
<figure class=diagram-medium>
<img src=/images/docs/pod.svg alt="Pod creation diagram">
</figure>
<p>Some Pods have <a class=glossary-tooltip title="One or more initialization containers that must run to completion before any app containers run." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-init-container" target=_blank aria-label="init containers">init containers</a> as well as <a class=glossary-tooltip title="A container used to run part of a workload. Compare with init container." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-app-container" target=_blank aria-label="app containers">app containers</a>. Init containers run and complete before the app containers are started.</p>
<p>Pods natively provide two kinds of shared resources for their constituent containers:
<a href=#pod-networking>networking</a> and <a href=#pod-storage>storage</a>.</p>
<h2 id=working-with-pods>Working with Pods</h2>
<p>You'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This
is because Pods are designed as relatively ephemeral, disposable entities. When
a Pod gets created (directly by you, or indirectly by a
<a class=glossary-tooltip title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>), the new Pod is
scheduled to run on a <a class=glossary-tooltip title="A node is a worker machine in Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> in your cluster.
The Pod remains on that node until the Pod finishes execution, the Pod object is deleted,
the Pod is <em>evicted</em> for lack of resources, or the node fails.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Restarting a container in a Pod should not be confused with restarting a Pod. A Pod
is not a process, but an environment for running container(s). A Pod persists until
it is deleted.
</div>
<p>When you create the manifest for a Pod object, make sure the name specified is a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p>
<h3 id=pods-and-controllers>Pods and controllers</h3>
<p>You can use workload resources to create and manage multiple Pods for you. A controller
for the resource handles replication and rollout and automatic healing in case of
Pod failure. For example, if a Node fails, a controller notices that Pods on that
Node have stopped working and creates a replacement Pod. The scheduler places the
replacement Pod onto a healthy Node.</p>
<p>Here are some examples of workload resources that manage one or more Pods:</p>
<ul>
<li><a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a></li>
<li><a class=glossary-tooltip title="Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a></li>
<li><a class=glossary-tooltip title="Ensures a copy of a Pod is running across a set of nodes in a cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a></li>
</ul>
<h3 id=pod-templates>Pod templates</h3>
<p>Controllers for <a class=glossary-tooltip title="A workload is an application running on Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/ target=_blank aria-label=workload>workload</a> resources create Pods
from a <em>pod template</em> and manage those Pods on your behalf.</p>
<p>PodTemplates are specifications for creating Pods, and are included in workload resources such as
<a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a>,
<a href=/docs/concepts/workloads/controllers/job/>Jobs</a>, and
<a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSets</a>.</p>
<p>Each controller for a workload resource uses the <code>PodTemplate</code> inside the workload
object to make actual Pods. The <code>PodTemplate</code> is part of the desired state of whatever
workload resource you used to run your app.</p>
<p>The sample below is a manifest for a simple Job with a <code>template</code> that starts one
container. The container in that Pod prints a message then pauses.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This is the pod template</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;echo &#34;Hello, Kubernetes!&#34; &amp;&amp; sleep 3600&#39;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>OnFailure<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># The pod template ends here</span><span style=color:#bbb>
</span></code></pre></div><p>Modifying the pod template or switching to a new pod template has no direct effect
on the Pods that already exist. If you change the pod template for a workload
resource, that resource needs to create replacement Pods that use the updated template.</p>
<p>For example, the StatefulSet controller ensures that the running Pods match the current
pod template for each StatefulSet object. If you edit the StatefulSet to change its pod
template, the StatefulSet starts to create new Pods based on the updated template.
Eventually, all of the old Pods are replaced with new Pods, and the update is complete.</p>
<p>Each workload resource implements its own rules for handling changes to the Pod template.
If you want to read more about StatefulSet specifically, read
<a href=/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets>Update strategy</a> in the StatefulSet Basics tutorial.</p>
<p>On Nodes, the <a class=glossary-tooltip title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> does not
directly observe or manage any of the details around pod templates and updates; those
details are abstracted away. That abstraction and separation of concerns simplifies
system semantics, and makes it feasible to extend the cluster's behavior without
changing existing code.</p>
<h2 id=pod-update-and-replacement>Pod update and replacement</h2>
<p>As mentioned in the previous section, when the Pod template for a workload
resource is changed, the controller creates new Pods based on the updated
template instead of updating or patching the existing Pods.</p>
<p>Kubernetes doesn't prevent you from managing Pods directly. It is possible to
update some fields of a running Pod, in place. However, Pod update operations
like
<a href=/docs/reference/generated/kubernetes-api/v1.23/#patch-pod-v1-core><code>patch</code></a>, and
<a href=/docs/reference/generated/kubernetes-api/v1.23/#replace-pod-v1-core><code>replace</code></a>
have some limitations:</p>
<ul>
<li>
<p>Most of the metadata about a Pod is immutable. For example, you cannot
change the <code>namespace</code>, <code>name</code>, <code>uid</code>, or <code>creationTimestamp</code> fields;
the <code>generation</code> field is unique. It only accepts updates that increment the
field's current value.</p>
</li>
<li>
<p>If the <code>metadata.deletionTimestamp</code> is set, no new entry can be added to the
<code>metadata.finalizers</code> list.</p>
</li>
<li>
<p>Pod updates may not change fields other than <code>spec.containers[*].image</code>,
<code>spec.initContainers[*].image</code>, <code>spec.activeDeadlineSeconds</code> or
<code>spec.tolerations</code>. For <code>spec.tolerations</code>, you can only add new entries.</p>
</li>
<li>
<p>When updating the <code>spec.activeDeadlineSeconds</code> field, two types of updates
are allowed:</p>
<ol>
<li>setting the unassigned field to a positive number;</li>
<li>updating the field from a positive number to a smaller, non-negative
number.</li>
</ol>
</li>
</ul>
<h2 id=resource-sharing-and-communication>Resource sharing and communication</h2>
<p>Pods enable data sharing and communication among their constituent
containers.</p>
<h3 id=pod-storage>Storage in Pods</h3>
<p>A Pod can specify a set of shared storage
<a class=glossary-tooltip title="A directory containing data, accessible to the containers in a pod." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volumes>volumes</a>. All containers
in the Pod can access the shared volumes, allowing those containers to
share data. Volumes also allow persistent data in a Pod to survive
in case one of the containers within needs to be restarted. See
<a href=/docs/concepts/storage/>Storage</a> for more information on how
Kubernetes implements shared storage and makes it available to Pods.</p>
<h3 id=pod-networking>Pod networking</h3>
<p>Each Pod is assigned a unique IP address for each address family. Every
container in a Pod shares the network namespace, including the IP address and
network ports. Inside a Pod (and <strong>only</strong> then), the containers that belong to the Pod
can communicate with one another using <code>localhost</code>. When containers in a Pod communicate
with entities <em>outside the Pod</em>,
they must coordinate how they use the shared network resources (such as ports).
Within a Pod, containers share an IP address and port space, and
can find each other via <code>localhost</code>. The containers in a Pod can also communicate
with each other using standard inter-process communications like SystemV semaphores
or POSIX shared memory. Containers in different Pods have distinct IP addresses
and can not communicate by OS-level IPC without special configuration.
Containers that want to interact with a container running in a different Pod can
use IP networking to communicate.</p>
<p>Containers within the Pod see the system hostname as being the same as the configured
<code>name</code> for the Pod. There's more about this in the <a href=/docs/concepts/cluster-administration/networking/>networking</a>
section.</p>
<h2 id=privileged-mode-for-containers>Privileged mode for containers</h2>
<p>In Linux, any container in a Pod can enable privileged mode using the <code>privileged</code> (Linux) flag on the <a href=/docs/tasks/configure-pod-container/security-context/>security context</a> of the container spec. This is useful for containers that want to use operating system administrative capabilities such as manipulating the network stack or accessing hardware devices.</p>
<p>If your cluster has the <code>WindowsHostProcessContainers</code> feature enabled, you can create a <a href=/docs/tasks/configure-pod-container/create-hostprocess-pod>Windows HostProcess pod</a> by setting the <code>windowsOptions.hostProcess</code> flag on the security context of the pod spec. All containers in these pods must run as Windows HostProcess containers. HostProcess pods run directly on the host and can also be used to perform administrative tasks as is done with Linux privileged containers.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> Your <a class=glossary-tooltip title="The container runtime is the software that is responsible for running containers." data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label="container runtime">container runtime</a> must support the concept of a privileged container for this setting to be relevant.
</div>
<h2 id=static-pods>Static Pods</h2>
<p><em>Static Pods</em> are managed directly by the kubelet daemon on a specific node,
without the <a class=glossary-tooltip title="Control plane component that serves the Kubernetes API." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label="API server">API server</a>
observing them.
Whereas most Pods are managed by the control plane (for example, a
<a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>), for static
Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).</p>
<p>Static Pods are always bound to one <a class=glossary-tooltip title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> on a specific node.
The main use for static Pods is to run a self-hosted control plane: in other words,
using the kubelet to supervise the individual <a href=/docs/concepts/overview/components/#control-plane-components>control plane components</a>.</p>
<p>The kubelet automatically tries to create a <a class=glossary-tooltip title="An object in the API server that tracks a static pod on a kubelet." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-mirror-pod" target=_blank aria-label="mirror Pod">mirror Pod</a>
on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server,
but cannot be controlled from there.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The <code>spec</code> of a static Pod cannot refer to other API objects
(e.g., <a class=glossary-tooltip title="Provides an identity for processes that run in a Pod." data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/configure-service-account/ target=_blank aria-label=ServiceAccount>ServiceAccount</a>,
<a class=glossary-tooltip title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a>,
<a class=glossary-tooltip title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a>, etc).
</div>
<h2 id=container-probes>Container probes</h2>
<p>A <em>probe</em> is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:</p>
<ul>
<li><code>ExecAction</code> (performed with the help of the container runtime)</li>
<li><code>TCPSocketAction</code> (checked directly by the kubelet)</li>
<li><code>HTTPGetAction</code> (checked directly by the kubelet)</li>
</ul>
<p>You can read more about <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>probes</a>
in the Pod Lifecycle documentation.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Learn about the <a href=/docs/concepts/workloads/pods/pod-lifecycle/>lifecycle of a Pod</a>.</li>
<li>Learn about <a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a> and how you can use it to
configure different Pods with different container runtime configurations.</li>
<li>Read about <a href=/docs/concepts/workloads/pods/pod-topology-spread-constraints/>Pod topology spread constraints</a>.</li>
<li>Read about <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> and how you can use it to manage application availability during disruptions.</li>
<li>Pod is a top-level resource in the Kubernetes REST API.
The
<a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/>Pod</a>
object definition describes the object in detail.</li>
<li><a href=/blog/2015/06/the-distributed-system-toolkit-patterns/>The Distributed System Toolkit: Patterns for Composite Containers</a> explains common layouts for Pods with more than one container.</li>
</ul>
<p>To understand the context for why Kubernetes wraps a common Pod API in other resources (such as <a class=glossary-tooltip title="Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSets>StatefulSets</a> or <a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployments>Deployments</a>), you can read about the prior art, including:</p>
<ul>
<li><a href=https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema>Aurora</a></li>
<li><a href=https://research.google.com/pubs/pub43438.html>Borg</a></li>
<li><a href=https://mesosphere.github.io/marathon/docs/rest-api.html>Marathon</a></li>
<li><a href=https://research.google/pubs/pub41684/>Omega</a></li>
<li><a href=https://engineering.fb.com/data-center-engineering/tupperware/>Tupperware</a>.</li>
</ul>
</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c3c2b9cf30915ec9d46c147201da3332>1 - Pod Lifecycle</h1>
<p>This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the <code>Pending</code> <a href=#pod-phase>phase</a>, moving through <code>Running</code> if at least one
of its primary containers starts OK, and then through either the <code>Succeeded</code> or
<code>Failed</code> phases depending on whether any container in the Pod terminated in failure.</p>
<p>Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
<a href=#container-states>states</a> and determines what action to take to make the Pod
healthy again.</p>
<p>In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of <a href=#pod-conditions>Pod conditions</a>.
You can also inject <a href=#pod-readiness-gate>custom readiness information</a> into the
condition data for a Pod, if that is useful to your application.</p>
<p>Pods are only <a href=/docs/concepts/scheduling-eviction/>scheduled</a> once in their lifetime.
Once a Pod is scheduled (assigned) to a Node, the Pod runs on that Node until it stops
or is <a href=#pod-termination>terminated</a>.</p>
<h2 id=pod-lifetime>Pod lifetime</h2>
<p>Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID (<a href=/docs/concepts/overview/working-with-objects/names/#uids>UID</a>), and scheduled
to nodes where they remain until termination (according to restart policy) or
deletion.
If a <a class=glossary-tooltip title="A node is a worker machine in Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> dies, the Pods scheduled to that node
are <a href=#pod-garbage-collection>scheduled for deletion</a> after a timeout period.</p>
<p>Pods do not, by themselves, self-heal. If a Pod is scheduled to a
<a class=glossary-tooltip title="A node is a worker machine in Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> that then fails, the Pod is deleted; likewise, a Pod won't
survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a
higher-level abstraction, called a
<a class=glossary-tooltip title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>, that handles the work of
managing the relatively disposable Pod instances.</p>
<p>A given Pod (as defined by a UID) is never "rescheduled" to a different node; instead,
that Pod can be replaced by a new, near-identical Pod, with even the same name if
desired, but with a different UID.</p>
<p>When something is said to have the same lifetime as a Pod, such as a
<a class=glossary-tooltip title="A directory containing data, accessible to the containers in a pod." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a>,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.</p>
<figure class=diagram-medium>
<img src=/images/docs/pod.svg> <figcaption>
<h4>Pod diagram</h4>
</figcaption>
</figure>
<p><em>A multi-container Pod that contains a file puller and a
web server that uses a persistent volume for shared storage between the containers.</em></p>
<h2 id=pod-phase>Pod phase</h2>
<p>A Pod's <code>status</code> field is a
<a href=/docs/reference/generated/kubernetes-api/v1.23/#podstatus-v1-core>PodStatus</a>
object, which has a <code>phase</code> field.</p>
<p>The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.</p>
<p>The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given <code>phase</code> value.</p>
<p>Here are the possible values for <code>phase</code>:</p>
<table>
<thead>
<tr>
<th style=text-align:left>Value</th>
<th style=text-align:left>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left><code>Pending</code></td>
<td style=text-align:left>The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.</td>
</tr>
<tr>
<td style=text-align:left><code>Running</code></td>
<td style=text-align:left>The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.</td>
</tr>
<tr>
<td style=text-align:left><code>Succeeded</code></td>
<td style=text-align:left>All containers in the Pod have terminated in success, and will not be restarted.</td>
</tr>
<tr>
<td style=text-align:left><code>Failed</code></td>
<td style=text-align:left>All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.</td>
</tr>
<tr>
<td style=text-align:left><code>Unknown</code></td>
<td style=text-align:left>For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> When a Pod is being deleted, it is shown as <code>Terminating</code> by some kubectl commands.
This <code>Terminating</code> status is not one of the Pod phases.
A Pod is granted a term to terminate gracefully, which defaults to 30 seconds.
You can use the flag <code>--force</code> to <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced>terminate a Pod by force</a>.
</div>
<p>If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the <code>phase</code> of all Pods on the lost node to Failed.</p>
<h2 id=container-states>Container states</h2>
<p>As well as the <a href=#pod-phase>phase</a> of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
<a href=/docs/concepts/containers/container-lifecycle-hooks/>container lifecycle hooks</a> to
trigger events to run at certain points in a container's lifecycle.</p>
<p>Once the <a class=glossary-tooltip title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a>
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a <a class=glossary-tooltip title="The container runtime is the software that is responsible for running containers." data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label="container runtime">container runtime</a>.
There are three possible container states: <code>Waiting</code>, <code>Running</code>, and <code>Terminated</code>.</p>
<p>To check the state of a Pod's containers, you can use
<code>kubectl describe pod &lt;name-of-pod></code>. The output shows the state for each container
within that Pod.</p>
<p>Each state has a specific meaning:</p>
<h3 id=container-state-waiting><code>Waiting</code></h3>
<p>If a container is not in either the <code>Running</code> or <code>Terminated</code> state, it is <code>Waiting</code>.
A container in the <code>Waiting</code> state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying <a class=glossary-tooltip title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a>
data.
When you use <code>kubectl</code> to query a Pod with a container that is <code>Waiting</code>, you also see
a Reason field to summarize why the container is in that state.</p>
<h3 id=container-state-running><code>Running</code></h3>
<p>The <code>Running</code> status indicates that a container is executing without issues. If there
was a <code>postStart</code> hook configured, it has already executed and finished. When you use
<code>kubectl</code> to query a Pod with a container that is <code>Running</code>, you also see information
about when the container entered the <code>Running</code> state.</p>
<h3 id=container-state-terminated><code>Terminated</code></h3>
<p>A container in the <code>Terminated</code> state began execution and then either ran to
completion or failed for some reason. When you use <code>kubectl</code> to query a Pod with
a container that is <code>Terminated</code>, you see a reason, an exit code, and the start and
finish time for that container's period of execution.</p>
<p>If a container has a <code>preStop</code> hook configured, this hook runs before the container enters
the <code>Terminated</code> state.</p>
<h2 id=restart-policy>Container restart policy</h2>
<p>The <code>spec</code> of a Pod has a <code>restartPolicy</code> field with possible values Always, OnFailure,
and Never. The default value is Always.</p>
<p>The <code>restartPolicy</code> applies to all containers in the Pod. <code>restartPolicy</code> only
refers to restarts of the containers by the kubelet on the same node. After containers
in a Pod exit, the kubelet restarts them with an exponential back-off delay (10s, 20s,
40s, …), that is capped at five minutes. Once a container has executed for 10 minutes
without any problems, the kubelet resets the restart backoff timer for that container.</p>
<h2 id=pod-conditions>Pod conditions</h2>
<p>A Pod has a PodStatus, which has an array of
<a href=/docs/reference/generated/kubernetes-api/v1.23/#podcondition-v1-core>PodConditions</a>
through which the Pod has or has not passed:</p>
<ul>
<li><code>PodScheduled</code>: the Pod has been scheduled to a node.</li>
<li><code>ContainersReady</code>: all containers in the Pod are ready.</li>
<li><code>Initialized</code>: all <a href=/docs/concepts/workloads/pods/init-containers/>init containers</a>
have completed successfully.</li>
<li><code>Ready</code>: the Pod is able to serve requests and should be added to the load
balancing pools of all matching Services.</li>
</ul>
<table>
<thead>
<tr>
<th style=text-align:left>Field name</th>
<th style=text-align:left>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left><code>type</code></td>
<td style=text-align:left>Name of this Pod condition.</td>
</tr>
<tr>
<td style=text-align:left><code>status</code></td>
<td style=text-align:left>Indicates whether that condition is applicable, with possible values "<code>True</code>", "<code>False</code>", or "<code>Unknown</code>".</td>
</tr>
<tr>
<td style=text-align:left><code>lastProbeTime</code></td>
<td style=text-align:left>Timestamp of when the Pod condition was last probed.</td>
</tr>
<tr>
<td style=text-align:left><code>lastTransitionTime</code></td>
<td style=text-align:left>Timestamp for when the Pod last transitioned from one status to another.</td>
</tr>
<tr>
<td style=text-align:left><code>reason</code></td>
<td style=text-align:left>Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.</td>
</tr>
<tr>
<td style=text-align:left><code>message</code></td>
<td style=text-align:left>Human-readable message indicating details about the last status transition.</td>
</tr>
</tbody>
</table>
<h3 id=pod-readiness-gate>Pod readiness</h3>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>
<p>Your application can inject extra feedback or signals into PodStatus:
<em>Pod readiness</em>. To use this, set <code>readinessGates</code> in the Pod's <code>spec</code> to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.</p>
<p>Readiness gates are determined by the current state of <code>status.condition</code>
fields for the Pod. If Kubernetes cannot find such a condition in the
<code>status.conditions</code> field of a Pod, the status of the condition
is defaulted to "<code>False</code>".</p>
<p>Here is an example:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>readinessGates</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>conditionType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;www.example.com/feature-1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>conditions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Ready                             <span style=color:#bbb> </span><span style=color:#080;font-style:italic># a built in PodCondition</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;False&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastProbeTime</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>null</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2018-01-01T00:00:00Z<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;www.example.com/feature-1&#34;</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># an extra PodCondition</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;False&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastProbeTime</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>null</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2018-01-01T00:00:00Z<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containerStatuses</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerID</span>:<span style=color:#bbb> </span>docker://abcd...<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ready</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></code></pre></div><p>The Pod conditions you add must have names that meet the Kubernetes <a href=/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set>label key format</a>.</p>
<h3 id=pod-readiness-status>Status for Pod readiness</h3>
<p>The <code>kubectl patch</code> command does not support patching object status.
To set these <code>status.conditions</code> for the pod, applications and
<a class=glossary-tooltip title="A specialized controller used to manage a custom resource" data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label=operators>operators</a> should use
the <code>PATCH</code> action.
You can use a <a href=/docs/reference/using-api/client-libraries/>Kubernetes client library</a> to
write code that sets custom Pod conditions for Pod readiness.</p>
<p>For a Pod that uses custom conditions, that Pod is evaluated to be ready <strong>only</strong>
when both the following statements apply:</p>
<ul>
<li>All containers in the Pod are ready.</li>
<li>All conditions specified in <code>readinessGates</code> are <code>True</code>.</li>
</ul>
<p>When a Pod's containers are Ready but at least one custom condition is missing or
<code>False</code>, the kubelet sets the Pod's <a href=#pod-conditions>condition</a> to <code>ContainersReady</code>.</p>
<h2 id=container-probes>Container probes</h2>
<p>A <em>probe</em> is a diagnostic
performed periodically by the
<a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a>
on a container. To perform a diagnostic,
the kubelet either executes code within the container, or makes
a network request.</p>
<h3 id=probe-check-methods>Check mechanisms</h3>
<p>There are four different ways to check a container using a probe.
Each probe must define exactly one of these four mechanisms:</p>
<dl>
<dt><code>exec</code></dt>
<dd>Executes a specified command inside the container. The diagnostic
is considered successful if the command exits with a status code of 0.</dd>
<dt><code>grpc</code></dt>
<dd>Performs a remote procedure call using <a href=https://grpc.io/>gRPC</a>.
The target should implement
<a href=https://grpc.io/grpc/core/md_doc_health-checking.html>gRPC health checks</a>.
The diagnostic is considered successful if the <code>status</code>
of the response is <code>SERVING</code>.<br>
gRPC probes are an alpha feature and are only available if you
enable the <code>GRPCContainerProbe</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>.</dd>
<dt><code>httpGet</code></dt>
<dd>Performs an HTTP <code>GET</code> request against the Pod's IP
address on a specified port and path. The diagnostic is
considered successful if the response has a status code
greater than or equal to 200 and less than 400.</dd>
<dt><code>tcpSocket</code></dt>
<dd>Performs a TCP check against the Pod's IP address on
a specified port. The diagnostic is considered successful if
the port is open. If the remote system (the container) closes
the connection immediately after it opens, this counts as healthy.</dd>
</dl>
<h3 id=probe-outcome>Probe outcome</h3>
<p>Each probe has one of three results:</p>
<dl>
<dt><code>Success</code></dt>
<dd>The container passed the diagnostic.</dd>
<dt><code>Failure</code></dt>
<dd>The container failed the diagnostic.</dd>
<dt><code>Unknown</code></dt>
<dd>The diagnostic failed (no action should be taken, and the kubelet
will make further checks).</dd>
</dl>
<h3 id=types-of-probe>Types of probe</h3>
<p>The kubelet can optionally perform and react to three kinds of probes on running
containers:</p>
<dl>
<dt><code>livenessProbe</code></dt>
<dd>Indicates whether the container is running. If
the liveness probe fails, the kubelet kills the container, and the container
is subjected to its <a href=#restart-policy>restart policy</a>. If a container does not
provide a liveness probe, the default state is <code>Success</code>.</dd>
<dt><code>readinessProbe</code></dt>
<dd>Indicates whether the container is ready to respond to requests.
If the readiness probe fails, the endpoints controller removes the Pod's IP
address from the endpoints of all Services that match the Pod. The default
state of readiness before the initial delay is <code>Failure</code>. If a container does
not provide a readiness probe, the default state is <code>Success</code>.</dd>
<dt><code>startupProbe</code></dt>
<dd>Indicates whether the application within the container is started.
All other probes are disabled if a startup probe is provided, until it succeeds.
If the startup probe fails, the kubelet kills the container, and the container
is subjected to its <a href=#restart-policy>restart policy</a>. If a container does not
provide a startup probe, the default state is <code>Success</code>.</dd>
</dl>
<p>For more information about how to set up a liveness, readiness, or startup probe,
see <a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>Configure Liveness, Readiness and Startup Probes</a>.</p>
<h4 id=when-should-you-use-a-liveness-probe>When should you use a liveness probe?</h4>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code>
</div>
<p>If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod's <code>restartPolicy</code>.</p>
<p>If you'd like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a <code>restartPolicy</code> of Always or OnFailure.</p>
<h4 id=when-should-you-use-a-readiness-probe>When should you use a readiness probe?</h4>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code>
</div>
<p>If you'd like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.</p>
<p>If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.</p>
<p>If your app has a strict dependency on back-end services, you can implement both
a liveness and a readiness probe. The liveness probe passes when the app itself
is healthy, but the readiness probe additionally checks that each required
back-end service is available. This helps you avoid directing traffic to Pods
that can only respond with error messages.</p>
<p>If your container needs to work on loading large data, configuration files, or
migrations during startup, you can use a
<a href=#when-should-you-use-a-startup-probe>startup probe</a>. However, if you want to
detect the difference between an app that has failed and an app that is still
processing its startup data, you might prefer a readiness probe.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If you want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; on deletion, the Pod automatically puts itself
into an unready state regardless of whether the readiness probe exists.
The Pod remains in the unready state while it waits for the containers in the Pod
to stop.
</div>
<h4 id=when-should-you-use-a-startup-probe>When should you use a startup probe?</h4>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>
<p>Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.</p>
<p>If your container usually starts in more than
<code>initialDelaySeconds + failureThreshold × periodSeconds</code>, you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
<code>periodSeconds</code> is 10s. You should then set its <code>failureThreshold</code> high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.</p>
<h2 id=pod-termination>Termination of Pods</h2>
<p>Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a <code>KILL</code> signal and having no chance to clean up).</p>
<p>The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the <a class=glossary-tooltip title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> attempts graceful
shutdown.</p>
<p>Typically, the container runtime sends a TERM signal to the main process in each
container. Many container runtimes respect the <code>STOPSIGNAL</code> value defined in the container
image and send this instead of TERM.
Once the grace period has expired, the KILL signal is sent to any remaining
processes, and the Pod is then deleted from the
<a class=glossary-tooltip title="Control plane component that serves the Kubernetes API." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label="API Server">API Server</a>. If the kubelet or the
container runtime's management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.</p>
<p>An example flow:</p>
<ol>
<li>You use the <code>kubectl</code> tool to manually delete a specific Pod, with the default grace period
(30 seconds).</li>
<li>The Pod in the API server is updated with the time beyond which the Pod is considered "dead"
along with the grace period.
If you use <code>kubectl describe</code> to check on the Pod you're deleting, that Pod shows up as
"Terminating".
On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
shutdown process.
<ol>
<li>If one of the Pod's containers has defined a <code>preStop</code>
<a href=/docs/concepts/containers/container-lifecycle-hooks>hook</a>, the kubelet
runs that hook inside of the container. If the <code>preStop</code> hook is still running after the
grace period expires, the kubelet requests a small, one-off grace period extension of 2
seconds.
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> If the <code>preStop</code> hook needs longer to complete than the default grace period allows,
you must modify <code>terminationGracePeriodSeconds</code> to suit this.
</div></li>
<li>The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
container.
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The containers in the Pod receive the TERM signal at different times and in an arbitrary
order. If the order of shutdowns matters, consider using a <code>preStop</code> hook to synchronize.
</div></li>
</ol>
</li>
<li>At the same time as the kubelet is starting graceful shutdown, the control plane removes that
shutting-down Pod from Endpoints (and, if enabled, EndpointSlice) objects where these represent
a <a class=glossary-tooltip title="A way to expose an application running on a set of Pods as a network service." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> with a configured
<a class=glossary-tooltip title="Allows users to filter a list of resources based on labels." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=selector>selector</a>.
<a class=glossary-tooltip title="ReplicaSet ensures that a specified number of Pod replicas are running at one time" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/replicaset/ target=_blank aria-label=ReplicaSets>ReplicaSets</a> and other workload resources
no longer treat the shutting-down Pod as a valid, in-service replica. Pods that shut down slowly
cannot continue to serve traffic as load balancers (like the service proxy) remove the Pod from
the list of endpoints as soon as the termination grace period <em>begins</em>.</li>
<li>When the grace period expires, the kubelet triggers forcible shutdown. The container runtime sends
<code>SIGKILL</code> to any processes still running in any container in the Pod.
The kubelet also cleans up a hidden <code>pause</code> container if that container runtime uses one.</li>
<li>The kubelet triggers forcible removal of Pod object from the API server, by setting grace period
to 0 (immediate deletion).</li>
<li>The API server deletes the Pod's API object, which is then no longer visible from any client.</li>
</ol>
<h3 id=pod-termination-forced>Forced Pod termination</h3>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> Forced deletions can be potentially disruptive for some workloads and their Pods.
</div>
<p>By default, all deletes are graceful within 30 seconds. The <code>kubectl delete</code> command supports
the <code>--grace-period=&lt;seconds></code> option which allows you to override the default and specify your
own value.</p>
<p>Setting the grace period to <code>0</code> forcibly and immediately deletes the Pod from the API
server. If the pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> You must specify an additional flag <code>--force</code> along with <code>--grace-period=0</code> in order to perform force deletions.
</div>
<p>When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.</p>
<p>If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
<a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>deleting Pods from a StatefulSet</a>.</p>
<h3 id=pod-garbage-collection>Garbage collection of failed Pods</h3>
<p>For failed Pods, the API objects remain in the cluster's API until a human or
<a class=glossary-tooltip title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> process
explicitly removes them.</p>
<p>The control plane cleans up terminated Pods (with a phase of <code>Succeeded</code> or
<code>Failed</code>), when the number of Pods exceeds the configured threshold
(determined by <code>terminated-pod-gc-threshold</code> in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>
<p>Get hands-on experience
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>attaching handlers to container lifecycle events</a>.</p>
</li>
<li>
<p>Get hands-on experience
<a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>configuring Liveness, Readiness and Startup Probes</a>.</p>
</li>
<li>
<p>Learn more about <a href=/docs/concepts/containers/container-lifecycle-hooks/>container lifecycle hooks</a>.</p>
</li>
<li>
<p>For detailed information about Pod and container status in the API, see
the API reference documentation covering
<a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus><code>.status</code></a> for Pod.</p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1ccbd4eeded6ab138d98b59175bd557e>2 - Init Containers</h1>
<p>This page provides an overview of init containers: specialized containers that run
before app containers in a <a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>.
Init containers can contain utilities or setup scripts not present in an app image.</p>
<p>You can specify init containers in the Pod specification alongside the <code>containers</code>
array (which describes app containers).</p>
<h2 id=understanding-init-containers>Understanding init containers</h2>
<p>A <a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.</p>
<p>Init containers are exactly like regular containers, except:</p>
<ul>
<li>Init containers always run to completion.</li>
<li>Each init container must complete successfully before the next one starts.</li>
</ul>
<p>If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.
However, if the Pod has a <code>restartPolicy</code> of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.</p>
<p>To specify an init container for a Pod, add the <code>initContainers</code> field into
the <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec>Pod specification</a>,
as an array of <code>container</code> items (similar to the app <code>containers</code> field and its contents).
See <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container>Container</a> in the
API reference for more details.</p>
<p>The status of the init containers is returned in <code>.status.initContainerStatuses</code>
field as an array of the container statuses (similar to the <code>.status.containerStatuses</code>
field).</p>
<h3 id=differences-from-regular-containers>Differences from regular containers</h3>
<p>Init containers support all the fields and features of app containers,
including resource limits, volumes, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in <a href=#resources>Resources</a>.</p>
<p>Also, init containers do not support <code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code>, or
<code>startupProbe</code> because they must run to completion before the Pod can be ready.</p>
<p>If you specify multiple init containers for a Pod, kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, kubelet initializes
the application containers for the Pod and runs them as usual.</p>
<h2 id=using-init-containers>Using init containers</h2>
<p>Because init containers have separate images from app containers, they
have some advantages for start-up related code:</p>
<ul>
<li>Init containers can contain utilities or custom code for setup that are not present in an app
image. For example, there is no need to make an image <code>FROM</code> another image just to use a tool like
<code>sed</code>, <code>awk</code>, <code>python</code>, or <code>dig</code> during setup.</li>
<li>The application image builder and deployer roles can work independently without
the need to jointly build a single app image.</li>
<li>Init containers can run with a different view of the filesystem than app containers in the
same Pod. Consequently, they can be given access to
<a class=glossary-tooltip title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a> that app containers cannot access.</li>
<li>Because init containers run to completion before any app containers start, init containers offer
a mechanism to block or delay app container startup until a set of preconditions are met. Once
preconditions are met, all of the app containers in a Pod can start in parallel.</li>
<li>Init containers can securely run utilities or custom code that would otherwise make an app
container image less secure. By keeping unnecessary tools separate you can limit the attack
surface of your app container image.</li>
</ul>
<h3 id=examples>Examples</h3>
<p>Here are some ideas for how to use init containers:</p>
<ul>
<li>
<p>Wait for a <a class=glossary-tooltip title="A way to expose an application running on a set of Pods as a network service." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> to
be created, using a shell one-line command like:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#666>{</span>1..100<span style=color:#666>}</span>; <span style=color:#a2f;font-weight:700>do</span> sleep 1; <span style=color:#a2f;font-weight:700>if</span> dig myservice; <span style=color:#a2f;font-weight:700>then</span> <span style=color:#a2f>exit</span> 0; <span style=color:#a2f;font-weight:700>fi</span>; <span style=color:#a2f;font-weight:700>done</span>; <span style=color:#a2f>exit</span> <span style=color:#666>1</span>
</code></pre></div></li>
<li>
<p>Register this Pod with a remote server from the downward API with a command like:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -X POST http://<span style=color:#b8860b>$MANAGEMENT_SERVICE_HOST</span>:<span style=color:#b8860b>$MANAGEMENT_SERVICE_PORT</span>/register -d <span style=color:#b44>&#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;</span>
</code></pre></div></li>
<li>
<p>Wait for some time before starting the app container with a command like</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sleep <span style=color:#666>60</span>
</code></pre></div></li>
<li>
<p>Clone a Git repository into a <a class=glossary-tooltip title="A directory containing data, accessible to the containers in a pod." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a></p>
</li>
<li>
<p>Place values into a configuration file and run a template tool to dynamically
generate a configuration file for the main app container. For example,
place the <code>POD_IP</code> value in a configuration and generate the main app
configuration file using Jinja.</p>
</li>
</ul>
<h4 id=init-containers-in-use>Init containers in use</h4>
<p>This example defines a simple Pod that has two init containers.
The first waits for <code>myservice</code>, and the second waits for <code>mydb</code>. Once both
init containers complete, the Pod runs the app container from its <code>spec</code> section.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-pod<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>myapp<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-container<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;echo The app is running! &amp;&amp; sleep 3600&#39;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>initContainers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-myservice<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-mydb<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></code></pre></div><p>You can start this Pod by running:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f myapp.yaml
</code></pre></div><p>The output is similar to this:</p>
<pre><code>pod/myapp-pod created
</code></pre><p>And check on its status with:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get -f myapp.yaml
</code></pre></div><p>The output is similar to this:</p>
<pre><code>NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
</code></pre><p>or for more details:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe -f myapp.yaml
</code></pre></div><p>The output is similar to this:</p>
<pre><code>Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &quot;busybox&quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &quot;busybox&quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
</code></pre><p>To see logs for the init containers in this Pod, run:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs myapp-pod -c init-myservice <span style=color:#080;font-style:italic># Inspect the first init container</span>
kubectl logs myapp-pod -c init-mydb      <span style=color:#080;font-style:italic># Inspect the second init container</span>
</code></pre></div><p>At this point, those init containers will be waiting to discover Services named
<code>mydb</code> and <code>myservice</code>.</p>
<p>Here's a configuration you can use to make those Services appear:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydb<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9377</span><span style=color:#bbb>
</span></code></pre></div><p>To create the <code>mydb</code> and <code>myservice</code> services:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f services.yaml
</code></pre></div><p>The output is similar to this:</p>
<pre><code>service/myservice created
service/mydb created
</code></pre><p>You'll then see that those init containers complete, and that the <code>myapp-pod</code>
Pod moves into the Running state:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get -f myapp.yaml
</code></pre></div><p>The output is similar to this:</p>
<pre><code>NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
</code></pre><p>This simple example should provide some inspiration for you to create your own
init containers. <a href=#what-s-next>What's next</a> contains a link to a more detailed example.</p>
<h2 id=detailed-behavior>Detailed behavior</h2>
<p>During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod's init containers in the order
they appear in the Pod's spec.</p>
<p>Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod <code>restartPolicy</code>. However,
if the Pod <code>restartPolicy</code> is set to Always, the init containers use
<code>restartPolicy</code> OnFailure.</p>
<p>A Pod cannot be <code>Ready</code> until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the <code>Pending</code> state but should have a condition <code>Initialized</code> set to false.</p>
<p>If the Pod <a href=#pod-restart-reasons>restarts</a>, or is restarted, all init containers
must execute again.</p>
<p>Changes to the init container spec are limited to the container image field.
Altering an init container image field is equivalent to restarting the Pod.</p>
<p>Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes to files on <code>EmptyDirs</code>
should be prepared for the possibility that an output file already exists.</p>
<p>Init containers have all of the fields of an app container. However, Kubernetes
prohibits <code>readinessProbe</code> from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.</p>
<p>Use <code>activeDeadlineSeconds</code> on the Pod to prevent init containers from failing forever.
The active deadline includes init containers.
However it is recommended to use <code>activeDeadlineSeconds</code> only if teams deploy their application
as a Job, because <code>activeDeadlineSeconds</code> has an effect even after initContainer finished.
The Pod which is already running correctly would be killed by <code>activeDeadlineSeconds</code> if you set.</p>
<p>The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.</p>
<h3 id=resources>Resources</h3>
<p>Given the ordering and execution for init containers, the following rules
for resource usage apply:</p>
<ul>
<li>The highest of any particular resource request or limit defined on all init
containers is the <em>effective init request/limit</em>. If any resource has no
resource limit specified this is considered as the highest limit.</li>
<li>The Pod's <em>effective request/limit</em> for a resource is the higher of:
<ul>
<li>the sum of all app containers request/limit for a resource</li>
<li>the effective init request/limit for a resource</li>
</ul>
</li>
<li>Scheduling is done based on effective requests/limits, which means
init containers can reserve resources for initialization that are not used
during the life of the Pod.</li>
<li>The QoS (quality of service) tier of the Pod's <em>effective QoS tier</em> is the
QoS tier for init containers and app containers alike.</li>
</ul>
<p>Quota and limits are applied based on the effective Pod request and
limit.</p>
<p>Pod level control groups (cgroups) are based on the effective Pod request and
limit, the same as the scheduler.</p>
<h3 id=pod-restart-reasons>Pod restart reasons</h3>
<p>A Pod can restart, causing re-execution of init containers, for the following
reasons:</p>
<ul>
<li>The Pod infrastructure container is restarted. This is uncommon and would
have to be done by someone with root access to nodes.</li>
<li>All containers in a Pod are terminated while <code>restartPolicy</code> is set to Always,
forcing a restart, and the init container completion record has been lost due
to garbage collection.</li>
</ul>
<p>The Pod will not be restarted when the init container image is changed, or the
init container completion record has been lost due to garbage collection. This
applies for Kubernetes v1.20 and later. If you are using an earlier version of
Kubernetes, consult the documentation for the version you are using.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Read about <a href=/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container>creating a Pod that has an init container</a></li>
<li>Learn how to <a href=/docs/tasks/debug/debug-application/debug-init-containers/>debug init containers</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c8d62295ca703fdcef1aaf89fb4c916a>3 - Pod Topology Spread Constraints</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code>
</div>
<p>You can use <em>topology spread constraints</em> to control how <a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization.</p>
<h2 id=prerequisites>Prerequisites</h2>
<h3 id=node-labels>Node Labels</h3>
<p>Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in. For example, a Node might have labels: <code>node=node1,zone=us-east-1a,region=us-east-1</code></p>
<p>Suppose you have a 4-node cluster with the following labels:</p>
<pre><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><p>Then the cluster is logically viewed as below:</p>
<figure>
<div class=mermaid>
graph TB
subgraph "zoneB"
n3(Node3)
n4(Node4)
end
subgraph "zoneA"
n1(Node1)
n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4 k8s;
class zoneA,zoneB cluster;
</div>
</figure>
<noscript>
<div class="alert alert-secondary callout" role=alert>
<em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em>
</div>
</noscript>
<p>Instead of manually applying labels, you can also reuse the <a href=/docs/reference/labels-annotations-taints/>well-known labels</a> that are created and populated automatically on most clusters.</p>
<h2 id=spread-constraints-for-pods>Spread Constraints for Pods</h2>
<h3 id=api>API</h3>
<p>The API field <code>pod.spec.topologySpreadConstraints</code> is defined as below:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span>&lt;integer&gt;<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>&lt;string&gt;<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>&lt;string&gt;<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb> </span>&lt;object&gt;<span style=color:#bbb>
</span></code></pre></div><p>You can define one or multiple <code>topologySpreadConstraint</code> to instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster. The fields are:</p>
<ul>
<li><strong>maxSkew</strong> describes the degree to which Pods may be unevenly distributed.
It must be greater than zero. Its semantics differs according to the value of <code>whenUnsatisfiable</code>:
<ul>
<li>when <code>whenUnsatisfiable</code> equals to "DoNotSchedule", <code>maxSkew</code> is the maximum
permitted difference between the number of matching pods in the target
topology and the global minimum
(the minimum number of pods that match the label selector in a topology domain. For example, if you have 3 zones with 0, 2 and 3 matching pods respectively, The global minimum is 0).</li>
<li>when <code>whenUnsatisfiable</code> equals to "ScheduleAnyway", scheduler gives higher
precedence to topologies that would help reduce the skew.</li>
</ul>
</li>
<li><strong>topologyKey</strong> is the key of node labels. If two Nodes are labelled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.</li>
<li><strong>whenUnsatisfiable</strong> indicates how to deal with a Pod if it doesn't satisfy the spread constraint:
<ul>
<li><code>DoNotSchedule</code> (default) tells the scheduler not to schedule it.</li>
<li><code>ScheduleAnyway</code> tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.</li>
</ul>
</li>
<li><strong>labelSelector</strong> is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods in their corresponding topology domain. See <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>Label Selectors</a> for more details.</li>
</ul>
<p>When a Pod defines more than one <code>topologySpreadConstraint</code>, those constraints are ANDed: The kube-scheduler looks for a node for the incoming Pod that satisfies all the constraints.</p>
<p>You can read more about this field by running <code>kubectl explain Pod.spec.topologySpreadConstraints</code>.</p>
<h3 id=example-one-topologyspreadconstraint>Example: One TopologySpreadConstraint</h3>
<p>Suppose you have a 4-node cluster where 3 Pods labeled <code>foo:bar</code> are located in node1, node2 and node3 respectively:</p>
<figure>
<div class=mermaid>
graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class zoneA,zoneB cluster;
</div>
</figure>
<noscript>
<div class="alert alert-secondary callout" role=alert>
<em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em>
</div>
</noscript>
<p>If we want an incoming Pod to be evenly spread with existing Pods across zones, the spec can be given as:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/pods/topology-spread-constraints/one-constraint.yaml download=pods/topology-spread-constraints/one-constraint.yaml><code>pods/topology-spread-constraints/one-constraint.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-topology-spread-constraints-one-constraint-yaml')" title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-topology-spread-constraints-one-constraint-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:3.1</code></pre></div>
</div>
</div>
<p><code>topologyKey: zone</code> implies the even distribution will only be applied to the nodes which have label pair "zone:&lt;any value>" present. <code>whenUnsatisfiable: DoNotSchedule</code> tells the scheduler to let it stay pending if the incoming Pod can't satisfy the constraint.</p>
<p>If the scheduler placed this incoming Pod into "zoneA", the Pods distribution would become [3, 1], hence the actual skew is 2 (3 - 1) - which violates <code>maxSkew: 1</code>. In this example, the incoming Pod can only be placed onto "zoneB":</p>
<figure>
<div class=mermaid>
graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
p4(mypod) --> n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;
</div>
</figure>
<noscript>
<div class="alert alert-secondary callout" role=alert>
<em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em>
</div>
</noscript>
<p>OR</p>
<figure>
<div class=mermaid>
graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
p4(mypod) --> n3
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;
</div>
</figure>
<noscript>
<div class="alert alert-secondary callout" role=alert>
<em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em>
</div>
</noscript>
<p>You can tweak the Pod spec to meet various kinds of requirements:</p>
<ul>
<li>Change <code>maxSkew</code> to a bigger value like "2" so that the incoming Pod can be placed onto "zoneA" as well.</li>
<li>Change <code>topologyKey</code> to "node" so as to distribute the Pods evenly across nodes instead of zones. In the above example, if <code>maxSkew</code> remains "1", the incoming Pod can only be placed onto "node4".</li>
<li>Change <code>whenUnsatisfiable: DoNotSchedule</code> to <code>whenUnsatisfiable: ScheduleAnyway</code> to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs are satisfied). However, it's preferred to be placed onto the topology domain which has fewer matching Pods. (Be aware that this preferability is jointly normalized with other internal scheduling priorities like resource usage ratio, etc.)</li>
</ul>
<h3 id=example-multiple-topologyspreadconstraints>Example: Multiple TopologySpreadConstraints</h3>
<p>This builds upon the previous example. Suppose you have a 4-node cluster where 3 Pods labeled <code>foo:bar</code> are located in node1, node2 and node3 respectively:</p>
<figure>
<div class=mermaid>
graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;
</div>
</figure>
<noscript>
<div class="alert alert-secondary callout" role=alert>
<em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em>
</div>
</noscript>
<p>You can use 2 TopologySpreadConstraints to control the Pods spreading on both zone and node:</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml download=pods/topology-spread-constraints/two-constraints.yaml><code>pods/topology-spread-constraints/two-constraints.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-topology-spread-constraints-two-constraints-yaml')" title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-topology-spread-constraints-two-constraints-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>node<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:3.1</code></pre></div>
</div>
</div>
<p>In this case, to match the first constraint, the incoming Pod can only be placed onto "zoneB"; while in terms of the second constraint, the incoming Pod can only be placed onto "node4". Then the results of 2 constraints are ANDed, so the only viable option is to place on "node4".</p>
<p>Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:</p>
<figure>
<div class=mermaid>
graph BT
subgraph "zoneB"
p4(Pod) --> n3(Node3)
p5(Pod) --> n3
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n1
p3(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;
class zoneA,zoneB cluster;
</div>
</figure>
<noscript>
<div class="alert alert-secondary callout" role=alert>
<em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em>
</div>
</noscript>
<p>If you apply "two-constraints.yaml" to this cluster, you will notice "mypod" stays in <code>Pending</code> state. This is because: to satisfy the first constraint, "mypod" can only be put to "zoneB"; while in terms of the second constraint, "mypod" can only put to "node2". Then a joint result of "zoneB" and "node2" returns nothing.</p>
<p>To overcome this situation, you can either increase the <code>maxSkew</code> or modify one of the constraints to use <code>whenUnsatisfiable: ScheduleAnyway</code>.</p>
<h3 id=interaction-with-node-affinity-and-node-selectors>Interaction With Node Affinity and Node Selectors</h3>
<p>The scheduler will skip the non-matching nodes from the skew calculations if the incoming Pod has <code>spec.nodeSelector</code> or <code>spec.affinity.nodeAffinity</code> defined.</p>
<h3 id=example-topologyspreadconstraints-with-nodeaffinity>Example: TopologySpreadConstraints with NodeAffinity</h3>
<p>Suppose you have a 5-node cluster ranging from zoneA to zoneC:</p>
<figure>
<div class=mermaid>
graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;
</div>
</figure>
<noscript>
<div class="alert alert-secondary callout" role=alert>
<em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em>
</div>
</noscript>
<figure>
<div class=mermaid>
graph BT
subgraph "zoneC"
n5(Node5)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n5 k8s;
class zoneC cluster;
</div>
</figure>
<noscript>
<div class="alert alert-secondary callout" role=alert>
<em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em>
</div>
</noscript>
<p>and you know that "zoneC" must be excluded. In this case, you can compose the yaml as below, so that "mypod" will be placed onto "zoneB" instead of "zoneC". Similarly <code>spec.nodeSelector</code> is also respected.</p>
<div class=highlight>
<div class=copy-code-icon style=text-align:right>
<a href=https://raw.githubusercontent.com/kubernetes/website/release-1.23/content/en/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml download=pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code>
</a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml')" title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard">
</img>
</div>
<div class=includecode id=pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>NotIn<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- zoneC<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:3.1</code></pre></div>
</div>
</div>
<p>The scheduler doesn't have prior knowledge of all the zones or other topology domains that a cluster has. They are determined from the existing nodes in the cluster. This could lead to a problem in autoscaled clusters, when a node pool (or node group) is scaled to zero nodes and the user is expecting them to scale up, because, in this case, those topology domains won't be considered until there is at least one node in them.</p>
<h3 id=other-noticeable-semantics>Other Noticeable Semantics</h3>
<p>There are some implicit conventions worth noting here:</p>
<ul>
<li>
<p>Only the Pods holding the same namespace as the incoming Pod can be matching candidates.</p>
</li>
<li>
<p>The scheduler will bypass the nodes without <code>topologySpreadConstraints[*].topologyKey</code> present. This implies that:</p>
<ol>
<li>the Pods located on those nodes do not impact <code>maxSkew</code> calculation - in the above example, suppose "node1" does not have label "zone", then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into "zoneA".</li>
<li>the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a "node5" carrying label <code>{zone-typo: zoneC}</code> joins the cluster, it will be bypassed due to the absence of label key "zone".</li>
</ol>
</li>
<li>
<p>Be aware of what will happen if the incomingPod's <code>topologySpreadConstraints[*].labelSelector</code> doesn't match its own labels. In the above example, if we remove the incoming Pod's labels, it can still be placed onto "zoneB" since the constraints are still satisfied. However, after the placement, the degree of imbalance of the cluster remains unchanged - it's still zoneA having 2 Pods which hold label {foo:bar}, and zoneB having 1 Pod which holds label {foo:bar}. So if this is not what you expect, we recommend the workload's <code>topologySpreadConstraints[*].labelSelector</code> to match its own labels.</p>
</li>
</ul>
<h3 id=cluster-level-default-constraints>Cluster-level default constraints</h3>
<p>It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:</p>
<ul>
<li>It doesn't define any constraints in its <code>.spec.topologySpreadConstraints</code>.</li>
<li>It belongs to a service, replication controller, replica set or stateful set.</li>
</ul>
<p>Default constraints can be set as part of the <code>PodTopologySpread</code> plugin args
in a <a href=/docs/reference/scheduling/config/#profiles>scheduling profile</a>.
The constraints are specified with the same <a href=#api>API above</a>, except that
<code>labelSelector</code> must be empty. The selectors are calculated from the services,
replication controllers, replica sets or stateful sets that the Pod belongs to.</p>
<p>An example configuration might look like follows:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultingType</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></code></pre></div><div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> The score produced by default scheduling constraints might conflict with the
score produced by the
<a href=/docs/reference/scheduling/config/#scheduling-plugins><code>SelectorSpread</code> plugin</a>.
It is recommended that you disable this plugin in the scheduling profile when
using default constraints for <code>PodTopologySpread</code>.
</div>
<h4 id=internal-default-constraints>Internal default constraints</h4>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code>
</div>
<p>With the <code>DefaultPodTopologySpread</code> feature gate, enabled by default, the
legacy <code>SelectorSpread</code> plugin is disabled.
kube-scheduler uses the following default topology constraints for the
<code>PodTopologySpread</code> plugin configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;topology.kubernetes.io/zone&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></code></pre></div><p>Also, the legacy <code>SelectorSpread</code> plugin, which provides an equivalent behavior,
is disabled.</p>
<div class="alert alert-info note callout" role=alert>
<strong>Note:</strong> <p>The <code>PodTopologySpread</code> plugin does not score the nodes that don't have
the topology keys specified in the spreading constraints. This might result
in a different default behavior compared to the legacy <code>SelectorSpread</code> plugin when
using the default topology constraints.</p>
<p>If your nodes are not expected to have <strong>both</strong> <code>kubernetes.io/hostname</code> and
<code>topology.kubernetes.io/zone</code> labels set, define your own constraints
instead of using the Kubernetes defaults.</p>
</div>
<p>If you don't want to use the default Pod spreading constraints for your cluster,
you can disable those defaults by setting <code>defaultingType</code> to <code>List</code> and leaving
empty <code>defaultConstraints</code> in the <code>PodTopologySpread</code> plugin configuration:</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb> </span>[]<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultingType</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></code></pre></div><h2 id=comparison-with-podaffinity-podantiaffinity>Comparison with PodAffinity/PodAntiAffinity</h2>
<p>In Kubernetes, directives related to "Affinity" control how Pods are
scheduled - more packed or more scattered.</p>
<ul>
<li>For <code>PodAffinity</code>, you can try to pack any number of Pods into qualifying
topology domain(s)</li>
<li>For <code>PodAntiAffinity</code>, only one Pod can be scheduled into a
single topology domain.</li>
</ul>
<p>For finer control, you can specify topology spread constraints to distribute
Pods across different topology domains - to achieve either high availability or
cost-saving. This can also help on rolling update workloads and scaling out
replicas smoothly. See
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation>Motivation</a>
for more details.</p>
<h2 id=known-limitations>Known Limitations</h2>
<ul>
<li>There's no guarantee that the constraints remain satisfied when Pods are removed. For example, scaling down a Deployment may result in imbalanced Pods distribution.
You can use <a href=https://github.com/kubernetes-sigs/descheduler>Descheduler</a> to rebalance the Pods distribution.</li>
<li>Pods matched on tainted nodes are respected. See <a href=https://github.com/kubernetes/kubernetes/issues/80921>Issue 80921</a></li>
</ul>
<h2 id=what-s-next>What's next</h2>
<ul>
<li><a href=https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/>Blog: Introducing PodTopologySpread</a>
explains <code>maxSkew</code> in details, as well as bringing up some advanced usage examples.</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-4aaf43c715cd764bc8ed4436f3537e68>4 - Disruptions</h1>
<p>This guide is for application owners who want to build
highly available applications, and thus need to understand
what types of disruptions can happen to Pods.</p>
<p>It is also for cluster administrators who want to perform automated
cluster actions, like upgrading and autoscaling clusters.</p>
<h2 id=voluntary-and-involuntary-disruptions>Voluntary and involuntary disruptions</h2>
<p>Pods do not disappear until someone (a person or a controller) destroys them, or
there is an unavoidable hardware or system software error.</p>
<p>We call these unavoidable cases <em>involuntary disruptions</em> to
an application. Examples are:</p>
<ul>
<li>a hardware failure of the physical machine backing the node</li>
<li>cluster administrator deletes VM (instance) by mistake</li>
<li>cloud provider or hypervisor failure makes VM disappear</li>
<li>a kernel panic</li>
<li>the node disappears from the cluster due to cluster network partition</li>
<li>eviction of a pod due to the node being <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>out-of-resources</a>.</li>
</ul>
<p>Except for the out-of-resources condition, all these conditions
should be familiar to most users; they are not specific
to Kubernetes.</p>
<p>We call other cases <em>voluntary disruptions</em>. These include both
actions initiated by the application owner and those initiated by a Cluster
Administrator. Typical application owner actions include:</p>
<ul>
<li>deleting the deployment or other controller that manages the pod</li>
<li>updating a deployment's pod template causing a restart</li>
<li>directly deleting a pod (e.g. by accident)</li>
</ul>
<p>Cluster administrator actions include:</p>
<ul>
<li><a href=/docs/tasks/administer-cluster/safely-drain-node/>Draining a node</a> for repair or upgrade.</li>
<li>Draining a node from a cluster to scale the cluster down (learn about
<a href=https://github.com/kubernetes/autoscaler/#readme>Cluster Autoscaling</a>
).</li>
<li>Removing a pod from a node to permit something else to fit on that node.</li>
</ul>
<p>These actions might be taken directly by the cluster administrator, or by automation
run by the cluster administrator, or by your cluster hosting provider.</p>
<p>Ask your cluster administrator or consult your cloud provider or distribution documentation
to determine if any sources of voluntary disruptions are enabled for your cluster.
If none are enabled, you can skip creating Pod Disruption Budgets.</p>
<div class="alert alert-warning caution callout" role=alert>
<strong>Caution:</strong> Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,
deleting deployments or pods bypasses Pod Disruption Budgets.
</div>
<h2 id=dealing-with-disruptions>Dealing with disruptions</h2>
<p>Here are some ways to mitigate involuntary disruptions:</p>
<ul>
<li>Ensure your pod <a href=/docs/tasks/configure-pod-container/assign-memory-resource>requests the resources</a> it needs.</li>
<li>Replicate your application if you need higher availability. (Learn about running replicated
<a href=/docs/tasks/run-application/run-stateless-application-deployment/>stateless</a>
and <a href=/docs/tasks/run-application/run-replicated-stateful-application/>stateful</a> applications.)</li>
<li>For even higher availability when running replicated applications,
spread applications across racks (using
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>anti-affinity</a>)
or across zones (if using a
<a href=/docs/setup/multiple-zones>multi-zone cluster</a>.)</li>
</ul>
<p>The frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are
no automated voluntary disruptions (only user-triggered ones). However, your cluster administrator or hosting provider
may run some additional services which cause voluntary disruptions. For example,
rolling out node software updates can cause voluntary disruptions. Also, some implementations
of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.
Your cluster administrator or hosting provider should have documented what level of voluntary
disruptions, if any, to expect. Certain configuration options, such as
<a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>using PriorityClasses</a>
in your pod spec can also cause voluntary (and involuntary) disruptions.</p>
<h2 id=pod-disruption-budgets>Pod disruption budgets</h2>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>
<p>Kubernetes offers features to help you run highly available applications even when you
introduce frequent voluntary disruptions.</p>
<p>As an application owner, you can create a PodDisruptionBudget (PDB) for each application.
A PDB limits the number of Pods of a replicated application that are down simultaneously from
voluntary disruptions. For example, a quorum-based application would
like to ensure that the number of replicas running is never brought below the
number needed for a quorum. A web front end might want to
ensure that the number of replicas serving load never falls below a certain
percentage of the total.</p>
<p>Cluster managers and hosting providers should use tools which
respect PodDisruptionBudgets by calling the <a href=/docs/tasks/administer-cluster/safely-drain-node/#eviction-api>Eviction API</a>
instead of directly deleting pods or deployments.</p>
<p>For example, the <code>kubectl drain</code> subcommand lets you mark a node as going out of
service. When you run <code>kubectl drain</code>, the tool tries to evict all of the Pods on
the Node you're taking out of service. The eviction request that <code>kubectl</code> submits on
your behalf may be temporarily rejected, so the tool periodically retries all failed
requests until all Pods on the target node are terminated, or until a configurable timeout
is reached.</p>
<p>A PDB specifies the number of replicas that an application can tolerate having, relative to how
many it is intended to have. For example, a Deployment which has a <code>.spec.replicas: 5</code> is
supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time,
then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.</p>
<p>The group of pods that comprise the application is specified using a label selector, the same
as the one used by the application's controller (deployment, stateful-set, etc).</p>
<p>The "intended" number of pods is computed from the <code>.spec.replicas</code> of the workload resource
that is managing those pods. The control plane discovers the owning workload resource by
examining the <code>.metadata.ownerReferences</code> of the Pod.</p>
<p><a href=#voluntary-and-involuntary-disruptions>Involuntary disruptions</a> cannot be prevented by PDBs; however they
do count against the budget.</p>
<p>Pods which are deleted or unavailable due to a rolling upgrade to an application do count
against the disruption budget, but workload resources (such as Deployment and StatefulSet)
are not limited by PDBs when doing rolling upgrades. Instead, the handling of failures
during application updates is configured in the spec for the specific workload resource.</p>
<p>When a pod is evicted using the eviction API, it is gracefully
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>terminated</a>, honoring the
<code>terminationGracePeriodSeconds</code> setting in its <a href=/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core>PodSpec</a>.</p>
<h2 id=pdb-example>PodDisruptionBudget example</h2>
<p>Consider a cluster with 3 nodes, <code>node-1</code> through <code>node-3</code>.
The cluster is running several applications. One of them has 3 replicas initially called
<code>pod-a</code>, <code>pod-b</code>, and <code>pod-c</code>. Another, unrelated pod without a PDB, called <code>pod-x</code>, is also shown.
Initially, the pods are laid out as follows:</p>
<table>
<thead>
<tr>
<th style=text-align:center>node-1</th>
<th style=text-align:center>node-2</th>
<th style=text-align:center>node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:center>pod-a <em>available</em></td>
<td style=text-align:center>pod-b <em>available</em></td>
<td style=text-align:center>pod-c <em>available</em></td>
</tr>
<tr>
<td style=text-align:center>pod-x <em>available</em></td>
<td style=text-align:center></td>
<td style=text-align:center></td>
</tr>
</tbody>
</table>
<p>All 3 pods are part of a deployment, and they collectively have a PDB which requires
there be at least 2 of the 3 pods to be available at all times.</p>
<p>For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.
The cluster administrator first tries to drain <code>node-1</code> using the <code>kubectl drain</code> command.
That tool tries to evict <code>pod-a</code> and <code>pod-x</code>. This succeeds immediately.
Both pods go into the <code>terminating</code> state at the same time.
This puts the cluster in this state:</p>
<table>
<thead>
<tr>
<th style=text-align:center>node-1 <em>draining</em></th>
<th style=text-align:center>node-2</th>
<th style=text-align:center>node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:center>pod-a <em>terminating</em></td>
<td style=text-align:center>pod-b <em>available</em></td>
<td style=text-align:center>pod-c <em>available</em></td>
</tr>
<tr>
<td style=text-align:center>pod-x <em>terminating</em></td>
<td style=text-align:center></td>
<td style=text-align:center></td>
</tr>
</tbody>
</table>
<p>The deployment notices that one of the pods is terminating, so it creates a replacement
called <code>pod-d</code>. Since <code>node-1</code> is cordoned, it lands on another node. Something has
also created <code>pod-y</code> as a replacement for <code>pod-x</code>.</p>
<p>(Note: for a StatefulSet, <code>pod-a</code>, which would be called something like <code>pod-0</code>, would need
to terminate completely before its replacement, which is also called <code>pod-0</code> but has a
different UID, could be created. Otherwise, the example applies to a StatefulSet as well.)</p>
<p>Now the cluster is in this state:</p>
<table>
<thead>
<tr>
<th style=text-align:center>node-1 <em>draining</em></th>
<th style=text-align:center>node-2</th>
<th style=text-align:center>node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:center>pod-a <em>terminating</em></td>
<td style=text-align:center>pod-b <em>available</em></td>
<td style=text-align:center>pod-c <em>available</em></td>
</tr>
<tr>
<td style=text-align:center>pod-x <em>terminating</em></td>
<td style=text-align:center>pod-d <em>starting</em></td>
<td style=text-align:center>pod-y</td>
</tr>
</tbody>
</table>
<p>At some point, the pods terminate, and the cluster looks like this:</p>
<table>
<thead>
<tr>
<th style=text-align:center>node-1 <em>drained</em></th>
<th style=text-align:center>node-2</th>
<th style=text-align:center>node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:center></td>
<td style=text-align:center>pod-b <em>available</em></td>
<td style=text-align:center>pod-c <em>available</em></td>
</tr>
<tr>
<td style=text-align:center></td>
<td style=text-align:center>pod-d <em>starting</em></td>
<td style=text-align:center>pod-y</td>
</tr>
</tbody>
</table>
<p>At this point, if an impatient cluster administrator tries to drain <code>node-2</code> or
<code>node-3</code>, the drain command will block, because there are only 2 available
pods for the deployment, and its PDB requires at least 2. After some time passes, <code>pod-d</code> becomes available.</p>
<p>The cluster state now looks like this:</p>
<table>
<thead>
<tr>
<th style=text-align:center>node-1 <em>drained</em></th>
<th style=text-align:center>node-2</th>
<th style=text-align:center>node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:center></td>
<td style=text-align:center>pod-b <em>available</em></td>
<td style=text-align:center>pod-c <em>available</em></td>
</tr>
<tr>
<td style=text-align:center></td>
<td style=text-align:center>pod-d <em>available</em></td>
<td style=text-align:center>pod-y</td>
</tr>
</tbody>
</table>
<p>Now, the cluster administrator tries to drain <code>node-2</code>.
The drain command will try to evict the two pods in some order, say
<code>pod-b</code> first and then <code>pod-d</code>. It will succeed at evicting <code>pod-b</code>.
But, when it tries to evict <code>pod-d</code>, it will be refused because that would leave only
one pod available for the deployment.</p>
<p>The deployment creates a replacement for <code>pod-b</code> called <code>pod-e</code>.
Because there are not enough resources in the cluster to schedule
<code>pod-e</code> the drain will again block. The cluster may end up in this
state:</p>
<table>
<thead>
<tr>
<th style=text-align:center>node-1 <em>drained</em></th>
<th style=text-align:center>node-2</th>
<th style=text-align:center>node-3</th>
<th style=text-align:center><em>no node</em></th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:center></td>
<td style=text-align:center>pod-b <em>terminating</em></td>
<td style=text-align:center>pod-c <em>available</em></td>
<td style=text-align:center>pod-e <em>pending</em></td>
</tr>
<tr>
<td style=text-align:center></td>
<td style=text-align:center>pod-d <em>available</em></td>
<td style=text-align:center>pod-y</td>
<td style=text-align:center></td>
</tr>
</tbody>
</table>
<p>At this point, the cluster administrator needs to
add a node back to the cluster to proceed with the upgrade.</p>
<p>You can see how Kubernetes varies the rate at which disruptions
can happen, according to:</p>
<ul>
<li>how many replicas an application needs</li>
<li>how long it takes to gracefully shutdown an instance</li>
<li>how long it takes a new instance to start up</li>
<li>the type of controller</li>
<li>the cluster's resource capacity</li>
</ul>
<h2 id=separating-cluster-owner-and-application-owner-roles>Separating Cluster Owner and Application Owner Roles</h2>
<p>Often, it is useful to think of the Cluster Manager
and Application Owner as separate roles with limited knowledge
of each other. This separation of responsibilities
may make sense in these scenarios:</p>
<ul>
<li>when there are many application teams sharing a Kubernetes cluster, and
there is natural specialization of roles</li>
<li>when third-party tools or services are used to automate cluster management</li>
</ul>
<p>Pod Disruption Budgets support this separation of roles by providing an
interface between the roles.</p>
<p>If you do not have such a separation of responsibilities in your organization,
you may not need to use Pod Disruption Budgets.</p>
<h2 id=how-to-perform-disruptive-actions-on-your-cluster>How to perform Disruptive Actions on your Cluster</h2>
<p>If you are a Cluster Administrator, and you need to perform a disruptive action on all
the nodes in your cluster, such as a node or system software upgrade, here are some options:</p>
<ul>
<li>Accept downtime during the upgrade.</li>
<li>Failover to another complete replica cluster.
<ul>
<li>No downtime, but may be costly both for the duplicated nodes
and for human effort to orchestrate the switchover.</li>
</ul>
</li>
<li>Write disruption tolerant applications and use PDBs.
<ul>
<li>No downtime.</li>
<li>Minimal resource duplication.</li>
<li>Allows more automation of cluster administration.</li>
<li>Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary
disruptions largely overlaps with work to support autoscaling and tolerating
involuntary disruptions.</li>
</ul>
</li>
</ul>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>
<p>Follow steps to protect your application by <a href=/docs/tasks/run-application/configure-pdb/>configuring a Pod Disruption Budget</a>.</p>
</li>
<li>
<p>Learn more about <a href=/docs/tasks/administer-cluster/safely-drain-node/>draining nodes</a></p>
</li>
<li>
<p>Learn about <a href=/docs/concepts/workloads/controllers/deployment/#updating-a-deployment>updating a deployment</a>
including steps to maintain its availability during the rollout.</p>
</li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-53a1005011e1bda2ce81819aad7c8b32>5 - Ephemeral Containers</h1>
<div style=margin-top:10px;margin-bottom:10px>
<b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>
<p>This page provides an overview of ephemeral containers: a special type of container
that runs temporarily in an existing <a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> to
accomplish user-initiated actions such as troubleshooting. You use ephemeral
containers to inspect services rather than to build applications.</p>
<h2 id=understanding-ephemeral-containers>Understanding ephemeral containers</h2>
<p><a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> are the fundamental building
block of Kubernetes applications. Since Pods are intended to be disposable and
replaceable, you cannot add a container to a Pod once it has been created.
Instead, you usually delete and replace Pods in a controlled fashion using
<a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=deployments>deployments</a>.</p>
<p>Sometimes it's necessary to inspect the state of an existing Pod, however, for
example to troubleshoot a hard-to-reproduce bug. In these cases you can run
an ephemeral container in an existing Pod to inspect its state and run
arbitrary commands.</p>
<h3 id=what-is-an-ephemeral-container>What is an ephemeral container?</h3>
<p>Ephemeral containers differ from other containers in that they lack guarantees
for resources or execution, and they will never be automatically restarted, so
they are not appropriate for building applications. Ephemeral containers are
described using the same <code>ContainerSpec</code> as regular containers, but many fields
are incompatible and disallowed for ephemeral containers.</p>
<ul>
<li>Ephemeral containers may not have ports, so fields such as <code>ports</code>,
<code>livenessProbe</code>, <code>readinessProbe</code> are disallowed.</li>
<li>Pod resource allocations are immutable, so setting <code>resources</code> is disallowed.</li>
<li>For a complete list of allowed fields, see the <a href=/docs/reference/generated/kubernetes-api/v1.23/#ephemeralcontainer-v1-core>EphemeralContainer reference
documentation</a>.</li>
</ul>
<p>Ephemeral containers are created using a special <code>ephemeralcontainers</code> handler
in the API rather than by adding them directly to <code>pod.spec</code>, so it's not
possible to add an ephemeral container using <code>kubectl edit</code>.</p>
<p>Like regular containers, you may not change or remove an ephemeral container
after you have added it to a Pod.</p>
<h2 id=uses-for-ephemeral-containers>Uses for ephemeral containers</h2>
<p>Ephemeral containers are useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient because a container has crashed or a container image
doesn't include debugging utilities.</p>
<p>In particular, <a href=https://github.com/GoogleContainerTools/distroless>distroless images</a>
enable you to deploy minimal container images that reduce attack surface
and exposure to bugs and vulnerabilities. Since distroless images do not include a
shell or any debugging utilities, it's difficult to troubleshoot distroless
images using <code>kubectl exec</code> alone.</p>
<p>When using ephemeral containers, it's helpful to enable <a href=/docs/tasks/configure-pod-container/share-process-namespace/>process namespace
sharing</a> so
you can view processes in other containers.</p>
<h2 id=what-s-next>What's next</h2>
<ul>
<li>Learn how to <a href=/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container>debug pods using ephemeral containers</a>.</li>
</ul>
</div>
</main>
</div>
</div>
<footer class=d-print-none>
<div class=footer__links>
<nav>
<a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a>
</nav>
</div>
<div class=container-fluid>
<div class=row>
<div class="col-6 col-sm-2 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list">
<a class=text-white target=_blank href=https://discuss.kubernetes.io>
<i class="fa fa-envelope"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter>
<a class=text-white target=_blank href=https://twitter.com/kubernetesio>
<i class="fab fa-twitter"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar>
<a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
<i class="fas fa-calendar-alt"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube>
<a class=text-white target=_blank href=https://youtube.com/kubernetescommunity>
<i class="fab fa-youtube"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank href=https://slack.k8s.io>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute>
<a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide>
<i class="fas fa-edit"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow">
<a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes>
<i class="fab fa-stack-overflow"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-8 text-center order-sm-2">
<small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small>
<br>
<small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small>
<br>
<small class=text-white>ICP license: 京ICP备17074266号-3</small>
</div>
</div>
</div>
</footer>
</div>
<script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script>
<script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script>
<script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script>
<script async src=/js/sweetalert-2.1.2.min.js></script>
<script type=text/javascript>function copyCode(c){var d,a,b;if(document.getElementById(c)){d="_hiddenCopyText_",a=document.getElementById(d),a||(a=document.createElement("textarea"),a.style.position="absolute",a.style.left="-9999px",a.style.top="0",a.id=d,document.body.appendChild(a)),a.value=document.getElementById(c).innerText,a.select();try{b=document.execCommand("copy")}catch(a){swal("Oh, no…","Sorry, your browser doesn't support copying this example to your clipboard."),b=!1}return b?(swal("Copied to clipboard: ",c),b):(swal("Oops!",c+" not found when trying to copy code"),!1)}}</script>
</body>
</html>